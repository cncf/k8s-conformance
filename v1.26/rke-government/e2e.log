I0104 22:11:30.989160      18 e2e.go:126] Starting e2e run "fcc81411-4ecc-4b5e-a2b9-1b71a19b6b05" on Ginkgo node 1
Jan  4 22:11:31.010: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1672870290 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan  4 22:11:31.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:11:31.274: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan  4 22:11:31.294: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan  4 22:11:31.370: INFO: The status of Pod helm-install-rke2-canal-r7b4c is Succeeded, skipping waiting
Jan  4 22:11:31.370: INFO: The status of Pod helm-install-rke2-coredns-8ff46 is Succeeded, skipping waiting
Jan  4 22:11:31.370: INFO: The status of Pod helm-install-rke2-ingress-nginx-qfntk is Succeeded, skipping waiting
Jan  4 22:11:31.370: INFO: The status of Pod helm-install-rke2-metrics-server-q46jz is Succeeded, skipping waiting
Jan  4 22:11:31.370: INFO: 31 / 35 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan  4 22:11:31.370: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jan  4 22:11:31.370: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan  4 22:11:31.377: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'rke2-canal' (0 seconds elapsed)
Jan  4 22:11:31.377: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'rke2-ingress-nginx-controller' (0 seconds elapsed)
Jan  4 22:11:31.377: INFO: e2e test version: v1.26.0
Jan  4 22:11:31.389: INFO: kube-apiserver version: v1.26.0+rke2r1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan  4 22:11:31.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:11:31.394: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.122 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan  4 22:11:31.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:11:31.274: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan  4 22:11:31.294: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan  4 22:11:31.370: INFO: The status of Pod helm-install-rke2-canal-r7b4c is Succeeded, skipping waiting
    Jan  4 22:11:31.370: INFO: The status of Pod helm-install-rke2-coredns-8ff46 is Succeeded, skipping waiting
    Jan  4 22:11:31.370: INFO: The status of Pod helm-install-rke2-ingress-nginx-qfntk is Succeeded, skipping waiting
    Jan  4 22:11:31.370: INFO: The status of Pod helm-install-rke2-metrics-server-q46jz is Succeeded, skipping waiting
    Jan  4 22:11:31.370: INFO: 31 / 35 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan  4 22:11:31.370: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Jan  4 22:11:31.370: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan  4 22:11:31.377: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'rke2-canal' (0 seconds elapsed)
    Jan  4 22:11:31.377: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'rke2-ingress-nginx-controller' (0 seconds elapsed)
    Jan  4 22:11:31.377: INFO: e2e test version: v1.26.0
    Jan  4 22:11:31.389: INFO: kube-apiserver version: v1.26.0+rke2r1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan  4 22:11:31.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:11:31.394: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:11:31.458
Jan  4 22:11:31.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 22:11:31.459
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:11:31.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:11:31.485
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan  4 22:11:31.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: creating the pod 01/04/23 22:11:31.487
STEP: submitting the pod to kubernetes 01/04/23 22:11:31.487
Jan  4 22:11:31.497: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501" in namespace "pods-636" to be "running and ready"
Jan  4 22:11:31.503: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501": Phase="Pending", Reason="", readiness=false. Elapsed: 5.945163ms
Jan  4 22:11:31.503: INFO: The phase of Pod pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:11:33.519: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021566761s
Jan  4 22:11:33.519: INFO: The phase of Pod pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:11:35.575: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0779165s
Jan  4 22:11:35.575: INFO: The phase of Pod pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:11:37.506: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501": Phase="Running", Reason="", readiness=true. Elapsed: 6.008713638s
Jan  4 22:11:37.506: INFO: The phase of Pod pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501 is Running (Ready = true)
Jan  4 22:11:37.506: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 22:11:37.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-636" for this suite. 01/04/23 22:11:37.581
------------------------------
• [SLOW TEST] [6.129 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:11:31.458
    Jan  4 22:11:31.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 22:11:31.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:11:31.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:11:31.485
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan  4 22:11:31.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: creating the pod 01/04/23 22:11:31.487
    STEP: submitting the pod to kubernetes 01/04/23 22:11:31.487
    Jan  4 22:11:31.497: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501" in namespace "pods-636" to be "running and ready"
    Jan  4 22:11:31.503: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501": Phase="Pending", Reason="", readiness=false. Elapsed: 5.945163ms
    Jan  4 22:11:31.503: INFO: The phase of Pod pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:11:33.519: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021566761s
    Jan  4 22:11:33.519: INFO: The phase of Pod pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:11:35.575: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0779165s
    Jan  4 22:11:35.575: INFO: The phase of Pod pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:11:37.506: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501": Phase="Running", Reason="", readiness=true. Elapsed: 6.008713638s
    Jan  4 22:11:37.506: INFO: The phase of Pod pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501 is Running (Ready = true)
    Jan  4 22:11:37.506: INFO: Pod "pod-logs-websocket-27edbf26-5bd0-42f9-adcd-85c1e7dac501" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:11:37.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-636" for this suite. 01/04/23 22:11:37.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:11:37.591
Jan  4 22:11:37.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename disruption 01/04/23 22:11:37.593
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:11:37.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:11:37.612
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:11:37.614
Jan  4 22:11:37.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename disruption-2 01/04/23 22:11:37.616
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:11:37.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:11:37.635
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/04/23 22:11:37.642
STEP: Waiting for the pdb to be processed 01/04/23 22:11:39.653
STEP: Waiting for the pdb to be processed 01/04/23 22:11:41.667
STEP: listing a collection of PDBs across all namespaces 01/04/23 22:11:43.678
STEP: listing a collection of PDBs in namespace disruption-7834 01/04/23 22:11:43.685
STEP: deleting a collection of PDBs 01/04/23 22:11:43.689
STEP: Waiting for the PDB collection to be deleted 01/04/23 22:11:43.701
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan  4 22:11:43.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  4 22:11:43.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-1105" for this suite. 01/04/23 22:11:43.713
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7834" for this suite. 01/04/23 22:11:43.719
------------------------------
• [SLOW TEST] [6.133 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:11:37.591
    Jan  4 22:11:37.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename disruption 01/04/23 22:11:37.593
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:11:37.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:11:37.612
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:11:37.614
    Jan  4 22:11:37.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename disruption-2 01/04/23 22:11:37.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:11:37.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:11:37.635
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/04/23 22:11:37.642
    STEP: Waiting for the pdb to be processed 01/04/23 22:11:39.653
    STEP: Waiting for the pdb to be processed 01/04/23 22:11:41.667
    STEP: listing a collection of PDBs across all namespaces 01/04/23 22:11:43.678
    STEP: listing a collection of PDBs in namespace disruption-7834 01/04/23 22:11:43.685
    STEP: deleting a collection of PDBs 01/04/23 22:11:43.689
    STEP: Waiting for the PDB collection to be deleted 01/04/23 22:11:43.701
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:11:43.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:11:43.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-1105" for this suite. 01/04/23 22:11:43.713
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7834" for this suite. 01/04/23 22:11:43.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:11:43.735
Jan  4 22:11:43.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename subpath 01/04/23 22:11:43.736
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:11:43.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:11:43.754
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/04/23 22:11:43.756
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-tmkw 01/04/23 22:11:43.765
STEP: Creating a pod to test atomic-volume-subpath 01/04/23 22:11:43.765
Jan  4 22:11:43.774: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tmkw" in namespace "subpath-3896" to be "Succeeded or Failed"
Jan  4 22:11:43.779: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.835669ms
Jan  4 22:11:45.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009598376s
Jan  4 22:11:47.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008680199s
Jan  4 22:11:49.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007942872s
Jan  4 22:11:51.785: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010083206s
Jan  4 22:11:53.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008430683s
Jan  4 22:11:55.787: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012115346s
Jan  4 22:11:57.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 14.008376363s
Jan  4 22:11:59.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 16.008553479s
Jan  4 22:12:01.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 18.009051209s
Jan  4 22:12:03.787: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 20.011978815s
Jan  4 22:12:05.785: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 22.010141806s
Jan  4 22:12:07.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 24.009361825s
Jan  4 22:12:09.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 26.00960832s
Jan  4 22:12:11.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 28.008025764s
Jan  4 22:12:13.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 30.009762664s
Jan  4 22:12:15.785: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 32.010287052s
Jan  4 22:12:17.785: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=false. Elapsed: 34.009939401s
Jan  4 22:12:19.787: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.011913422s
STEP: Saw pod success 01/04/23 22:12:19.787
Jan  4 22:12:19.787: INFO: Pod "pod-subpath-test-configmap-tmkw" satisfied condition "Succeeded or Failed"
Jan  4 22:12:19.790: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-configmap-tmkw container test-container-subpath-configmap-tmkw: <nil>
STEP: delete the pod 01/04/23 22:12:19.813
Jan  4 22:12:19.830: INFO: Waiting for pod pod-subpath-test-configmap-tmkw to disappear
Jan  4 22:12:19.833: INFO: Pod pod-subpath-test-configmap-tmkw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tmkw 01/04/23 22:12:19.833
Jan  4 22:12:19.834: INFO: Deleting pod "pod-subpath-test-configmap-tmkw" in namespace "subpath-3896"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  4 22:12:19.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3896" for this suite. 01/04/23 22:12:19.846
------------------------------
• [SLOW TEST] [36.118 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:11:43.735
    Jan  4 22:11:43.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename subpath 01/04/23 22:11:43.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:11:43.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:11:43.754
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/04/23 22:11:43.756
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-tmkw 01/04/23 22:11:43.765
    STEP: Creating a pod to test atomic-volume-subpath 01/04/23 22:11:43.765
    Jan  4 22:11:43.774: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tmkw" in namespace "subpath-3896" to be "Succeeded or Failed"
    Jan  4 22:11:43.779: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.835669ms
    Jan  4 22:11:45.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009598376s
    Jan  4 22:11:47.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008680199s
    Jan  4 22:11:49.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007942872s
    Jan  4 22:11:51.785: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010083206s
    Jan  4 22:11:53.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008430683s
    Jan  4 22:11:55.787: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012115346s
    Jan  4 22:11:57.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 14.008376363s
    Jan  4 22:11:59.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 16.008553479s
    Jan  4 22:12:01.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 18.009051209s
    Jan  4 22:12:03.787: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 20.011978815s
    Jan  4 22:12:05.785: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 22.010141806s
    Jan  4 22:12:07.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 24.009361825s
    Jan  4 22:12:09.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 26.00960832s
    Jan  4 22:12:11.783: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 28.008025764s
    Jan  4 22:12:13.784: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 30.009762664s
    Jan  4 22:12:15.785: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=true. Elapsed: 32.010287052s
    Jan  4 22:12:17.785: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Running", Reason="", readiness=false. Elapsed: 34.009939401s
    Jan  4 22:12:19.787: INFO: Pod "pod-subpath-test-configmap-tmkw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.011913422s
    STEP: Saw pod success 01/04/23 22:12:19.787
    Jan  4 22:12:19.787: INFO: Pod "pod-subpath-test-configmap-tmkw" satisfied condition "Succeeded or Failed"
    Jan  4 22:12:19.790: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-configmap-tmkw container test-container-subpath-configmap-tmkw: <nil>
    STEP: delete the pod 01/04/23 22:12:19.813
    Jan  4 22:12:19.830: INFO: Waiting for pod pod-subpath-test-configmap-tmkw to disappear
    Jan  4 22:12:19.833: INFO: Pod pod-subpath-test-configmap-tmkw no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-tmkw 01/04/23 22:12:19.833
    Jan  4 22:12:19.834: INFO: Deleting pod "pod-subpath-test-configmap-tmkw" in namespace "subpath-3896"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:12:19.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3896" for this suite. 01/04/23 22:12:19.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:12:19.856
Jan  4 22:12:19.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replication-controller 01/04/23 22:12:19.858
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:19.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:19.885
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/04/23 22:12:19.887
STEP: When the matched label of one of its pods change 01/04/23 22:12:19.893
Jan  4 22:12:19.901: INFO: Pod name pod-release: Found 0 pods out of 1
Jan  4 22:12:24.909: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/04/23 22:12:24.917
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  4 22:12:25.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6533" for this suite. 01/04/23 22:12:25.932
------------------------------
• [SLOW TEST] [6.082 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:12:19.856
    Jan  4 22:12:19.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replication-controller 01/04/23 22:12:19.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:19.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:19.885
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/04/23 22:12:19.887
    STEP: When the matched label of one of its pods change 01/04/23 22:12:19.893
    Jan  4 22:12:19.901: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan  4 22:12:24.909: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/04/23 22:12:24.917
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:12:25.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6533" for this suite. 01/04/23 22:12:25.932
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:12:25.94
Jan  4 22:12:25.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:12:25.941
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:25.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:25.958
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:12:25.96
Jan  4 22:12:25.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe" in namespace "projected-8212" to be "Succeeded or Failed"
Jan  4 22:12:25.970: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724455ms
Jan  4 22:12:27.973: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.006168223s
Jan  4 22:12:29.974: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe": Phase="Running", Reason="", readiness=false. Elapsed: 4.00690197s
Jan  4 22:12:31.974: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006292845s
STEP: Saw pod success 01/04/23 22:12:31.974
Jan  4 22:12:31.974: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe" satisfied condition "Succeeded or Failed"
Jan  4 22:12:31.976: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe container client-container: <nil>
STEP: delete the pod 01/04/23 22:12:31.982
Jan  4 22:12:31.992: INFO: Waiting for pod downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe to disappear
Jan  4 22:12:31.994: INFO: Pod downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 22:12:31.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8212" for this suite. 01/04/23 22:12:32
------------------------------
• [SLOW TEST] [6.067 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:12:25.94
    Jan  4 22:12:25.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:12:25.941
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:25.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:25.958
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:12:25.96
    Jan  4 22:12:25.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe" in namespace "projected-8212" to be "Succeeded or Failed"
    Jan  4 22:12:25.970: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724455ms
    Jan  4 22:12:27.973: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.006168223s
    Jan  4 22:12:29.974: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe": Phase="Running", Reason="", readiness=false. Elapsed: 4.00690197s
    Jan  4 22:12:31.974: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006292845s
    STEP: Saw pod success 01/04/23 22:12:31.974
    Jan  4 22:12:31.974: INFO: Pod "downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe" satisfied condition "Succeeded or Failed"
    Jan  4 22:12:31.976: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe container client-container: <nil>
    STEP: delete the pod 01/04/23 22:12:31.982
    Jan  4 22:12:31.992: INFO: Waiting for pod downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe to disappear
    Jan  4 22:12:31.994: INFO: Pod downwardapi-volume-41171ef3-5b10-4a11-ac59-f68ec797a8fe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:12:31.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8212" for this suite. 01/04/23 22:12:32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:12:32.01
Jan  4 22:12:32.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:12:32.012
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:32.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:32.032
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/04/23 22:12:32.035
Jan  4 22:12:32.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-3666 cluster-info'
Jan  4 22:12:32.116: INFO: stderr: ""
Jan  4 22:12:32.116: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:12:32.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3666" for this suite. 01/04/23 22:12:32.12
------------------------------
• [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:12:32.01
    Jan  4 22:12:32.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:12:32.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:32.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:32.032
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/04/23 22:12:32.035
    Jan  4 22:12:32.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-3666 cluster-info'
    Jan  4 22:12:32.116: INFO: stderr: ""
    Jan  4 22:12:32.116: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:12:32.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3666" for this suite. 01/04/23 22:12:32.12
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:12:32.127
Jan  4 22:12:32.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 22:12:32.128
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:32.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:32.144
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-00fbb063-7c14-4303-8d7b-5b9c86f13f35 01/04/23 22:12:32.149
STEP: Creating configMap with name cm-test-opt-upd-e1189ea2-02f3-4c7c-a017-6b5b3de3f069 01/04/23 22:12:32.153
STEP: Creating the pod 01/04/23 22:12:32.157
Jan  4 22:12:32.164: INFO: Waiting up to 5m0s for pod "pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa" in namespace "configmap-6894" to be "running and ready"
Jan  4 22:12:32.170: INFO: Pod "pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.740576ms
Jan  4 22:12:32.170: INFO: The phase of Pod pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:12:34.174: INFO: Pod "pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.009200322s
Jan  4 22:12:34.174: INFO: The phase of Pod pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa is Running (Ready = true)
Jan  4 22:12:34.174: INFO: Pod "pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-00fbb063-7c14-4303-8d7b-5b9c86f13f35 01/04/23 22:12:34.194
STEP: Updating configmap cm-test-opt-upd-e1189ea2-02f3-4c7c-a017-6b5b3de3f069 01/04/23 22:12:34.2
STEP: Creating configMap with name cm-test-opt-create-8c49c47c-5ce8-4a4c-a720-e7b004f4455b 01/04/23 22:12:34.205
STEP: waiting to observe update in volume 01/04/23 22:12:34.21
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:12:36.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6894" for this suite. 01/04/23 22:12:36.244
------------------------------
• [4.125 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:12:32.127
    Jan  4 22:12:32.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 22:12:32.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:32.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:32.144
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-00fbb063-7c14-4303-8d7b-5b9c86f13f35 01/04/23 22:12:32.149
    STEP: Creating configMap with name cm-test-opt-upd-e1189ea2-02f3-4c7c-a017-6b5b3de3f069 01/04/23 22:12:32.153
    STEP: Creating the pod 01/04/23 22:12:32.157
    Jan  4 22:12:32.164: INFO: Waiting up to 5m0s for pod "pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa" in namespace "configmap-6894" to be "running and ready"
    Jan  4 22:12:32.170: INFO: Pod "pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.740576ms
    Jan  4 22:12:32.170: INFO: The phase of Pod pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:12:34.174: INFO: Pod "pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.009200322s
    Jan  4 22:12:34.174: INFO: The phase of Pod pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa is Running (Ready = true)
    Jan  4 22:12:34.174: INFO: Pod "pod-configmaps-28acca8a-a47b-4026-9426-7d572b5730fa" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-00fbb063-7c14-4303-8d7b-5b9c86f13f35 01/04/23 22:12:34.194
    STEP: Updating configmap cm-test-opt-upd-e1189ea2-02f3-4c7c-a017-6b5b3de3f069 01/04/23 22:12:34.2
    STEP: Creating configMap with name cm-test-opt-create-8c49c47c-5ce8-4a4c-a720-e7b004f4455b 01/04/23 22:12:34.205
    STEP: waiting to observe update in volume 01/04/23 22:12:34.21
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:12:36.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6894" for this suite. 01/04/23 22:12:36.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:12:36.254
Jan  4 22:12:36.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:12:36.255
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:36.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:36.286
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:12:36.31
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:12:36.969
STEP: Deploying the webhook pod 01/04/23 22:12:36.978
STEP: Wait for the deployment to be ready 01/04/23 22:12:36.99
Jan  4 22:12:37.000: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/04/23 22:12:39.01
STEP: Verifying the service has paired with the endpoint 01/04/23 22:12:39.021
Jan  4 22:12:40.022: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/04/23 22:12:40.024
STEP: create a pod that should be denied by the webhook 01/04/23 22:12:40.066
STEP: create a pod that causes the webhook to hang 01/04/23 22:12:40.104
STEP: create a configmap that should be denied by the webhook 01/04/23 22:12:50.111
STEP: create a configmap that should be admitted by the webhook 01/04/23 22:12:50.133
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/04/23 22:12:50.144
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/04/23 22:12:50.151
STEP: create a namespace that bypass the webhook 01/04/23 22:12:50.156
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/04/23 22:12:50.162
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:12:50.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5901" for this suite. 01/04/23 22:12:50.278
STEP: Destroying namespace "webhook-5901-markers" for this suite. 01/04/23 22:12:50.289
------------------------------
• [SLOW TEST] [14.054 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:12:36.254
    Jan  4 22:12:36.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:12:36.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:36.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:36.286
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:12:36.31
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:12:36.969
    STEP: Deploying the webhook pod 01/04/23 22:12:36.978
    STEP: Wait for the deployment to be ready 01/04/23 22:12:36.99
    Jan  4 22:12:37.000: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/04/23 22:12:39.01
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:12:39.021
    Jan  4 22:12:40.022: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/04/23 22:12:40.024
    STEP: create a pod that should be denied by the webhook 01/04/23 22:12:40.066
    STEP: create a pod that causes the webhook to hang 01/04/23 22:12:40.104
    STEP: create a configmap that should be denied by the webhook 01/04/23 22:12:50.111
    STEP: create a configmap that should be admitted by the webhook 01/04/23 22:12:50.133
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/04/23 22:12:50.144
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/04/23 22:12:50.151
    STEP: create a namespace that bypass the webhook 01/04/23 22:12:50.156
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/04/23 22:12:50.162
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:12:50.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5901" for this suite. 01/04/23 22:12:50.278
    STEP: Destroying namespace "webhook-5901-markers" for this suite. 01/04/23 22:12:50.289
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:12:50.308
Jan  4 22:12:50.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename var-expansion 01/04/23 22:12:50.309
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:50.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:50.337
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/04/23 22:12:51.033
Jan  4 22:12:51.041: INFO: Waiting up to 5m0s for pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212" in namespace "var-expansion-4626" to be "Succeeded or Failed"
Jan  4 22:12:51.043: INFO: Pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.308757ms
Jan  4 22:12:53.046: INFO: Pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005724149s
Jan  4 22:12:55.046: INFO: Pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00578857s
STEP: Saw pod success 01/04/23 22:12:55.046
Jan  4 22:12:55.047: INFO: Pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212" satisfied condition "Succeeded or Failed"
Jan  4 22:12:55.049: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod var-expansion-42f506bb-9c31-493d-915a-472856de1212 container dapi-container: <nil>
STEP: delete the pod 01/04/23 22:12:55.06
Jan  4 22:12:55.072: INFO: Waiting for pod var-expansion-42f506bb-9c31-493d-915a-472856de1212 to disappear
Jan  4 22:12:55.074: INFO: Pod var-expansion-42f506bb-9c31-493d-915a-472856de1212 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  4 22:12:55.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4626" for this suite. 01/04/23 22:12:55.08
------------------------------
• [4.780 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:12:50.308
    Jan  4 22:12:50.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename var-expansion 01/04/23 22:12:50.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:50.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:50.337
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/04/23 22:12:51.033
    Jan  4 22:12:51.041: INFO: Waiting up to 5m0s for pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212" in namespace "var-expansion-4626" to be "Succeeded or Failed"
    Jan  4 22:12:51.043: INFO: Pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.308757ms
    Jan  4 22:12:53.046: INFO: Pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005724149s
    Jan  4 22:12:55.046: INFO: Pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00578857s
    STEP: Saw pod success 01/04/23 22:12:55.046
    Jan  4 22:12:55.047: INFO: Pod "var-expansion-42f506bb-9c31-493d-915a-472856de1212" satisfied condition "Succeeded or Failed"
    Jan  4 22:12:55.049: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod var-expansion-42f506bb-9c31-493d-915a-472856de1212 container dapi-container: <nil>
    STEP: delete the pod 01/04/23 22:12:55.06
    Jan  4 22:12:55.072: INFO: Waiting for pod var-expansion-42f506bb-9c31-493d-915a-472856de1212 to disappear
    Jan  4 22:12:55.074: INFO: Pod var-expansion-42f506bb-9c31-493d-915a-472856de1212 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:12:55.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4626" for this suite. 01/04/23 22:12:55.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:12:55.092
Jan  4 22:12:55.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 22:12:55.093
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:55.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:55.117
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/04/23 22:12:55.126
STEP: submitting the pod to kubernetes 01/04/23 22:12:55.126
Jan  4 22:12:55.142: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261" in namespace "pods-5004" to be "running and ready"
Jan  4 22:12:55.146: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Pending", Reason="", readiness=false. Elapsed: 3.564339ms
Jan  4 22:12:55.146: INFO: The phase of Pod pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:12:57.150: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007220245s
Jan  4 22:12:57.150: INFO: The phase of Pod pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:12:59.151: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Running", Reason="", readiness=true. Elapsed: 4.008134022s
Jan  4 22:12:59.151: INFO: The phase of Pod pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261 is Running (Ready = true)
Jan  4 22:12:59.151: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/04/23 22:12:59.153
STEP: updating the pod 01/04/23 22:12:59.156
Jan  4 22:12:59.680: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261"
Jan  4 22:12:59.681: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261" in namespace "pods-5004" to be "terminated with reason DeadlineExceeded"
Jan  4 22:12:59.689: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Running", Reason="", readiness=true. Elapsed: 7.923606ms
Jan  4 22:13:01.711: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Running", Reason="", readiness=false. Elapsed: 2.029634862s
Jan  4 22:13:03.694: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012731081s
Jan  4 22:13:03.694: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 22:13:03.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5004" for this suite. 01/04/23 22:13:03.701
------------------------------
• [SLOW TEST] [8.620 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:12:55.092
    Jan  4 22:12:55.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 22:12:55.093
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:12:55.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:12:55.117
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/04/23 22:12:55.126
    STEP: submitting the pod to kubernetes 01/04/23 22:12:55.126
    Jan  4 22:12:55.142: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261" in namespace "pods-5004" to be "running and ready"
    Jan  4 22:12:55.146: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Pending", Reason="", readiness=false. Elapsed: 3.564339ms
    Jan  4 22:12:55.146: INFO: The phase of Pod pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:12:57.150: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007220245s
    Jan  4 22:12:57.150: INFO: The phase of Pod pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:12:59.151: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Running", Reason="", readiness=true. Elapsed: 4.008134022s
    Jan  4 22:12:59.151: INFO: The phase of Pod pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261 is Running (Ready = true)
    Jan  4 22:12:59.151: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/04/23 22:12:59.153
    STEP: updating the pod 01/04/23 22:12:59.156
    Jan  4 22:12:59.680: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261"
    Jan  4 22:12:59.681: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261" in namespace "pods-5004" to be "terminated with reason DeadlineExceeded"
    Jan  4 22:12:59.689: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Running", Reason="", readiness=true. Elapsed: 7.923606ms
    Jan  4 22:13:01.711: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Running", Reason="", readiness=false. Elapsed: 2.029634862s
    Jan  4 22:13:03.694: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012731081s
    Jan  4 22:13:03.694: INFO: Pod "pod-update-activedeadlineseconds-5194ffb0-81df-4e41-8cf7-f64a8bddc261" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:13:03.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5004" for this suite. 01/04/23 22:13:03.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:13:03.716
Jan  4 22:13:03.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:13:03.717
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:03.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:03.75
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-4a5b69de-a34d-4ba9-9d97-4c6d3f03bc9f 01/04/23 22:13:03.754
STEP: Creating a pod to test consume configMaps 01/04/23 22:13:03.761
Jan  4 22:13:03.771: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d" in namespace "projected-3503" to be "Succeeded or Failed"
Jan  4 22:13:03.781: INFO: Pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.535457ms
Jan  4 22:13:05.785: INFO: Pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013882053s
Jan  4 22:13:07.785: INFO: Pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014286134s
STEP: Saw pod success 01/04/23 22:13:07.786
Jan  4 22:13:07.786: INFO: Pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d" satisfied condition "Succeeded or Failed"
Jan  4 22:13:07.789: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:13:07.795
Jan  4 22:13:07.805: INFO: Waiting for pod pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d to disappear
Jan  4 22:13:07.807: INFO: Pod pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:13:07.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3503" for this suite. 01/04/23 22:13:07.811
------------------------------
• [4.101 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:13:03.716
    Jan  4 22:13:03.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:13:03.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:03.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:03.75
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-4a5b69de-a34d-4ba9-9d97-4c6d3f03bc9f 01/04/23 22:13:03.754
    STEP: Creating a pod to test consume configMaps 01/04/23 22:13:03.761
    Jan  4 22:13:03.771: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d" in namespace "projected-3503" to be "Succeeded or Failed"
    Jan  4 22:13:03.781: INFO: Pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.535457ms
    Jan  4 22:13:05.785: INFO: Pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013882053s
    Jan  4 22:13:07.785: INFO: Pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014286134s
    STEP: Saw pod success 01/04/23 22:13:07.786
    Jan  4 22:13:07.786: INFO: Pod "pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d" satisfied condition "Succeeded or Failed"
    Jan  4 22:13:07.789: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:13:07.795
    Jan  4 22:13:07.805: INFO: Waiting for pod pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d to disappear
    Jan  4 22:13:07.807: INFO: Pod pod-projected-configmaps-9092d639-575c-4f25-8a86-204b1c6d976d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:13:07.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3503" for this suite. 01/04/23 22:13:07.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:13:07.818
Jan  4 22:13:07.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:13:07.819
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:07.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:07.837
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-5433 01/04/23 22:13:07.84
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[] 01/04/23 22:13:07.853
Jan  4 22:13:07.871: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5433 01/04/23 22:13:07.871
Jan  4 22:13:07.881: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5433" to be "running and ready"
Jan  4 22:13:07.899: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.193514ms
Jan  4 22:13:07.899: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:13:09.903: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022085802s
Jan  4 22:13:09.903: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  4 22:13:09.903: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[pod1:[80]] 01/04/23 22:13:09.906
Jan  4 22:13:09.916: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/04/23 22:13:09.916
Jan  4 22:13:09.916: INFO: Creating new exec pod
Jan  4 22:13:09.922: INFO: Waiting up to 5m0s for pod "execpodtft8z" in namespace "services-5433" to be "running"
Jan  4 22:13:09.935: INFO: Pod "execpodtft8z": Phase="Pending", Reason="", readiness=false. Elapsed: 12.834742ms
Jan  4 22:13:11.944: INFO: Pod "execpodtft8z": Phase="Running", Reason="", readiness=true. Elapsed: 2.021920101s
Jan  4 22:13:11.944: INFO: Pod "execpodtft8z" satisfied condition "running"
Jan  4 22:13:12.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  4 22:13:13.246: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  4 22:13:13.246: INFO: stdout: ""
Jan  4 22:13:13.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 10.43.185.203 80'
Jan  4 22:13:13.414: INFO: stderr: "+ nc -v -z -w 2 10.43.185.203 80\nConnection to 10.43.185.203 80 port [tcp/http] succeeded!\n"
Jan  4 22:13:13.414: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-5433 01/04/23 22:13:13.414
Jan  4 22:13:13.427: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5433" to be "running and ready"
Jan  4 22:13:13.434: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.698198ms
Jan  4 22:13:13.434: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:13:15.439: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012685982s
Jan  4 22:13:15.439: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:13:17.438: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011691752s
Jan  4 22:13:17.438: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:13:19.439: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 6.012036822s
Jan  4 22:13:19.439: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  4 22:13:19.439: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[pod1:[80] pod2:[80]] 01/04/23 22:13:19.442
Jan  4 22:13:19.451: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/04/23 22:13:19.451
Jan  4 22:13:20.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  4 22:13:20.605: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  4 22:13:20.605: INFO: stdout: ""
Jan  4 22:13:20.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 10.43.185.203 80'
Jan  4 22:13:20.755: INFO: stderr: "+ nc -v -z -w 2 10.43.185.203 80\nConnection to 10.43.185.203 80 port [tcp/http] succeeded!\n"
Jan  4 22:13:20.755: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5433 01/04/23 22:13:20.755
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[pod2:[80]] 01/04/23 22:13:20.776
Jan  4 22:13:21.818: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/04/23 22:13:21.818
Jan  4 22:13:22.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  4 22:13:23.055: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  4 22:13:23.055: INFO: stdout: ""
Jan  4 22:13:23.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 10.43.185.203 80'
Jan  4 22:13:23.329: INFO: stderr: "+ nc -v -z -w 2 10.43.185.203 80\nConnection to 10.43.185.203 80 port [tcp/http] succeeded!\n"
Jan  4 22:13:23.329: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-5433 01/04/23 22:13:23.329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[] 01/04/23 22:13:23.354
Jan  4 22:13:24.403: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:13:24.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5433" for this suite. 01/04/23 22:13:24.434
------------------------------
• [SLOW TEST] [16.627 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:13:07.818
    Jan  4 22:13:07.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:13:07.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:07.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:07.837
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-5433 01/04/23 22:13:07.84
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[] 01/04/23 22:13:07.853
    Jan  4 22:13:07.871: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5433 01/04/23 22:13:07.871
    Jan  4 22:13:07.881: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5433" to be "running and ready"
    Jan  4 22:13:07.899: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.193514ms
    Jan  4 22:13:07.899: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:13:09.903: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022085802s
    Jan  4 22:13:09.903: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  4 22:13:09.903: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[pod1:[80]] 01/04/23 22:13:09.906
    Jan  4 22:13:09.916: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/04/23 22:13:09.916
    Jan  4 22:13:09.916: INFO: Creating new exec pod
    Jan  4 22:13:09.922: INFO: Waiting up to 5m0s for pod "execpodtft8z" in namespace "services-5433" to be "running"
    Jan  4 22:13:09.935: INFO: Pod "execpodtft8z": Phase="Pending", Reason="", readiness=false. Elapsed: 12.834742ms
    Jan  4 22:13:11.944: INFO: Pod "execpodtft8z": Phase="Running", Reason="", readiness=true. Elapsed: 2.021920101s
    Jan  4 22:13:11.944: INFO: Pod "execpodtft8z" satisfied condition "running"
    Jan  4 22:13:12.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  4 22:13:13.246: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  4 22:13:13.246: INFO: stdout: ""
    Jan  4 22:13:13.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 10.43.185.203 80'
    Jan  4 22:13:13.414: INFO: stderr: "+ nc -v -z -w 2 10.43.185.203 80\nConnection to 10.43.185.203 80 port [tcp/http] succeeded!\n"
    Jan  4 22:13:13.414: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-5433 01/04/23 22:13:13.414
    Jan  4 22:13:13.427: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5433" to be "running and ready"
    Jan  4 22:13:13.434: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.698198ms
    Jan  4 22:13:13.434: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:13:15.439: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012685982s
    Jan  4 22:13:15.439: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:13:17.438: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011691752s
    Jan  4 22:13:17.438: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:13:19.439: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 6.012036822s
    Jan  4 22:13:19.439: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  4 22:13:19.439: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[pod1:[80] pod2:[80]] 01/04/23 22:13:19.442
    Jan  4 22:13:19.451: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/04/23 22:13:19.451
    Jan  4 22:13:20.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  4 22:13:20.605: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  4 22:13:20.605: INFO: stdout: ""
    Jan  4 22:13:20.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 10.43.185.203 80'
    Jan  4 22:13:20.755: INFO: stderr: "+ nc -v -z -w 2 10.43.185.203 80\nConnection to 10.43.185.203 80 port [tcp/http] succeeded!\n"
    Jan  4 22:13:20.755: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5433 01/04/23 22:13:20.755
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[pod2:[80]] 01/04/23 22:13:20.776
    Jan  4 22:13:21.818: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/04/23 22:13:21.818
    Jan  4 22:13:22.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  4 22:13:23.055: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  4 22:13:23.055: INFO: stdout: ""
    Jan  4 22:13:23.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5433 exec execpodtft8z -- /bin/sh -x -c nc -v -z -w 2 10.43.185.203 80'
    Jan  4 22:13:23.329: INFO: stderr: "+ nc -v -z -w 2 10.43.185.203 80\nConnection to 10.43.185.203 80 port [tcp/http] succeeded!\n"
    Jan  4 22:13:23.329: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-5433 01/04/23 22:13:23.329
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5433 to expose endpoints map[] 01/04/23 22:13:23.354
    Jan  4 22:13:24.403: INFO: successfully validated that service endpoint-test2 in namespace services-5433 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:13:24.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5433" for this suite. 01/04/23 22:13:24.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:13:24.445
Jan  4 22:13:24.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename daemonsets 01/04/23 22:13:24.446
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:24.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:24.467
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/04/23 22:13:24.485
STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 22:13:24.49
Jan  4 22:13:24.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:13:24.497: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:13:25.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:13:25.509: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:13:26.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  4 22:13:26.506: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:13:27.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  4 22:13:27.504: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:13:28.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  4 22:13:28.503: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:13:29.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 22:13:29.511: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/04/23 22:13:29.514
Jan  4 22:13:29.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  4 22:13:29.543: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:13:30.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 22:13:30.552: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/04/23 22:13:30.552
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:13:30.558
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-429, will wait for the garbage collector to delete the pods 01/04/23 22:13:30.558
Jan  4 22:13:30.617: INFO: Deleting DaemonSet.extensions daemon-set took: 6.488908ms
Jan  4 22:13:30.718: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.873776ms
Jan  4 22:13:36.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:13:36.723: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  4 22:13:36.736: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29628"},"items":null}

Jan  4 22:13:36.746: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29628"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:13:36.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-429" for this suite. 01/04/23 22:13:36.806
------------------------------
• [SLOW TEST] [12.376 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:13:24.445
    Jan  4 22:13:24.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename daemonsets 01/04/23 22:13:24.446
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:24.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:24.467
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/04/23 22:13:24.485
    STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 22:13:24.49
    Jan  4 22:13:24.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:13:24.497: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:13:25.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:13:25.509: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:13:26.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  4 22:13:26.506: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:13:27.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  4 22:13:27.504: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:13:28.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  4 22:13:28.503: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:13:29.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 22:13:29.511: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/04/23 22:13:29.514
    Jan  4 22:13:29.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  4 22:13:29.543: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:13:30.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 22:13:30.552: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/04/23 22:13:30.552
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:13:30.558
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-429, will wait for the garbage collector to delete the pods 01/04/23 22:13:30.558
    Jan  4 22:13:30.617: INFO: Deleting DaemonSet.extensions daemon-set took: 6.488908ms
    Jan  4 22:13:30.718: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.873776ms
    Jan  4 22:13:36.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:13:36.723: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  4 22:13:36.736: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29628"},"items":null}

    Jan  4 22:13:36.746: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29628"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:13:36.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-429" for this suite. 01/04/23 22:13:36.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:13:36.822
Jan  4 22:13:36.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 22:13:36.823
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:36.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:36.85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan  4 22:13:36.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:13:37.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9309" for this suite. 01/04/23 22:13:37.452
------------------------------
• [0.667 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:13:36.822
    Jan  4 22:13:36.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 22:13:36.823
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:36.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:36.85
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan  4 22:13:36.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:13:37.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9309" for this suite. 01/04/23 22:13:37.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:13:37.494
Jan  4 22:13:37.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename dns 01/04/23 22:13:37.496
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:37.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:37.55
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/04/23 22:13:37.552
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local;sleep 1; done
 01/04/23 22:13:37.556
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local;sleep 1; done
 01/04/23 22:13:37.556
STEP: creating a pod to probe DNS 01/04/23 22:13:37.556
STEP: submitting the pod to kubernetes 01/04/23 22:13:37.556
Jan  4 22:13:37.565: INFO: Waiting up to 15m0s for pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec" in namespace "dns-9622" to be "running"
Jan  4 22:13:37.570: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658071ms
Jan  4 22:13:39.652: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086253091s
Jan  4 22:13:41.573: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008012178s
Jan  4 22:13:43.574: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008947857s
Jan  4 22:13:45.576: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010194533s
Jan  4 22:13:47.578: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012686648s
Jan  4 22:13:49.574: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Running", Reason="", readiness=true. Elapsed: 12.008647337s
Jan  4 22:13:49.574: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec" satisfied condition "running"
STEP: retrieving the pod 01/04/23 22:13:49.574
STEP: looking for the results for each expected name from probers 01/04/23 22:13:49.577
Jan  4 22:13:49.585: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:49.588: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:49.590: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:49.594: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:49.597: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:49.599: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:49.602: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:49.604: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:49.604: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

Jan  4 22:13:54.608: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:54.611: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:54.614: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:54.616: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:54.619: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:54.622: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:54.624: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:54.627: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:54.627: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

Jan  4 22:13:59.610: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:59.614: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:59.617: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:59.624: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:59.627: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:59.632: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:59.635: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:59.640: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:13:59.641: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

Jan  4 22:14:04.612: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:04.615: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:04.618: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:04.624: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:04.628: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:04.631: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:04.634: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:04.636: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:04.637: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

Jan  4 22:14:09.610: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:09.613: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:09.618: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:09.625: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:09.628: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:09.631: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
Jan  4 22:14:09.631: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

Jan  4 22:14:14.630: INFO: DNS probes using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec succeeded

STEP: deleting the pod 01/04/23 22:14:14.63
STEP: deleting the test headless service 01/04/23 22:14:14.647
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  4 22:14:14.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9622" for this suite. 01/04/23 22:14:14.697
------------------------------
• [SLOW TEST] [37.211 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:13:37.494
    Jan  4 22:13:37.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename dns 01/04/23 22:13:37.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:13:37.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:13:37.55
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/04/23 22:13:37.552
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local;sleep 1; done
     01/04/23 22:13:37.556
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local;sleep 1; done
     01/04/23 22:13:37.556
    STEP: creating a pod to probe DNS 01/04/23 22:13:37.556
    STEP: submitting the pod to kubernetes 01/04/23 22:13:37.556
    Jan  4 22:13:37.565: INFO: Waiting up to 15m0s for pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec" in namespace "dns-9622" to be "running"
    Jan  4 22:13:37.570: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658071ms
    Jan  4 22:13:39.652: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086253091s
    Jan  4 22:13:41.573: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008012178s
    Jan  4 22:13:43.574: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008947857s
    Jan  4 22:13:45.576: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010194533s
    Jan  4 22:13:47.578: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012686648s
    Jan  4 22:13:49.574: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec": Phase="Running", Reason="", readiness=true. Elapsed: 12.008647337s
    Jan  4 22:13:49.574: INFO: Pod "dns-test-b62f1e47-2918-450e-9a38-9d03901790ec" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 22:13:49.574
    STEP: looking for the results for each expected name from probers 01/04/23 22:13:49.577
    Jan  4 22:13:49.585: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:49.588: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:49.590: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:49.594: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:49.597: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:49.599: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:49.602: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:49.604: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:49.604: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

    Jan  4 22:13:54.608: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:54.611: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:54.614: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:54.616: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:54.619: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:54.622: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:54.624: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:54.627: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:54.627: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

    Jan  4 22:13:59.610: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:59.614: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:59.617: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:59.624: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:59.627: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:59.632: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:59.635: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:59.640: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:13:59.641: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

    Jan  4 22:14:04.612: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:04.615: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:04.618: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:04.624: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:04.628: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:04.631: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:04.634: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:04.636: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:04.637: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

    Jan  4 22:14:09.610: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:09.613: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:09.618: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:09.625: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:09.628: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:09.631: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local from pod dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec: the server could not find the requested resource (get pods dns-test-b62f1e47-2918-450e-9a38-9d03901790ec)
    Jan  4 22:14:09.631: INFO: Lookups using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9622.svc.cluster.local jessie_udp@dns-test-service-2.dns-9622.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9622.svc.cluster.local]

    Jan  4 22:14:14.630: INFO: DNS probes using dns-9622/dns-test-b62f1e47-2918-450e-9a38-9d03901790ec succeeded

    STEP: deleting the pod 01/04/23 22:14:14.63
    STEP: deleting the test headless service 01/04/23 22:14:14.647
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:14:14.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9622" for this suite. 01/04/23 22:14:14.697
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:14:14.708
Jan  4 22:14:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename var-expansion 01/04/23 22:14:14.71
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:14.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:14.731
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/04/23 22:14:14.745
Jan  4 22:14:14.755: INFO: Waiting up to 5m0s for pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa" in namespace "var-expansion-8226" to be "Succeeded or Failed"
Jan  4 22:14:14.760: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.069882ms
Jan  4 22:14:16.768: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01230118s
Jan  4 22:14:18.766: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Running", Reason="", readiness=true. Elapsed: 4.010548925s
Jan  4 22:14:20.766: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Running", Reason="", readiness=false. Elapsed: 6.01034593s
Jan  4 22:14:22.765: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009693135s
STEP: Saw pod success 01/04/23 22:14:22.765
Jan  4 22:14:22.765: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa" satisfied condition "Succeeded or Failed"
Jan  4 22:14:22.768: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa container dapi-container: <nil>
STEP: delete the pod 01/04/23 22:14:22.774
Jan  4 22:14:22.785: INFO: Waiting for pod var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa to disappear
Jan  4 22:14:22.787: INFO: Pod var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  4 22:14:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8226" for this suite. 01/04/23 22:14:22.792
------------------------------
• [SLOW TEST] [8.091 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:14:14.708
    Jan  4 22:14:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename var-expansion 01/04/23 22:14:14.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:14.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:14.731
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/04/23 22:14:14.745
    Jan  4 22:14:14.755: INFO: Waiting up to 5m0s for pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa" in namespace "var-expansion-8226" to be "Succeeded or Failed"
    Jan  4 22:14:14.760: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.069882ms
    Jan  4 22:14:16.768: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01230118s
    Jan  4 22:14:18.766: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Running", Reason="", readiness=true. Elapsed: 4.010548925s
    Jan  4 22:14:20.766: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Running", Reason="", readiness=false. Elapsed: 6.01034593s
    Jan  4 22:14:22.765: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009693135s
    STEP: Saw pod success 01/04/23 22:14:22.765
    Jan  4 22:14:22.765: INFO: Pod "var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa" satisfied condition "Succeeded or Failed"
    Jan  4 22:14:22.768: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa container dapi-container: <nil>
    STEP: delete the pod 01/04/23 22:14:22.774
    Jan  4 22:14:22.785: INFO: Waiting for pod var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa to disappear
    Jan  4 22:14:22.787: INFO: Pod var-expansion-d5abd249-13a6-454f-8d22-bbe0fd1883aa no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:14:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8226" for this suite. 01/04/23 22:14:22.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:14:22.801
Jan  4 22:14:22.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-pred 01/04/23 22:14:22.803
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:22.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:22.828
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  4 22:14:22.831: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  4 22:14:22.840: INFO: Waiting for terminating namespaces to be deleted...
Jan  4 22:14:22.851: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-54.us-east-2.compute.internal before test
Jan  4 22:14:22.862: INFO: cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:39 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.862: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 22:14:22.862: INFO: etcd-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:18 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.862: INFO: 	Container etcd ready: true, restart count 0
Jan  4 22:14:22.862: INFO: helm-install-rke2-canal-r7b4c from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.862: INFO: 	Container helm ready: false, restart count 0
Jan  4 22:14:22.864: INFO: helm-install-rke2-coredns-8ff46 from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.864: INFO: 	Container helm ready: false, restart count 0
Jan  4 22:14:22.865: INFO: helm-install-rke2-ingress-nginx-qfntk from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.865: INFO: 	Container helm ready: false, restart count 0
Jan  4 22:14:22.866: INFO: helm-install-rke2-metrics-server-q46jz from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.866: INFO: 	Container helm ready: false, restart count 0
Jan  4 22:14:22.866: INFO: kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:32 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.866: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 22:14:22.866: INFO: kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:38 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.866: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 22:14:22.866: INFO: kube-proxy-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:42 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.866: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 22:14:22.868: INFO: kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:37 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.868: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 22:14:22.868: INFO: rke2-canal-ggwd4 from kube-system started at 2023-01-04 20:05:59 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.868: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 22:14:22.868: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 22:14:22.868: INFO: rke2-coredns-rke2-coredns-854779488f-mwkvw from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.868: INFO: 	Container coredns ready: true, restart count 0
Jan  4 22:14:22.869: INFO: rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.871: INFO: 	Container autoscaler ready: true, restart count 0
Jan  4 22:14:22.871: INFO: rke2-ingress-nginx-controller-97km7 from kube-system started at 2023-01-04 20:06:54 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.871: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 22:14:22.871: INFO: rke2-metrics-server-778467dc76-4rtdk from kube-system started at 2023-01-04 20:06:31 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.871: INFO: 	Container metrics-server ready: true, restart count 0
Jan  4 22:14:22.872: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:14:22.872: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 22:14:22.872: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-117.us-east-2.compute.internal before test
Jan  4 22:14:22.879: INFO: kube-proxy-ip-172-31-13-117.us-east-2.compute.internal from kube-system started at 2023-01-04 20:10:23 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.879: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 22:14:22.879: INFO: rke2-canal-mprb9 from kube-system started at 2023-01-04 20:10:24 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.879: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 22:14:22.879: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 22:14:22.879: INFO: rke2-ingress-nginx-controller-8mjvf from kube-system started at 2023-01-04 20:10:52 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.879: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 22:14:22.879: INFO: sonobuoy from sonobuoy started at 2023-01-04 22:11:19 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.879: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  4 22:14:22.879: INFO: sonobuoy-e2e-job-6a70417ebe254b91 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.879: INFO: 	Container e2e ready: true, restart count 0
Jan  4 22:14:22.879: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:14:22.879: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.879: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:14:22.879: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 22:14:22.879: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-240.us-east-2.compute.internal before test
Jan  4 22:14:22.886: INFO: cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.886: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 22:14:22.887: INFO: etcd-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:07:58 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.887: INFO: 	Container etcd ready: true, restart count 0
Jan  4 22:14:22.887: INFO: kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:18 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.887: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 22:14:22.887: INFO: kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.887: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 22:14:22.887: INFO: kube-proxy-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:27 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.887: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 22:14:22.887: INFO: kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.887: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 22:14:22.887: INFO: rke2-canal-wspdm from kube-system started at 2023-01-04 20:08:32 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.887: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 22:14:22.887: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 22:14:22.887: INFO: rke2-coredns-rke2-coredns-854779488f-n8z2r from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.887: INFO: 	Container coredns ready: true, restart count 0
Jan  4 22:14:22.887: INFO: rke2-ingress-nginx-controller-rv4dm from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.887: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 22:14:22.887: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:14:22.888: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 22:14:22.888: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-62.us-east-2.compute.internal before test
Jan  4 22:14:22.896: INFO: cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 22:14:22.896: INFO: etcd-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:22 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container etcd ready: true, restart count 0
Jan  4 22:14:22.896: INFO: kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:36 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 22:14:22.896: INFO: kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 22:14:22.896: INFO: kube-proxy-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:47 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 22:14:22.896: INFO: kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 22:14:22.896: INFO: rke2-canal-44lz9 from kube-system started at 2023-01-04 20:08:42 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 22:14:22.896: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 22:14:22.896: INFO: rke2-ingress-nginx-controller-glxqt from kube-system started at 2023-01-04 20:08:54 +0000 UTC (1 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 22:14:22.896: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:14:22.896: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:14:22.896: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/04/23 22:14:22.896
Jan  4 22:14:22.905: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8565" to be "running"
Jan  4 22:14:22.908: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669083ms
Jan  4 22:14:24.912: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006809298s
Jan  4 22:14:24.912: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/04/23 22:14:24.914
STEP: Trying to apply a random label on the found node. 01/04/23 22:14:24.923
STEP: verifying the node has the label kubernetes.io/e2e-9aa504d8-cd5c-4ba1-a74b-74fe1e4e8e7f 42 01/04/23 22:14:24.941
STEP: Trying to relaunch the pod, now with labels. 01/04/23 22:14:24.944
Jan  4 22:14:24.949: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8565" to be "not pending"
Jan  4 22:14:24.953: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.718181ms
Jan  4 22:14:26.957: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008302996s
Jan  4 22:14:26.957: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-9aa504d8-cd5c-4ba1-a74b-74fe1e4e8e7f off the node ip-172-31-13-117.us-east-2.compute.internal 01/04/23 22:14:26.959
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9aa504d8-cd5c-4ba1-a74b-74fe1e4e8e7f 01/04/23 22:14:26.973
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:14:26.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8565" for this suite. 01/04/23 22:14:27.003
------------------------------
• [4.212 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:14:22.801
    Jan  4 22:14:22.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-pred 01/04/23 22:14:22.803
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:22.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:22.828
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  4 22:14:22.831: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  4 22:14:22.840: INFO: Waiting for terminating namespaces to be deleted...
    Jan  4 22:14:22.851: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-11-54.us-east-2.compute.internal before test
    Jan  4 22:14:22.862: INFO: cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:39 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.862: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 22:14:22.862: INFO: etcd-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:18 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.862: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 22:14:22.862: INFO: helm-install-rke2-canal-r7b4c from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.862: INFO: 	Container helm ready: false, restart count 0
    Jan  4 22:14:22.864: INFO: helm-install-rke2-coredns-8ff46 from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.864: INFO: 	Container helm ready: false, restart count 0
    Jan  4 22:14:22.865: INFO: helm-install-rke2-ingress-nginx-qfntk from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.865: INFO: 	Container helm ready: false, restart count 0
    Jan  4 22:14:22.866: INFO: helm-install-rke2-metrics-server-q46jz from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.866: INFO: 	Container helm ready: false, restart count 0
    Jan  4 22:14:22.866: INFO: kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:32 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.866: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 22:14:22.866: INFO: kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:38 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.866: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 22:14:22.866: INFO: kube-proxy-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:42 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.866: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 22:14:22.868: INFO: kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:37 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.868: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 22:14:22.868: INFO: rke2-canal-ggwd4 from kube-system started at 2023-01-04 20:05:59 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.868: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 22:14:22.868: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 22:14:22.868: INFO: rke2-coredns-rke2-coredns-854779488f-mwkvw from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.868: INFO: 	Container coredns ready: true, restart count 0
    Jan  4 22:14:22.869: INFO: rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.871: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  4 22:14:22.871: INFO: rke2-ingress-nginx-controller-97km7 from kube-system started at 2023-01-04 20:06:54 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.871: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 22:14:22.871: INFO: rke2-metrics-server-778467dc76-4rtdk from kube-system started at 2023-01-04 20:06:31 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.871: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  4 22:14:22.872: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:14:22.872: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 22:14:22.872: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-13-117.us-east-2.compute.internal before test
    Jan  4 22:14:22.879: INFO: kube-proxy-ip-172-31-13-117.us-east-2.compute.internal from kube-system started at 2023-01-04 20:10:23 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.879: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: rke2-canal-mprb9 from kube-system started at 2023-01-04 20:10:24 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.879: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: rke2-ingress-nginx-controller-8mjvf from kube-system started at 2023-01-04 20:10:52 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.879: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: sonobuoy from sonobuoy started at 2023-01-04 22:11:19 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.879: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: sonobuoy-e2e-job-6a70417ebe254b91 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.879: INFO: 	Container e2e ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.879: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 22:14:22.879: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-3-240.us-east-2.compute.internal before test
    Jan  4 22:14:22.886: INFO: cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.886: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: etcd-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:07:58 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.887: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:18 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.887: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.887: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: kube-proxy-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:27 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.887: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.887: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: rke2-canal-wspdm from kube-system started at 2023-01-04 20:08:32 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.887: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: rke2-coredns-rke2-coredns-854779488f-n8z2r from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.887: INFO: 	Container coredns ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: rke2-ingress-nginx-controller-rv4dm from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.887: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 22:14:22.887: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:14:22.888: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 22:14:22.888: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-9-62.us-east-2.compute.internal before test
    Jan  4 22:14:22.896: INFO: cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: etcd-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:22 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:36 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: kube-proxy-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:47 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: rke2-canal-44lz9 from kube-system started at 2023-01-04 20:08:42 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: rke2-ingress-nginx-controller-glxqt from kube-system started at 2023-01-04 20:08:54 +0000 UTC (1 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:14:22.896: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:14:22.896: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/04/23 22:14:22.896
    Jan  4 22:14:22.905: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8565" to be "running"
    Jan  4 22:14:22.908: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669083ms
    Jan  4 22:14:24.912: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006809298s
    Jan  4 22:14:24.912: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/04/23 22:14:24.914
    STEP: Trying to apply a random label on the found node. 01/04/23 22:14:24.923
    STEP: verifying the node has the label kubernetes.io/e2e-9aa504d8-cd5c-4ba1-a74b-74fe1e4e8e7f 42 01/04/23 22:14:24.941
    STEP: Trying to relaunch the pod, now with labels. 01/04/23 22:14:24.944
    Jan  4 22:14:24.949: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8565" to be "not pending"
    Jan  4 22:14:24.953: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.718181ms
    Jan  4 22:14:26.957: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008302996s
    Jan  4 22:14:26.957: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-9aa504d8-cd5c-4ba1-a74b-74fe1e4e8e7f off the node ip-172-31-13-117.us-east-2.compute.internal 01/04/23 22:14:26.959
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-9aa504d8-cd5c-4ba1-a74b-74fe1e4e8e7f 01/04/23 22:14:26.973
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:14:26.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8565" for this suite. 01/04/23 22:14:27.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:14:27.013
Jan  4 22:14:27.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-runtime 01/04/23 22:14:27.014
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:27.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:27.038
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/04/23 22:14:27.047
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/04/23 22:14:42.117
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/04/23 22:14:42.12
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/04/23 22:14:42.124
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/04/23 22:14:42.124
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/04/23 22:14:42.151
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/04/23 22:14:45.165
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/04/23 22:14:47.177
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/04/23 22:14:47.182
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/04/23 22:14:47.182
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/04/23 22:14:47.197
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/04/23 22:14:48.202
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/04/23 22:14:51.219
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/04/23 22:14:51.224
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/04/23 22:14:51.224
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  4 22:14:51.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1166" for this suite. 01/04/23 22:14:51.267
------------------------------
• [SLOW TEST] [24.263 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:14:27.013
    Jan  4 22:14:27.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-runtime 01/04/23 22:14:27.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:27.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:27.038
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/04/23 22:14:27.047
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/04/23 22:14:42.117
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/04/23 22:14:42.12
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/04/23 22:14:42.124
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/04/23 22:14:42.124
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/04/23 22:14:42.151
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/04/23 22:14:45.165
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/04/23 22:14:47.177
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/04/23 22:14:47.182
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/04/23 22:14:47.182
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/04/23 22:14:47.197
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/04/23 22:14:48.202
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/04/23 22:14:51.219
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/04/23 22:14:51.224
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/04/23 22:14:51.224
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:14:51.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1166" for this suite. 01/04/23 22:14:51.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:14:51.288
Jan  4 22:14:51.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename var-expansion 01/04/23 22:14:51.289
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:52.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:52.129
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan  4 22:14:52.173: INFO: Waiting up to 2m0s for pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7" in namespace "var-expansion-8243" to be "container 0 failed with reason CreateContainerConfigError"
Jan  4 22:14:52.185: INFO: Pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.893189ms
Jan  4 22:14:54.188: INFO: Pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015233055s
Jan  4 22:14:54.188: INFO: Pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  4 22:14:54.189: INFO: Deleting pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7" in namespace "var-expansion-8243"
Jan  4 22:14:54.195: INFO: Wait up to 5m0s for pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  4 22:14:56.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8243" for this suite. 01/04/23 22:14:56.215
------------------------------
• [4.957 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:14:51.288
    Jan  4 22:14:51.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename var-expansion 01/04/23 22:14:51.289
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:52.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:52.129
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan  4 22:14:52.173: INFO: Waiting up to 2m0s for pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7" in namespace "var-expansion-8243" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  4 22:14:52.185: INFO: Pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.893189ms
    Jan  4 22:14:54.188: INFO: Pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015233055s
    Jan  4 22:14:54.188: INFO: Pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  4 22:14:54.189: INFO: Deleting pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7" in namespace "var-expansion-8243"
    Jan  4 22:14:54.195: INFO: Wait up to 5m0s for pod "var-expansion-badc01d3-79c0-4f8b-a24a-57f95d580da7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:14:56.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8243" for this suite. 01/04/23 22:14:56.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:14:56.246
Jan  4 22:14:56.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 22:14:56.247
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:56.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:56.282
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/04/23 22:14:56.284
STEP: Creating a ResourceQuota 01/04/23 22:15:01.29
STEP: Ensuring resource quota status is calculated 01/04/23 22:15:01.309
STEP: Creating a Pod that fits quota 01/04/23 22:15:03.321
STEP: Ensuring ResourceQuota status captures the pod usage 01/04/23 22:15:03.34
STEP: Not allowing a pod to be created that exceeds remaining quota 01/04/23 22:15:05.343
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/04/23 22:15:05.346
STEP: Ensuring a pod cannot update its resource requirements 01/04/23 22:15:05.348
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/04/23 22:15:05.352
STEP: Deleting the pod 01/04/23 22:15:07.358
STEP: Ensuring resource quota status released the pod usage 01/04/23 22:15:07.376
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 22:15:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7343" for this suite. 01/04/23 22:15:09.385
------------------------------
• [SLOW TEST] [13.145 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:14:56.246
    Jan  4 22:14:56.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 22:14:56.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:14:56.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:14:56.282
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/04/23 22:14:56.284
    STEP: Creating a ResourceQuota 01/04/23 22:15:01.29
    STEP: Ensuring resource quota status is calculated 01/04/23 22:15:01.309
    STEP: Creating a Pod that fits quota 01/04/23 22:15:03.321
    STEP: Ensuring ResourceQuota status captures the pod usage 01/04/23 22:15:03.34
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/04/23 22:15:05.343
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/04/23 22:15:05.346
    STEP: Ensuring a pod cannot update its resource requirements 01/04/23 22:15:05.348
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/04/23 22:15:05.352
    STEP: Deleting the pod 01/04/23 22:15:07.358
    STEP: Ensuring resource quota status released the pod usage 01/04/23 22:15:07.376
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:15:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7343" for this suite. 01/04/23 22:15:09.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:15:09.394
Jan  4 22:15:09.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename runtimeclass 01/04/23 22:15:09.395
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:15:09.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:15:09.419
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan  4 22:15:09.435: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-463 to be scheduled
Jan  4 22:15:09.438: INFO: 1 pods are not scheduled: [runtimeclass-463/test-runtimeclass-runtimeclass-463-preconfigured-handler-q86rs(12e29573-fa80-4a87-b0c2-f0d3458a2002)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  4 22:15:11.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-463" for this suite. 01/04/23 22:15:11.451
------------------------------
• [2.065 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:15:09.394
    Jan  4 22:15:09.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename runtimeclass 01/04/23 22:15:09.395
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:15:09.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:15:09.419
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan  4 22:15:09.435: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-463 to be scheduled
    Jan  4 22:15:09.438: INFO: 1 pods are not scheduled: [runtimeclass-463/test-runtimeclass-runtimeclass-463-preconfigured-handler-q86rs(12e29573-fa80-4a87-b0c2-f0d3458a2002)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:15:11.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-463" for this suite. 01/04/23 22:15:11.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:15:11.46
Jan  4 22:15:11.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename watch 01/04/23 22:15:11.461
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:15:11.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:15:11.487
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/04/23 22:15:11.489
STEP: creating a watch on configmaps with label B 01/04/23 22:15:11.49
STEP: creating a watch on configmaps with label A or B 01/04/23 22:15:11.491
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/04/23 22:15:11.492
Jan  4 22:15:11.497: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30301 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:15:11.497: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30301 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/04/23 22:15:11.497
Jan  4 22:15:11.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30302 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:15:11.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30302 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/04/23 22:15:11.506
Jan  4 22:15:11.516: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30303 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:15:11.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30303 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/04/23 22:15:11.517
Jan  4 22:15:11.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30304 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:15:11.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30304 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/04/23 22:15:11.522
Jan  4 22:15:11.527: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1970  6dda24b8-dc88-494b-b0fb-88414c9151e3 30305 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:15:11.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1970  6dda24b8-dc88-494b-b0fb-88414c9151e3 30305 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/04/23 22:15:21.529
Jan  4 22:15:21.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1970  6dda24b8-dc88-494b-b0fb-88414c9151e3 30360 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:15:21.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1970  6dda24b8-dc88-494b-b0fb-88414c9151e3 30360 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  4 22:15:31.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1970" for this suite. 01/04/23 22:15:31.54
------------------------------
• [SLOW TEST] [20.086 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:15:11.46
    Jan  4 22:15:11.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename watch 01/04/23 22:15:11.461
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:15:11.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:15:11.487
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/04/23 22:15:11.489
    STEP: creating a watch on configmaps with label B 01/04/23 22:15:11.49
    STEP: creating a watch on configmaps with label A or B 01/04/23 22:15:11.491
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/04/23 22:15:11.492
    Jan  4 22:15:11.497: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30301 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:15:11.497: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30301 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/04/23 22:15:11.497
    Jan  4 22:15:11.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30302 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:15:11.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30302 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/04/23 22:15:11.506
    Jan  4 22:15:11.516: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30303 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:15:11.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30303 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/04/23 22:15:11.517
    Jan  4 22:15:11.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30304 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:15:11.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1970  58102706-8dbd-4fdb-85bd-b84ec8afa91e 30304 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/04/23 22:15:11.522
    Jan  4 22:15:11.527: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1970  6dda24b8-dc88-494b-b0fb-88414c9151e3 30305 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:15:11.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1970  6dda24b8-dc88-494b-b0fb-88414c9151e3 30305 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/04/23 22:15:21.529
    Jan  4 22:15:21.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1970  6dda24b8-dc88-494b-b0fb-88414c9151e3 30360 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:15:21.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1970  6dda24b8-dc88-494b-b0fb-88414c9151e3 30360 0 2023-01-04 22:15:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-04 22:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:15:31.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1970" for this suite. 01/04/23 22:15:31.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:15:31.546
Jan  4 22:15:31.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:15:31.548
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:15:31.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:15:31.571
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:15:31.585
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:15:32.109
STEP: Deploying the webhook pod 01/04/23 22:15:32.116
STEP: Wait for the deployment to be ready 01/04/23 22:15:32.127
Jan  4 22:15:32.136: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 22:15:34.145
STEP: Verifying the service has paired with the endpoint 01/04/23 22:15:34.155
Jan  4 22:15:35.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/04/23 22:15:35.207
STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:15:35.267
STEP: Deleting the collection of validation webhooks 01/04/23 22:15:35.324
STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:15:35.386
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:15:35.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4203" for this suite. 01/04/23 22:15:35.464
STEP: Destroying namespace "webhook-4203-markers" for this suite. 01/04/23 22:15:35.475
------------------------------
• [3.937 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:15:31.546
    Jan  4 22:15:31.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:15:31.548
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:15:31.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:15:31.571
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:15:31.585
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:15:32.109
    STEP: Deploying the webhook pod 01/04/23 22:15:32.116
    STEP: Wait for the deployment to be ready 01/04/23 22:15:32.127
    Jan  4 22:15:32.136: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 22:15:34.145
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:15:34.155
    Jan  4 22:15:35.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/04/23 22:15:35.207
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:15:35.267
    STEP: Deleting the collection of validation webhooks 01/04/23 22:15:35.324
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:15:35.386
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:15:35.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4203" for this suite. 01/04/23 22:15:35.464
    STEP: Destroying namespace "webhook-4203-markers" for this suite. 01/04/23 22:15:35.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:15:35.487
Jan  4 22:15:35.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename dns 01/04/23 22:15:35.488
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:15:35.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:15:35.53
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/04/23 22:15:35.538
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8731.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8731.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 15.19.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.19.15_udp@PTR;check="$$(dig +tcp +noall +answer +search 15.19.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.19.15_tcp@PTR;sleep 1; done
 01/04/23 22:15:35.558
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8731.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8731.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 15.19.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.19.15_udp@PTR;check="$$(dig +tcp +noall +answer +search 15.19.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.19.15_tcp@PTR;sleep 1; done
 01/04/23 22:15:35.558
STEP: creating a pod to probe DNS 01/04/23 22:15:35.558
STEP: submitting the pod to kubernetes 01/04/23 22:15:35.559
Jan  4 22:15:35.572: INFO: Waiting up to 15m0s for pod "dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8" in namespace "dns-8731" to be "running"
Jan  4 22:15:35.579: INFO: Pod "dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.548349ms
Jan  4 22:15:37.584: INFO: Pod "dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011430316s
Jan  4 22:15:37.584: INFO: Pod "dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8" satisfied condition "running"
STEP: retrieving the pod 01/04/23 22:15:37.584
STEP: looking for the results for each expected name from probers 01/04/23 22:15:37.586
Jan  4 22:15:37.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:37.597: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:37.600: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:37.603: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:37.617: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:37.619: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:37.622: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:37.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:37.634: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

Jan  4 22:15:42.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:42.642: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:42.644: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:42.649: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:42.674: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:42.678: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:42.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:42.705: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:42.716: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

Jan  4 22:15:47.641: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:47.644: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:47.647: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:47.649: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:47.662: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:47.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:47.668: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:47.670: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:47.694: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

Jan  4 22:15:52.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:52.643: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:52.647: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:52.650: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:52.667: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:52.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:52.672: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:52.674: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:52.685: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

Jan  4 22:15:57.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:57.641: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:57.644: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:57.651: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:57.667: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:57.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:57.673: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:57.676: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:15:57.691: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

Jan  4 22:16:02.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:16:02.643: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:16:02.646: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:16:02.649: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:16:02.668: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:16:02.674: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:16:02.679: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:16:02.682: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
Jan  4 22:16:02.695: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

Jan  4 22:16:07.751: INFO: DNS probes using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 succeeded

STEP: deleting the pod 01/04/23 22:16:07.751
STEP: deleting the test service 01/04/23 22:16:07.805
STEP: deleting the test headless service 01/04/23 22:16:07.838
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  4 22:16:07.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8731" for this suite. 01/04/23 22:16:07.871
------------------------------
• [SLOW TEST] [32.396 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:15:35.487
    Jan  4 22:15:35.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename dns 01/04/23 22:15:35.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:15:35.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:15:35.53
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/04/23 22:15:35.538
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8731.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8731.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 15.19.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.19.15_udp@PTR;check="$$(dig +tcp +noall +answer +search 15.19.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.19.15_tcp@PTR;sleep 1; done
     01/04/23 22:15:35.558
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8731.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8731.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8731.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8731.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8731.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 15.19.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.19.15_udp@PTR;check="$$(dig +tcp +noall +answer +search 15.19.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.19.15_tcp@PTR;sleep 1; done
     01/04/23 22:15:35.558
    STEP: creating a pod to probe DNS 01/04/23 22:15:35.558
    STEP: submitting the pod to kubernetes 01/04/23 22:15:35.559
    Jan  4 22:15:35.572: INFO: Waiting up to 15m0s for pod "dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8" in namespace "dns-8731" to be "running"
    Jan  4 22:15:35.579: INFO: Pod "dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.548349ms
    Jan  4 22:15:37.584: INFO: Pod "dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011430316s
    Jan  4 22:15:37.584: INFO: Pod "dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 22:15:37.584
    STEP: looking for the results for each expected name from probers 01/04/23 22:15:37.586
    Jan  4 22:15:37.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:37.597: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:37.600: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:37.603: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:37.617: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:37.619: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:37.622: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:37.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:37.634: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

    Jan  4 22:15:42.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:42.642: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:42.644: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:42.649: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:42.674: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:42.678: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:42.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:42.705: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:42.716: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

    Jan  4 22:15:47.641: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:47.644: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:47.647: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:47.649: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:47.662: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:47.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:47.668: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:47.670: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:47.694: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

    Jan  4 22:15:52.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:52.643: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:52.647: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:52.650: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:52.667: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:52.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:52.672: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:52.674: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:52.685: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

    Jan  4 22:15:57.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:57.641: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:57.644: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:57.651: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:57.667: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:57.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:57.673: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:57.676: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:15:57.691: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

    Jan  4 22:16:02.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:16:02.643: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:16:02.646: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:16:02.649: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:16:02.668: INFO: Unable to read jessie_udp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:16:02.674: INFO: Unable to read jessie_tcp@dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:16:02.679: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:16:02.682: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local from pod dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8: the server could not find the requested resource (get pods dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8)
    Jan  4 22:16:02.695: INFO: Lookups using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 failed for: [wheezy_udp@dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@dns-test-service.dns-8731.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_udp@dns-test-service.dns-8731.svc.cluster.local jessie_tcp@dns-test-service.dns-8731.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8731.svc.cluster.local]

    Jan  4 22:16:07.751: INFO: DNS probes using dns-8731/dns-test-96fd7640-62d2-464b-82ec-2e868ec001a8 succeeded

    STEP: deleting the pod 01/04/23 22:16:07.751
    STEP: deleting the test service 01/04/23 22:16:07.805
    STEP: deleting the test headless service 01/04/23 22:16:07.838
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:16:07.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8731" for this suite. 01/04/23 22:16:07.871
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:16:07.886
Jan  4 22:16:07.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:16:07.888
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:07.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:07.916
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-40f706c0-1032-4dcf-9152-e3b94dad17c7 01/04/23 22:16:07.918
STEP: Creating a pod to test consume secrets 01/04/23 22:16:07.924
Jan  4 22:16:07.931: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba" in namespace "projected-6958" to be "Succeeded or Failed"
Jan  4 22:16:07.934: INFO: Pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842965ms
Jan  4 22:16:10.298: INFO: Pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366292914s
Jan  4 22:16:11.938: INFO: Pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006292295s
STEP: Saw pod success 01/04/23 22:16:11.938
Jan  4 22:16:11.938: INFO: Pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba" satisfied condition "Succeeded or Failed"
Jan  4 22:16:11.940: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba container projected-secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:16:11.951
Jan  4 22:16:11.961: INFO: Waiting for pod pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba to disappear
Jan  4 22:16:11.963: INFO: Pod pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  4 22:16:11.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6958" for this suite. 01/04/23 22:16:11.967
------------------------------
• [4.087 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:16:07.886
    Jan  4 22:16:07.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:16:07.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:07.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:07.916
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-40f706c0-1032-4dcf-9152-e3b94dad17c7 01/04/23 22:16:07.918
    STEP: Creating a pod to test consume secrets 01/04/23 22:16:07.924
    Jan  4 22:16:07.931: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba" in namespace "projected-6958" to be "Succeeded or Failed"
    Jan  4 22:16:07.934: INFO: Pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842965ms
    Jan  4 22:16:10.298: INFO: Pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366292914s
    Jan  4 22:16:11.938: INFO: Pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006292295s
    STEP: Saw pod success 01/04/23 22:16:11.938
    Jan  4 22:16:11.938: INFO: Pod "pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba" satisfied condition "Succeeded or Failed"
    Jan  4 22:16:11.940: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:16:11.951
    Jan  4 22:16:11.961: INFO: Waiting for pod pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba to disappear
    Jan  4 22:16:11.963: INFO: Pod pod-projected-secrets-840e139f-eaf4-4d3d-b6a5-b7bf0a487eba no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:16:11.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6958" for this suite. 01/04/23 22:16:11.967
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:16:11.975
Jan  4 22:16:11.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 22:16:11.975
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:11.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:11.992
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-67f23f1a-6058-4459-8006-745b9c480405 01/04/23 22:16:11.994
STEP: Creating a pod to test consume configMaps 01/04/23 22:16:11.998
Jan  4 22:16:12.008: INFO: Waiting up to 5m0s for pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35" in namespace "configmap-7130" to be "Succeeded or Failed"
Jan  4 22:16:12.011: INFO: Pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868106ms
Jan  4 22:16:14.014: INFO: Pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006005504s
Jan  4 22:16:16.014: INFO: Pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006210021s
STEP: Saw pod success 01/04/23 22:16:16.014
Jan  4 22:16:16.014: INFO: Pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35" satisfied condition "Succeeded or Failed"
Jan  4 22:16:16.017: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:16:16.024
Jan  4 22:16:16.043: INFO: Waiting for pod pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35 to disappear
Jan  4 22:16:16.048: INFO: Pod pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:16:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7130" for this suite. 01/04/23 22:16:16.054
------------------------------
• [4.091 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:16:11.975
    Jan  4 22:16:11.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 22:16:11.975
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:11.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:11.992
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-67f23f1a-6058-4459-8006-745b9c480405 01/04/23 22:16:11.994
    STEP: Creating a pod to test consume configMaps 01/04/23 22:16:11.998
    Jan  4 22:16:12.008: INFO: Waiting up to 5m0s for pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35" in namespace "configmap-7130" to be "Succeeded or Failed"
    Jan  4 22:16:12.011: INFO: Pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868106ms
    Jan  4 22:16:14.014: INFO: Pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006005504s
    Jan  4 22:16:16.014: INFO: Pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006210021s
    STEP: Saw pod success 01/04/23 22:16:16.014
    Jan  4 22:16:16.014: INFO: Pod "pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35" satisfied condition "Succeeded or Failed"
    Jan  4 22:16:16.017: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:16:16.024
    Jan  4 22:16:16.043: INFO: Waiting for pod pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35 to disappear
    Jan  4 22:16:16.048: INFO: Pod pod-configmaps-67a2304a-f1fe-4d1b-beb7-e80cc6f1af35 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:16:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7130" for this suite. 01/04/23 22:16:16.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:16:16.067
Jan  4 22:16:16.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:16:16.068
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:16.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:16.096
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:16:16.11
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:16:16.671
STEP: Deploying the webhook pod 01/04/23 22:16:16.694
STEP: Wait for the deployment to be ready 01/04/23 22:16:16.714
Jan  4 22:16:16.738: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 22:16:18.748
STEP: Verifying the service has paired with the endpoint 01/04/23 22:16:18.767
Jan  4 22:16:19.768: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/04/23 22:16:19.771
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/04/23 22:16:19.772
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/04/23 22:16:19.772
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/04/23 22:16:19.772
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/04/23 22:16:19.773
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/04/23 22:16:19.773
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/04/23 22:16:19.774
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:16:19.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5200" for this suite. 01/04/23 22:16:19.844
STEP: Destroying namespace "webhook-5200-markers" for this suite. 01/04/23 22:16:19.861
------------------------------
• [3.830 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:16:16.067
    Jan  4 22:16:16.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:16:16.068
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:16.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:16.096
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:16:16.11
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:16:16.671
    STEP: Deploying the webhook pod 01/04/23 22:16:16.694
    STEP: Wait for the deployment to be ready 01/04/23 22:16:16.714
    Jan  4 22:16:16.738: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 22:16:18.748
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:16:18.767
    Jan  4 22:16:19.768: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/04/23 22:16:19.771
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/04/23 22:16:19.772
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/04/23 22:16:19.772
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/04/23 22:16:19.772
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/04/23 22:16:19.773
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/04/23 22:16:19.773
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/04/23 22:16:19.774
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:16:19.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5200" for this suite. 01/04/23 22:16:19.844
    STEP: Destroying namespace "webhook-5200-markers" for this suite. 01/04/23 22:16:19.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:16:19.914
Jan  4 22:16:19.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:16:19.915
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:19.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:19.94
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4550 01/04/23 22:16:19.944
STEP: changing the ExternalName service to type=NodePort 01/04/23 22:16:19.949
STEP: creating replication controller externalname-service in namespace services-4550 01/04/23 22:16:19.973
I0104 22:16:19.980296      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4550, replica count: 2
I0104 22:16:23.031987      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 22:16:23.032: INFO: Creating new exec pod
Jan  4 22:16:23.040: INFO: Waiting up to 5m0s for pod "execpodvltpz" in namespace "services-4550" to be "running"
Jan  4 22:16:23.043: INFO: Pod "execpodvltpz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.627389ms
Jan  4 22:16:25.047: INFO: Pod "execpodvltpz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006558896s
Jan  4 22:16:25.047: INFO: Pod "execpodvltpz" satisfied condition "running"
Jan  4 22:16:26.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4550 exec execpodvltpz -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan  4 22:16:26.212: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  4 22:16:26.212: INFO: stdout: ""
Jan  4 22:16:26.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4550 exec execpodvltpz -- /bin/sh -x -c nc -v -z -w 2 10.43.130.125 80'
Jan  4 22:16:26.392: INFO: stderr: "+ nc -v -z -w 2 10.43.130.125 80\nConnection to 10.43.130.125 80 port [tcp/http] succeeded!\n"
Jan  4 22:16:26.392: INFO: stdout: ""
Jan  4 22:16:26.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4550 exec execpodvltpz -- /bin/sh -x -c nc -v -z -w 2 172.31.3.240 32032'
Jan  4 22:16:26.580: INFO: stderr: "+ nc -v -z -w 2 172.31.3.240 32032\nConnection to 172.31.3.240 32032 port [tcp/*] succeeded!\n"
Jan  4 22:16:26.580: INFO: stdout: ""
Jan  4 22:16:26.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4550 exec execpodvltpz -- /bin/sh -x -c nc -v -z -w 2 172.31.9.62 32032'
Jan  4 22:16:26.744: INFO: stderr: "+ nc -v -z -w 2 172.31.9.62 32032\nConnection to 172.31.9.62 32032 port [tcp/*] succeeded!\n"
Jan  4 22:16:26.744: INFO: stdout: ""
Jan  4 22:16:26.744: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:16:26.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4550" for this suite. 01/04/23 22:16:26.821
------------------------------
• [SLOW TEST] [6.919 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:16:19.914
    Jan  4 22:16:19.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:16:19.915
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:19.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:19.94
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4550 01/04/23 22:16:19.944
    STEP: changing the ExternalName service to type=NodePort 01/04/23 22:16:19.949
    STEP: creating replication controller externalname-service in namespace services-4550 01/04/23 22:16:19.973
    I0104 22:16:19.980296      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4550, replica count: 2
    I0104 22:16:23.031987      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 22:16:23.032: INFO: Creating new exec pod
    Jan  4 22:16:23.040: INFO: Waiting up to 5m0s for pod "execpodvltpz" in namespace "services-4550" to be "running"
    Jan  4 22:16:23.043: INFO: Pod "execpodvltpz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.627389ms
    Jan  4 22:16:25.047: INFO: Pod "execpodvltpz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006558896s
    Jan  4 22:16:25.047: INFO: Pod "execpodvltpz" satisfied condition "running"
    Jan  4 22:16:26.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4550 exec execpodvltpz -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan  4 22:16:26.212: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  4 22:16:26.212: INFO: stdout: ""
    Jan  4 22:16:26.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4550 exec execpodvltpz -- /bin/sh -x -c nc -v -z -w 2 10.43.130.125 80'
    Jan  4 22:16:26.392: INFO: stderr: "+ nc -v -z -w 2 10.43.130.125 80\nConnection to 10.43.130.125 80 port [tcp/http] succeeded!\n"
    Jan  4 22:16:26.392: INFO: stdout: ""
    Jan  4 22:16:26.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4550 exec execpodvltpz -- /bin/sh -x -c nc -v -z -w 2 172.31.3.240 32032'
    Jan  4 22:16:26.580: INFO: stderr: "+ nc -v -z -w 2 172.31.3.240 32032\nConnection to 172.31.3.240 32032 port [tcp/*] succeeded!\n"
    Jan  4 22:16:26.580: INFO: stdout: ""
    Jan  4 22:16:26.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4550 exec execpodvltpz -- /bin/sh -x -c nc -v -z -w 2 172.31.9.62 32032'
    Jan  4 22:16:26.744: INFO: stderr: "+ nc -v -z -w 2 172.31.9.62 32032\nConnection to 172.31.9.62 32032 port [tcp/*] succeeded!\n"
    Jan  4 22:16:26.744: INFO: stdout: ""
    Jan  4 22:16:26.744: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:16:26.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4550" for this suite. 01/04/23 22:16:26.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:16:26.836
Jan  4 22:16:26.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename gc 01/04/23 22:16:26.841
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:26.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:26.873
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/04/23 22:16:26.899
STEP: create the rc2 01/04/23 22:16:26.905
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/04/23 22:16:31.924
STEP: delete the rc simpletest-rc-to-be-deleted 01/04/23 22:16:33.017
STEP: wait for the rc to be deleted 01/04/23 22:16:33.028
Jan  4 22:16:38.045: INFO: 67 pods remaining
Jan  4 22:16:38.045: INFO: 67 pods has nil DeletionTimestamp
Jan  4 22:16:38.045: INFO: 
STEP: Gathering metrics 01/04/23 22:16:43.037
Jan  4 22:16:43.365: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
Jan  4 22:16:43.368: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.840584ms
Jan  4 22:16:43.368: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
Jan  4 22:16:43.368: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
Jan  4 22:16:43.787: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  4 22:16:43.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-255k4" in namespace "gc-6679"
Jan  4 22:16:43.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-2phk7" in namespace "gc-6679"
Jan  4 22:16:43.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s72g" in namespace "gc-6679"
Jan  4 22:16:43.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-2thqx" in namespace "gc-6679"
Jan  4 22:16:43.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-45b4c" in namespace "gc-6679"
Jan  4 22:16:43.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-48nft" in namespace "gc-6679"
Jan  4 22:16:44.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dxp5" in namespace "gc-6679"
Jan  4 22:16:44.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pd75" in namespace "gc-6679"
Jan  4 22:16:44.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-59xwc" in namespace "gc-6679"
Jan  4 22:16:44.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k6fj" in namespace "gc-6679"
Jan  4 22:16:44.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-64t8j" in namespace "gc-6679"
Jan  4 22:16:44.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-679hv" in namespace "gc-6679"
Jan  4 22:16:44.238: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zsrc" in namespace "gc-6679"
Jan  4 22:16:44.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-7cnkm" in namespace "gc-6679"
Jan  4 22:16:44.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ff66" in namespace "gc-6679"
Jan  4 22:16:44.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hcnt" in namespace "gc-6679"
Jan  4 22:16:44.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hfvc" in namespace "gc-6679"
Jan  4 22:16:44.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jmmk" in namespace "gc-6679"
Jan  4 22:16:44.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-7knzk" in namespace "gc-6679"
Jan  4 22:16:44.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v6rs" in namespace "gc-6679"
Jan  4 22:16:44.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-8875h" in namespace "gc-6679"
Jan  4 22:16:44.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hm27" in namespace "gc-6679"
Jan  4 22:16:44.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v4rm" in namespace "gc-6679"
Jan  4 22:16:44.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ztf4" in namespace "gc-6679"
Jan  4 22:16:44.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vjww" in namespace "gc-6679"
Jan  4 22:16:44.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vmhv" in namespace "gc-6679"
Jan  4 22:16:44.756: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wskr" in namespace "gc-6679"
Jan  4 22:16:44.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7g76" in namespace "gc-6679"
Jan  4 22:16:44.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkqlk" in namespace "gc-6679"
Jan  4 22:16:44.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm426" in namespace "gc-6679"
Jan  4 22:16:44.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5z58" in namespace "gc-6679"
Jan  4 22:16:44.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6rbx" in namespace "gc-6679"
Jan  4 22:16:44.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-c997g" in namespace "gc-6679"
Jan  4 22:16:44.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-chnj5" in namespace "gc-6679"
Jan  4 22:16:44.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-cp8q9" in namespace "gc-6679"
Jan  4 22:16:44.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpdpf" in namespace "gc-6679"
Jan  4 22:16:45.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-csf9v" in namespace "gc-6679"
Jan  4 22:16:45.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct2h8" in namespace "gc-6679"
Jan  4 22:16:45.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvmj9" in namespace "gc-6679"
Jan  4 22:16:45.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxdq9" in namespace "gc-6679"
Jan  4 22:16:45.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfrpb" in namespace "gc-6679"
Jan  4 22:16:45.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnkxv" in namespace "gc-6679"
Jan  4 22:16:45.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv66h" in namespace "gc-6679"
Jan  4 22:16:45.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvzxk" in namespace "gc-6679"
Jan  4 22:16:45.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-fk69x" in namespace "gc-6679"
Jan  4 22:16:45.256: INFO: Deleting pod "simpletest-rc-to-be-deleted-fssch" in namespace "gc-6679"
Jan  4 22:16:45.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxkq2" in namespace "gc-6679"
Jan  4 22:16:45.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzw6j" in namespace "gc-6679"
Jan  4 22:16:45.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-g87b4" in namespace "gc-6679"
Jan  4 22:16:45.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghbb6" in namespace "gc-6679"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  4 22:16:45.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6679" for this suite. 01/04/23 22:16:45.461
------------------------------
• [SLOW TEST] [18.643 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:16:26.836
    Jan  4 22:16:26.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename gc 01/04/23 22:16:26.841
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:26.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:26.873
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/04/23 22:16:26.899
    STEP: create the rc2 01/04/23 22:16:26.905
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/04/23 22:16:31.924
    STEP: delete the rc simpletest-rc-to-be-deleted 01/04/23 22:16:33.017
    STEP: wait for the rc to be deleted 01/04/23 22:16:33.028
    Jan  4 22:16:38.045: INFO: 67 pods remaining
    Jan  4 22:16:38.045: INFO: 67 pods has nil DeletionTimestamp
    Jan  4 22:16:38.045: INFO: 
    STEP: Gathering metrics 01/04/23 22:16:43.037
    Jan  4 22:16:43.365: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Jan  4 22:16:43.368: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.840584ms
    Jan  4 22:16:43.368: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
    Jan  4 22:16:43.368: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
    Jan  4 22:16:43.787: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  4 22:16:43.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-255k4" in namespace "gc-6679"
    Jan  4 22:16:43.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-2phk7" in namespace "gc-6679"
    Jan  4 22:16:43.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s72g" in namespace "gc-6679"
    Jan  4 22:16:43.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-2thqx" in namespace "gc-6679"
    Jan  4 22:16:43.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-45b4c" in namespace "gc-6679"
    Jan  4 22:16:43.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-48nft" in namespace "gc-6679"
    Jan  4 22:16:44.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dxp5" in namespace "gc-6679"
    Jan  4 22:16:44.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pd75" in namespace "gc-6679"
    Jan  4 22:16:44.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-59xwc" in namespace "gc-6679"
    Jan  4 22:16:44.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k6fj" in namespace "gc-6679"
    Jan  4 22:16:44.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-64t8j" in namespace "gc-6679"
    Jan  4 22:16:44.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-679hv" in namespace "gc-6679"
    Jan  4 22:16:44.238: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zsrc" in namespace "gc-6679"
    Jan  4 22:16:44.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-7cnkm" in namespace "gc-6679"
    Jan  4 22:16:44.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ff66" in namespace "gc-6679"
    Jan  4 22:16:44.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hcnt" in namespace "gc-6679"
    Jan  4 22:16:44.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hfvc" in namespace "gc-6679"
    Jan  4 22:16:44.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jmmk" in namespace "gc-6679"
    Jan  4 22:16:44.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-7knzk" in namespace "gc-6679"
    Jan  4 22:16:44.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v6rs" in namespace "gc-6679"
    Jan  4 22:16:44.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-8875h" in namespace "gc-6679"
    Jan  4 22:16:44.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hm27" in namespace "gc-6679"
    Jan  4 22:16:44.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v4rm" in namespace "gc-6679"
    Jan  4 22:16:44.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ztf4" in namespace "gc-6679"
    Jan  4 22:16:44.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vjww" in namespace "gc-6679"
    Jan  4 22:16:44.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vmhv" in namespace "gc-6679"
    Jan  4 22:16:44.756: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wskr" in namespace "gc-6679"
    Jan  4 22:16:44.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7g76" in namespace "gc-6679"
    Jan  4 22:16:44.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkqlk" in namespace "gc-6679"
    Jan  4 22:16:44.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm426" in namespace "gc-6679"
    Jan  4 22:16:44.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5z58" in namespace "gc-6679"
    Jan  4 22:16:44.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6rbx" in namespace "gc-6679"
    Jan  4 22:16:44.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-c997g" in namespace "gc-6679"
    Jan  4 22:16:44.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-chnj5" in namespace "gc-6679"
    Jan  4 22:16:44.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-cp8q9" in namespace "gc-6679"
    Jan  4 22:16:44.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpdpf" in namespace "gc-6679"
    Jan  4 22:16:45.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-csf9v" in namespace "gc-6679"
    Jan  4 22:16:45.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct2h8" in namespace "gc-6679"
    Jan  4 22:16:45.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvmj9" in namespace "gc-6679"
    Jan  4 22:16:45.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxdq9" in namespace "gc-6679"
    Jan  4 22:16:45.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfrpb" in namespace "gc-6679"
    Jan  4 22:16:45.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnkxv" in namespace "gc-6679"
    Jan  4 22:16:45.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv66h" in namespace "gc-6679"
    Jan  4 22:16:45.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvzxk" in namespace "gc-6679"
    Jan  4 22:16:45.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-fk69x" in namespace "gc-6679"
    Jan  4 22:16:45.256: INFO: Deleting pod "simpletest-rc-to-be-deleted-fssch" in namespace "gc-6679"
    Jan  4 22:16:45.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxkq2" in namespace "gc-6679"
    Jan  4 22:16:45.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzw6j" in namespace "gc-6679"
    Jan  4 22:16:45.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-g87b4" in namespace "gc-6679"
    Jan  4 22:16:45.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghbb6" in namespace "gc-6679"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:16:45.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6679" for this suite. 01/04/23 22:16:45.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:16:45.482
Jan  4 22:16:45.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 22:16:45.483
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:45.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:45.508
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/04/23 22:16:45.512
Jan  4 22:16:45.521: INFO: Waiting up to 5m0s for pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9" in namespace "emptydir-5742" to be "Succeeded or Failed"
Jan  4 22:16:45.527: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.257221ms
Jan  4 22:16:47.532: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010950842s
Jan  4 22:16:49.531: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010487439s
Jan  4 22:16:51.533: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011848799s
STEP: Saw pod success 01/04/23 22:16:51.533
Jan  4 22:16:51.533: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9" satisfied condition "Succeeded or Failed"
Jan  4 22:16:51.536: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-ca7137bb-0ef9-4f33-825c-d27e236989e9 container test-container: <nil>
STEP: delete the pod 01/04/23 22:16:51.545
Jan  4 22:16:51.559: INFO: Waiting for pod pod-ca7137bb-0ef9-4f33-825c-d27e236989e9 to disappear
Jan  4 22:16:51.562: INFO: Pod pod-ca7137bb-0ef9-4f33-825c-d27e236989e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 22:16:51.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5742" for this suite. 01/04/23 22:16:51.566
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:16:45.482
    Jan  4 22:16:45.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 22:16:45.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:45.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:45.508
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/04/23 22:16:45.512
    Jan  4 22:16:45.521: INFO: Waiting up to 5m0s for pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9" in namespace "emptydir-5742" to be "Succeeded or Failed"
    Jan  4 22:16:45.527: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.257221ms
    Jan  4 22:16:47.532: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010950842s
    Jan  4 22:16:49.531: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010487439s
    Jan  4 22:16:51.533: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011848799s
    STEP: Saw pod success 01/04/23 22:16:51.533
    Jan  4 22:16:51.533: INFO: Pod "pod-ca7137bb-0ef9-4f33-825c-d27e236989e9" satisfied condition "Succeeded or Failed"
    Jan  4 22:16:51.536: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-ca7137bb-0ef9-4f33-825c-d27e236989e9 container test-container: <nil>
    STEP: delete the pod 01/04/23 22:16:51.545
    Jan  4 22:16:51.559: INFO: Waiting for pod pod-ca7137bb-0ef9-4f33-825c-d27e236989e9 to disappear
    Jan  4 22:16:51.562: INFO: Pod pod-ca7137bb-0ef9-4f33-825c-d27e236989e9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:16:51.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5742" for this suite. 01/04/23 22:16:51.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:16:51.573
Jan  4 22:16:51.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename svcaccounts 01/04/23 22:16:51.574
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:51.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:51.594
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan  4 22:16:51.625: INFO: created pod pod-service-account-defaultsa
Jan  4 22:16:51.625: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan  4 22:16:51.633: INFO: created pod pod-service-account-mountsa
Jan  4 22:16:51.633: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan  4 22:16:51.641: INFO: created pod pod-service-account-nomountsa
Jan  4 22:16:51.641: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan  4 22:16:51.651: INFO: created pod pod-service-account-defaultsa-mountspec
Jan  4 22:16:51.651: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan  4 22:16:51.663: INFO: created pod pod-service-account-mountsa-mountspec
Jan  4 22:16:51.669: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan  4 22:16:51.679: INFO: created pod pod-service-account-nomountsa-mountspec
Jan  4 22:16:51.680: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan  4 22:16:51.709: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan  4 22:16:51.709: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan  4 22:16:51.726: INFO: created pod pod-service-account-mountsa-nomountspec
Jan  4 22:16:51.726: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan  4 22:16:51.760: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan  4 22:16:51.760: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  4 22:16:51.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7135" for this suite. 01/04/23 22:16:51.773
------------------------------
• [0.219 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:16:51.573
    Jan  4 22:16:51.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename svcaccounts 01/04/23 22:16:51.574
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:51.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:51.594
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan  4 22:16:51.625: INFO: created pod pod-service-account-defaultsa
    Jan  4 22:16:51.625: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan  4 22:16:51.633: INFO: created pod pod-service-account-mountsa
    Jan  4 22:16:51.633: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan  4 22:16:51.641: INFO: created pod pod-service-account-nomountsa
    Jan  4 22:16:51.641: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan  4 22:16:51.651: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan  4 22:16:51.651: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan  4 22:16:51.663: INFO: created pod pod-service-account-mountsa-mountspec
    Jan  4 22:16:51.669: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan  4 22:16:51.679: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan  4 22:16:51.680: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan  4 22:16:51.709: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan  4 22:16:51.709: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan  4 22:16:51.726: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan  4 22:16:51.726: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan  4 22:16:51.760: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan  4 22:16:51.760: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:16:51.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7135" for this suite. 01/04/23 22:16:51.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:16:51.796
Jan  4 22:16:51.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:16:51.802
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:51.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:51.83
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-9755 01/04/23 22:16:51.836
STEP: creating service affinity-nodeport-transition in namespace services-9755 01/04/23 22:16:51.836
STEP: creating replication controller affinity-nodeport-transition in namespace services-9755 01/04/23 22:16:51.871
I0104 22:16:51.896060      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9755, replica count: 3
I0104 22:16:54.956294      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0104 22:16:57.957980      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0104 22:17:00.958564      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0104 22:17:03.958919      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 22:17:03.972: INFO: Creating new exec pod
Jan  4 22:17:03.998: INFO: Waiting up to 5m0s for pod "execpod-affinity9r8jb" in namespace "services-9755" to be "running"
Jan  4 22:17:04.016: INFO: Pod "execpod-affinity9r8jb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.314648ms
Jan  4 22:17:06.020: INFO: Pod "execpod-affinity9r8jb": Phase="Running", Reason="", readiness=true. Elapsed: 2.022216709s
Jan  4 22:17:06.020: INFO: Pod "execpod-affinity9r8jb" satisfied condition "running"
Jan  4 22:17:07.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan  4 22:17:07.203: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan  4 22:17:07.203: INFO: stdout: ""
Jan  4 22:17:07.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c nc -v -z -w 2 10.43.198.76 80'
Jan  4 22:17:07.384: INFO: stderr: "+ nc -v -z -w 2 10.43.198.76 80\nConnection to 10.43.198.76 80 port [tcp/http] succeeded!\n"
Jan  4 22:17:07.384: INFO: stdout: ""
Jan  4 22:17:07.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c nc -v -z -w 2 172.31.9.62 32601'
Jan  4 22:17:07.536: INFO: stderr: "+ nc -v -z -w 2 172.31.9.62 32601\nConnection to 172.31.9.62 32601 port [tcp/*] succeeded!\n"
Jan  4 22:17:07.536: INFO: stdout: ""
Jan  4 22:17:07.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c nc -v -z -w 2 172.31.13.117 32601'
Jan  4 22:17:07.693: INFO: stderr: "+ nc -v -z -w 2 172.31.13.117 32601\nConnection to 172.31.13.117 32601 port [tcp/*] succeeded!\n"
Jan  4 22:17:07.693: INFO: stdout: ""
Jan  4 22:17:07.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.54:32601/ ; done'
Jan  4 22:17:07.940: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n"
Jan  4 22:17:07.940: INFO: stdout: "\naffinity-nodeport-transition-gl597\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-gl597\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-gl597\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-gl597"
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-gl597
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-gl597
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-gl597
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-gl597
Jan  4 22:17:07.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.54:32601/ ; done'
Jan  4 22:17:08.214: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n"
Jan  4 22:17:08.214: INFO: stdout: "\naffinity-nodeport-transition-gl597\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm"
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-gl597
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
Jan  4 22:17:08.214: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9755, will wait for the garbage collector to delete the pods 01/04/23 22:17:08.224
Jan  4 22:17:08.284: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.822603ms
Jan  4 22:17:08.385: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.632215ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:17:13.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9755" for this suite. 01/04/23 22:17:13.128
------------------------------
• [SLOW TEST] [21.339 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:16:51.796
    Jan  4 22:16:51.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:16:51.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:16:51.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:16:51.83
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-9755 01/04/23 22:16:51.836
    STEP: creating service affinity-nodeport-transition in namespace services-9755 01/04/23 22:16:51.836
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9755 01/04/23 22:16:51.871
    I0104 22:16:51.896060      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9755, replica count: 3
    I0104 22:16:54.956294      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0104 22:16:57.957980      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0104 22:17:00.958564      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0104 22:17:03.958919      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 22:17:03.972: INFO: Creating new exec pod
    Jan  4 22:17:03.998: INFO: Waiting up to 5m0s for pod "execpod-affinity9r8jb" in namespace "services-9755" to be "running"
    Jan  4 22:17:04.016: INFO: Pod "execpod-affinity9r8jb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.314648ms
    Jan  4 22:17:06.020: INFO: Pod "execpod-affinity9r8jb": Phase="Running", Reason="", readiness=true. Elapsed: 2.022216709s
    Jan  4 22:17:06.020: INFO: Pod "execpod-affinity9r8jb" satisfied condition "running"
    Jan  4 22:17:07.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan  4 22:17:07.203: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan  4 22:17:07.203: INFO: stdout: ""
    Jan  4 22:17:07.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c nc -v -z -w 2 10.43.198.76 80'
    Jan  4 22:17:07.384: INFO: stderr: "+ nc -v -z -w 2 10.43.198.76 80\nConnection to 10.43.198.76 80 port [tcp/http] succeeded!\n"
    Jan  4 22:17:07.384: INFO: stdout: ""
    Jan  4 22:17:07.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c nc -v -z -w 2 172.31.9.62 32601'
    Jan  4 22:17:07.536: INFO: stderr: "+ nc -v -z -w 2 172.31.9.62 32601\nConnection to 172.31.9.62 32601 port [tcp/*] succeeded!\n"
    Jan  4 22:17:07.536: INFO: stdout: ""
    Jan  4 22:17:07.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c nc -v -z -w 2 172.31.13.117 32601'
    Jan  4 22:17:07.693: INFO: stderr: "+ nc -v -z -w 2 172.31.13.117 32601\nConnection to 172.31.13.117 32601 port [tcp/*] succeeded!\n"
    Jan  4 22:17:07.693: INFO: stdout: ""
    Jan  4 22:17:07.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.54:32601/ ; done'
    Jan  4 22:17:07.940: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n"
    Jan  4 22:17:07.940: INFO: stdout: "\naffinity-nodeport-transition-gl597\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-gl597\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-gl597\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-56ndr\naffinity-nodeport-transition-gl597"
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-gl597
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-gl597
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-gl597
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-56ndr
    Jan  4 22:17:07.940: INFO: Received response from host: affinity-nodeport-transition-gl597
    Jan  4 22:17:07.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9755 exec execpod-affinity9r8jb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.54:32601/ ; done'
    Jan  4 22:17:08.214: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:32601/\n"
    Jan  4 22:17:08.214: INFO: stdout: "\naffinity-nodeport-transition-gl597\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm\naffinity-nodeport-transition-44ltm"
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-gl597
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Received response from host: affinity-nodeport-transition-44ltm
    Jan  4 22:17:08.214: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9755, will wait for the garbage collector to delete the pods 01/04/23 22:17:08.224
    Jan  4 22:17:08.284: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.822603ms
    Jan  4 22:17:08.385: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.632215ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:17:13.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9755" for this suite. 01/04/23 22:17:13.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:17:13.138
Jan  4 22:17:13.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename runtimeclass 01/04/23 22:17:13.139
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:17:13.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:17:13.157
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3774-delete-me 01/04/23 22:17:13.163
STEP: Waiting for the RuntimeClass to disappear 01/04/23 22:17:13.168
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  4 22:17:13.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3774" for this suite. 01/04/23 22:17:13.182
------------------------------
• [0.049 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:17:13.138
    Jan  4 22:17:13.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename runtimeclass 01/04/23 22:17:13.139
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:17:13.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:17:13.157
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3774-delete-me 01/04/23 22:17:13.163
    STEP: Waiting for the RuntimeClass to disappear 01/04/23 22:17:13.168
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:17:13.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3774" for this suite. 01/04/23 22:17:13.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:17:13.201
Jan  4 22:17:13.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:17:13.205
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:17:13.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:17:13.225
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/04/23 22:17:13.227
Jan  4 22:17:13.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6289 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan  4 22:17:13.463: INFO: stderr: ""
Jan  4 22:17:13.463: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/04/23 22:17:13.463
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan  4 22:17:13.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6289 delete pods e2e-test-httpd-pod'
Jan  4 22:17:16.175: INFO: stderr: ""
Jan  4 22:17:16.175: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:17:16.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6289" for this suite. 01/04/23 22:17:16.179
------------------------------
• [2.984 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:17:13.201
    Jan  4 22:17:13.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:17:13.205
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:17:13.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:17:13.225
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/04/23 22:17:13.227
    Jan  4 22:17:13.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6289 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan  4 22:17:13.463: INFO: stderr: ""
    Jan  4 22:17:13.463: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/04/23 22:17:13.463
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan  4 22:17:13.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6289 delete pods e2e-test-httpd-pod'
    Jan  4 22:17:16.175: INFO: stderr: ""
    Jan  4 22:17:16.175: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:17:16.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6289" for this suite. 01/04/23 22:17:16.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:17:16.186
Jan  4 22:17:16.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename proxy 01/04/23 22:17:16.187
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:17:16.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:17:16.22
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/04/23 22:17:16.237
STEP: creating replication controller proxy-service-44dcr in namespace proxy-968 01/04/23 22:17:16.237
I0104 22:17:16.248926      18 runners.go:193] Created replication controller with name: proxy-service-44dcr, namespace: proxy-968, replica count: 1
I0104 22:17:17.300533      18 runners.go:193] proxy-service-44dcr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0104 22:17:18.300693      18 runners.go:193] proxy-service-44dcr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 22:17:18.305: INFO: setup took 2.080465638s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/04/23 22:17:18.305
Jan  4 22:17:18.351: INFO: (0) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 45.211567ms)
Jan  4 22:17:18.351: INFO: (0) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 44.884101ms)
Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 63.40679ms)
Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 63.565457ms)
Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 63.022289ms)
Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 63.305007ms)
Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 63.270697ms)
Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 63.65632ms)
Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 63.247919ms)
Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 63.724192ms)
Jan  4 22:17:18.374: INFO: (0) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 67.828093ms)
Jan  4 22:17:18.374: INFO: (0) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 68.046813ms)
Jan  4 22:17:18.374: INFO: (0) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 68.015712ms)
Jan  4 22:17:18.378: INFO: (0) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 72.642425ms)
Jan  4 22:17:18.378: INFO: (0) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 72.605279ms)
Jan  4 22:17:18.384: INFO: (0) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 78.004797ms)
Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 28.102092ms)
Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 27.950028ms)
Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 27.247564ms)
Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 27.218718ms)
Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 27.111953ms)
Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 26.897964ms)
Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 30.34598ms)
Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 30.361949ms)
Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 30.31662ms)
Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 29.783045ms)
Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 31.211988ms)
Jan  4 22:17:18.431: INFO: (1) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 45.802796ms)
Jan  4 22:17:18.431: INFO: (1) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 46.000765ms)
Jan  4 22:17:18.432: INFO: (1) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 47.24521ms)
Jan  4 22:17:18.432: INFO: (1) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 46.601087ms)
Jan  4 22:17:18.433: INFO: (1) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 47.629949ms)
Jan  4 22:17:18.467: INFO: (2) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 33.936051ms)
Jan  4 22:17:18.470: INFO: (2) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 36.487775ms)
Jan  4 22:17:18.470: INFO: (2) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 36.602395ms)
Jan  4 22:17:18.470: INFO: (2) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 36.915774ms)
Jan  4 22:17:18.470: INFO: (2) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 37.1903ms)
Jan  4 22:17:18.472: INFO: (2) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 38.728299ms)
Jan  4 22:17:18.472: INFO: (2) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 38.907476ms)
Jan  4 22:17:18.473: INFO: (2) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 39.313378ms)
Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 40.907972ms)
Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 41.631563ms)
Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 41.47776ms)
Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 41.979254ms)
Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 41.590325ms)
Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 41.71533ms)
Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 41.819318ms)
Jan  4 22:17:18.476: INFO: (2) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 42.17539ms)
Jan  4 22:17:18.492: INFO: (3) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 15.955958ms)
Jan  4 22:17:18.492: INFO: (3) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 16.260427ms)
Jan  4 22:17:18.495: INFO: (3) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 18.636376ms)
Jan  4 22:17:18.495: INFO: (3) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 18.874768ms)
Jan  4 22:17:18.495: INFO: (3) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 19.018041ms)
Jan  4 22:17:18.502: INFO: (3) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 25.865922ms)
Jan  4 22:17:18.502: INFO: (3) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 25.97581ms)
Jan  4 22:17:18.502: INFO: (3) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 26.152581ms)
Jan  4 22:17:18.502: INFO: (3) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 26.366485ms)
Jan  4 22:17:18.503: INFO: (3) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 26.416157ms)
Jan  4 22:17:18.506: INFO: (3) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 30.260073ms)
Jan  4 22:17:18.506: INFO: (3) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 30.205123ms)
Jan  4 22:17:18.506: INFO: (3) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 30.141222ms)
Jan  4 22:17:18.506: INFO: (3) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 30.422207ms)
Jan  4 22:17:18.509: INFO: (3) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 32.626311ms)
Jan  4 22:17:18.509: INFO: (3) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 32.836615ms)
Jan  4 22:17:18.520: INFO: (4) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 10.467955ms)
Jan  4 22:17:18.525: INFO: (4) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 15.595352ms)
Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 16.008544ms)
Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 16.383988ms)
Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 16.601605ms)
Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 16.357011ms)
Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 16.629376ms)
Jan  4 22:17:18.527: INFO: (4) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 17.426535ms)
Jan  4 22:17:18.528: INFO: (4) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 17.941293ms)
Jan  4 22:17:18.528: INFO: (4) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.410863ms)
Jan  4 22:17:18.530: INFO: (4) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 21.099413ms)
Jan  4 22:17:18.535: INFO: (4) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 25.412447ms)
Jan  4 22:17:18.535: INFO: (4) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 25.806195ms)
Jan  4 22:17:18.535: INFO: (4) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 25.833721ms)
Jan  4 22:17:18.538: INFO: (4) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 28.280816ms)
Jan  4 22:17:18.538: INFO: (4) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 28.467498ms)
Jan  4 22:17:18.553: INFO: (5) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 14.58443ms)
Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 15.174082ms)
Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 15.258063ms)
Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 14.988404ms)
Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 15.428934ms)
Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 15.279608ms)
Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 15.298692ms)
Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 15.488354ms)
Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 15.632106ms)
Jan  4 22:17:18.559: INFO: (5) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 20.334681ms)
Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 26.902967ms)
Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 26.847664ms)
Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 27.066333ms)
Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 27.166958ms)
Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 27.672889ms)
Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 27.590056ms)
Jan  4 22:17:18.589: INFO: (6) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 22.738427ms)
Jan  4 22:17:18.589: INFO: (6) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 22.682703ms)
Jan  4 22:17:18.589: INFO: (6) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 22.788086ms)
Jan  4 22:17:18.590: INFO: (6) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 23.230561ms)
Jan  4 22:17:18.590: INFO: (6) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 22.970719ms)
Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 32.16655ms)
Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 32.532786ms)
Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 32.468841ms)
Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 32.866533ms)
Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 33.111561ms)
Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 34.35205ms)
Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 34.269671ms)
Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 34.47554ms)
Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 34.689229ms)
Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 34.808443ms)
Jan  4 22:17:18.603: INFO: (6) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 35.938987ms)
Jan  4 22:17:18.611: INFO: (7) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 7.579095ms)
Jan  4 22:17:18.617: INFO: (7) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 12.69162ms)
Jan  4 22:17:18.622: INFO: (7) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 16.900888ms)
Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 18.081367ms)
Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 18.020758ms)
Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 18.224699ms)
Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 18.075051ms)
Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 18.797772ms)
Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 19.058125ms)
Jan  4 22:17:18.625: INFO: (7) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 19.941962ms)
Jan  4 22:17:18.625: INFO: (7) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 20.195467ms)
Jan  4 22:17:18.625: INFO: (7) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 20.513693ms)
Jan  4 22:17:18.625: INFO: (7) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 20.780369ms)
Jan  4 22:17:18.626: INFO: (7) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 20.760415ms)
Jan  4 22:17:18.626: INFO: (7) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 21.441212ms)
Jan  4 22:17:18.626: INFO: (7) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 21.129771ms)
Jan  4 22:17:18.635: INFO: (8) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 9.141153ms)
Jan  4 22:17:18.644: INFO: (8) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.826439ms)
Jan  4 22:17:18.644: INFO: (8) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 17.7762ms)
Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 21.987542ms)
Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 22.489421ms)
Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 22.851247ms)
Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 22.395ms)
Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 22.634869ms)
Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 22.723584ms)
Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 22.763667ms)
Jan  4 22:17:18.650: INFO: (8) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 23.776591ms)
Jan  4 22:17:18.651: INFO: (8) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 24.964014ms)
Jan  4 22:17:18.653: INFO: (8) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 26.343841ms)
Jan  4 22:17:18.654: INFO: (8) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 27.166687ms)
Jan  4 22:17:18.654: INFO: (8) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 27.311713ms)
Jan  4 22:17:18.654: INFO: (8) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 27.678374ms)
Jan  4 22:17:18.662: INFO: (9) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 7.77998ms)
Jan  4 22:17:18.667: INFO: (9) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 12.727328ms)
Jan  4 22:17:18.667: INFO: (9) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 12.789313ms)
Jan  4 22:17:18.668: INFO: (9) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 12.895102ms)
Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 17.777992ms)
Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 18.554842ms)
Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 18.297541ms)
Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 18.230607ms)
Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.324471ms)
Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 18.376251ms)
Jan  4 22:17:18.674: INFO: (9) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 19.156236ms)
Jan  4 22:17:18.678: INFO: (9) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 22.950405ms)
Jan  4 22:17:18.679: INFO: (9) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 23.681163ms)
Jan  4 22:17:18.679: INFO: (9) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 24.008923ms)
Jan  4 22:17:18.679: INFO: (9) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 24.244158ms)
Jan  4 22:17:18.683: INFO: (9) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 27.910212ms)
Jan  4 22:17:18.691: INFO: (10) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 7.495223ms)
Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 17.054693ms)
Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 17.8858ms)
Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 17.819908ms)
Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 17.774123ms)
Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 17.6426ms)
Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.871165ms)
Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 17.849599ms)
Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.279154ms)
Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 19.379036ms)
Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 19.234937ms)
Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 19.368849ms)
Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 19.243591ms)
Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 19.626748ms)
Jan  4 22:17:18.709: INFO: (10) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 25.73291ms)
Jan  4 22:17:18.710: INFO: (10) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 26.185044ms)
Jan  4 22:17:18.731: INFO: (11) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 21.324064ms)
Jan  4 22:17:18.732: INFO: (11) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 21.63741ms)
Jan  4 22:17:18.732: INFO: (11) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 21.501173ms)
Jan  4 22:17:18.732: INFO: (11) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 21.938395ms)
Jan  4 22:17:18.733: INFO: (11) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 23.659813ms)
Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 24.895603ms)
Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 25.237491ms)
Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 25.589546ms)
Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 25.146394ms)
Jan  4 22:17:18.736: INFO: (11) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 25.527831ms)
Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 25.355529ms)
Jan  4 22:17:18.736: INFO: (11) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 26.567959ms)
Jan  4 22:17:18.738: INFO: (11) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 27.267865ms)
Jan  4 22:17:18.738: INFO: (11) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 28.117304ms)
Jan  4 22:17:18.738: INFO: (11) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 27.82614ms)
Jan  4 22:17:18.738: INFO: (11) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 28.109996ms)
Jan  4 22:17:18.755: INFO: (12) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 15.815163ms)
Jan  4 22:17:18.755: INFO: (12) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 15.997925ms)
Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 17.479984ms)
Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.644058ms)
Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 17.383326ms)
Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 18.09158ms)
Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 17.817227ms)
Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 18.225822ms)
Jan  4 22:17:18.757: INFO: (12) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.103876ms)
Jan  4 22:17:18.757: INFO: (12) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 18.570498ms)
Jan  4 22:17:18.757: INFO: (12) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 18.491192ms)
Jan  4 22:17:18.757: INFO: (12) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 18.943725ms)
Jan  4 22:17:18.764: INFO: (12) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 25.019438ms)
Jan  4 22:17:18.765: INFO: (12) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 27.117327ms)
Jan  4 22:17:18.766: INFO: (12) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 27.367091ms)
Jan  4 22:17:18.766: INFO: (12) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 27.186384ms)
Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 18.117069ms)
Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 17.855772ms)
Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 17.965619ms)
Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 18.199717ms)
Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.455032ms)
Jan  4 22:17:18.785: INFO: (13) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.575511ms)
Jan  4 22:17:18.785: INFO: (13) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 18.929068ms)
Jan  4 22:17:18.787: INFO: (13) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 20.535172ms)
Jan  4 22:17:18.787: INFO: (13) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 20.483519ms)
Jan  4 22:17:18.789: INFO: (13) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 23.017255ms)
Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 26.561081ms)
Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 26.222462ms)
Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 26.464712ms)
Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 26.568579ms)
Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 26.207166ms)
Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 26.148443ms)
Jan  4 22:17:18.803: INFO: (14) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 10.421076ms)
Jan  4 22:17:18.809: INFO: (14) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 15.431321ms)
Jan  4 22:17:18.809: INFO: (14) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 15.240616ms)
Jan  4 22:17:18.809: INFO: (14) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 16.401834ms)
Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 16.5693ms)
Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 16.815183ms)
Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 16.563375ms)
Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 16.658536ms)
Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 16.92641ms)
Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 16.820816ms)
Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.156471ms)
Jan  4 22:17:18.812: INFO: (14) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 18.828016ms)
Jan  4 22:17:18.814: INFO: (14) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 20.656774ms)
Jan  4 22:17:18.815: INFO: (14) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 21.330414ms)
Jan  4 22:17:18.815: INFO: (14) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 21.718172ms)
Jan  4 22:17:18.815: INFO: (14) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 21.656682ms)
Jan  4 22:17:18.828: INFO: (15) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 12.491351ms)
Jan  4 22:17:18.828: INFO: (15) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 12.972134ms)
Jan  4 22:17:18.829: INFO: (15) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 13.366893ms)
Jan  4 22:17:18.829: INFO: (15) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 13.131668ms)
Jan  4 22:17:18.829: INFO: (15) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 13.634214ms)
Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 13.714593ms)
Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 14.419411ms)
Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 14.311073ms)
Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 14.280044ms)
Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 14.510687ms)
Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 14.024114ms)
Jan  4 22:17:18.831: INFO: (15) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 15.052812ms)
Jan  4 22:17:18.833: INFO: (15) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 16.83162ms)
Jan  4 22:17:18.834: INFO: (15) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 17.895357ms)
Jan  4 22:17:18.834: INFO: (15) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 17.707857ms)
Jan  4 22:17:18.834: INFO: (15) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 18.009207ms)
Jan  4 22:17:18.845: INFO: (16) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 10.888929ms)
Jan  4 22:17:18.845: INFO: (16) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 11.250872ms)
Jan  4 22:17:18.846: INFO: (16) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 11.714638ms)
Jan  4 22:17:18.846: INFO: (16) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 11.153634ms)
Jan  4 22:17:18.847: INFO: (16) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 12.788986ms)
Jan  4 22:17:18.847: INFO: (16) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 12.956749ms)
Jan  4 22:17:18.849: INFO: (16) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 14.113879ms)
Jan  4 22:17:18.849: INFO: (16) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 14.562144ms)
Jan  4 22:17:18.851: INFO: (16) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 17.393025ms)
Jan  4 22:17:18.852: INFO: (16) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 17.498221ms)
Jan  4 22:17:18.852: INFO: (16) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 17.311292ms)
Jan  4 22:17:18.852: INFO: (16) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 18.128283ms)
Jan  4 22:17:18.852: INFO: (16) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 18.042148ms)
Jan  4 22:17:18.855: INFO: (16) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 20.763909ms)
Jan  4 22:17:18.856: INFO: (16) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 21.700848ms)
Jan  4 22:17:18.856: INFO: (16) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 21.757009ms)
Jan  4 22:17:18.873: INFO: (17) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 17.001036ms)
Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 17.333805ms)
Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.490759ms)
Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 17.608031ms)
Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 17.645777ms)
Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 17.820952ms)
Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 17.604755ms)
Jan  4 22:17:18.875: INFO: (17) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 18.398867ms)
Jan  4 22:17:18.875: INFO: (17) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 18.987393ms)
Jan  4 22:17:18.880: INFO: (17) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 23.211225ms)
Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 24.079203ms)
Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 24.75153ms)
Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 24.614836ms)
Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 24.494284ms)
Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 24.648905ms)
Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 25.376349ms)
Jan  4 22:17:18.889: INFO: (18) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 8.084635ms)
Jan  4 22:17:18.890: INFO: (18) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 8.392645ms)
Jan  4 22:17:18.891: INFO: (18) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 9.241279ms)
Jan  4 22:17:18.891: INFO: (18) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 9.294915ms)
Jan  4 22:17:18.893: INFO: (18) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 11.216247ms)
Jan  4 22:17:18.894: INFO: (18) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 11.88604ms)
Jan  4 22:17:18.894: INFO: (18) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 12.038559ms)
Jan  4 22:17:18.896: INFO: (18) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 14.126687ms)
Jan  4 22:17:18.897: INFO: (18) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 15.270391ms)
Jan  4 22:17:18.897: INFO: (18) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 15.371173ms)
Jan  4 22:17:18.898: INFO: (18) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 16.23062ms)
Jan  4 22:17:18.898: INFO: (18) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 16.335221ms)
Jan  4 22:17:18.898: INFO: (18) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 16.26668ms)
Jan  4 22:17:18.899: INFO: (18) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 17.533058ms)
Jan  4 22:17:18.900: INFO: (18) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 17.668171ms)
Jan  4 22:17:18.901: INFO: (18) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 18.599106ms)
Jan  4 22:17:18.908: INFO: (19) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 7.117769ms)
Jan  4 22:17:18.909: INFO: (19) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 7.728566ms)
Jan  4 22:17:18.914: INFO: (19) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 12.433068ms)
Jan  4 22:17:18.914: INFO: (19) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 13.115047ms)
Jan  4 22:17:18.915: INFO: (19) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 13.821226ms)
Jan  4 22:17:18.915: INFO: (19) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 13.834222ms)
Jan  4 22:17:18.915: INFO: (19) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 13.70545ms)
Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 14.869642ms)
Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 14.522331ms)
Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 14.650088ms)
Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 14.724592ms)
Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 14.701719ms)
Jan  4 22:17:18.917: INFO: (19) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 15.425577ms)
Jan  4 22:17:18.919: INFO: (19) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 18.182309ms)
Jan  4 22:17:18.919: INFO: (19) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 18.156576ms)
Jan  4 22:17:18.921: INFO: (19) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 20.036109ms)
STEP: deleting ReplicationController proxy-service-44dcr in namespace proxy-968, will wait for the garbage collector to delete the pods 01/04/23 22:17:18.921
Jan  4 22:17:18.984: INFO: Deleting ReplicationController proxy-service-44dcr took: 8.664438ms
Jan  4 22:17:19.084: INFO: Terminating ReplicationController proxy-service-44dcr pods took: 100.508767ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  4 22:17:21.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-968" for this suite. 01/04/23 22:17:21.205
------------------------------
• [SLOW TEST] [5.047 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:17:16.186
    Jan  4 22:17:16.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename proxy 01/04/23 22:17:16.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:17:16.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:17:16.22
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/04/23 22:17:16.237
    STEP: creating replication controller proxy-service-44dcr in namespace proxy-968 01/04/23 22:17:16.237
    I0104 22:17:16.248926      18 runners.go:193] Created replication controller with name: proxy-service-44dcr, namespace: proxy-968, replica count: 1
    I0104 22:17:17.300533      18 runners.go:193] proxy-service-44dcr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0104 22:17:18.300693      18 runners.go:193] proxy-service-44dcr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 22:17:18.305: INFO: setup took 2.080465638s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/04/23 22:17:18.305
    Jan  4 22:17:18.351: INFO: (0) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 45.211567ms)
    Jan  4 22:17:18.351: INFO: (0) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 44.884101ms)
    Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 63.40679ms)
    Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 63.565457ms)
    Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 63.022289ms)
    Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 63.305007ms)
    Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 63.270697ms)
    Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 63.65632ms)
    Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 63.247919ms)
    Jan  4 22:17:18.369: INFO: (0) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 63.724192ms)
    Jan  4 22:17:18.374: INFO: (0) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 67.828093ms)
    Jan  4 22:17:18.374: INFO: (0) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 68.046813ms)
    Jan  4 22:17:18.374: INFO: (0) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 68.015712ms)
    Jan  4 22:17:18.378: INFO: (0) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 72.642425ms)
    Jan  4 22:17:18.378: INFO: (0) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 72.605279ms)
    Jan  4 22:17:18.384: INFO: (0) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 78.004797ms)
    Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 28.102092ms)
    Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 27.950028ms)
    Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 27.247564ms)
    Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 27.218718ms)
    Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 27.111953ms)
    Jan  4 22:17:18.412: INFO: (1) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 26.897964ms)
    Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 30.34598ms)
    Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 30.361949ms)
    Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 30.31662ms)
    Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 29.783045ms)
    Jan  4 22:17:18.415: INFO: (1) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 31.211988ms)
    Jan  4 22:17:18.431: INFO: (1) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 45.802796ms)
    Jan  4 22:17:18.431: INFO: (1) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 46.000765ms)
    Jan  4 22:17:18.432: INFO: (1) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 47.24521ms)
    Jan  4 22:17:18.432: INFO: (1) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 46.601087ms)
    Jan  4 22:17:18.433: INFO: (1) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 47.629949ms)
    Jan  4 22:17:18.467: INFO: (2) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 33.936051ms)
    Jan  4 22:17:18.470: INFO: (2) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 36.487775ms)
    Jan  4 22:17:18.470: INFO: (2) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 36.602395ms)
    Jan  4 22:17:18.470: INFO: (2) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 36.915774ms)
    Jan  4 22:17:18.470: INFO: (2) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 37.1903ms)
    Jan  4 22:17:18.472: INFO: (2) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 38.728299ms)
    Jan  4 22:17:18.472: INFO: (2) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 38.907476ms)
    Jan  4 22:17:18.473: INFO: (2) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 39.313378ms)
    Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 40.907972ms)
    Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 41.631563ms)
    Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 41.47776ms)
    Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 41.979254ms)
    Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 41.590325ms)
    Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 41.71533ms)
    Jan  4 22:17:18.475: INFO: (2) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 41.819318ms)
    Jan  4 22:17:18.476: INFO: (2) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 42.17539ms)
    Jan  4 22:17:18.492: INFO: (3) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 15.955958ms)
    Jan  4 22:17:18.492: INFO: (3) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 16.260427ms)
    Jan  4 22:17:18.495: INFO: (3) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 18.636376ms)
    Jan  4 22:17:18.495: INFO: (3) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 18.874768ms)
    Jan  4 22:17:18.495: INFO: (3) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 19.018041ms)
    Jan  4 22:17:18.502: INFO: (3) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 25.865922ms)
    Jan  4 22:17:18.502: INFO: (3) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 25.97581ms)
    Jan  4 22:17:18.502: INFO: (3) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 26.152581ms)
    Jan  4 22:17:18.502: INFO: (3) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 26.366485ms)
    Jan  4 22:17:18.503: INFO: (3) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 26.416157ms)
    Jan  4 22:17:18.506: INFO: (3) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 30.260073ms)
    Jan  4 22:17:18.506: INFO: (3) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 30.205123ms)
    Jan  4 22:17:18.506: INFO: (3) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 30.141222ms)
    Jan  4 22:17:18.506: INFO: (3) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 30.422207ms)
    Jan  4 22:17:18.509: INFO: (3) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 32.626311ms)
    Jan  4 22:17:18.509: INFO: (3) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 32.836615ms)
    Jan  4 22:17:18.520: INFO: (4) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 10.467955ms)
    Jan  4 22:17:18.525: INFO: (4) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 15.595352ms)
    Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 16.008544ms)
    Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 16.383988ms)
    Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 16.601605ms)
    Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 16.357011ms)
    Jan  4 22:17:18.526: INFO: (4) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 16.629376ms)
    Jan  4 22:17:18.527: INFO: (4) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 17.426535ms)
    Jan  4 22:17:18.528: INFO: (4) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 17.941293ms)
    Jan  4 22:17:18.528: INFO: (4) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.410863ms)
    Jan  4 22:17:18.530: INFO: (4) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 21.099413ms)
    Jan  4 22:17:18.535: INFO: (4) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 25.412447ms)
    Jan  4 22:17:18.535: INFO: (4) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 25.806195ms)
    Jan  4 22:17:18.535: INFO: (4) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 25.833721ms)
    Jan  4 22:17:18.538: INFO: (4) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 28.280816ms)
    Jan  4 22:17:18.538: INFO: (4) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 28.467498ms)
    Jan  4 22:17:18.553: INFO: (5) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 14.58443ms)
    Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 15.174082ms)
    Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 15.258063ms)
    Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 14.988404ms)
    Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 15.428934ms)
    Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 15.279608ms)
    Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 15.298692ms)
    Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 15.488354ms)
    Jan  4 22:17:18.554: INFO: (5) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 15.632106ms)
    Jan  4 22:17:18.559: INFO: (5) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 20.334681ms)
    Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 26.902967ms)
    Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 26.847664ms)
    Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 27.066333ms)
    Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 27.166958ms)
    Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 27.672889ms)
    Jan  4 22:17:18.566: INFO: (5) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 27.590056ms)
    Jan  4 22:17:18.589: INFO: (6) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 22.738427ms)
    Jan  4 22:17:18.589: INFO: (6) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 22.682703ms)
    Jan  4 22:17:18.589: INFO: (6) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 22.788086ms)
    Jan  4 22:17:18.590: INFO: (6) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 23.230561ms)
    Jan  4 22:17:18.590: INFO: (6) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 22.970719ms)
    Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 32.16655ms)
    Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 32.532786ms)
    Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 32.468841ms)
    Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 32.866533ms)
    Jan  4 22:17:18.599: INFO: (6) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 33.111561ms)
    Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 34.35205ms)
    Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 34.269671ms)
    Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 34.47554ms)
    Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 34.689229ms)
    Jan  4 22:17:18.601: INFO: (6) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 34.808443ms)
    Jan  4 22:17:18.603: INFO: (6) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 35.938987ms)
    Jan  4 22:17:18.611: INFO: (7) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 7.579095ms)
    Jan  4 22:17:18.617: INFO: (7) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 12.69162ms)
    Jan  4 22:17:18.622: INFO: (7) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 16.900888ms)
    Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 18.081367ms)
    Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 18.020758ms)
    Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 18.224699ms)
    Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 18.075051ms)
    Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 18.797772ms)
    Jan  4 22:17:18.623: INFO: (7) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 19.058125ms)
    Jan  4 22:17:18.625: INFO: (7) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 19.941962ms)
    Jan  4 22:17:18.625: INFO: (7) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 20.195467ms)
    Jan  4 22:17:18.625: INFO: (7) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 20.513693ms)
    Jan  4 22:17:18.625: INFO: (7) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 20.780369ms)
    Jan  4 22:17:18.626: INFO: (7) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 20.760415ms)
    Jan  4 22:17:18.626: INFO: (7) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 21.441212ms)
    Jan  4 22:17:18.626: INFO: (7) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 21.129771ms)
    Jan  4 22:17:18.635: INFO: (8) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 9.141153ms)
    Jan  4 22:17:18.644: INFO: (8) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.826439ms)
    Jan  4 22:17:18.644: INFO: (8) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 17.7762ms)
    Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 21.987542ms)
    Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 22.489421ms)
    Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 22.851247ms)
    Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 22.395ms)
    Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 22.634869ms)
    Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 22.723584ms)
    Jan  4 22:17:18.649: INFO: (8) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 22.763667ms)
    Jan  4 22:17:18.650: INFO: (8) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 23.776591ms)
    Jan  4 22:17:18.651: INFO: (8) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 24.964014ms)
    Jan  4 22:17:18.653: INFO: (8) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 26.343841ms)
    Jan  4 22:17:18.654: INFO: (8) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 27.166687ms)
    Jan  4 22:17:18.654: INFO: (8) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 27.311713ms)
    Jan  4 22:17:18.654: INFO: (8) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 27.678374ms)
    Jan  4 22:17:18.662: INFO: (9) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 7.77998ms)
    Jan  4 22:17:18.667: INFO: (9) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 12.727328ms)
    Jan  4 22:17:18.667: INFO: (9) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 12.789313ms)
    Jan  4 22:17:18.668: INFO: (9) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 12.895102ms)
    Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 17.777992ms)
    Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 18.554842ms)
    Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 18.297541ms)
    Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 18.230607ms)
    Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.324471ms)
    Jan  4 22:17:18.673: INFO: (9) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 18.376251ms)
    Jan  4 22:17:18.674: INFO: (9) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 19.156236ms)
    Jan  4 22:17:18.678: INFO: (9) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 22.950405ms)
    Jan  4 22:17:18.679: INFO: (9) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 23.681163ms)
    Jan  4 22:17:18.679: INFO: (9) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 24.008923ms)
    Jan  4 22:17:18.679: INFO: (9) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 24.244158ms)
    Jan  4 22:17:18.683: INFO: (9) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 27.910212ms)
    Jan  4 22:17:18.691: INFO: (10) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 7.495223ms)
    Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 17.054693ms)
    Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 17.8858ms)
    Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 17.819908ms)
    Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 17.774123ms)
    Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 17.6426ms)
    Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.871165ms)
    Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 17.849599ms)
    Jan  4 22:17:18.701: INFO: (10) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.279154ms)
    Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 19.379036ms)
    Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 19.234937ms)
    Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 19.368849ms)
    Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 19.243591ms)
    Jan  4 22:17:18.703: INFO: (10) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 19.626748ms)
    Jan  4 22:17:18.709: INFO: (10) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 25.73291ms)
    Jan  4 22:17:18.710: INFO: (10) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 26.185044ms)
    Jan  4 22:17:18.731: INFO: (11) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 21.324064ms)
    Jan  4 22:17:18.732: INFO: (11) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 21.63741ms)
    Jan  4 22:17:18.732: INFO: (11) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 21.501173ms)
    Jan  4 22:17:18.732: INFO: (11) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 21.938395ms)
    Jan  4 22:17:18.733: INFO: (11) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 23.659813ms)
    Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 24.895603ms)
    Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 25.237491ms)
    Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 25.589546ms)
    Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 25.146394ms)
    Jan  4 22:17:18.736: INFO: (11) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 25.527831ms)
    Jan  4 22:17:18.735: INFO: (11) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 25.355529ms)
    Jan  4 22:17:18.736: INFO: (11) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 26.567959ms)
    Jan  4 22:17:18.738: INFO: (11) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 27.267865ms)
    Jan  4 22:17:18.738: INFO: (11) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 28.117304ms)
    Jan  4 22:17:18.738: INFO: (11) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 27.82614ms)
    Jan  4 22:17:18.738: INFO: (11) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 28.109996ms)
    Jan  4 22:17:18.755: INFO: (12) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 15.815163ms)
    Jan  4 22:17:18.755: INFO: (12) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 15.997925ms)
    Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 17.479984ms)
    Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.644058ms)
    Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 17.383326ms)
    Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 18.09158ms)
    Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 17.817227ms)
    Jan  4 22:17:18.756: INFO: (12) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 18.225822ms)
    Jan  4 22:17:18.757: INFO: (12) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.103876ms)
    Jan  4 22:17:18.757: INFO: (12) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 18.570498ms)
    Jan  4 22:17:18.757: INFO: (12) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 18.491192ms)
    Jan  4 22:17:18.757: INFO: (12) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 18.943725ms)
    Jan  4 22:17:18.764: INFO: (12) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 25.019438ms)
    Jan  4 22:17:18.765: INFO: (12) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 27.117327ms)
    Jan  4 22:17:18.766: INFO: (12) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 27.367091ms)
    Jan  4 22:17:18.766: INFO: (12) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 27.186384ms)
    Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 18.117069ms)
    Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 17.855772ms)
    Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 17.965619ms)
    Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 18.199717ms)
    Jan  4 22:17:18.784: INFO: (13) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.455032ms)
    Jan  4 22:17:18.785: INFO: (13) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 18.575511ms)
    Jan  4 22:17:18.785: INFO: (13) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 18.929068ms)
    Jan  4 22:17:18.787: INFO: (13) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 20.535172ms)
    Jan  4 22:17:18.787: INFO: (13) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 20.483519ms)
    Jan  4 22:17:18.789: INFO: (13) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 23.017255ms)
    Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 26.561081ms)
    Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 26.222462ms)
    Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 26.464712ms)
    Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 26.568579ms)
    Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 26.207166ms)
    Jan  4 22:17:18.793: INFO: (13) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 26.148443ms)
    Jan  4 22:17:18.803: INFO: (14) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 10.421076ms)
    Jan  4 22:17:18.809: INFO: (14) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 15.431321ms)
    Jan  4 22:17:18.809: INFO: (14) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 15.240616ms)
    Jan  4 22:17:18.809: INFO: (14) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 16.401834ms)
    Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 16.5693ms)
    Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 16.815183ms)
    Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 16.563375ms)
    Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 16.658536ms)
    Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 16.92641ms)
    Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 16.820816ms)
    Jan  4 22:17:18.810: INFO: (14) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.156471ms)
    Jan  4 22:17:18.812: INFO: (14) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 18.828016ms)
    Jan  4 22:17:18.814: INFO: (14) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 20.656774ms)
    Jan  4 22:17:18.815: INFO: (14) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 21.330414ms)
    Jan  4 22:17:18.815: INFO: (14) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 21.718172ms)
    Jan  4 22:17:18.815: INFO: (14) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 21.656682ms)
    Jan  4 22:17:18.828: INFO: (15) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 12.491351ms)
    Jan  4 22:17:18.828: INFO: (15) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 12.972134ms)
    Jan  4 22:17:18.829: INFO: (15) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 13.366893ms)
    Jan  4 22:17:18.829: INFO: (15) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 13.131668ms)
    Jan  4 22:17:18.829: INFO: (15) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 13.634214ms)
    Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 13.714593ms)
    Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 14.419411ms)
    Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 14.311073ms)
    Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 14.280044ms)
    Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 14.510687ms)
    Jan  4 22:17:18.830: INFO: (15) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 14.024114ms)
    Jan  4 22:17:18.831: INFO: (15) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 15.052812ms)
    Jan  4 22:17:18.833: INFO: (15) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 16.83162ms)
    Jan  4 22:17:18.834: INFO: (15) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 17.895357ms)
    Jan  4 22:17:18.834: INFO: (15) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 17.707857ms)
    Jan  4 22:17:18.834: INFO: (15) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 18.009207ms)
    Jan  4 22:17:18.845: INFO: (16) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 10.888929ms)
    Jan  4 22:17:18.845: INFO: (16) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 11.250872ms)
    Jan  4 22:17:18.846: INFO: (16) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 11.714638ms)
    Jan  4 22:17:18.846: INFO: (16) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 11.153634ms)
    Jan  4 22:17:18.847: INFO: (16) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 12.788986ms)
    Jan  4 22:17:18.847: INFO: (16) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 12.956749ms)
    Jan  4 22:17:18.849: INFO: (16) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 14.113879ms)
    Jan  4 22:17:18.849: INFO: (16) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 14.562144ms)
    Jan  4 22:17:18.851: INFO: (16) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 17.393025ms)
    Jan  4 22:17:18.852: INFO: (16) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 17.498221ms)
    Jan  4 22:17:18.852: INFO: (16) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 17.311292ms)
    Jan  4 22:17:18.852: INFO: (16) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 18.128283ms)
    Jan  4 22:17:18.852: INFO: (16) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 18.042148ms)
    Jan  4 22:17:18.855: INFO: (16) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 20.763909ms)
    Jan  4 22:17:18.856: INFO: (16) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 21.700848ms)
    Jan  4 22:17:18.856: INFO: (16) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 21.757009ms)
    Jan  4 22:17:18.873: INFO: (17) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 17.001036ms)
    Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 17.333805ms)
    Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 17.490759ms)
    Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 17.608031ms)
    Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 17.645777ms)
    Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 17.820952ms)
    Jan  4 22:17:18.874: INFO: (17) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 17.604755ms)
    Jan  4 22:17:18.875: INFO: (17) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 18.398867ms)
    Jan  4 22:17:18.875: INFO: (17) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 18.987393ms)
    Jan  4 22:17:18.880: INFO: (17) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 23.211225ms)
    Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 24.079203ms)
    Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 24.75153ms)
    Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 24.614836ms)
    Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 24.494284ms)
    Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 24.648905ms)
    Jan  4 22:17:18.881: INFO: (17) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 25.376349ms)
    Jan  4 22:17:18.889: INFO: (18) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 8.084635ms)
    Jan  4 22:17:18.890: INFO: (18) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 8.392645ms)
    Jan  4 22:17:18.891: INFO: (18) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 9.241279ms)
    Jan  4 22:17:18.891: INFO: (18) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 9.294915ms)
    Jan  4 22:17:18.893: INFO: (18) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 11.216247ms)
    Jan  4 22:17:18.894: INFO: (18) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 11.88604ms)
    Jan  4 22:17:18.894: INFO: (18) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 12.038559ms)
    Jan  4 22:17:18.896: INFO: (18) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 14.126687ms)
    Jan  4 22:17:18.897: INFO: (18) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 15.270391ms)
    Jan  4 22:17:18.897: INFO: (18) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 15.371173ms)
    Jan  4 22:17:18.898: INFO: (18) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 16.23062ms)
    Jan  4 22:17:18.898: INFO: (18) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 16.335221ms)
    Jan  4 22:17:18.898: INFO: (18) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 16.26668ms)
    Jan  4 22:17:18.899: INFO: (18) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 17.533058ms)
    Jan  4 22:17:18.900: INFO: (18) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 17.668171ms)
    Jan  4 22:17:18.901: INFO: (18) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 18.599106ms)
    Jan  4 22:17:18.908: INFO: (19) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc/proxy/rewriteme">test</a> (200; 7.117769ms)
    Jan  4 22:17:18.909: INFO: (19) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 7.728566ms)
    Jan  4 22:17:18.914: INFO: (19) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 12.433068ms)
    Jan  4 22:17:18.914: INFO: (19) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:160/proxy/: foo (200; 13.115047ms)
    Jan  4 22:17:18.915: INFO: (19) /api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">test</... (200; 13.821226ms)
    Jan  4 22:17:18.915: INFO: (19) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname1/proxy/: foo (200; 13.834222ms)
    Jan  4 22:17:18.915: INFO: (19) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:1080/proxy/rewriteme">t... (200; 13.70545ms)
    Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname1/proxy/: foo (200; 14.869642ms)
    Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/: <a href="/api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:443/proxy/tlsrewriteme... (200; 14.522331ms)
    Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:462/proxy/: tls qux (200; 14.650088ms)
    Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/pods/http:proxy-service-44dcr-fxdwc:162/proxy/: bar (200; 14.724592ms)
    Jan  4 22:17:18.916: INFO: (19) /api/v1/namespaces/proxy-968/pods/https:proxy-service-44dcr-fxdwc:460/proxy/: tls baz (200; 14.701719ms)
    Jan  4 22:17:18.917: INFO: (19) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname1/proxy/: tls baz (200; 15.425577ms)
    Jan  4 22:17:18.919: INFO: (19) /api/v1/namespaces/proxy-968/services/https:proxy-service-44dcr:tlsportname2/proxy/: tls qux (200; 18.182309ms)
    Jan  4 22:17:18.919: INFO: (19) /api/v1/namespaces/proxy-968/services/http:proxy-service-44dcr:portname2/proxy/: bar (200; 18.156576ms)
    Jan  4 22:17:18.921: INFO: (19) /api/v1/namespaces/proxy-968/services/proxy-service-44dcr:portname2/proxy/: bar (200; 20.036109ms)
    STEP: deleting ReplicationController proxy-service-44dcr in namespace proxy-968, will wait for the garbage collector to delete the pods 01/04/23 22:17:18.921
    Jan  4 22:17:18.984: INFO: Deleting ReplicationController proxy-service-44dcr took: 8.664438ms
    Jan  4 22:17:19.084: INFO: Terminating ReplicationController proxy-service-44dcr pods took: 100.508767ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:17:21.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-968" for this suite. 01/04/23 22:17:21.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:17:21.24
Jan  4 22:17:21.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename statefulset 01/04/23 22:17:21.241
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:17:21.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:17:21.283
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1941 01/04/23 22:17:21.286
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/04/23 22:17:21.296
Jan  4 22:17:21.320: INFO: Found 0 stateful pods, waiting for 3
Jan  4 22:17:31.329: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 22:17:31.329: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 22:17:31.329: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 22:17:31.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-1941 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 22:17:31.500: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 22:17:31.500: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 22:17:31.500: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/04/23 22:17:41.512
Jan  4 22:17:41.532: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/04/23 22:17:41.532
STEP: Updating Pods in reverse ordinal order 01/04/23 22:17:51.554
Jan  4 22:17:51.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-1941 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 22:17:51.731: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  4 22:17:51.731: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 22:17:51.731: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  4 22:18:11.761: INFO: Waiting for StatefulSet statefulset-1941/ss2 to complete update
STEP: Rolling back to a previous revision 01/04/23 22:18:21.783
Jan  4 22:18:21.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-1941 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 22:18:21.939: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 22:18:21.939: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 22:18:21.939: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 22:18:31.974: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/04/23 22:18:42.004
Jan  4 22:18:42.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-1941 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 22:18:42.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  4 22:18:42.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 22:18:42.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  4 22:18:52.220: INFO: Deleting all statefulset in ns statefulset-1941
Jan  4 22:18:52.222: INFO: Scaling statefulset ss2 to 0
Jan  4 22:19:02.235: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 22:19:02.238: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:02.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1941" for this suite. 01/04/23 22:19:02.264
------------------------------
• [SLOW TEST] [101.033 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:17:21.24
    Jan  4 22:17:21.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename statefulset 01/04/23 22:17:21.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:17:21.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:17:21.283
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1941 01/04/23 22:17:21.286
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/04/23 22:17:21.296
    Jan  4 22:17:21.320: INFO: Found 0 stateful pods, waiting for 3
    Jan  4 22:17:31.329: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 22:17:31.329: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 22:17:31.329: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 22:17:31.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-1941 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 22:17:31.500: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 22:17:31.500: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 22:17:31.500: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/04/23 22:17:41.512
    Jan  4 22:17:41.532: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/04/23 22:17:41.532
    STEP: Updating Pods in reverse ordinal order 01/04/23 22:17:51.554
    Jan  4 22:17:51.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-1941 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 22:17:51.731: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  4 22:17:51.731: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 22:17:51.731: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  4 22:18:11.761: INFO: Waiting for StatefulSet statefulset-1941/ss2 to complete update
    STEP: Rolling back to a previous revision 01/04/23 22:18:21.783
    Jan  4 22:18:21.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-1941 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 22:18:21.939: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 22:18:21.939: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 22:18:21.939: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 22:18:31.974: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/04/23 22:18:42.004
    Jan  4 22:18:42.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-1941 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 22:18:42.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  4 22:18:42.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 22:18:42.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  4 22:18:52.220: INFO: Deleting all statefulset in ns statefulset-1941
    Jan  4 22:18:52.222: INFO: Scaling statefulset ss2 to 0
    Jan  4 22:19:02.235: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 22:19:02.238: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:02.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1941" for this suite. 01/04/23 22:19:02.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:02.276
Jan  4 22:19:02.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-lifecycle-hook 01/04/23 22:19:02.277
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:02.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:02.295
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/04/23 22:19:02.309
Jan  4 22:19:02.317: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1525" to be "running and ready"
Jan  4 22:19:02.323: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.542378ms
Jan  4 22:19:02.323: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:19:04.327: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010161074s
Jan  4 22:19:04.327: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:19:06.327: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.010558349s
Jan  4 22:19:06.327: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  4 22:19:06.327: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/04/23 22:19:06.331
Jan  4 22:19:06.337: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1525" to be "running and ready"
Jan  4 22:19:06.342: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.074013ms
Jan  4 22:19:06.342: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:19:08.346: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009479177s
Jan  4 22:19:08.346: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan  4 22:19:08.346: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/04/23 22:19:08.35
STEP: delete the pod with lifecycle hook 01/04/23 22:19:08.363
Jan  4 22:19:08.373: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  4 22:19:08.378: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  4 22:19:10.379: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  4 22:19:10.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  4 22:19:12.379: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  4 22:19:12.382: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:12.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1525" for this suite. 01/04/23 22:19:12.386
------------------------------
• [SLOW TEST] [10.114 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:02.276
    Jan  4 22:19:02.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/04/23 22:19:02.277
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:02.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:02.295
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/04/23 22:19:02.309
    Jan  4 22:19:02.317: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1525" to be "running and ready"
    Jan  4 22:19:02.323: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.542378ms
    Jan  4 22:19:02.323: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:19:04.327: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010161074s
    Jan  4 22:19:04.327: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:19:06.327: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.010558349s
    Jan  4 22:19:06.327: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  4 22:19:06.327: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/04/23 22:19:06.331
    Jan  4 22:19:06.337: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1525" to be "running and ready"
    Jan  4 22:19:06.342: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.074013ms
    Jan  4 22:19:06.342: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:19:08.346: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009479177s
    Jan  4 22:19:08.346: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan  4 22:19:08.346: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/04/23 22:19:08.35
    STEP: delete the pod with lifecycle hook 01/04/23 22:19:08.363
    Jan  4 22:19:08.373: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  4 22:19:08.378: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  4 22:19:10.379: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  4 22:19:10.383: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  4 22:19:12.379: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  4 22:19:12.382: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:12.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1525" for this suite. 01/04/23 22:19:12.386
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:12.392
Jan  4 22:19:12.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:19:12.393
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:12.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:12.413
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:12.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5835" for this suite. 01/04/23 22:19:12.467
------------------------------
• [0.083 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:12.392
    Jan  4 22:19:12.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:19:12.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:12.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:12.413
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:12.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5835" for this suite. 01/04/23 22:19:12.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:12.475
Jan  4 22:19:12.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename security-context 01/04/23 22:19:12.476
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:12.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:12.499
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/04/23 22:19:12.501
Jan  4 22:19:12.509: INFO: Waiting up to 5m0s for pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc" in namespace "security-context-9508" to be "Succeeded or Failed"
Jan  4 22:19:12.513: INFO: Pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453131ms
Jan  4 22:19:14.517: INFO: Pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008539908s
Jan  4 22:19:16.518: INFO: Pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008749973s
STEP: Saw pod success 01/04/23 22:19:16.518
Jan  4 22:19:16.518: INFO: Pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc" satisfied condition "Succeeded or Failed"
Jan  4 22:19:16.522: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc container test-container: <nil>
STEP: delete the pod 01/04/23 22:19:16.535
Jan  4 22:19:16.555: INFO: Waiting for pod security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc to disappear
Jan  4 22:19:16.560: INFO: Pod security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:16.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9508" for this suite. 01/04/23 22:19:16.566
------------------------------
• [4.097 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:12.475
    Jan  4 22:19:12.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename security-context 01/04/23 22:19:12.476
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:12.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:12.499
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/04/23 22:19:12.501
    Jan  4 22:19:12.509: INFO: Waiting up to 5m0s for pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc" in namespace "security-context-9508" to be "Succeeded or Failed"
    Jan  4 22:19:12.513: INFO: Pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453131ms
    Jan  4 22:19:14.517: INFO: Pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008539908s
    Jan  4 22:19:16.518: INFO: Pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008749973s
    STEP: Saw pod success 01/04/23 22:19:16.518
    Jan  4 22:19:16.518: INFO: Pod "security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc" satisfied condition "Succeeded or Failed"
    Jan  4 22:19:16.522: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc container test-container: <nil>
    STEP: delete the pod 01/04/23 22:19:16.535
    Jan  4 22:19:16.555: INFO: Waiting for pod security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc to disappear
    Jan  4 22:19:16.560: INFO: Pod security-context-cc9f8441-8d4c-4106-a10e-cf7411f150fc no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:16.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9508" for this suite. 01/04/23 22:19:16.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:16.574
Jan  4 22:19:16.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename csistoragecapacity 01/04/23 22:19:16.575
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:16.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:16.591
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/04/23 22:19:16.594
STEP: getting /apis/storage.k8s.io 01/04/23 22:19:16.597
STEP: getting /apis/storage.k8s.io/v1 01/04/23 22:19:16.598
STEP: creating 01/04/23 22:19:16.599
STEP: watching 01/04/23 22:19:16.613
Jan  4 22:19:16.614: INFO: starting watch
STEP: getting 01/04/23 22:19:16.629
STEP: listing in namespace 01/04/23 22:19:16.633
STEP: listing across namespaces 01/04/23 22:19:16.64
STEP: patching 01/04/23 22:19:16.643
STEP: updating 01/04/23 22:19:16.653
Jan  4 22:19:16.665: INFO: waiting for watch events with expected annotations in namespace
Jan  4 22:19:16.665: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/04/23 22:19:16.665
STEP: deleting a collection 01/04/23 22:19:16.678
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:16.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-8572" for this suite. 01/04/23 22:19:16.716
------------------------------
• [0.152 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:16.574
    Jan  4 22:19:16.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename csistoragecapacity 01/04/23 22:19:16.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:16.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:16.591
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/04/23 22:19:16.594
    STEP: getting /apis/storage.k8s.io 01/04/23 22:19:16.597
    STEP: getting /apis/storage.k8s.io/v1 01/04/23 22:19:16.598
    STEP: creating 01/04/23 22:19:16.599
    STEP: watching 01/04/23 22:19:16.613
    Jan  4 22:19:16.614: INFO: starting watch
    STEP: getting 01/04/23 22:19:16.629
    STEP: listing in namespace 01/04/23 22:19:16.633
    STEP: listing across namespaces 01/04/23 22:19:16.64
    STEP: patching 01/04/23 22:19:16.643
    STEP: updating 01/04/23 22:19:16.653
    Jan  4 22:19:16.665: INFO: waiting for watch events with expected annotations in namespace
    Jan  4 22:19:16.665: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/04/23 22:19:16.665
    STEP: deleting a collection 01/04/23 22:19:16.678
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:16.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-8572" for this suite. 01/04/23 22:19:16.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:16.732
Jan  4 22:19:16.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 22:19:16.734
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:16.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:16.763
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/04/23 22:19:16.771
STEP: submitting the pod to kubernetes 01/04/23 22:19:16.771
Jan  4 22:19:16.791: INFO: Waiting up to 5m0s for pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2" in namespace "pods-9281" to be "running and ready"
Jan  4 22:19:16.798: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17112ms
Jan  4 22:19:16.798: INFO: The phase of Pod pod-update-d4964099-ea18-4125-b287-40bad1b501e2 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:19:18.802: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011097308s
Jan  4 22:19:18.802: INFO: The phase of Pod pod-update-d4964099-ea18-4125-b287-40bad1b501e2 is Running (Ready = true)
Jan  4 22:19:18.802: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/04/23 22:19:18.808
STEP: updating the pod 01/04/23 22:19:18.81
Jan  4 22:19:19.322: INFO: Successfully updated pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2"
Jan  4 22:19:19.322: INFO: Waiting up to 5m0s for pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2" in namespace "pods-9281" to be "running"
Jan  4 22:19:19.326: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2": Phase="Running", Reason="", readiness=true. Elapsed: 3.571778ms
Jan  4 22:19:19.326: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/04/23 22:19:19.326
Jan  4 22:19:19.328: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:19.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9281" for this suite. 01/04/23 22:19:19.332
------------------------------
• [2.605 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:16.732
    Jan  4 22:19:16.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 22:19:16.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:16.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:16.763
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/04/23 22:19:16.771
    STEP: submitting the pod to kubernetes 01/04/23 22:19:16.771
    Jan  4 22:19:16.791: INFO: Waiting up to 5m0s for pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2" in namespace "pods-9281" to be "running and ready"
    Jan  4 22:19:16.798: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17112ms
    Jan  4 22:19:16.798: INFO: The phase of Pod pod-update-d4964099-ea18-4125-b287-40bad1b501e2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:19:18.802: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011097308s
    Jan  4 22:19:18.802: INFO: The phase of Pod pod-update-d4964099-ea18-4125-b287-40bad1b501e2 is Running (Ready = true)
    Jan  4 22:19:18.802: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/04/23 22:19:18.808
    STEP: updating the pod 01/04/23 22:19:18.81
    Jan  4 22:19:19.322: INFO: Successfully updated pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2"
    Jan  4 22:19:19.322: INFO: Waiting up to 5m0s for pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2" in namespace "pods-9281" to be "running"
    Jan  4 22:19:19.326: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2": Phase="Running", Reason="", readiness=true. Elapsed: 3.571778ms
    Jan  4 22:19:19.326: INFO: Pod "pod-update-d4964099-ea18-4125-b287-40bad1b501e2" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/04/23 22:19:19.326
    Jan  4 22:19:19.328: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:19.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9281" for this suite. 01/04/23 22:19:19.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:19.34
Jan  4 22:19:19.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 22:19:19.341
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:19.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:19.358
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-3570/configmap-test-40baedf4-8150-40f6-8ff3-92a3666a0be0 01/04/23 22:19:19.362
STEP: Creating a pod to test consume configMaps 01/04/23 22:19:19.367
Jan  4 22:19:19.383: INFO: Waiting up to 5m0s for pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6" in namespace "configmap-3570" to be "Succeeded or Failed"
Jan  4 22:19:19.385: INFO: Pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.628391ms
Jan  4 22:19:21.390: INFO: Pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007265699s
Jan  4 22:19:23.389: INFO: Pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006230912s
STEP: Saw pod success 01/04/23 22:19:23.389
Jan  4 22:19:23.389: INFO: Pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6" satisfied condition "Succeeded or Failed"
Jan  4 22:19:23.392: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6 container env-test: <nil>
STEP: delete the pod 01/04/23 22:19:23.398
Jan  4 22:19:23.407: INFO: Waiting for pod pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6 to disappear
Jan  4 22:19:23.412: INFO: Pod pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:23.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3570" for this suite. 01/04/23 22:19:23.42
------------------------------
• [4.087 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:19.34
    Jan  4 22:19:19.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 22:19:19.341
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:19.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:19.358
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-3570/configmap-test-40baedf4-8150-40f6-8ff3-92a3666a0be0 01/04/23 22:19:19.362
    STEP: Creating a pod to test consume configMaps 01/04/23 22:19:19.367
    Jan  4 22:19:19.383: INFO: Waiting up to 5m0s for pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6" in namespace "configmap-3570" to be "Succeeded or Failed"
    Jan  4 22:19:19.385: INFO: Pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.628391ms
    Jan  4 22:19:21.390: INFO: Pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007265699s
    Jan  4 22:19:23.389: INFO: Pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006230912s
    STEP: Saw pod success 01/04/23 22:19:23.389
    Jan  4 22:19:23.389: INFO: Pod "pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6" satisfied condition "Succeeded or Failed"
    Jan  4 22:19:23.392: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6 container env-test: <nil>
    STEP: delete the pod 01/04/23 22:19:23.398
    Jan  4 22:19:23.407: INFO: Waiting for pod pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6 to disappear
    Jan  4 22:19:23.412: INFO: Pod pod-configmaps-602a0642-9c4d-4e12-96a1-49bc26ed6ec6 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:23.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3570" for this suite. 01/04/23 22:19:23.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:23.428
Jan  4 22:19:23.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename deployment 01/04/23 22:19:23.429
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:23.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:23.452
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan  4 22:19:23.455: INFO: Creating simple deployment test-new-deployment
Jan  4 22:19:23.466: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 01/04/23 22:19:25.488
STEP: updating a scale subresource 01/04/23 22:19:25.492
STEP: verifying the deployment Spec.Replicas was modified 01/04/23 22:19:25.498
STEP: Patch a scale subresource 01/04/23 22:19:25.502
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  4 22:19:25.526: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9223  6fd20ebc-f51f-47b9-adfc-90d5765535a8 34250 3 2023-01-04 22:19:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-04 22:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:19:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004089f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-04 22:19:24 +0000 UTC,LastTransitionTime:2023-01-04 22:19:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-04 22:19:24 +0000 UTC,LastTransitionTime:2023-01-04 22:19:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  4 22:19:25.541: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9223  52984533-bfe2-4739-92a4-ed3bf7975c63 34252 3 2023-01-04 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 6fd20ebc-f51f-47b9-adfc-90d5765535a8 0xc004d1ac37 0xc004d1ac38}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:19:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-04 22:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd20ebc-f51f-47b9-adfc-90d5765535a8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d1acc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  4 22:19:25.554: INFO: Pod "test-new-deployment-7f5969cbc7-lcrn9" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-lcrn9 test-new-deployment-7f5969cbc7- deployment-9223  2f950bf6-f902-46c7-bee9-d2c06bce97da 34259 0 2023-01-04 22:19:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 52984533-bfe2-4739-92a4-ed3bf7975c63 0xc004d1b0b7 0xc004d1b0b8}] [] [{kube-controller-manager Update v1 2023-01-04 22:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52984533-bfe2-4739-92a4-ed3bf7975c63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jq8m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jq8m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 22:19:25.554: INFO: Pod "test-new-deployment-7f5969cbc7-lps87" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-lps87 test-new-deployment-7f5969cbc7- deployment-9223  e35b7deb-5dd3-4234-b7d2-dfda6ca0270d 34251 0 2023-01-04 22:19:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 52984533-bfe2-4739-92a4-ed3bf7975c63 0xc004d1b1f7 0xc004d1b1f8}] [] [{kube-controller-manager Update v1 2023-01-04 22:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52984533-bfe2-4739-92a4-ed3bf7975c63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwxhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwxhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 22:19:25.555: INFO: Pod "test-new-deployment-7f5969cbc7-nfbnm" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-nfbnm test-new-deployment-7f5969cbc7- deployment-9223  43a0df5e-dca8-4c5a-b64c-3fed6b57c8a9 34227 0 2023-01-04 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bb5079bb861072cccec94bcb832d9576b664c68b0b91b57c56a763467394c4b6 cni.projectcalico.org/podIP:10.42.3.72/32 cni.projectcalico.org/podIPs:10.42.3.72/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 52984533-bfe2-4739-92a4-ed3bf7975c63 0xc004d1b380 0xc004d1b381}] [] [{calico Update v1 2023-01-04 22:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 22:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52984533-bfe2-4739-92a4-ed3bf7975c63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 22:19:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntggc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntggc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.72,StartTime:2023-01-04 22:19:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 22:19:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://be45aa4d953edff390aa17611db5d5f56e6693a3d067eab9153fa4b4cc02ee93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:25.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9223" for this suite. 01/04/23 22:19:25.563
------------------------------
• [2.162 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:23.428
    Jan  4 22:19:23.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename deployment 01/04/23 22:19:23.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:23.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:23.452
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan  4 22:19:23.455: INFO: Creating simple deployment test-new-deployment
    Jan  4 22:19:23.466: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 01/04/23 22:19:25.488
    STEP: updating a scale subresource 01/04/23 22:19:25.492
    STEP: verifying the deployment Spec.Replicas was modified 01/04/23 22:19:25.498
    STEP: Patch a scale subresource 01/04/23 22:19:25.502
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  4 22:19:25.526: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9223  6fd20ebc-f51f-47b9-adfc-90d5765535a8 34250 3 2023-01-04 22:19:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-04 22:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:19:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004089f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-04 22:19:24 +0000 UTC,LastTransitionTime:2023-01-04 22:19:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-04 22:19:24 +0000 UTC,LastTransitionTime:2023-01-04 22:19:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  4 22:19:25.541: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9223  52984533-bfe2-4739-92a4-ed3bf7975c63 34252 3 2023-01-04 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 6fd20ebc-f51f-47b9-adfc-90d5765535a8 0xc004d1ac37 0xc004d1ac38}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:19:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-04 22:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd20ebc-f51f-47b9-adfc-90d5765535a8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d1acc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 22:19:25.554: INFO: Pod "test-new-deployment-7f5969cbc7-lcrn9" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-lcrn9 test-new-deployment-7f5969cbc7- deployment-9223  2f950bf6-f902-46c7-bee9-d2c06bce97da 34259 0 2023-01-04 22:19:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 52984533-bfe2-4739-92a4-ed3bf7975c63 0xc004d1b0b7 0xc004d1b0b8}] [] [{kube-controller-manager Update v1 2023-01-04 22:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52984533-bfe2-4739-92a4-ed3bf7975c63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jq8m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jq8m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 22:19:25.554: INFO: Pod "test-new-deployment-7f5969cbc7-lps87" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-lps87 test-new-deployment-7f5969cbc7- deployment-9223  e35b7deb-5dd3-4234-b7d2-dfda6ca0270d 34251 0 2023-01-04 22:19:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 52984533-bfe2-4739-92a4-ed3bf7975c63 0xc004d1b1f7 0xc004d1b1f8}] [] [{kube-controller-manager Update v1 2023-01-04 22:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52984533-bfe2-4739-92a4-ed3bf7975c63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwxhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwxhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 22:19:25.555: INFO: Pod "test-new-deployment-7f5969cbc7-nfbnm" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-nfbnm test-new-deployment-7f5969cbc7- deployment-9223  43a0df5e-dca8-4c5a-b64c-3fed6b57c8a9 34227 0 2023-01-04 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bb5079bb861072cccec94bcb832d9576b664c68b0b91b57c56a763467394c4b6 cni.projectcalico.org/podIP:10.42.3.72/32 cni.projectcalico.org/podIPs:10.42.3.72/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 52984533-bfe2-4739-92a4-ed3bf7975c63 0xc004d1b380 0xc004d1b381}] [] [{calico Update v1 2023-01-04 22:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 22:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52984533-bfe2-4739-92a4-ed3bf7975c63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 22:19:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntggc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntggc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.72,StartTime:2023-01-04 22:19:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 22:19:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://be45aa4d953edff390aa17611db5d5f56e6693a3d067eab9153fa4b4cc02ee93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:25.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9223" for this suite. 01/04/23 22:19:25.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:25.592
Jan  4 22:19:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename ephemeral-containers-test 01/04/23 22:19:25.593
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:25.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:25.63
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/04/23 22:19:25.638
Jan  4 22:19:25.648: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4556" to be "running and ready"
Jan  4 22:19:25.651: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.703444ms
Jan  4 22:19:25.652: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:19:27.655: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007587542s
Jan  4 22:19:27.655: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan  4 22:19:27.655: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/04/23 22:19:27.658
Jan  4 22:19:27.666: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4556" to be "container debugger running"
Jan  4 22:19:27.672: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.135124ms
Jan  4 22:19:29.677: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010354987s
Jan  4 22:19:29.677: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/04/23 22:19:29.677
Jan  4 22:19:29.677: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4556 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:19:29.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:19:29.677: INFO: ExecWithOptions: Clientset creation
Jan  4 22:19:29.677: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/ephemeral-containers-test-4556/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan  4 22:19:29.748: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:19:29.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4556" for this suite. 01/04/23 22:19:29.758
------------------------------
• [4.171 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:25.592
    Jan  4 22:19:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/04/23 22:19:25.593
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:25.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:25.63
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/04/23 22:19:25.638
    Jan  4 22:19:25.648: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4556" to be "running and ready"
    Jan  4 22:19:25.651: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.703444ms
    Jan  4 22:19:25.652: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:19:27.655: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007587542s
    Jan  4 22:19:27.655: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan  4 22:19:27.655: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/04/23 22:19:27.658
    Jan  4 22:19:27.666: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4556" to be "container debugger running"
    Jan  4 22:19:27.672: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.135124ms
    Jan  4 22:19:29.677: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010354987s
    Jan  4 22:19:29.677: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/04/23 22:19:29.677
    Jan  4 22:19:29.677: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4556 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:19:29.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:19:29.677: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:19:29.677: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/ephemeral-containers-test-4556/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan  4 22:19:29.748: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:19:29.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4556" for this suite. 01/04/23 22:19:29.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:19:29.764
Jan  4 22:19:29.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-probe 01/04/23 22:19:29.765
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:29.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:29.783
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  4 22:20:29.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4749" for this suite. 01/04/23 22:20:29.804
------------------------------
• [SLOW TEST] [60.046 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:19:29.764
    Jan  4 22:19:29.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-probe 01/04/23 22:19:29.765
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:19:29.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:19:29.783
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:20:29.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4749" for this suite. 01/04/23 22:20:29.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:20:29.81
Jan  4 22:20:29.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename disruption 01/04/23 22:20:29.812
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:29.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:29.828
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/04/23 22:20:29.83
STEP: Waiting for the pdb to be processed 01/04/23 22:20:29.835
STEP: updating the pdb 01/04/23 22:20:31.842
STEP: Waiting for the pdb to be processed 01/04/23 22:20:31.853
STEP: patching the pdb 01/04/23 22:20:33.863
STEP: Waiting for the pdb to be processed 01/04/23 22:20:33.877
STEP: Waiting for the pdb to be deleted 01/04/23 22:20:35.895
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  4 22:20:35.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4208" for this suite. 01/04/23 22:20:35.905
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:20:29.81
    Jan  4 22:20:29.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename disruption 01/04/23 22:20:29.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:29.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:29.828
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/04/23 22:20:29.83
    STEP: Waiting for the pdb to be processed 01/04/23 22:20:29.835
    STEP: updating the pdb 01/04/23 22:20:31.842
    STEP: Waiting for the pdb to be processed 01/04/23 22:20:31.853
    STEP: patching the pdb 01/04/23 22:20:33.863
    STEP: Waiting for the pdb to be processed 01/04/23 22:20:33.877
    STEP: Waiting for the pdb to be deleted 01/04/23 22:20:35.895
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:20:35.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4208" for this suite. 01/04/23 22:20:35.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:20:35.914
Jan  4 22:20:35.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:20:35.915
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:35.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:35.938
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:20:35.941
Jan  4 22:20:35.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2" in namespace "projected-8873" to be "Succeeded or Failed"
Jan  4 22:20:35.964: INFO: Pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047274ms
Jan  4 22:20:37.969: INFO: Pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014833586s
Jan  4 22:20:39.969: INFO: Pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014498468s
STEP: Saw pod success 01/04/23 22:20:39.969
Jan  4 22:20:39.969: INFO: Pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2" satisfied condition "Succeeded or Failed"
Jan  4 22:20:39.973: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2 container client-container: <nil>
STEP: delete the pod 01/04/23 22:20:39.98
Jan  4 22:20:39.994: INFO: Waiting for pod downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2 to disappear
Jan  4 22:20:39.997: INFO: Pod downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 22:20:39.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8873" for this suite. 01/04/23 22:20:40.008
------------------------------
• [4.107 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:20:35.914
    Jan  4 22:20:35.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:20:35.915
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:35.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:35.938
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:20:35.941
    Jan  4 22:20:35.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2" in namespace "projected-8873" to be "Succeeded or Failed"
    Jan  4 22:20:35.964: INFO: Pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047274ms
    Jan  4 22:20:37.969: INFO: Pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014833586s
    Jan  4 22:20:39.969: INFO: Pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014498468s
    STEP: Saw pod success 01/04/23 22:20:39.969
    Jan  4 22:20:39.969: INFO: Pod "downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2" satisfied condition "Succeeded or Failed"
    Jan  4 22:20:39.973: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2 container client-container: <nil>
    STEP: delete the pod 01/04/23 22:20:39.98
    Jan  4 22:20:39.994: INFO: Waiting for pod downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2 to disappear
    Jan  4 22:20:39.997: INFO: Pod downwardapi-volume-b6c0977e-782e-497f-b5cc-4254d48c96a2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:20:39.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8873" for this suite. 01/04/23 22:20:40.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:20:40.021
Jan  4 22:20:40.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:20:40.023
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:40.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:40.045
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/04/23 22:20:40.048
Jan  4 22:20:40.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan  4 22:20:40.143: INFO: stderr: ""
Jan  4 22:20:40.143: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/04/23 22:20:40.143
Jan  4 22:20:40.143: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan  4 22:20:40.143: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9248" to be "running and ready, or succeeded"
Jan  4 22:20:40.153: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.815665ms
Jan  4 22:20:40.153: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-13-117.us-east-2.compute.internal' to be 'Running' but was 'Pending'
Jan  4 22:20:42.156: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.013002031s
Jan  4 22:20:42.156: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan  4 22:20:42.156: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/04/23 22:20:42.156
Jan  4 22:20:42.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator'
Jan  4 22:20:42.244: INFO: stderr: ""
Jan  4 22:20:42.244: INFO: stdout: "I0104 22:20:40.946053       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/28b 315\nI0104 22:20:41.146143       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/f4gr 271\nI0104 22:20:41.346675       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/q2d 220\nI0104 22:20:41.546980       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/kpq 524\nI0104 22:20:41.746137       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/c8bm 290\nI0104 22:20:41.946463       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/cdct 459\nI0104 22:20:42.146790       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/wjkp 404\n"
STEP: limiting log lines 01/04/23 22:20:42.245
Jan  4 22:20:42.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --tail=1'
Jan  4 22:20:42.384: INFO: stderr: ""
Jan  4 22:20:42.384: INFO: stdout: "I0104 22:20:42.349679       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/kfc 538\n"
Jan  4 22:20:42.384: INFO: got output "I0104 22:20:42.349679       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/kfc 538\n"
STEP: limiting log bytes 01/04/23 22:20:42.384
Jan  4 22:20:42.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --limit-bytes=1'
Jan  4 22:20:42.511: INFO: stderr: ""
Jan  4 22:20:42.511: INFO: stdout: "I"
Jan  4 22:20:42.511: INFO: got output "I"
STEP: exposing timestamps 01/04/23 22:20:42.511
Jan  4 22:20:42.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --tail=1 --timestamps'
Jan  4 22:20:42.688: INFO: stderr: ""
Jan  4 22:20:42.688: INFO: stdout: "2023-01-04T22:20:42.548765422Z I0104 22:20:42.548330       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tmk 567\n"
Jan  4 22:20:42.688: INFO: got output "2023-01-04T22:20:42.548765422Z I0104 22:20:42.548330       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tmk 567\n"
STEP: restricting to a time range 01/04/23 22:20:42.688
Jan  4 22:20:45.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --since=1s'
Jan  4 22:20:45.281: INFO: stderr: ""
Jan  4 22:20:45.281: INFO: stdout: "I0104 22:20:44.346456       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/gt7 239\nI0104 22:20:44.546723       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/l96 218\nI0104 22:20:44.747060       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/km5 468\nI0104 22:20:44.946402       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/knd 527\nI0104 22:20:45.146751       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/8np 458\n"
Jan  4 22:20:45.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --since=24h'
Jan  4 22:20:45.369: INFO: stderr: ""
Jan  4 22:20:45.369: INFO: stdout: "I0104 22:20:40.946053       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/28b 315\nI0104 22:20:41.146143       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/f4gr 271\nI0104 22:20:41.346675       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/q2d 220\nI0104 22:20:41.546980       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/kpq 524\nI0104 22:20:41.746137       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/c8bm 290\nI0104 22:20:41.946463       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/cdct 459\nI0104 22:20:42.146790       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/wjkp 404\nI0104 22:20:42.349679       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/kfc 538\nI0104 22:20:42.548330       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tmk 567\nI0104 22:20:42.746638       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/f9x 498\nI0104 22:20:42.946062       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/wgt 584\nI0104 22:20:43.146400       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/pz2 329\nI0104 22:20:43.346771       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/574r 350\nI0104 22:20:43.547117       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/k2z 243\nI0104 22:20:43.746470       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/tq8 260\nI0104 22:20:43.946836       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/dzq9 455\nI0104 22:20:44.146117       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/zxh 458\nI0104 22:20:44.346456       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/gt7 239\nI0104 22:20:44.546723       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/l96 218\nI0104 22:20:44.747060       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/km5 468\nI0104 22:20:44.946402       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/knd 527\nI0104 22:20:45.146751       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/8np 458\nI0104 22:20:45.347199       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/zcl 574\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan  4 22:20:45.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 delete pod logs-generator'
Jan  4 22:20:46.698: INFO: stderr: ""
Jan  4 22:20:46.698: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:20:46.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9248" for this suite. 01/04/23 22:20:46.711
------------------------------
• [SLOW TEST] [6.698 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:20:40.021
    Jan  4 22:20:40.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:20:40.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:40.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:40.045
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/04/23 22:20:40.048
    Jan  4 22:20:40.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan  4 22:20:40.143: INFO: stderr: ""
    Jan  4 22:20:40.143: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/04/23 22:20:40.143
    Jan  4 22:20:40.143: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan  4 22:20:40.143: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9248" to be "running and ready, or succeeded"
    Jan  4 22:20:40.153: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.815665ms
    Jan  4 22:20:40.153: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-13-117.us-east-2.compute.internal' to be 'Running' but was 'Pending'
    Jan  4 22:20:42.156: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.013002031s
    Jan  4 22:20:42.156: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan  4 22:20:42.156: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/04/23 22:20:42.156
    Jan  4 22:20:42.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator'
    Jan  4 22:20:42.244: INFO: stderr: ""
    Jan  4 22:20:42.244: INFO: stdout: "I0104 22:20:40.946053       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/28b 315\nI0104 22:20:41.146143       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/f4gr 271\nI0104 22:20:41.346675       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/q2d 220\nI0104 22:20:41.546980       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/kpq 524\nI0104 22:20:41.746137       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/c8bm 290\nI0104 22:20:41.946463       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/cdct 459\nI0104 22:20:42.146790       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/wjkp 404\n"
    STEP: limiting log lines 01/04/23 22:20:42.245
    Jan  4 22:20:42.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --tail=1'
    Jan  4 22:20:42.384: INFO: stderr: ""
    Jan  4 22:20:42.384: INFO: stdout: "I0104 22:20:42.349679       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/kfc 538\n"
    Jan  4 22:20:42.384: INFO: got output "I0104 22:20:42.349679       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/kfc 538\n"
    STEP: limiting log bytes 01/04/23 22:20:42.384
    Jan  4 22:20:42.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --limit-bytes=1'
    Jan  4 22:20:42.511: INFO: stderr: ""
    Jan  4 22:20:42.511: INFO: stdout: "I"
    Jan  4 22:20:42.511: INFO: got output "I"
    STEP: exposing timestamps 01/04/23 22:20:42.511
    Jan  4 22:20:42.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan  4 22:20:42.688: INFO: stderr: ""
    Jan  4 22:20:42.688: INFO: stdout: "2023-01-04T22:20:42.548765422Z I0104 22:20:42.548330       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tmk 567\n"
    Jan  4 22:20:42.688: INFO: got output "2023-01-04T22:20:42.548765422Z I0104 22:20:42.548330       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tmk 567\n"
    STEP: restricting to a time range 01/04/23 22:20:42.688
    Jan  4 22:20:45.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --since=1s'
    Jan  4 22:20:45.281: INFO: stderr: ""
    Jan  4 22:20:45.281: INFO: stdout: "I0104 22:20:44.346456       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/gt7 239\nI0104 22:20:44.546723       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/l96 218\nI0104 22:20:44.747060       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/km5 468\nI0104 22:20:44.946402       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/knd 527\nI0104 22:20:45.146751       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/8np 458\n"
    Jan  4 22:20:45.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 logs logs-generator logs-generator --since=24h'
    Jan  4 22:20:45.369: INFO: stderr: ""
    Jan  4 22:20:45.369: INFO: stdout: "I0104 22:20:40.946053       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/28b 315\nI0104 22:20:41.146143       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/f4gr 271\nI0104 22:20:41.346675       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/q2d 220\nI0104 22:20:41.546980       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/kpq 524\nI0104 22:20:41.746137       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/c8bm 290\nI0104 22:20:41.946463       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/cdct 459\nI0104 22:20:42.146790       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/wjkp 404\nI0104 22:20:42.349679       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/kfc 538\nI0104 22:20:42.548330       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tmk 567\nI0104 22:20:42.746638       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/f9x 498\nI0104 22:20:42.946062       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/wgt 584\nI0104 22:20:43.146400       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/pz2 329\nI0104 22:20:43.346771       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/574r 350\nI0104 22:20:43.547117       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/k2z 243\nI0104 22:20:43.746470       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/tq8 260\nI0104 22:20:43.946836       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/dzq9 455\nI0104 22:20:44.146117       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/zxh 458\nI0104 22:20:44.346456       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/gt7 239\nI0104 22:20:44.546723       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/l96 218\nI0104 22:20:44.747060       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/km5 468\nI0104 22:20:44.946402       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/knd 527\nI0104 22:20:45.146751       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/8np 458\nI0104 22:20:45.347199       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/zcl 574\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan  4 22:20:45.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9248 delete pod logs-generator'
    Jan  4 22:20:46.698: INFO: stderr: ""
    Jan  4 22:20:46.698: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:20:46.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9248" for this suite. 01/04/23 22:20:46.711
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:20:46.722
Jan  4 22:20:46.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:20:46.723
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:46.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:46.759
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-2771 01/04/23 22:20:46.761
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[] 01/04/23 22:20:46.787
Jan  4 22:20:46.811: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2771 01/04/23 22:20:46.811
Jan  4 22:20:46.849: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2771" to be "running and ready"
Jan  4 22:20:46.857: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.319857ms
Jan  4 22:20:46.857: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:20:48.861: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011838255s
Jan  4 22:20:48.861: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  4 22:20:48.861: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod1:[100]] 01/04/23 22:20:48.863
Jan  4 22:20:48.871: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2771 01/04/23 22:20:48.871
Jan  4 22:20:48.877: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2771" to be "running and ready"
Jan  4 22:20:48.881: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.760586ms
Jan  4 22:20:48.881: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:20:50.885: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008248768s
Jan  4 22:20:50.885: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  4 22:20:50.885: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod1:[100] pod2:[101]] 01/04/23 22:20:50.888
Jan  4 22:20:50.899: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/04/23 22:20:50.899
Jan  4 22:20:50.899: INFO: Creating new exec pod
Jan  4 22:20:50.904: INFO: Waiting up to 5m0s for pod "execpodb7fx5" in namespace "services-2771" to be "running"
Jan  4 22:20:50.908: INFO: Pod "execpodb7fx5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.779618ms
Jan  4 22:20:52.912: INFO: Pod "execpodb7fx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008309371s
Jan  4 22:20:52.912: INFO: Pod "execpodb7fx5" satisfied condition "running"
Jan  4 22:20:53.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-2771 exec execpodb7fx5 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan  4 22:20:54.070: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan  4 22:20:54.070: INFO: stdout: ""
Jan  4 22:20:54.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-2771 exec execpodb7fx5 -- /bin/sh -x -c nc -v -z -w 2 10.43.251.144 80'
Jan  4 22:20:54.197: INFO: stderr: "+ nc -v -z -w 2 10.43.251.144 80\nConnection to 10.43.251.144 80 port [tcp/http] succeeded!\n"
Jan  4 22:20:54.197: INFO: stdout: ""
Jan  4 22:20:54.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-2771 exec execpodb7fx5 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan  4 22:20:54.323: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan  4 22:20:54.323: INFO: stdout: ""
Jan  4 22:20:54.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-2771 exec execpodb7fx5 -- /bin/sh -x -c nc -v -z -w 2 10.43.251.144 81'
Jan  4 22:20:54.446: INFO: stderr: "+ nc -v -z -w 2 10.43.251.144 81\nConnection to 10.43.251.144 81 port [tcp/*] succeeded!\n"
Jan  4 22:20:54.446: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2771 01/04/23 22:20:54.446
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod2:[101]] 01/04/23 22:20:54.465
Jan  4 22:20:54.499: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2771 01/04/23 22:20:54.499
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[] 01/04/23 22:20:54.528
Jan  4 22:20:54.563: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:20:54.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2771" for this suite. 01/04/23 22:20:54.599
------------------------------
• [SLOW TEST] [7.885 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:20:46.722
    Jan  4 22:20:46.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:20:46.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:46.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:46.759
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-2771 01/04/23 22:20:46.761
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[] 01/04/23 22:20:46.787
    Jan  4 22:20:46.811: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2771 01/04/23 22:20:46.811
    Jan  4 22:20:46.849: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2771" to be "running and ready"
    Jan  4 22:20:46.857: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.319857ms
    Jan  4 22:20:46.857: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:20:48.861: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011838255s
    Jan  4 22:20:48.861: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  4 22:20:48.861: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod1:[100]] 01/04/23 22:20:48.863
    Jan  4 22:20:48.871: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-2771 01/04/23 22:20:48.871
    Jan  4 22:20:48.877: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2771" to be "running and ready"
    Jan  4 22:20:48.881: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.760586ms
    Jan  4 22:20:48.881: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:20:50.885: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008248768s
    Jan  4 22:20:50.885: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  4 22:20:50.885: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod1:[100] pod2:[101]] 01/04/23 22:20:50.888
    Jan  4 22:20:50.899: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/04/23 22:20:50.899
    Jan  4 22:20:50.899: INFO: Creating new exec pod
    Jan  4 22:20:50.904: INFO: Waiting up to 5m0s for pod "execpodb7fx5" in namespace "services-2771" to be "running"
    Jan  4 22:20:50.908: INFO: Pod "execpodb7fx5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.779618ms
    Jan  4 22:20:52.912: INFO: Pod "execpodb7fx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008309371s
    Jan  4 22:20:52.912: INFO: Pod "execpodb7fx5" satisfied condition "running"
    Jan  4 22:20:53.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-2771 exec execpodb7fx5 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan  4 22:20:54.070: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan  4 22:20:54.070: INFO: stdout: ""
    Jan  4 22:20:54.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-2771 exec execpodb7fx5 -- /bin/sh -x -c nc -v -z -w 2 10.43.251.144 80'
    Jan  4 22:20:54.197: INFO: stderr: "+ nc -v -z -w 2 10.43.251.144 80\nConnection to 10.43.251.144 80 port [tcp/http] succeeded!\n"
    Jan  4 22:20:54.197: INFO: stdout: ""
    Jan  4 22:20:54.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-2771 exec execpodb7fx5 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan  4 22:20:54.323: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan  4 22:20:54.323: INFO: stdout: ""
    Jan  4 22:20:54.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-2771 exec execpodb7fx5 -- /bin/sh -x -c nc -v -z -w 2 10.43.251.144 81'
    Jan  4 22:20:54.446: INFO: stderr: "+ nc -v -z -w 2 10.43.251.144 81\nConnection to 10.43.251.144 81 port [tcp/*] succeeded!\n"
    Jan  4 22:20:54.446: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2771 01/04/23 22:20:54.446
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod2:[101]] 01/04/23 22:20:54.465
    Jan  4 22:20:54.499: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-2771 01/04/23 22:20:54.499
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[] 01/04/23 22:20:54.528
    Jan  4 22:20:54.563: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:20:54.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2771" for this suite. 01/04/23 22:20:54.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:20:54.607
Jan  4 22:20:54.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename cronjob 01/04/23 22:20:54.608
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:54.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:54.627
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/04/23 22:20:54.632
STEP: Ensuring no jobs are scheduled 01/04/23 22:20:54.641
STEP: Ensuring no job exists by listing jobs explicitly 01/04/23 22:25:54.65
STEP: Removing cronjob 01/04/23 22:25:54.653
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  4 22:25:54.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-955" for this suite. 01/04/23 22:25:54.665
------------------------------
• [SLOW TEST] [300.064 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:20:54.607
    Jan  4 22:20:54.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename cronjob 01/04/23 22:20:54.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:20:54.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:20:54.627
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/04/23 22:20:54.632
    STEP: Ensuring no jobs are scheduled 01/04/23 22:20:54.641
    STEP: Ensuring no job exists by listing jobs explicitly 01/04/23 22:25:54.65
    STEP: Removing cronjob 01/04/23 22:25:54.653
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:25:54.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-955" for this suite. 01/04/23 22:25:54.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:25:54.676
Jan  4 22:25:54.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename watch 01/04/23 22:25:54.688
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:25:54.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:25:54.712
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/04/23 22:25:54.726
STEP: starting a background goroutine to produce watch events 01/04/23 22:25:54.73
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/04/23 22:25:54.73
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  4 22:25:57.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2257" for this suite. 01/04/23 22:25:57.548
------------------------------
• [2.918 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:25:54.676
    Jan  4 22:25:54.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename watch 01/04/23 22:25:54.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:25:54.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:25:54.712
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/04/23 22:25:54.726
    STEP: starting a background goroutine to produce watch events 01/04/23 22:25:54.73
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/04/23 22:25:54.73
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:25:57.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2257" for this suite. 01/04/23 22:25:57.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:25:57.595
Jan  4 22:25:57.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename podtemplate 01/04/23 22:25:57.596
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:25:57.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:25:57.614
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  4 22:25:57.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4821" for this suite. 01/04/23 22:25:57.653
------------------------------
• [0.068 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:25:57.595
    Jan  4 22:25:57.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename podtemplate 01/04/23 22:25:57.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:25:57.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:25:57.614
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:25:57.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4821" for this suite. 01/04/23 22:25:57.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:25:57.663
Jan  4 22:25:57.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-preemption 01/04/23 22:25:57.664
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:25:57.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:25:57.683
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan  4 22:25:57.702: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  4 22:26:57.743: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 01/04/23 22:26:57.745
Jan  4 22:26:57.771: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  4 22:26:57.777: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  4 22:26:57.802: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  4 22:26:57.808: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  4 22:26:57.827: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  4 22:26:57.833: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan  4 22:26:57.865: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan  4 22:26:57.877: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/04/23 22:26:57.877
Jan  4 22:26:57.877: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:26:57.884: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.282808ms
Jan  4 22:26:59.888: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010266215s
Jan  4 22:27:01.916: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03893646s
Jan  4 22:27:03.888: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010331922s
Jan  4 22:27:05.889: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011199988s
Jan  4 22:27:07.889: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011407245s
Jan  4 22:27:09.888: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.01067147s
Jan  4 22:27:09.888: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  4 22:27:09.888: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:27:09.891: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.582018ms
Jan  4 22:27:09.891: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 22:27:09.891: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:27:09.893: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.488765ms
Jan  4 22:27:09.893: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 22:27:09.893: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:27:09.897: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.440915ms
Jan  4 22:27:09.897: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 22:27:09.897: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:27:09.900: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.719702ms
Jan  4 22:27:09.900: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 22:27:09.900: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:27:09.902: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.471693ms
Jan  4 22:27:09.902: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 22:27:09.902: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:27:09.906: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.345788ms
Jan  4 22:27:09.906: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 22:27:09.906: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:27:09.908: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.266851ms
Jan  4 22:27:09.908: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/04/23 22:27:09.908
Jan  4 22:27:09.914: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5001" to be "running"
Jan  4 22:27:09.917: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.309319ms
Jan  4 22:27:11.921: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007317361s
Jan  4 22:27:13.923: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008700247s
Jan  4 22:27:13.923: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:27:13.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5001" for this suite. 01/04/23 22:27:14.076
------------------------------
• [SLOW TEST] [76.426 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:25:57.663
    Jan  4 22:25:57.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-preemption 01/04/23 22:25:57.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:25:57.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:25:57.683
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan  4 22:25:57.702: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  4 22:26:57.743: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 01/04/23 22:26:57.745
    Jan  4 22:26:57.771: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  4 22:26:57.777: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  4 22:26:57.802: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  4 22:26:57.808: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  4 22:26:57.827: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  4 22:26:57.833: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jan  4 22:26:57.865: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jan  4 22:26:57.877: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/04/23 22:26:57.877
    Jan  4 22:26:57.877: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:26:57.884: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.282808ms
    Jan  4 22:26:59.888: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010266215s
    Jan  4 22:27:01.916: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03893646s
    Jan  4 22:27:03.888: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010331922s
    Jan  4 22:27:05.889: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011199988s
    Jan  4 22:27:07.889: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011407245s
    Jan  4 22:27:09.888: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.01067147s
    Jan  4 22:27:09.888: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  4 22:27:09.888: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:27:09.891: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.582018ms
    Jan  4 22:27:09.891: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 22:27:09.891: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:27:09.893: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.488765ms
    Jan  4 22:27:09.893: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 22:27:09.893: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:27:09.897: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.440915ms
    Jan  4 22:27:09.897: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 22:27:09.897: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:27:09.900: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.719702ms
    Jan  4 22:27:09.900: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 22:27:09.900: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:27:09.902: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.471693ms
    Jan  4 22:27:09.902: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 22:27:09.902: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:27:09.906: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.345788ms
    Jan  4 22:27:09.906: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 22:27:09.906: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:27:09.908: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.266851ms
    Jan  4 22:27:09.908: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/04/23 22:27:09.908
    Jan  4 22:27:09.914: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5001" to be "running"
    Jan  4 22:27:09.917: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.309319ms
    Jan  4 22:27:11.921: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007317361s
    Jan  4 22:27:13.923: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008700247s
    Jan  4 22:27:13.923: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:27:13.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5001" for this suite. 01/04/23 22:27:14.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:27:14.091
Jan  4 22:27:14.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:27:14.092
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:14.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:14.126
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:27:14.133
Jan  4 22:27:14.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d" in namespace "projected-3436" to be "Succeeded or Failed"
Jan  4 22:27:14.144: INFO: Pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.704422ms
Jan  4 22:27:16.148: INFO: Pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007062239s
Jan  4 22:27:18.148: INFO: Pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006469337s
STEP: Saw pod success 01/04/23 22:27:18.148
Jan  4 22:27:18.148: INFO: Pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d" satisfied condition "Succeeded or Failed"
Jan  4 22:27:18.151: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d container client-container: <nil>
STEP: delete the pod 01/04/23 22:27:18.164
Jan  4 22:27:18.177: INFO: Waiting for pod downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d to disappear
Jan  4 22:27:18.181: INFO: Pod downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 22:27:18.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3436" for this suite. 01/04/23 22:27:18.188
------------------------------
• [4.110 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:27:14.091
    Jan  4 22:27:14.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:27:14.092
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:14.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:14.126
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:27:14.133
    Jan  4 22:27:14.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d" in namespace "projected-3436" to be "Succeeded or Failed"
    Jan  4 22:27:14.144: INFO: Pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.704422ms
    Jan  4 22:27:16.148: INFO: Pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007062239s
    Jan  4 22:27:18.148: INFO: Pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006469337s
    STEP: Saw pod success 01/04/23 22:27:18.148
    Jan  4 22:27:18.148: INFO: Pod "downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d" satisfied condition "Succeeded or Failed"
    Jan  4 22:27:18.151: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d container client-container: <nil>
    STEP: delete the pod 01/04/23 22:27:18.164
    Jan  4 22:27:18.177: INFO: Waiting for pod downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d to disappear
    Jan  4 22:27:18.181: INFO: Pod downwardapi-volume-350fe112-9fac-4124-ac62-c4c74130ac1d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:27:18.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3436" for this suite. 01/04/23 22:27:18.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:27:18.209
Jan  4 22:27:18.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename events 01/04/23 22:27:18.212
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:18.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:18.284
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/04/23 22:27:18.288
STEP: listing events in all namespaces 01/04/23 22:27:18.301
STEP: listing events in test namespace 01/04/23 22:27:18.306
STEP: listing events with field selection filtering on source 01/04/23 22:27:18.309
STEP: listing events with field selection filtering on reportingController 01/04/23 22:27:18.312
STEP: getting the test event 01/04/23 22:27:18.315
STEP: patching the test event 01/04/23 22:27:18.321
STEP: getting the test event 01/04/23 22:27:18.332
STEP: updating the test event 01/04/23 22:27:18.339
STEP: getting the test event 01/04/23 22:27:18.347
STEP: deleting the test event 01/04/23 22:27:18.352
STEP: listing events in all namespaces 01/04/23 22:27:18.359
STEP: listing events in test namespace 01/04/23 22:27:18.363
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan  4 22:27:18.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9478" for this suite. 01/04/23 22:27:18.37
------------------------------
• [0.168 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:27:18.209
    Jan  4 22:27:18.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename events 01/04/23 22:27:18.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:18.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:18.284
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/04/23 22:27:18.288
    STEP: listing events in all namespaces 01/04/23 22:27:18.301
    STEP: listing events in test namespace 01/04/23 22:27:18.306
    STEP: listing events with field selection filtering on source 01/04/23 22:27:18.309
    STEP: listing events with field selection filtering on reportingController 01/04/23 22:27:18.312
    STEP: getting the test event 01/04/23 22:27:18.315
    STEP: patching the test event 01/04/23 22:27:18.321
    STEP: getting the test event 01/04/23 22:27:18.332
    STEP: updating the test event 01/04/23 22:27:18.339
    STEP: getting the test event 01/04/23 22:27:18.347
    STEP: deleting the test event 01/04/23 22:27:18.352
    STEP: listing events in all namespaces 01/04/23 22:27:18.359
    STEP: listing events in test namespace 01/04/23 22:27:18.363
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:27:18.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9478" for this suite. 01/04/23 22:27:18.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:27:18.379
Jan  4 22:27:18.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:27:18.38
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:18.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:18.412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:27:18.441
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:27:18.99
STEP: Deploying the webhook pod 01/04/23 22:27:18.998
STEP: Wait for the deployment to be ready 01/04/23 22:27:19.01
Jan  4 22:27:19.019: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 22:27:21.029
STEP: Verifying the service has paired with the endpoint 01/04/23 22:27:21.041
Jan  4 22:27:22.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/04/23 22:27:22.097
STEP: Creating a configMap that should be mutated 01/04/23 22:27:22.114
STEP: Deleting the collection of validation webhooks 01/04/23 22:27:22.142
STEP: Creating a configMap that should not be mutated 01/04/23 22:27:22.181
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:27:22.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9413" for this suite. 01/04/23 22:27:22.27
STEP: Destroying namespace "webhook-9413-markers" for this suite. 01/04/23 22:27:22.282
------------------------------
• [3.910 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:27:18.379
    Jan  4 22:27:18.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:27:18.38
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:18.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:18.412
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:27:18.441
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:27:18.99
    STEP: Deploying the webhook pod 01/04/23 22:27:18.998
    STEP: Wait for the deployment to be ready 01/04/23 22:27:19.01
    Jan  4 22:27:19.019: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 22:27:21.029
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:27:21.041
    Jan  4 22:27:22.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/04/23 22:27:22.097
    STEP: Creating a configMap that should be mutated 01/04/23 22:27:22.114
    STEP: Deleting the collection of validation webhooks 01/04/23 22:27:22.142
    STEP: Creating a configMap that should not be mutated 01/04/23 22:27:22.181
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:27:22.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9413" for this suite. 01/04/23 22:27:22.27
    STEP: Destroying namespace "webhook-9413-markers" for this suite. 01/04/23 22:27:22.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:27:22.295
Jan  4 22:27:22.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 22:27:22.296
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:22.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:22.315
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan  4 22:27:22.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:27:25.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6121" for this suite. 01/04/23 22:27:25.446
------------------------------
• [3.181 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:27:22.295
    Jan  4 22:27:22.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 22:27:22.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:22.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:22.315
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan  4 22:27:22.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:27:25.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6121" for this suite. 01/04/23 22:27:25.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:27:25.477
Jan  4 22:27:25.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-probe 01/04/23 22:27:25.478
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:25.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:25.521
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7 in namespace container-probe-9020 01/04/23 22:27:25.523
Jan  4 22:27:25.543: INFO: Waiting up to 5m0s for pod "busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7" in namespace "container-probe-9020" to be "not pending"
Jan  4 22:27:25.557: INFO: Pod "busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.554583ms
Jan  4 22:27:27.560: INFO: Pod "busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.016873912s
Jan  4 22:27:27.560: INFO: Pod "busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7" satisfied condition "not pending"
Jan  4 22:27:27.560: INFO: Started pod busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7 in namespace container-probe-9020
STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 22:27:27.56
Jan  4 22:27:27.563: INFO: Initial restart count of pod busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7 is 0
STEP: deleting the pod 01/04/23 22:31:28.334
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  4 22:31:28.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9020" for this suite. 01/04/23 22:31:28.399
------------------------------
• [SLOW TEST] [242.931 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:27:25.477
    Jan  4 22:27:25.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-probe 01/04/23 22:27:25.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:27:25.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:27:25.521
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7 in namespace container-probe-9020 01/04/23 22:27:25.523
    Jan  4 22:27:25.543: INFO: Waiting up to 5m0s for pod "busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7" in namespace "container-probe-9020" to be "not pending"
    Jan  4 22:27:25.557: INFO: Pod "busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.554583ms
    Jan  4 22:27:27.560: INFO: Pod "busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.016873912s
    Jan  4 22:27:27.560: INFO: Pod "busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7" satisfied condition "not pending"
    Jan  4 22:27:27.560: INFO: Started pod busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7 in namespace container-probe-9020
    STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 22:27:27.56
    Jan  4 22:27:27.563: INFO: Initial restart count of pod busybox-0c967d10-0a32-47fe-8f69-e0ced4e0aaf7 is 0
    STEP: deleting the pod 01/04/23 22:31:28.334
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:31:28.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9020" for this suite. 01/04/23 22:31:28.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:31:28.42
Jan  4 22:31:28.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 22:31:28.426
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:28.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:28.475
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-7c4bc5b2-8fa2-40a8-940d-32efd1c11021 01/04/23 22:31:28.477
STEP: Creating a pod to test consume configMaps 01/04/23 22:31:28.485
Jan  4 22:31:28.494: INFO: Waiting up to 5m0s for pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb" in namespace "configmap-4110" to be "Succeeded or Failed"
Jan  4 22:31:28.498: INFO: Pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.853012ms
Jan  4 22:31:30.502: INFO: Pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008000495s
Jan  4 22:31:32.504: INFO: Pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009432066s
STEP: Saw pod success 01/04/23 22:31:32.504
Jan  4 22:31:32.504: INFO: Pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb" satisfied condition "Succeeded or Failed"
Jan  4 22:31:32.507: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:31:32.518
Jan  4 22:31:32.535: INFO: Waiting for pod pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb to disappear
Jan  4 22:31:32.544: INFO: Pod pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:31:32.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4110" for this suite. 01/04/23 22:31:32.554
------------------------------
• [4.142 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:31:28.42
    Jan  4 22:31:28.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 22:31:28.426
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:28.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:28.475
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-7c4bc5b2-8fa2-40a8-940d-32efd1c11021 01/04/23 22:31:28.477
    STEP: Creating a pod to test consume configMaps 01/04/23 22:31:28.485
    Jan  4 22:31:28.494: INFO: Waiting up to 5m0s for pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb" in namespace "configmap-4110" to be "Succeeded or Failed"
    Jan  4 22:31:28.498: INFO: Pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.853012ms
    Jan  4 22:31:30.502: INFO: Pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008000495s
    Jan  4 22:31:32.504: INFO: Pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009432066s
    STEP: Saw pod success 01/04/23 22:31:32.504
    Jan  4 22:31:32.504: INFO: Pod "pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb" satisfied condition "Succeeded or Failed"
    Jan  4 22:31:32.507: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:31:32.518
    Jan  4 22:31:32.535: INFO: Waiting for pod pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb to disappear
    Jan  4 22:31:32.544: INFO: Pod pod-configmaps-31d0ae74-3931-4119-906c-e3fd20949edb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:31:32.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4110" for this suite. 01/04/23 22:31:32.554
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:31:32.563
Jan  4 22:31:32.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename svcaccounts 01/04/23 22:31:32.564
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:32.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:32.59
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan  4 22:31:32.600: INFO: Got root ca configmap in namespace "svcaccounts-1701"
Jan  4 22:31:32.607: INFO: Deleted root ca configmap in namespace "svcaccounts-1701"
STEP: waiting for a new root ca configmap created 01/04/23 22:31:33.108
Jan  4 22:31:33.111: INFO: Recreated root ca configmap in namespace "svcaccounts-1701"
Jan  4 22:31:33.116: INFO: Updated root ca configmap in namespace "svcaccounts-1701"
STEP: waiting for the root ca configmap reconciled 01/04/23 22:31:33.617
Jan  4 22:31:33.625: INFO: Reconciled root ca configmap in namespace "svcaccounts-1701"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  4 22:31:33.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1701" for this suite. 01/04/23 22:31:33.63
------------------------------
• [1.078 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:31:32.563
    Jan  4 22:31:32.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename svcaccounts 01/04/23 22:31:32.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:32.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:32.59
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan  4 22:31:32.600: INFO: Got root ca configmap in namespace "svcaccounts-1701"
    Jan  4 22:31:32.607: INFO: Deleted root ca configmap in namespace "svcaccounts-1701"
    STEP: waiting for a new root ca configmap created 01/04/23 22:31:33.108
    Jan  4 22:31:33.111: INFO: Recreated root ca configmap in namespace "svcaccounts-1701"
    Jan  4 22:31:33.116: INFO: Updated root ca configmap in namespace "svcaccounts-1701"
    STEP: waiting for the root ca configmap reconciled 01/04/23 22:31:33.617
    Jan  4 22:31:33.625: INFO: Reconciled root ca configmap in namespace "svcaccounts-1701"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:31:33.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1701" for this suite. 01/04/23 22:31:33.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:31:33.642
Jan  4 22:31:33.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:31:33.643
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:33.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:33.672
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/04/23 22:31:33.676
Jan  4 22:31:33.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9453 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  4 22:31:33.770: INFO: stderr: ""
Jan  4 22:31:33.770: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/04/23 22:31:33.77
Jan  4 22:31:33.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9453 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan  4 22:31:34.875: INFO: stderr: ""
Jan  4 22:31:34.875: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/04/23 22:31:34.875
Jan  4 22:31:34.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9453 delete pods e2e-test-httpd-pod'
Jan  4 22:31:37.971: INFO: stderr: ""
Jan  4 22:31:37.971: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:31:37.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9453" for this suite. 01/04/23 22:31:37.974
------------------------------
• [4.337 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:31:33.642
    Jan  4 22:31:33.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:31:33.643
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:33.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:33.672
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/04/23 22:31:33.676
    Jan  4 22:31:33.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9453 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  4 22:31:33.770: INFO: stderr: ""
    Jan  4 22:31:33.770: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/04/23 22:31:33.77
    Jan  4 22:31:33.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9453 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan  4 22:31:34.875: INFO: stderr: ""
    Jan  4 22:31:34.875: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/04/23 22:31:34.875
    Jan  4 22:31:34.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9453 delete pods e2e-test-httpd-pod'
    Jan  4 22:31:37.971: INFO: stderr: ""
    Jan  4 22:31:37.971: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:31:37.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9453" for this suite. 01/04/23 22:31:37.974
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:31:37.98
Jan  4 22:31:37.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename runtimeclass 01/04/23 22:31:37.981
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:37.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:38.007
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  4 22:31:38.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9161" for this suite. 01/04/23 22:31:38.018
------------------------------
• [0.044 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:31:37.98
    Jan  4 22:31:37.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename runtimeclass 01/04/23 22:31:37.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:37.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:38.007
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:31:38.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9161" for this suite. 01/04/23 22:31:38.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:31:38.024
Jan  4 22:31:38.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 22:31:38.027
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:38.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:38.045
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/04/23 22:31:38.048
Jan  4 22:31:38.056: INFO: Waiting up to 5m0s for pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad" in namespace "emptydir-6288" to be "Succeeded or Failed"
Jan  4 22:31:38.061: INFO: Pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23334ms
Jan  4 22:31:40.064: INFO: Pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008690751s
Jan  4 22:31:42.065: INFO: Pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009722893s
STEP: Saw pod success 01/04/23 22:31:42.065
Jan  4 22:31:42.066: INFO: Pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad" satisfied condition "Succeeded or Failed"
Jan  4 22:31:42.068: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-190b4166-147d-494f-9a6a-baf8d6e656ad container test-container: <nil>
STEP: delete the pod 01/04/23 22:31:42.073
Jan  4 22:31:42.083: INFO: Waiting for pod pod-190b4166-147d-494f-9a6a-baf8d6e656ad to disappear
Jan  4 22:31:42.085: INFO: Pod pod-190b4166-147d-494f-9a6a-baf8d6e656ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 22:31:42.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6288" for this suite. 01/04/23 22:31:42.089
------------------------------
• [4.075 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:31:38.024
    Jan  4 22:31:38.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 22:31:38.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:38.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:38.045
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/04/23 22:31:38.048
    Jan  4 22:31:38.056: INFO: Waiting up to 5m0s for pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad" in namespace "emptydir-6288" to be "Succeeded or Failed"
    Jan  4 22:31:38.061: INFO: Pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23334ms
    Jan  4 22:31:40.064: INFO: Pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008690751s
    Jan  4 22:31:42.065: INFO: Pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009722893s
    STEP: Saw pod success 01/04/23 22:31:42.065
    Jan  4 22:31:42.066: INFO: Pod "pod-190b4166-147d-494f-9a6a-baf8d6e656ad" satisfied condition "Succeeded or Failed"
    Jan  4 22:31:42.068: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-190b4166-147d-494f-9a6a-baf8d6e656ad container test-container: <nil>
    STEP: delete the pod 01/04/23 22:31:42.073
    Jan  4 22:31:42.083: INFO: Waiting for pod pod-190b4166-147d-494f-9a6a-baf8d6e656ad to disappear
    Jan  4 22:31:42.085: INFO: Pod pod-190b4166-147d-494f-9a6a-baf8d6e656ad no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:31:42.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6288" for this suite. 01/04/23 22:31:42.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:31:42.108
Jan  4 22:31:42.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename job 01/04/23 22:31:42.108
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:42.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:42.124
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/04/23 22:31:42.127
STEP: Ensuring active pods == parallelism 01/04/23 22:31:42.132
STEP: delete a job 01/04/23 22:31:44.136
STEP: deleting Job.batch foo in namespace job-7148, will wait for the garbage collector to delete the pods 01/04/23 22:31:44.137
Jan  4 22:31:44.198: INFO: Deleting Job.batch foo took: 9.169283ms
Jan  4 22:31:44.299: INFO: Terminating Job.batch foo pods took: 100.737823ms
STEP: Ensuring job was deleted 01/04/23 22:32:17.199
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:17.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7148" for this suite. 01/04/23 22:32:17.206
------------------------------
• [SLOW TEST] [35.103 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:31:42.108
    Jan  4 22:31:42.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename job 01/04/23 22:31:42.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:31:42.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:31:42.124
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/04/23 22:31:42.127
    STEP: Ensuring active pods == parallelism 01/04/23 22:31:42.132
    STEP: delete a job 01/04/23 22:31:44.136
    STEP: deleting Job.batch foo in namespace job-7148, will wait for the garbage collector to delete the pods 01/04/23 22:31:44.137
    Jan  4 22:31:44.198: INFO: Deleting Job.batch foo took: 9.169283ms
    Jan  4 22:31:44.299: INFO: Terminating Job.batch foo pods took: 100.737823ms
    STEP: Ensuring job was deleted 01/04/23 22:32:17.199
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:17.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7148" for this suite. 01/04/23 22:32:17.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:17.213
Jan  4 22:32:17.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 22:32:17.214
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:17.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:17.23
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/04/23 22:32:17.232
STEP: Creating a ResourceQuota 01/04/23 22:32:22.234
STEP: Ensuring resource quota status is calculated 01/04/23 22:32:22.24
STEP: Creating a Service 01/04/23 22:32:24.243
STEP: Creating a NodePort Service 01/04/23 22:32:24.26
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/04/23 22:32:24.284
STEP: Ensuring resource quota status captures service creation 01/04/23 22:32:24.314
STEP: Deleting Services 01/04/23 22:32:26.318
STEP: Ensuring resource quota status released usage 01/04/23 22:32:26.368
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:28.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5867" for this suite. 01/04/23 22:32:28.381
------------------------------
• [SLOW TEST] [11.179 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:17.213
    Jan  4 22:32:17.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 22:32:17.214
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:17.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:17.23
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/04/23 22:32:17.232
    STEP: Creating a ResourceQuota 01/04/23 22:32:22.234
    STEP: Ensuring resource quota status is calculated 01/04/23 22:32:22.24
    STEP: Creating a Service 01/04/23 22:32:24.243
    STEP: Creating a NodePort Service 01/04/23 22:32:24.26
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/04/23 22:32:24.284
    STEP: Ensuring resource quota status captures service creation 01/04/23 22:32:24.314
    STEP: Deleting Services 01/04/23 22:32:26.318
    STEP: Ensuring resource quota status released usage 01/04/23 22:32:26.368
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:28.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5867" for this suite. 01/04/23 22:32:28.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:28.408
Jan  4 22:32:28.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename daemonsets 01/04/23 22:32:28.409
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:28.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:28.438
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/04/23 22:32:28.483
STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 22:32:28.491
Jan  4 22:32:28.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:32:28.501: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:32:29.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:32:29.509: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:32:30.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 22:32:30.509: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status 01/04/23 22:32:30.512
Jan  4 22:32:30.515: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/04/23 22:32:30.515
Jan  4 22:32:30.522: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/04/23 22:32:30.522
Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: ADDED
Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.524: INFO: Found daemon set daemon-set in namespace daemonsets-8618 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  4 22:32:30.524: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/04/23 22:32:30.524
STEP: watching for the daemon set status to be patched 01/04/23 22:32:30.532
Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: ADDED
Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.536: INFO: Observed daemon set daemon-set in namespace daemonsets-8618 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  4 22:32:30.536: INFO: Observed &DaemonSet event: MODIFIED
Jan  4 22:32:30.536: INFO: Found daemon set daemon-set in namespace daemonsets-8618 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan  4 22:32:30.536: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:32:30.538
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8618, will wait for the garbage collector to delete the pods 01/04/23 22:32:30.538
Jan  4 22:32:30.598: INFO: Deleting DaemonSet.extensions daemon-set took: 5.785587ms
Jan  4 22:32:30.698: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.881295ms
Jan  4 22:32:33.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:32:33.202: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  4 22:32:33.205: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38194"},"items":null}

Jan  4 22:32:33.207: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38194"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:33.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8618" for this suite. 01/04/23 22:32:33.223
------------------------------
• [4.821 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:28.408
    Jan  4 22:32:28.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename daemonsets 01/04/23 22:32:28.409
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:28.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:28.438
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/04/23 22:32:28.483
    STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 22:32:28.491
    Jan  4 22:32:28.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:32:28.501: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:32:29.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:32:29.509: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:32:30.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 22:32:30.509: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Getting /status 01/04/23 22:32:30.512
    Jan  4 22:32:30.515: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/04/23 22:32:30.515
    Jan  4 22:32:30.522: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/04/23 22:32:30.522
    Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: ADDED
    Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.524: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.524: INFO: Found daemon set daemon-set in namespace daemonsets-8618 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  4 22:32:30.524: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/04/23 22:32:30.524
    STEP: watching for the daemon set status to be patched 01/04/23 22:32:30.532
    Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: ADDED
    Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.535: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.536: INFO: Observed daemon set daemon-set in namespace daemonsets-8618 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  4 22:32:30.536: INFO: Observed &DaemonSet event: MODIFIED
    Jan  4 22:32:30.536: INFO: Found daemon set daemon-set in namespace daemonsets-8618 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan  4 22:32:30.536: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:32:30.538
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8618, will wait for the garbage collector to delete the pods 01/04/23 22:32:30.538
    Jan  4 22:32:30.598: INFO: Deleting DaemonSet.extensions daemon-set took: 5.785587ms
    Jan  4 22:32:30.698: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.881295ms
    Jan  4 22:32:33.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:32:33.202: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  4 22:32:33.205: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38194"},"items":null}

    Jan  4 22:32:33.207: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38194"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:33.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8618" for this suite. 01/04/23 22:32:33.223
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:33.23
Jan  4 22:32:33.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:32:33.231
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:33.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:33.246
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:32:33.249
Jan  4 22:32:33.257: INFO: Waiting up to 5m0s for pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b" in namespace "projected-7826" to be "Succeeded or Failed"
Jan  4 22:32:33.265: INFO: Pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.578822ms
Jan  4 22:32:35.270: INFO: Pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013857955s
Jan  4 22:32:37.269: INFO: Pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012242287s
STEP: Saw pod success 01/04/23 22:32:37.269
Jan  4 22:32:37.269: INFO: Pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b" satisfied condition "Succeeded or Failed"
Jan  4 22:32:37.272: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b container client-container: <nil>
STEP: delete the pod 01/04/23 22:32:37.279
Jan  4 22:32:37.296: INFO: Waiting for pod downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b to disappear
Jan  4 22:32:37.299: INFO: Pod downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:37.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7826" for this suite. 01/04/23 22:32:37.303
------------------------------
• [4.083 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:33.23
    Jan  4 22:32:33.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:32:33.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:33.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:33.246
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:32:33.249
    Jan  4 22:32:33.257: INFO: Waiting up to 5m0s for pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b" in namespace "projected-7826" to be "Succeeded or Failed"
    Jan  4 22:32:33.265: INFO: Pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.578822ms
    Jan  4 22:32:35.270: INFO: Pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013857955s
    Jan  4 22:32:37.269: INFO: Pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012242287s
    STEP: Saw pod success 01/04/23 22:32:37.269
    Jan  4 22:32:37.269: INFO: Pod "downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b" satisfied condition "Succeeded or Failed"
    Jan  4 22:32:37.272: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b container client-container: <nil>
    STEP: delete the pod 01/04/23 22:32:37.279
    Jan  4 22:32:37.296: INFO: Waiting for pod downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b to disappear
    Jan  4 22:32:37.299: INFO: Pod downwardapi-volume-703981ce-7b51-4974-88af-d501088d512b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:37.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7826" for this suite. 01/04/23 22:32:37.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:37.321
Jan  4 22:32:37.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 22:32:37.325
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:37.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:37.346
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/04/23 22:32:37.348
Jan  4 22:32:37.358: INFO: Waiting up to 5m0s for pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb" in namespace "emptydir-3926" to be "Succeeded or Failed"
Jan  4 22:32:37.371: INFO: Pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.605259ms
Jan  4 22:32:39.376: INFO: Pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018183002s
Jan  4 22:32:41.376: INFO: Pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018238053s
STEP: Saw pod success 01/04/23 22:32:41.376
Jan  4 22:32:41.377: INFO: Pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb" satisfied condition "Succeeded or Failed"
Jan  4 22:32:41.380: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb container test-container: <nil>
STEP: delete the pod 01/04/23 22:32:41.387
Jan  4 22:32:41.401: INFO: Waiting for pod pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb to disappear
Jan  4 22:32:41.405: INFO: Pod pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:41.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3926" for this suite. 01/04/23 22:32:41.411
------------------------------
• [4.103 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:37.321
    Jan  4 22:32:37.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 22:32:37.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:37.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:37.346
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/04/23 22:32:37.348
    Jan  4 22:32:37.358: INFO: Waiting up to 5m0s for pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb" in namespace "emptydir-3926" to be "Succeeded or Failed"
    Jan  4 22:32:37.371: INFO: Pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.605259ms
    Jan  4 22:32:39.376: INFO: Pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018183002s
    Jan  4 22:32:41.376: INFO: Pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018238053s
    STEP: Saw pod success 01/04/23 22:32:41.376
    Jan  4 22:32:41.377: INFO: Pod "pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb" satisfied condition "Succeeded or Failed"
    Jan  4 22:32:41.380: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb container test-container: <nil>
    STEP: delete the pod 01/04/23 22:32:41.387
    Jan  4 22:32:41.401: INFO: Waiting for pod pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb to disappear
    Jan  4 22:32:41.405: INFO: Pod pod-8bb29ded-3a76-44a8-a67f-82e6fb4b1bcb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:41.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3926" for this suite. 01/04/23 22:32:41.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:41.429
Jan  4 22:32:41.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:32:41.431
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:41.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:41.452
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-8091462b-d73f-421a-9252-b5e7fca19904 01/04/23 22:32:41.454
STEP: Creating a pod to test consume secrets 01/04/23 22:32:41.459
Jan  4 22:32:41.466: INFO: Waiting up to 5m0s for pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62" in namespace "secrets-7785" to be "Succeeded or Failed"
Jan  4 22:32:41.469: INFO: Pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542476ms
Jan  4 22:32:43.474: INFO: Pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007724698s
Jan  4 22:32:45.475: INFO: Pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008408527s
STEP: Saw pod success 01/04/23 22:32:45.475
Jan  4 22:32:45.475: INFO: Pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62" satisfied condition "Succeeded or Failed"
Jan  4 22:32:45.481: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62 container secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:32:45.495
Jan  4 22:32:45.508: INFO: Waiting for pod pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62 to disappear
Jan  4 22:32:45.511: INFO: Pod pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:45.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7785" for this suite. 01/04/23 22:32:45.516
------------------------------
• [4.094 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:41.429
    Jan  4 22:32:41.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:32:41.431
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:41.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:41.452
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-8091462b-d73f-421a-9252-b5e7fca19904 01/04/23 22:32:41.454
    STEP: Creating a pod to test consume secrets 01/04/23 22:32:41.459
    Jan  4 22:32:41.466: INFO: Waiting up to 5m0s for pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62" in namespace "secrets-7785" to be "Succeeded or Failed"
    Jan  4 22:32:41.469: INFO: Pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542476ms
    Jan  4 22:32:43.474: INFO: Pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007724698s
    Jan  4 22:32:45.475: INFO: Pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008408527s
    STEP: Saw pod success 01/04/23 22:32:45.475
    Jan  4 22:32:45.475: INFO: Pod "pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62" satisfied condition "Succeeded or Failed"
    Jan  4 22:32:45.481: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62 container secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:32:45.495
    Jan  4 22:32:45.508: INFO: Waiting for pod pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62 to disappear
    Jan  4 22:32:45.511: INFO: Pod pod-secrets-dbe69112-aec3-4746-a94a-1d581d47da62 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:45.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7785" for this suite. 01/04/23 22:32:45.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:45.528
Jan  4 22:32:45.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename endpointslice 01/04/23 22:32:45.529
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:45.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:45.548
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/04/23 22:32:45.551
STEP: getting /apis/discovery.k8s.io 01/04/23 22:32:45.552
STEP: getting /apis/discovery.k8s.iov1 01/04/23 22:32:45.553
STEP: creating 01/04/23 22:32:45.554
STEP: getting 01/04/23 22:32:45.57
STEP: listing 01/04/23 22:32:45.572
STEP: watching 01/04/23 22:32:45.575
Jan  4 22:32:45.575: INFO: starting watch
STEP: cluster-wide listing 01/04/23 22:32:45.576
STEP: cluster-wide watching 01/04/23 22:32:45.584
Jan  4 22:32:45.584: INFO: starting watch
STEP: patching 01/04/23 22:32:45.585
STEP: updating 01/04/23 22:32:45.59
Jan  4 22:32:45.598: INFO: waiting for watch events with expected annotations
Jan  4 22:32:45.598: INFO: saw patched and updated annotations
STEP: deleting 01/04/23 22:32:45.598
STEP: deleting a collection 01/04/23 22:32:45.61
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:45.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6832" for this suite. 01/04/23 22:32:45.631
------------------------------
• [0.108 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:45.528
    Jan  4 22:32:45.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename endpointslice 01/04/23 22:32:45.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:45.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:45.548
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/04/23 22:32:45.551
    STEP: getting /apis/discovery.k8s.io 01/04/23 22:32:45.552
    STEP: getting /apis/discovery.k8s.iov1 01/04/23 22:32:45.553
    STEP: creating 01/04/23 22:32:45.554
    STEP: getting 01/04/23 22:32:45.57
    STEP: listing 01/04/23 22:32:45.572
    STEP: watching 01/04/23 22:32:45.575
    Jan  4 22:32:45.575: INFO: starting watch
    STEP: cluster-wide listing 01/04/23 22:32:45.576
    STEP: cluster-wide watching 01/04/23 22:32:45.584
    Jan  4 22:32:45.584: INFO: starting watch
    STEP: patching 01/04/23 22:32:45.585
    STEP: updating 01/04/23 22:32:45.59
    Jan  4 22:32:45.598: INFO: waiting for watch events with expected annotations
    Jan  4 22:32:45.598: INFO: saw patched and updated annotations
    STEP: deleting 01/04/23 22:32:45.598
    STEP: deleting a collection 01/04/23 22:32:45.61
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:45.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6832" for this suite. 01/04/23 22:32:45.631
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:45.638
Jan  4 22:32:45.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 22:32:45.638
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:45.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:45.659
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/04/23 22:32:45.662
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/04/23 22:32:45.663
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/04/23 22:32:45.663
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/04/23 22:32:45.663
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/04/23 22:32:45.664
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/04/23 22:32:45.665
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/04/23 22:32:45.666
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:45.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1515" for this suite. 01/04/23 22:32:45.671
------------------------------
• [0.045 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:45.638
    Jan  4 22:32:45.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 22:32:45.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:45.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:45.659
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/04/23 22:32:45.662
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/04/23 22:32:45.663
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/04/23 22:32:45.663
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/04/23 22:32:45.663
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/04/23 22:32:45.664
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/04/23 22:32:45.665
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/04/23 22:32:45.666
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:45.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1515" for this suite. 01/04/23 22:32:45.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:45.683
Jan  4 22:32:45.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 22:32:45.684
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:45.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:45.704
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/04/23 22:32:45.709
STEP: Creating a ResourceQuota 01/04/23 22:32:50.715
STEP: Ensuring resource quota status is calculated 01/04/23 22:32:50.725
STEP: Creating a ReplicaSet 01/04/23 22:32:52.729
STEP: Ensuring resource quota status captures replicaset creation 01/04/23 22:32:52.739
STEP: Deleting a ReplicaSet 01/04/23 22:32:54.743
STEP: Ensuring resource quota status released usage 01/04/23 22:32:54.748
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 22:32:56.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2328" for this suite. 01/04/23 22:32:56.776
------------------------------
• [SLOW TEST] [11.119 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:45.683
    Jan  4 22:32:45.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 22:32:45.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:45.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:45.704
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/04/23 22:32:45.709
    STEP: Creating a ResourceQuota 01/04/23 22:32:50.715
    STEP: Ensuring resource quota status is calculated 01/04/23 22:32:50.725
    STEP: Creating a ReplicaSet 01/04/23 22:32:52.729
    STEP: Ensuring resource quota status captures replicaset creation 01/04/23 22:32:52.739
    STEP: Deleting a ReplicaSet 01/04/23 22:32:54.743
    STEP: Ensuring resource quota status released usage 01/04/23 22:32:54.748
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:32:56.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2328" for this suite. 01/04/23 22:32:56.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:32:56.803
Jan  4 22:32:56.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:32:56.804
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:56.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:56.872
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:32:56.879
Jan  4 22:32:56.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37" in namespace "projected-399" to be "Succeeded or Failed"
Jan  4 22:32:56.936: INFO: Pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37": Phase="Pending", Reason="", readiness=false. Elapsed: 15.755753ms
Jan  4 22:32:58.940: INFO: Pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019640284s
Jan  4 22:33:00.940: INFO: Pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01995715s
STEP: Saw pod success 01/04/23 22:33:00.94
Jan  4 22:33:00.940: INFO: Pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37" satisfied condition "Succeeded or Failed"
Jan  4 22:33:00.943: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37 container client-container: <nil>
STEP: delete the pod 01/04/23 22:33:00.949
Jan  4 22:33:00.958: INFO: Waiting for pod downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37 to disappear
Jan  4 22:33:00.960: INFO: Pod downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 22:33:00.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-399" for this suite. 01/04/23 22:33:00.965
------------------------------
• [4.167 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:32:56.803
    Jan  4 22:32:56.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:32:56.804
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:32:56.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:32:56.872
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:32:56.879
    Jan  4 22:32:56.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37" in namespace "projected-399" to be "Succeeded or Failed"
    Jan  4 22:32:56.936: INFO: Pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37": Phase="Pending", Reason="", readiness=false. Elapsed: 15.755753ms
    Jan  4 22:32:58.940: INFO: Pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019640284s
    Jan  4 22:33:00.940: INFO: Pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01995715s
    STEP: Saw pod success 01/04/23 22:33:00.94
    Jan  4 22:33:00.940: INFO: Pod "downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37" satisfied condition "Succeeded or Failed"
    Jan  4 22:33:00.943: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37 container client-container: <nil>
    STEP: delete the pod 01/04/23 22:33:00.949
    Jan  4 22:33:00.958: INFO: Waiting for pod downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37 to disappear
    Jan  4 22:33:00.960: INFO: Pod downwardapi-volume-4938155d-1181-499e-b01c-ec06ae46fd37 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:33:00.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-399" for this suite. 01/04/23 22:33:00.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:33:00.972
Jan  4 22:33:00.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename gc 01/04/23 22:33:00.973
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:00.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:00.99
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/04/23 22:33:00.998
STEP: delete the rc 01/04/23 22:33:06.031
STEP: wait for the rc to be deleted 01/04/23 22:33:06.048
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/04/23 22:33:11.065
STEP: Gathering metrics 01/04/23 22:33:41.076
Jan  4 22:33:41.105: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
Jan  4 22:33:41.108: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.215919ms
Jan  4 22:33:41.108: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
Jan  4 22:33:41.108: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
Jan  4 22:33:41.169: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  4 22:33:41.170: INFO: Deleting pod "simpletest.rc-2d4xd" in namespace "gc-1043"
Jan  4 22:33:41.189: INFO: Deleting pod "simpletest.rc-2ljxs" in namespace "gc-1043"
Jan  4 22:33:41.226: INFO: Deleting pod "simpletest.rc-2msmw" in namespace "gc-1043"
Jan  4 22:33:41.272: INFO: Deleting pod "simpletest.rc-2qpxb" in namespace "gc-1043"
Jan  4 22:33:41.304: INFO: Deleting pod "simpletest.rc-46tw9" in namespace "gc-1043"
Jan  4 22:33:41.338: INFO: Deleting pod "simpletest.rc-48gp5" in namespace "gc-1043"
Jan  4 22:33:41.375: INFO: Deleting pod "simpletest.rc-4h49c" in namespace "gc-1043"
Jan  4 22:33:41.410: INFO: Deleting pod "simpletest.rc-4rnlg" in namespace "gc-1043"
Jan  4 22:33:41.480: INFO: Deleting pod "simpletest.rc-55lmq" in namespace "gc-1043"
Jan  4 22:33:41.514: INFO: Deleting pod "simpletest.rc-62fzj" in namespace "gc-1043"
Jan  4 22:33:41.545: INFO: Deleting pod "simpletest.rc-67zk2" in namespace "gc-1043"
Jan  4 22:33:41.560: INFO: Deleting pod "simpletest.rc-6g6ks" in namespace "gc-1043"
Jan  4 22:33:41.590: INFO: Deleting pod "simpletest.rc-6ns8r" in namespace "gc-1043"
Jan  4 22:33:41.628: INFO: Deleting pod "simpletest.rc-6pmqf" in namespace "gc-1043"
Jan  4 22:33:41.652: INFO: Deleting pod "simpletest.rc-6ptmr" in namespace "gc-1043"
Jan  4 22:33:41.683: INFO: Deleting pod "simpletest.rc-6swpj" in namespace "gc-1043"
Jan  4 22:33:41.712: INFO: Deleting pod "simpletest.rc-6w2z4" in namespace "gc-1043"
Jan  4 22:33:41.756: INFO: Deleting pod "simpletest.rc-7jfft" in namespace "gc-1043"
Jan  4 22:33:41.799: INFO: Deleting pod "simpletest.rc-7l6t7" in namespace "gc-1043"
Jan  4 22:33:41.851: INFO: Deleting pod "simpletest.rc-7s8dd" in namespace "gc-1043"
Jan  4 22:33:41.915: INFO: Deleting pod "simpletest.rc-877gf" in namespace "gc-1043"
Jan  4 22:33:41.953: INFO: Deleting pod "simpletest.rc-8nnrg" in namespace "gc-1043"
Jan  4 22:33:41.971: INFO: Deleting pod "simpletest.rc-8tt9s" in namespace "gc-1043"
Jan  4 22:33:41.991: INFO: Deleting pod "simpletest.rc-9k9nr" in namespace "gc-1043"
Jan  4 22:33:42.020: INFO: Deleting pod "simpletest.rc-9lff7" in namespace "gc-1043"
Jan  4 22:33:42.047: INFO: Deleting pod "simpletest.rc-9m9q8" in namespace "gc-1043"
Jan  4 22:33:42.092: INFO: Deleting pod "simpletest.rc-9ssqn" in namespace "gc-1043"
Jan  4 22:33:42.120: INFO: Deleting pod "simpletest.rc-9szg6" in namespace "gc-1043"
Jan  4 22:33:42.140: INFO: Deleting pod "simpletest.rc-b2lfl" in namespace "gc-1043"
Jan  4 22:33:42.163: INFO: Deleting pod "simpletest.rc-b4dj9" in namespace "gc-1043"
Jan  4 22:33:42.178: INFO: Deleting pod "simpletest.rc-b4wbt" in namespace "gc-1043"
Jan  4 22:33:42.201: INFO: Deleting pod "simpletest.rc-b6dhj" in namespace "gc-1043"
Jan  4 22:33:42.267: INFO: Deleting pod "simpletest.rc-bcvn9" in namespace "gc-1043"
Jan  4 22:33:42.296: INFO: Deleting pod "simpletest.rc-bdf4p" in namespace "gc-1043"
Jan  4 22:33:42.325: INFO: Deleting pod "simpletest.rc-bwsrc" in namespace "gc-1043"
Jan  4 22:33:42.340: INFO: Deleting pod "simpletest.rc-c76c5" in namespace "gc-1043"
Jan  4 22:33:42.371: INFO: Deleting pod "simpletest.rc-cbqqn" in namespace "gc-1043"
Jan  4 22:33:42.405: INFO: Deleting pod "simpletest.rc-cbwfk" in namespace "gc-1043"
Jan  4 22:33:42.427: INFO: Deleting pod "simpletest.rc-cnm7r" in namespace "gc-1043"
Jan  4 22:33:42.454: INFO: Deleting pod "simpletest.rc-cpbc2" in namespace "gc-1043"
Jan  4 22:33:42.481: INFO: Deleting pod "simpletest.rc-dg5bg" in namespace "gc-1043"
Jan  4 22:33:42.496: INFO: Deleting pod "simpletest.rc-dl2xh" in namespace "gc-1043"
Jan  4 22:33:42.506: INFO: Deleting pod "simpletest.rc-dm7lj" in namespace "gc-1043"
Jan  4 22:33:42.532: INFO: Deleting pod "simpletest.rc-dqzjb" in namespace "gc-1043"
Jan  4 22:33:42.552: INFO: Deleting pod "simpletest.rc-dxs26" in namespace "gc-1043"
Jan  4 22:33:42.574: INFO: Deleting pod "simpletest.rc-fh5vf" in namespace "gc-1043"
Jan  4 22:33:42.614: INFO: Deleting pod "simpletest.rc-fsmjl" in namespace "gc-1043"
Jan  4 22:33:42.644: INFO: Deleting pod "simpletest.rc-fzsx4" in namespace "gc-1043"
Jan  4 22:33:42.658: INFO: Deleting pod "simpletest.rc-g9l5n" in namespace "gc-1043"
Jan  4 22:33:42.680: INFO: Deleting pod "simpletest.rc-g9xth" in namespace "gc-1043"
Jan  4 22:33:42.709: INFO: Deleting pod "simpletest.rc-gn4b5" in namespace "gc-1043"
Jan  4 22:33:42.727: INFO: Deleting pod "simpletest.rc-h5knx" in namespace "gc-1043"
Jan  4 22:33:42.765: INFO: Deleting pod "simpletest.rc-h6nnc" in namespace "gc-1043"
Jan  4 22:33:42.794: INFO: Deleting pod "simpletest.rc-htp7d" in namespace "gc-1043"
Jan  4 22:33:42.821: INFO: Deleting pod "simpletest.rc-hvf89" in namespace "gc-1043"
Jan  4 22:33:42.834: INFO: Deleting pod "simpletest.rc-hwdq6" in namespace "gc-1043"
Jan  4 22:33:42.872: INFO: Deleting pod "simpletest.rc-j8c4t" in namespace "gc-1043"
Jan  4 22:33:42.884: INFO: Deleting pod "simpletest.rc-jd8nx" in namespace "gc-1043"
Jan  4 22:33:42.897: INFO: Deleting pod "simpletest.rc-jffvk" in namespace "gc-1043"
Jan  4 22:33:42.911: INFO: Deleting pod "simpletest.rc-jfrdv" in namespace "gc-1043"
Jan  4 22:33:42.923: INFO: Deleting pod "simpletest.rc-jktfp" in namespace "gc-1043"
Jan  4 22:33:42.940: INFO: Deleting pod "simpletest.rc-jlzxd" in namespace "gc-1043"
Jan  4 22:33:42.958: INFO: Deleting pod "simpletest.rc-k6zzq" in namespace "gc-1043"
Jan  4 22:33:42.994: INFO: Deleting pod "simpletest.rc-kfqln" in namespace "gc-1043"
Jan  4 22:33:43.050: INFO: Deleting pod "simpletest.rc-lm7ff" in namespace "gc-1043"
Jan  4 22:33:43.094: INFO: Deleting pod "simpletest.rc-lq56q" in namespace "gc-1043"
Jan  4 22:33:43.118: INFO: Deleting pod "simpletest.rc-lszwb" in namespace "gc-1043"
Jan  4 22:33:43.158: INFO: Deleting pod "simpletest.rc-m6jxf" in namespace "gc-1043"
Jan  4 22:33:43.216: INFO: Deleting pod "simpletest.rc-mlwg4" in namespace "gc-1043"
Jan  4 22:33:43.236: INFO: Deleting pod "simpletest.rc-nwshr" in namespace "gc-1043"
Jan  4 22:33:43.296: INFO: Deleting pod "simpletest.rc-nz2xt" in namespace "gc-1043"
Jan  4 22:33:43.307: INFO: Deleting pod "simpletest.rc-pftr9" in namespace "gc-1043"
Jan  4 22:33:43.323: INFO: Deleting pod "simpletest.rc-pgz77" in namespace "gc-1043"
Jan  4 22:33:43.337: INFO: Deleting pod "simpletest.rc-pxn4d" in namespace "gc-1043"
Jan  4 22:33:43.381: INFO: Deleting pod "simpletest.rc-qfg99" in namespace "gc-1043"
Jan  4 22:33:43.399: INFO: Deleting pod "simpletest.rc-qnb5q" in namespace "gc-1043"
Jan  4 22:33:43.498: INFO: Deleting pod "simpletest.rc-qskdf" in namespace "gc-1043"
Jan  4 22:33:43.514: INFO: Deleting pod "simpletest.rc-qv82g" in namespace "gc-1043"
Jan  4 22:33:43.533: INFO: Deleting pod "simpletest.rc-r4ks5" in namespace "gc-1043"
Jan  4 22:33:43.557: INFO: Deleting pod "simpletest.rc-r8djq" in namespace "gc-1043"
Jan  4 22:33:43.608: INFO: Deleting pod "simpletest.rc-rlsh8" in namespace "gc-1043"
Jan  4 22:33:43.625: INFO: Deleting pod "simpletest.rc-rz66h" in namespace "gc-1043"
Jan  4 22:33:43.645: INFO: Deleting pod "simpletest.rc-s9x6g" in namespace "gc-1043"
Jan  4 22:33:43.657: INFO: Deleting pod "simpletest.rc-skt4c" in namespace "gc-1043"
Jan  4 22:33:43.679: INFO: Deleting pod "simpletest.rc-sqcnw" in namespace "gc-1043"
Jan  4 22:33:43.701: INFO: Deleting pod "simpletest.rc-v49mw" in namespace "gc-1043"
Jan  4 22:33:43.727: INFO: Deleting pod "simpletest.rc-vmr4l" in namespace "gc-1043"
Jan  4 22:33:43.746: INFO: Deleting pod "simpletest.rc-vn6hv" in namespace "gc-1043"
Jan  4 22:33:43.769: INFO: Deleting pod "simpletest.rc-w5xrn" in namespace "gc-1043"
Jan  4 22:33:43.790: INFO: Deleting pod "simpletest.rc-w975d" in namespace "gc-1043"
Jan  4 22:33:43.811: INFO: Deleting pod "simpletest.rc-wbwsb" in namespace "gc-1043"
Jan  4 22:33:43.857: INFO: Deleting pod "simpletest.rc-ws8m2" in namespace "gc-1043"
Jan  4 22:33:43.889: INFO: Deleting pod "simpletest.rc-xkj2f" in namespace "gc-1043"
Jan  4 22:33:43.908: INFO: Deleting pod "simpletest.rc-xmg5x" in namespace "gc-1043"
Jan  4 22:33:43.928: INFO: Deleting pod "simpletest.rc-xpnk6" in namespace "gc-1043"
Jan  4 22:33:43.943: INFO: Deleting pod "simpletest.rc-xzbk9" in namespace "gc-1043"
Jan  4 22:33:43.959: INFO: Deleting pod "simpletest.rc-zf7rk" in namespace "gc-1043"
Jan  4 22:33:43.971: INFO: Deleting pod "simpletest.rc-zpk5r" in namespace "gc-1043"
Jan  4 22:33:43.987: INFO: Deleting pod "simpletest.rc-zt4d6" in namespace "gc-1043"
Jan  4 22:33:44.016: INFO: Deleting pod "simpletest.rc-zvksc" in namespace "gc-1043"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  4 22:33:44.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1043" for this suite. 01/04/23 22:33:44.033
------------------------------
• [SLOW TEST] [43.069 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:33:00.972
    Jan  4 22:33:00.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename gc 01/04/23 22:33:00.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:00.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:00.99
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/04/23 22:33:00.998
    STEP: delete the rc 01/04/23 22:33:06.031
    STEP: wait for the rc to be deleted 01/04/23 22:33:06.048
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/04/23 22:33:11.065
    STEP: Gathering metrics 01/04/23 22:33:41.076
    Jan  4 22:33:41.105: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Jan  4 22:33:41.108: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.215919ms
    Jan  4 22:33:41.108: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
    Jan  4 22:33:41.108: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
    Jan  4 22:33:41.169: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  4 22:33:41.170: INFO: Deleting pod "simpletest.rc-2d4xd" in namespace "gc-1043"
    Jan  4 22:33:41.189: INFO: Deleting pod "simpletest.rc-2ljxs" in namespace "gc-1043"
    Jan  4 22:33:41.226: INFO: Deleting pod "simpletest.rc-2msmw" in namespace "gc-1043"
    Jan  4 22:33:41.272: INFO: Deleting pod "simpletest.rc-2qpxb" in namespace "gc-1043"
    Jan  4 22:33:41.304: INFO: Deleting pod "simpletest.rc-46tw9" in namespace "gc-1043"
    Jan  4 22:33:41.338: INFO: Deleting pod "simpletest.rc-48gp5" in namespace "gc-1043"
    Jan  4 22:33:41.375: INFO: Deleting pod "simpletest.rc-4h49c" in namespace "gc-1043"
    Jan  4 22:33:41.410: INFO: Deleting pod "simpletest.rc-4rnlg" in namespace "gc-1043"
    Jan  4 22:33:41.480: INFO: Deleting pod "simpletest.rc-55lmq" in namespace "gc-1043"
    Jan  4 22:33:41.514: INFO: Deleting pod "simpletest.rc-62fzj" in namespace "gc-1043"
    Jan  4 22:33:41.545: INFO: Deleting pod "simpletest.rc-67zk2" in namespace "gc-1043"
    Jan  4 22:33:41.560: INFO: Deleting pod "simpletest.rc-6g6ks" in namespace "gc-1043"
    Jan  4 22:33:41.590: INFO: Deleting pod "simpletest.rc-6ns8r" in namespace "gc-1043"
    Jan  4 22:33:41.628: INFO: Deleting pod "simpletest.rc-6pmqf" in namespace "gc-1043"
    Jan  4 22:33:41.652: INFO: Deleting pod "simpletest.rc-6ptmr" in namespace "gc-1043"
    Jan  4 22:33:41.683: INFO: Deleting pod "simpletest.rc-6swpj" in namespace "gc-1043"
    Jan  4 22:33:41.712: INFO: Deleting pod "simpletest.rc-6w2z4" in namespace "gc-1043"
    Jan  4 22:33:41.756: INFO: Deleting pod "simpletest.rc-7jfft" in namespace "gc-1043"
    Jan  4 22:33:41.799: INFO: Deleting pod "simpletest.rc-7l6t7" in namespace "gc-1043"
    Jan  4 22:33:41.851: INFO: Deleting pod "simpletest.rc-7s8dd" in namespace "gc-1043"
    Jan  4 22:33:41.915: INFO: Deleting pod "simpletest.rc-877gf" in namespace "gc-1043"
    Jan  4 22:33:41.953: INFO: Deleting pod "simpletest.rc-8nnrg" in namespace "gc-1043"
    Jan  4 22:33:41.971: INFO: Deleting pod "simpletest.rc-8tt9s" in namespace "gc-1043"
    Jan  4 22:33:41.991: INFO: Deleting pod "simpletest.rc-9k9nr" in namespace "gc-1043"
    Jan  4 22:33:42.020: INFO: Deleting pod "simpletest.rc-9lff7" in namespace "gc-1043"
    Jan  4 22:33:42.047: INFO: Deleting pod "simpletest.rc-9m9q8" in namespace "gc-1043"
    Jan  4 22:33:42.092: INFO: Deleting pod "simpletest.rc-9ssqn" in namespace "gc-1043"
    Jan  4 22:33:42.120: INFO: Deleting pod "simpletest.rc-9szg6" in namespace "gc-1043"
    Jan  4 22:33:42.140: INFO: Deleting pod "simpletest.rc-b2lfl" in namespace "gc-1043"
    Jan  4 22:33:42.163: INFO: Deleting pod "simpletest.rc-b4dj9" in namespace "gc-1043"
    Jan  4 22:33:42.178: INFO: Deleting pod "simpletest.rc-b4wbt" in namespace "gc-1043"
    Jan  4 22:33:42.201: INFO: Deleting pod "simpletest.rc-b6dhj" in namespace "gc-1043"
    Jan  4 22:33:42.267: INFO: Deleting pod "simpletest.rc-bcvn9" in namespace "gc-1043"
    Jan  4 22:33:42.296: INFO: Deleting pod "simpletest.rc-bdf4p" in namespace "gc-1043"
    Jan  4 22:33:42.325: INFO: Deleting pod "simpletest.rc-bwsrc" in namespace "gc-1043"
    Jan  4 22:33:42.340: INFO: Deleting pod "simpletest.rc-c76c5" in namespace "gc-1043"
    Jan  4 22:33:42.371: INFO: Deleting pod "simpletest.rc-cbqqn" in namespace "gc-1043"
    Jan  4 22:33:42.405: INFO: Deleting pod "simpletest.rc-cbwfk" in namespace "gc-1043"
    Jan  4 22:33:42.427: INFO: Deleting pod "simpletest.rc-cnm7r" in namespace "gc-1043"
    Jan  4 22:33:42.454: INFO: Deleting pod "simpletest.rc-cpbc2" in namespace "gc-1043"
    Jan  4 22:33:42.481: INFO: Deleting pod "simpletest.rc-dg5bg" in namespace "gc-1043"
    Jan  4 22:33:42.496: INFO: Deleting pod "simpletest.rc-dl2xh" in namespace "gc-1043"
    Jan  4 22:33:42.506: INFO: Deleting pod "simpletest.rc-dm7lj" in namespace "gc-1043"
    Jan  4 22:33:42.532: INFO: Deleting pod "simpletest.rc-dqzjb" in namespace "gc-1043"
    Jan  4 22:33:42.552: INFO: Deleting pod "simpletest.rc-dxs26" in namespace "gc-1043"
    Jan  4 22:33:42.574: INFO: Deleting pod "simpletest.rc-fh5vf" in namespace "gc-1043"
    Jan  4 22:33:42.614: INFO: Deleting pod "simpletest.rc-fsmjl" in namespace "gc-1043"
    Jan  4 22:33:42.644: INFO: Deleting pod "simpletest.rc-fzsx4" in namespace "gc-1043"
    Jan  4 22:33:42.658: INFO: Deleting pod "simpletest.rc-g9l5n" in namespace "gc-1043"
    Jan  4 22:33:42.680: INFO: Deleting pod "simpletest.rc-g9xth" in namespace "gc-1043"
    Jan  4 22:33:42.709: INFO: Deleting pod "simpletest.rc-gn4b5" in namespace "gc-1043"
    Jan  4 22:33:42.727: INFO: Deleting pod "simpletest.rc-h5knx" in namespace "gc-1043"
    Jan  4 22:33:42.765: INFO: Deleting pod "simpletest.rc-h6nnc" in namespace "gc-1043"
    Jan  4 22:33:42.794: INFO: Deleting pod "simpletest.rc-htp7d" in namespace "gc-1043"
    Jan  4 22:33:42.821: INFO: Deleting pod "simpletest.rc-hvf89" in namespace "gc-1043"
    Jan  4 22:33:42.834: INFO: Deleting pod "simpletest.rc-hwdq6" in namespace "gc-1043"
    Jan  4 22:33:42.872: INFO: Deleting pod "simpletest.rc-j8c4t" in namespace "gc-1043"
    Jan  4 22:33:42.884: INFO: Deleting pod "simpletest.rc-jd8nx" in namespace "gc-1043"
    Jan  4 22:33:42.897: INFO: Deleting pod "simpletest.rc-jffvk" in namespace "gc-1043"
    Jan  4 22:33:42.911: INFO: Deleting pod "simpletest.rc-jfrdv" in namespace "gc-1043"
    Jan  4 22:33:42.923: INFO: Deleting pod "simpletest.rc-jktfp" in namespace "gc-1043"
    Jan  4 22:33:42.940: INFO: Deleting pod "simpletest.rc-jlzxd" in namespace "gc-1043"
    Jan  4 22:33:42.958: INFO: Deleting pod "simpletest.rc-k6zzq" in namespace "gc-1043"
    Jan  4 22:33:42.994: INFO: Deleting pod "simpletest.rc-kfqln" in namespace "gc-1043"
    Jan  4 22:33:43.050: INFO: Deleting pod "simpletest.rc-lm7ff" in namespace "gc-1043"
    Jan  4 22:33:43.094: INFO: Deleting pod "simpletest.rc-lq56q" in namespace "gc-1043"
    Jan  4 22:33:43.118: INFO: Deleting pod "simpletest.rc-lszwb" in namespace "gc-1043"
    Jan  4 22:33:43.158: INFO: Deleting pod "simpletest.rc-m6jxf" in namespace "gc-1043"
    Jan  4 22:33:43.216: INFO: Deleting pod "simpletest.rc-mlwg4" in namespace "gc-1043"
    Jan  4 22:33:43.236: INFO: Deleting pod "simpletest.rc-nwshr" in namespace "gc-1043"
    Jan  4 22:33:43.296: INFO: Deleting pod "simpletest.rc-nz2xt" in namespace "gc-1043"
    Jan  4 22:33:43.307: INFO: Deleting pod "simpletest.rc-pftr9" in namespace "gc-1043"
    Jan  4 22:33:43.323: INFO: Deleting pod "simpletest.rc-pgz77" in namespace "gc-1043"
    Jan  4 22:33:43.337: INFO: Deleting pod "simpletest.rc-pxn4d" in namespace "gc-1043"
    Jan  4 22:33:43.381: INFO: Deleting pod "simpletest.rc-qfg99" in namespace "gc-1043"
    Jan  4 22:33:43.399: INFO: Deleting pod "simpletest.rc-qnb5q" in namespace "gc-1043"
    Jan  4 22:33:43.498: INFO: Deleting pod "simpletest.rc-qskdf" in namespace "gc-1043"
    Jan  4 22:33:43.514: INFO: Deleting pod "simpletest.rc-qv82g" in namespace "gc-1043"
    Jan  4 22:33:43.533: INFO: Deleting pod "simpletest.rc-r4ks5" in namespace "gc-1043"
    Jan  4 22:33:43.557: INFO: Deleting pod "simpletest.rc-r8djq" in namespace "gc-1043"
    Jan  4 22:33:43.608: INFO: Deleting pod "simpletest.rc-rlsh8" in namespace "gc-1043"
    Jan  4 22:33:43.625: INFO: Deleting pod "simpletest.rc-rz66h" in namespace "gc-1043"
    Jan  4 22:33:43.645: INFO: Deleting pod "simpletest.rc-s9x6g" in namespace "gc-1043"
    Jan  4 22:33:43.657: INFO: Deleting pod "simpletest.rc-skt4c" in namespace "gc-1043"
    Jan  4 22:33:43.679: INFO: Deleting pod "simpletest.rc-sqcnw" in namespace "gc-1043"
    Jan  4 22:33:43.701: INFO: Deleting pod "simpletest.rc-v49mw" in namespace "gc-1043"
    Jan  4 22:33:43.727: INFO: Deleting pod "simpletest.rc-vmr4l" in namespace "gc-1043"
    Jan  4 22:33:43.746: INFO: Deleting pod "simpletest.rc-vn6hv" in namespace "gc-1043"
    Jan  4 22:33:43.769: INFO: Deleting pod "simpletest.rc-w5xrn" in namespace "gc-1043"
    Jan  4 22:33:43.790: INFO: Deleting pod "simpletest.rc-w975d" in namespace "gc-1043"
    Jan  4 22:33:43.811: INFO: Deleting pod "simpletest.rc-wbwsb" in namespace "gc-1043"
    Jan  4 22:33:43.857: INFO: Deleting pod "simpletest.rc-ws8m2" in namespace "gc-1043"
    Jan  4 22:33:43.889: INFO: Deleting pod "simpletest.rc-xkj2f" in namespace "gc-1043"
    Jan  4 22:33:43.908: INFO: Deleting pod "simpletest.rc-xmg5x" in namespace "gc-1043"
    Jan  4 22:33:43.928: INFO: Deleting pod "simpletest.rc-xpnk6" in namespace "gc-1043"
    Jan  4 22:33:43.943: INFO: Deleting pod "simpletest.rc-xzbk9" in namespace "gc-1043"
    Jan  4 22:33:43.959: INFO: Deleting pod "simpletest.rc-zf7rk" in namespace "gc-1043"
    Jan  4 22:33:43.971: INFO: Deleting pod "simpletest.rc-zpk5r" in namespace "gc-1043"
    Jan  4 22:33:43.987: INFO: Deleting pod "simpletest.rc-zt4d6" in namespace "gc-1043"
    Jan  4 22:33:44.016: INFO: Deleting pod "simpletest.rc-zvksc" in namespace "gc-1043"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:33:44.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1043" for this suite. 01/04/23 22:33:44.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:33:44.042
Jan  4 22:33:44.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 22:33:44.043
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:44.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:44.073
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/04/23 22:33:44.085
Jan  4 22:33:44.096: INFO: Waiting up to 5m0s for pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff" in namespace "emptydir-2409" to be "Succeeded or Failed"
Jan  4 22:33:44.107: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff": Phase="Pending", Reason="", readiness=false. Elapsed: 10.80301ms
Jan  4 22:33:46.116: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020333773s
Jan  4 22:33:48.110: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014484536s
Jan  4 22:33:50.112: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015925192s
STEP: Saw pod success 01/04/23 22:33:50.112
Jan  4 22:33:50.112: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff" satisfied condition "Succeeded or Failed"
Jan  4 22:33:50.115: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-6e3348a8-96af-4338-933c-05a96e4b91ff container test-container: <nil>
STEP: delete the pod 01/04/23 22:33:50.124
Jan  4 22:33:50.133: INFO: Waiting for pod pod-6e3348a8-96af-4338-933c-05a96e4b91ff to disappear
Jan  4 22:33:50.135: INFO: Pod pod-6e3348a8-96af-4338-933c-05a96e4b91ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 22:33:50.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2409" for this suite. 01/04/23 22:33:50.139
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:33:44.042
    Jan  4 22:33:44.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 22:33:44.043
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:44.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:44.073
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/04/23 22:33:44.085
    Jan  4 22:33:44.096: INFO: Waiting up to 5m0s for pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff" in namespace "emptydir-2409" to be "Succeeded or Failed"
    Jan  4 22:33:44.107: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff": Phase="Pending", Reason="", readiness=false. Elapsed: 10.80301ms
    Jan  4 22:33:46.116: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020333773s
    Jan  4 22:33:48.110: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014484536s
    Jan  4 22:33:50.112: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015925192s
    STEP: Saw pod success 01/04/23 22:33:50.112
    Jan  4 22:33:50.112: INFO: Pod "pod-6e3348a8-96af-4338-933c-05a96e4b91ff" satisfied condition "Succeeded or Failed"
    Jan  4 22:33:50.115: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-6e3348a8-96af-4338-933c-05a96e4b91ff container test-container: <nil>
    STEP: delete the pod 01/04/23 22:33:50.124
    Jan  4 22:33:50.133: INFO: Waiting for pod pod-6e3348a8-96af-4338-933c-05a96e4b91ff to disappear
    Jan  4 22:33:50.135: INFO: Pod pod-6e3348a8-96af-4338-933c-05a96e4b91ff no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:33:50.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2409" for this suite. 01/04/23 22:33:50.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:33:50.149
Jan  4 22:33:50.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:33:50.15
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:50.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:50.165
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:33:50.18
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:33:50.596
STEP: Deploying the webhook pod 01/04/23 22:33:50.603
STEP: Wait for the deployment to be ready 01/04/23 22:33:50.614
Jan  4 22:33:50.628: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 22:33:52.636
STEP: Verifying the service has paired with the endpoint 01/04/23 22:33:52.649
Jan  4 22:33:53.650: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/04/23 22:33:53.654
STEP: create a namespace for the webhook 01/04/23 22:33:53.676
STEP: create a configmap should be unconditionally rejected by the webhook 01/04/23 22:33:53.685
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:33:53.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2216" for this suite. 01/04/23 22:33:53.829
STEP: Destroying namespace "webhook-2216-markers" for this suite. 01/04/23 22:33:53.839
------------------------------
• [3.700 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:33:50.149
    Jan  4 22:33:50.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:33:50.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:50.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:50.165
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:33:50.18
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:33:50.596
    STEP: Deploying the webhook pod 01/04/23 22:33:50.603
    STEP: Wait for the deployment to be ready 01/04/23 22:33:50.614
    Jan  4 22:33:50.628: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 22:33:52.636
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:33:52.649
    Jan  4 22:33:53.650: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/04/23 22:33:53.654
    STEP: create a namespace for the webhook 01/04/23 22:33:53.676
    STEP: create a configmap should be unconditionally rejected by the webhook 01/04/23 22:33:53.685
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:33:53.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2216" for this suite. 01/04/23 22:33:53.829
    STEP: Destroying namespace "webhook-2216-markers" for this suite. 01/04/23 22:33:53.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:33:53.85
Jan  4 22:33:53.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 22:33:53.85
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:53.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:53.865
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/04/23 22:33:53.872
STEP: watching for Pod to be ready 01/04/23 22:33:53.88
Jan  4 22:33:53.881: INFO: observed Pod pod-test in namespace pods-2512 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan  4 22:33:53.884: INFO: observed Pod pod-test in namespace pods-2512 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  }]
Jan  4 22:33:53.898: INFO: observed Pod pod-test in namespace pods-2512 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  }]
Jan  4 22:33:54.326: INFO: observed Pod pod-test in namespace pods-2512 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  }]
Jan  4 22:33:55.253: INFO: Found Pod pod-test in namespace pods-2512 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/04/23 22:33:55.257
STEP: getting the Pod and ensuring that it's patched 01/04/23 22:33:55.267
STEP: replacing the Pod's status Ready condition to False 01/04/23 22:33:55.272
STEP: check the Pod again to ensure its Ready conditions are False 01/04/23 22:33:55.285
STEP: deleting the Pod via a Collection with a LabelSelector 01/04/23 22:33:55.286
STEP: watching for the Pod to be deleted 01/04/23 22:33:55.297
Jan  4 22:33:55.299: INFO: observed event type MODIFIED
Jan  4 22:33:57.269: INFO: observed event type MODIFIED
Jan  4 22:33:57.541: INFO: observed event type MODIFIED
Jan  4 22:33:58.266: INFO: observed event type MODIFIED
Jan  4 22:33:58.285: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 22:33:58.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2512" for this suite. 01/04/23 22:33:58.309
------------------------------
• [4.467 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:33:53.85
    Jan  4 22:33:53.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 22:33:53.85
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:53.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:53.865
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/04/23 22:33:53.872
    STEP: watching for Pod to be ready 01/04/23 22:33:53.88
    Jan  4 22:33:53.881: INFO: observed Pod pod-test in namespace pods-2512 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan  4 22:33:53.884: INFO: observed Pod pod-test in namespace pods-2512 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  }]
    Jan  4 22:33:53.898: INFO: observed Pod pod-test in namespace pods-2512 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  }]
    Jan  4 22:33:54.326: INFO: observed Pod pod-test in namespace pods-2512 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  }]
    Jan  4 22:33:55.253: INFO: Found Pod pod-test in namespace pods-2512 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 22:33:53 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/04/23 22:33:55.257
    STEP: getting the Pod and ensuring that it's patched 01/04/23 22:33:55.267
    STEP: replacing the Pod's status Ready condition to False 01/04/23 22:33:55.272
    STEP: check the Pod again to ensure its Ready conditions are False 01/04/23 22:33:55.285
    STEP: deleting the Pod via a Collection with a LabelSelector 01/04/23 22:33:55.286
    STEP: watching for the Pod to be deleted 01/04/23 22:33:55.297
    Jan  4 22:33:55.299: INFO: observed event type MODIFIED
    Jan  4 22:33:57.269: INFO: observed event type MODIFIED
    Jan  4 22:33:57.541: INFO: observed event type MODIFIED
    Jan  4 22:33:58.266: INFO: observed event type MODIFIED
    Jan  4 22:33:58.285: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:33:58.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2512" for this suite. 01/04/23 22:33:58.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:33:58.323
Jan  4 22:33:58.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 22:33:58.324
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:58.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:58.352
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:33:58.354
Jan  4 22:33:58.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333" in namespace "downward-api-2730" to be "Succeeded or Failed"
Jan  4 22:33:58.368: INFO: Pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831249ms
Jan  4 22:34:00.371: INFO: Pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006476927s
Jan  4 22:34:02.372: INFO: Pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006874411s
STEP: Saw pod success 01/04/23 22:34:02.372
Jan  4 22:34:02.372: INFO: Pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333" satisfied condition "Succeeded or Failed"
Jan  4 22:34:02.375: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333 container client-container: <nil>
STEP: delete the pod 01/04/23 22:34:02.384
Jan  4 22:34:02.399: INFO: Waiting for pod downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333 to disappear
Jan  4 22:34:02.406: INFO: Pod downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:02.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2730" for this suite. 01/04/23 22:34:02.416
------------------------------
• [4.104 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:33:58.323
    Jan  4 22:33:58.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 22:33:58.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:33:58.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:33:58.352
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:33:58.354
    Jan  4 22:33:58.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333" in namespace "downward-api-2730" to be "Succeeded or Failed"
    Jan  4 22:33:58.368: INFO: Pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831249ms
    Jan  4 22:34:00.371: INFO: Pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006476927s
    Jan  4 22:34:02.372: INFO: Pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006874411s
    STEP: Saw pod success 01/04/23 22:34:02.372
    Jan  4 22:34:02.372: INFO: Pod "downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333" satisfied condition "Succeeded or Failed"
    Jan  4 22:34:02.375: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333 container client-container: <nil>
    STEP: delete the pod 01/04/23 22:34:02.384
    Jan  4 22:34:02.399: INFO: Waiting for pod downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333 to disappear
    Jan  4 22:34:02.406: INFO: Pod downwardapi-volume-55a97925-bb1c-40bc-8087-a22ce9b8b333 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:02.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2730" for this suite. 01/04/23 22:34:02.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:02.427
Jan  4 22:34:02.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename security-context-test 01/04/23 22:34:02.428
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:02.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:02.449
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan  4 22:34:02.467: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2" in namespace "security-context-test-7269" to be "Succeeded or Failed"
Jan  4 22:34:02.474: INFO: Pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.052379ms
Jan  4 22:34:04.480: INFO: Pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012888701s
Jan  4 22:34:06.480: INFO: Pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013332664s
Jan  4 22:34:06.481: INFO: Pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2" satisfied condition "Succeeded or Failed"
Jan  4 22:34:06.487: INFO: Got logs for pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:06.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7269" for this suite. 01/04/23 22:34:06.491
------------------------------
• [4.069 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:02.427
    Jan  4 22:34:02.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename security-context-test 01/04/23 22:34:02.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:02.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:02.449
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan  4 22:34:02.467: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2" in namespace "security-context-test-7269" to be "Succeeded or Failed"
    Jan  4 22:34:02.474: INFO: Pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.052379ms
    Jan  4 22:34:04.480: INFO: Pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012888701s
    Jan  4 22:34:06.480: INFO: Pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013332664s
    Jan  4 22:34:06.481: INFO: Pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2" satisfied condition "Succeeded or Failed"
    Jan  4 22:34:06.487: INFO: Got logs for pod "busybox-privileged-false-b8cf8f6d-d2cd-4fdb-adac-e93fc232d7b2": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:06.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7269" for this suite. 01/04/23 22:34:06.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:06.501
Jan  4 22:34:06.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubelet-test 01/04/23 22:34:06.502
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:06.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:06.525
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan  4 22:34:06.536: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16" in namespace "kubelet-test-3284" to be "running and ready"
Jan  4 22:34:06.539: INFO: Pod "busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525642ms
Jan  4 22:34:06.540: INFO: The phase of Pod busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:34:08.544: INFO: Pod "busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16": Phase="Running", Reason="", readiness=true. Elapsed: 2.008320937s
Jan  4 22:34:08.544: INFO: The phase of Pod busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16 is Running (Ready = true)
Jan  4 22:34:08.544: INFO: Pod "busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3284" for this suite. 01/04/23 22:34:08.58
------------------------------
• [2.087 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:06.501
    Jan  4 22:34:06.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubelet-test 01/04/23 22:34:06.502
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:06.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:06.525
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan  4 22:34:06.536: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16" in namespace "kubelet-test-3284" to be "running and ready"
    Jan  4 22:34:06.539: INFO: Pod "busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525642ms
    Jan  4 22:34:06.540: INFO: The phase of Pod busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:34:08.544: INFO: Pod "busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16": Phase="Running", Reason="", readiness=true. Elapsed: 2.008320937s
    Jan  4 22:34:08.544: INFO: The phase of Pod busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16 is Running (Ready = true)
    Jan  4 22:34:08.544: INFO: Pod "busybox-scheduling-3b7458fe-4f78-4ce1-ad09-6216899c6b16" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3284" for this suite. 01/04/23 22:34:08.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:08.588
Jan  4 22:34:08.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename endpointslicemirroring 01/04/23 22:34:08.59
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:08.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:08.606
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/04/23 22:34:08.62
Jan  4 22:34:08.632: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/04/23 22:34:10.639
STEP: mirroring deletion of a custom Endpoint 01/04/23 22:34:10.654
Jan  4 22:34:10.671: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:12.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-5662" for this suite. 01/04/23 22:34:12.681
------------------------------
• [4.098 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:08.588
    Jan  4 22:34:08.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename endpointslicemirroring 01/04/23 22:34:08.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:08.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:08.606
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/04/23 22:34:08.62
    Jan  4 22:34:08.632: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/04/23 22:34:10.639
    STEP: mirroring deletion of a custom Endpoint 01/04/23 22:34:10.654
    Jan  4 22:34:10.671: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:12.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-5662" for this suite. 01/04/23 22:34:12.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:12.688
Jan  4 22:34:12.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:34:12.689
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:12.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:12.705
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-fef9fdb8-b314-418a-8ec6-8a7b5e692f1f 01/04/23 22:34:12.707
STEP: Creating a pod to test consume secrets 01/04/23 22:34:12.711
Jan  4 22:34:12.718: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72" in namespace "projected-5343" to be "Succeeded or Failed"
Jan  4 22:34:12.720: INFO: Pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.272119ms
Jan  4 22:34:14.724: INFO: Pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006004753s
Jan  4 22:34:16.731: INFO: Pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013195726s
STEP: Saw pod success 01/04/23 22:34:16.731
Jan  4 22:34:16.731: INFO: Pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72" satisfied condition "Succeeded or Failed"
Jan  4 22:34:16.743: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:34:16.767
Jan  4 22:34:16.810: INFO: Waiting for pod pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72 to disappear
Jan  4 22:34:16.830: INFO: Pod pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:16.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5343" for this suite. 01/04/23 22:34:16.849
------------------------------
• [4.180 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:12.688
    Jan  4 22:34:12.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:34:12.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:12.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:12.705
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-fef9fdb8-b314-418a-8ec6-8a7b5e692f1f 01/04/23 22:34:12.707
    STEP: Creating a pod to test consume secrets 01/04/23 22:34:12.711
    Jan  4 22:34:12.718: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72" in namespace "projected-5343" to be "Succeeded or Failed"
    Jan  4 22:34:12.720: INFO: Pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.272119ms
    Jan  4 22:34:14.724: INFO: Pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006004753s
    Jan  4 22:34:16.731: INFO: Pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013195726s
    STEP: Saw pod success 01/04/23 22:34:16.731
    Jan  4 22:34:16.731: INFO: Pod "pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72" satisfied condition "Succeeded or Failed"
    Jan  4 22:34:16.743: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:34:16.767
    Jan  4 22:34:16.810: INFO: Waiting for pod pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72 to disappear
    Jan  4 22:34:16.830: INFO: Pod pod-projected-secrets-fdf0a365-fe08-44ca-b67d-f21e4c3a7c72 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:16.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5343" for this suite. 01/04/23 22:34:16.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:16.87
Jan  4 22:34:16.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:34:16.871
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:16.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:16.92
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-b21867fb-5982-4341-b3a2-f99a349f8f8c 01/04/23 22:34:16.927
STEP: Creating a pod to test consume configMaps 01/04/23 22:34:16.932
Jan  4 22:34:16.941: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1" in namespace "projected-4840" to be "Succeeded or Failed"
Jan  4 22:34:16.945: INFO: Pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.775551ms
Jan  4 22:34:18.949: INFO: Pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007938215s
Jan  4 22:34:20.948: INFO: Pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007027696s
STEP: Saw pod success 01/04/23 22:34:20.949
Jan  4 22:34:20.949: INFO: Pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1" satisfied condition "Succeeded or Failed"
Jan  4 22:34:20.951: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/04/23 22:34:20.958
Jan  4 22:34:20.970: INFO: Waiting for pod pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1 to disappear
Jan  4 22:34:20.973: INFO: Pod pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:20.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4840" for this suite. 01/04/23 22:34:20.979
------------------------------
• [4.116 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:16.87
    Jan  4 22:34:16.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:34:16.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:16.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:16.92
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-b21867fb-5982-4341-b3a2-f99a349f8f8c 01/04/23 22:34:16.927
    STEP: Creating a pod to test consume configMaps 01/04/23 22:34:16.932
    Jan  4 22:34:16.941: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1" in namespace "projected-4840" to be "Succeeded or Failed"
    Jan  4 22:34:16.945: INFO: Pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.775551ms
    Jan  4 22:34:18.949: INFO: Pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007938215s
    Jan  4 22:34:20.948: INFO: Pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007027696s
    STEP: Saw pod success 01/04/23 22:34:20.949
    Jan  4 22:34:20.949: INFO: Pod "pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1" satisfied condition "Succeeded or Failed"
    Jan  4 22:34:20.951: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:34:20.958
    Jan  4 22:34:20.970: INFO: Waiting for pod pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1 to disappear
    Jan  4 22:34:20.973: INFO: Pod pod-projected-configmaps-9aeb32e4-63cb-4c89-aed6-13e84e0d01a1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:20.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4840" for this suite. 01/04/23 22:34:20.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:20.998
Jan  4 22:34:20.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:34:21.001
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:21.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:21.02
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-0566caae-b5b5-47bd-8703-3877fc762fdc 01/04/23 22:34:21.023
STEP: Creating a pod to test consume secrets 01/04/23 22:34:21.027
Jan  4 22:34:21.036: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3" in namespace "projected-2278" to be "Succeeded or Failed"
Jan  4 22:34:21.040: INFO: Pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.862632ms
Jan  4 22:34:23.045: INFO: Pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008995454s
Jan  4 22:34:25.045: INFO: Pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009268694s
STEP: Saw pod success 01/04/23 22:34:25.045
Jan  4 22:34:25.045: INFO: Pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3" satisfied condition "Succeeded or Failed"
Jan  4 22:34:25.048: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:34:25.057
Jan  4 22:34:25.072: INFO: Waiting for pod pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3 to disappear
Jan  4 22:34:25.075: INFO: Pod pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:25.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2278" for this suite. 01/04/23 22:34:25.079
------------------------------
• [4.086 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:20.998
    Jan  4 22:34:20.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:34:21.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:21.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:21.02
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-0566caae-b5b5-47bd-8703-3877fc762fdc 01/04/23 22:34:21.023
    STEP: Creating a pod to test consume secrets 01/04/23 22:34:21.027
    Jan  4 22:34:21.036: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3" in namespace "projected-2278" to be "Succeeded or Failed"
    Jan  4 22:34:21.040: INFO: Pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.862632ms
    Jan  4 22:34:23.045: INFO: Pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008995454s
    Jan  4 22:34:25.045: INFO: Pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009268694s
    STEP: Saw pod success 01/04/23 22:34:25.045
    Jan  4 22:34:25.045: INFO: Pod "pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3" satisfied condition "Succeeded or Failed"
    Jan  4 22:34:25.048: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:34:25.057
    Jan  4 22:34:25.072: INFO: Waiting for pod pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3 to disappear
    Jan  4 22:34:25.075: INFO: Pod pod-projected-secrets-709f182f-fdf5-4cdf-9685-2c053ddfd6a3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:25.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2278" for this suite. 01/04/23 22:34:25.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:25.087
Jan  4 22:34:25.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:34:25.088
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:25.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:25.111
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-437664b5-d390-439d-90bf-3fb24f7d79b5 01/04/23 22:34:25.114
STEP: Creating a pod to test consume secrets 01/04/23 22:34:25.124
Jan  4 22:34:25.135: INFO: Waiting up to 5m0s for pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee" in namespace "secrets-2186" to be "Succeeded or Failed"
Jan  4 22:34:25.143: INFO: Pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.974392ms
Jan  4 22:34:27.147: INFO: Pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011245072s
Jan  4 22:34:29.149: INFO: Pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012948307s
STEP: Saw pod success 01/04/23 22:34:29.149
Jan  4 22:34:29.149: INFO: Pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee" satisfied condition "Succeeded or Failed"
Jan  4 22:34:29.151: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee container secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:34:29.158
Jan  4 22:34:29.177: INFO: Waiting for pod pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee to disappear
Jan  4 22:34:29.180: INFO: Pod pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:29.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2186" for this suite. 01/04/23 22:34:29.184
------------------------------
• [4.102 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:25.087
    Jan  4 22:34:25.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:34:25.088
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:25.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:25.111
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-437664b5-d390-439d-90bf-3fb24f7d79b5 01/04/23 22:34:25.114
    STEP: Creating a pod to test consume secrets 01/04/23 22:34:25.124
    Jan  4 22:34:25.135: INFO: Waiting up to 5m0s for pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee" in namespace "secrets-2186" to be "Succeeded or Failed"
    Jan  4 22:34:25.143: INFO: Pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.974392ms
    Jan  4 22:34:27.147: INFO: Pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011245072s
    Jan  4 22:34:29.149: INFO: Pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012948307s
    STEP: Saw pod success 01/04/23 22:34:29.149
    Jan  4 22:34:29.149: INFO: Pod "pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee" satisfied condition "Succeeded or Failed"
    Jan  4 22:34:29.151: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee container secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:34:29.158
    Jan  4 22:34:29.177: INFO: Waiting for pod pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee to disappear
    Jan  4 22:34:29.180: INFO: Pod pod-secrets-2973c184-40cb-45c3-bb8e-39ecabb318ee no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:29.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2186" for this suite. 01/04/23 22:34:29.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:29.193
Jan  4 22:34:29.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:34:29.196
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:29.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:29.221
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:34:29.224
Jan  4 22:34:29.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b" in namespace "projected-2948" to be "Succeeded or Failed"
Jan  4 22:34:29.239: INFO: Pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822965ms
Jan  4 22:34:31.244: INFO: Pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01033045s
Jan  4 22:34:33.244: INFO: Pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010793247s
STEP: Saw pod success 01/04/23 22:34:33.244
Jan  4 22:34:33.245: INFO: Pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b" satisfied condition "Succeeded or Failed"
Jan  4 22:34:33.248: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b container client-container: <nil>
STEP: delete the pod 01/04/23 22:34:33.255
Jan  4 22:34:33.266: INFO: Waiting for pod downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b to disappear
Jan  4 22:34:33.268: INFO: Pod downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:33.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2948" for this suite. 01/04/23 22:34:33.273
------------------------------
• [4.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:29.193
    Jan  4 22:34:29.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:34:29.196
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:29.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:29.221
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:34:29.224
    Jan  4 22:34:29.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b" in namespace "projected-2948" to be "Succeeded or Failed"
    Jan  4 22:34:29.239: INFO: Pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822965ms
    Jan  4 22:34:31.244: INFO: Pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01033045s
    Jan  4 22:34:33.244: INFO: Pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010793247s
    STEP: Saw pod success 01/04/23 22:34:33.244
    Jan  4 22:34:33.245: INFO: Pod "downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b" satisfied condition "Succeeded or Failed"
    Jan  4 22:34:33.248: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b container client-container: <nil>
    STEP: delete the pod 01/04/23 22:34:33.255
    Jan  4 22:34:33.266: INFO: Waiting for pod downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b to disappear
    Jan  4 22:34:33.268: INFO: Pod downwardapi-volume-aa6aff30-b742-44a9-8824-25d74a68051b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:33.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2948" for this suite. 01/04/23 22:34:33.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:33.284
Jan  4 22:34:33.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:34:33.285
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:33.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:33.302
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-d9ff97d4-0e40-4d27-a23c-5f2db422657f 01/04/23 22:34:33.304
STEP: Creating a pod to test consume secrets 01/04/23 22:34:33.311
Jan  4 22:34:33.322: INFO: Waiting up to 5m0s for pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149" in namespace "secrets-2128" to be "Succeeded or Failed"
Jan  4 22:34:33.327: INFO: Pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149": Phase="Pending", Reason="", readiness=false. Elapsed: 4.873793ms
Jan  4 22:34:35.332: INFO: Pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009797564s
Jan  4 22:34:37.330: INFO: Pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008181927s
STEP: Saw pod success 01/04/23 22:34:37.33
Jan  4 22:34:37.330: INFO: Pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149" satisfied condition "Succeeded or Failed"
Jan  4 22:34:37.333: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149 container secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:34:37.339
Jan  4 22:34:37.349: INFO: Waiting for pod pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149 to disappear
Jan  4 22:34:37.353: INFO: Pod pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:34:37.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2128" for this suite. 01/04/23 22:34:37.357
------------------------------
• [4.085 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:33.284
    Jan  4 22:34:33.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:34:33.285
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:33.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:33.302
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-d9ff97d4-0e40-4d27-a23c-5f2db422657f 01/04/23 22:34:33.304
    STEP: Creating a pod to test consume secrets 01/04/23 22:34:33.311
    Jan  4 22:34:33.322: INFO: Waiting up to 5m0s for pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149" in namespace "secrets-2128" to be "Succeeded or Failed"
    Jan  4 22:34:33.327: INFO: Pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149": Phase="Pending", Reason="", readiness=false. Elapsed: 4.873793ms
    Jan  4 22:34:35.332: INFO: Pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009797564s
    Jan  4 22:34:37.330: INFO: Pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008181927s
    STEP: Saw pod success 01/04/23 22:34:37.33
    Jan  4 22:34:37.330: INFO: Pod "pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149" satisfied condition "Succeeded or Failed"
    Jan  4 22:34:37.333: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149 container secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:34:37.339
    Jan  4 22:34:37.349: INFO: Waiting for pod pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149 to disappear
    Jan  4 22:34:37.353: INFO: Pod pod-secrets-1c97f106-fed8-4df0-aa98-16f485359149 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:34:37.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2128" for this suite. 01/04/23 22:34:37.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:34:37.37
Jan  4 22:34:37.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-probe 01/04/23 22:34:37.371
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:37.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:37.391
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe in namespace container-probe-3210 01/04/23 22:34:37.395
Jan  4 22:34:37.422: INFO: Waiting up to 5m0s for pod "liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe" in namespace "container-probe-3210" to be "not pending"
Jan  4 22:34:37.430: INFO: Pod "liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016844ms
Jan  4 22:34:39.433: INFO: Pod "liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.011826116s
Jan  4 22:34:39.434: INFO: Pod "liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe" satisfied condition "not pending"
Jan  4 22:34:39.434: INFO: Started pod liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe in namespace container-probe-3210
STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 22:34:39.434
Jan  4 22:34:39.436: INFO: Initial restart count of pod liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe is 0
STEP: deleting the pod 01/04/23 22:38:39.979
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  4 22:38:39.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3210" for this suite. 01/04/23 22:38:40.008
------------------------------
• [SLOW TEST] [242.654 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:34:37.37
    Jan  4 22:34:37.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-probe 01/04/23 22:34:37.371
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:34:37.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:34:37.391
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe in namespace container-probe-3210 01/04/23 22:34:37.395
    Jan  4 22:34:37.422: INFO: Waiting up to 5m0s for pod "liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe" in namespace "container-probe-3210" to be "not pending"
    Jan  4 22:34:37.430: INFO: Pod "liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016844ms
    Jan  4 22:34:39.433: INFO: Pod "liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.011826116s
    Jan  4 22:34:39.434: INFO: Pod "liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe" satisfied condition "not pending"
    Jan  4 22:34:39.434: INFO: Started pod liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe in namespace container-probe-3210
    STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 22:34:39.434
    Jan  4 22:34:39.436: INFO: Initial restart count of pod liveness-433733ef-bb7a-4b37-9cb3-5867a3363dbe is 0
    STEP: deleting the pod 01/04/23 22:38:39.979
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:38:39.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3210" for this suite. 01/04/23 22:38:40.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:38:40.029
Jan  4 22:38:40.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pod-network-test 01/04/23 22:38:40.031
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:38:40.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:38:40.048
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-264 01/04/23 22:38:40.056
STEP: creating a selector 01/04/23 22:38:40.056
STEP: Creating the service pods in kubernetes 01/04/23 22:38:40.056
Jan  4 22:38:40.057: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  4 22:38:40.110: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-264" to be "running and ready"
Jan  4 22:38:40.121: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.38105ms
Jan  4 22:38:40.121: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:38:42.124: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013880745s
Jan  4 22:38:42.124: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:38:44.125: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015098532s
Jan  4 22:38:44.125: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:38:46.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015054262s
Jan  4 22:38:46.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:38:48.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014987767s
Jan  4 22:38:48.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:38:50.124: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013989983s
Jan  4 22:38:50.124: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:38:52.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015381918s
Jan  4 22:38:52.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:38:54.124: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01379854s
Jan  4 22:38:54.124: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:38:56.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015322374s
Jan  4 22:38:56.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:38:58.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015713151s
Jan  4 22:38:58.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:39:00.128: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017994957s
Jan  4 22:39:00.128: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:39:02.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015511256s
Jan  4 22:39:02.126: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  4 22:39:02.126: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  4 22:39:02.129: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-264" to be "running and ready"
Jan  4 22:39:02.131: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.633127ms
Jan  4 22:39:02.132: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  4 22:39:02.132: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  4 22:39:02.134: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-264" to be "running and ready"
Jan  4 22:39:02.137: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.896998ms
Jan  4 22:39:02.137: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  4 22:39:02.137: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan  4 22:39:02.140: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-264" to be "running and ready"
Jan  4 22:39:02.144: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.509442ms
Jan  4 22:39:02.144: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan  4 22:39:02.144: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 01/04/23 22:39:02.147
Jan  4 22:39:02.165: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-264" to be "running"
Jan  4 22:39:02.174: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.886431ms
Jan  4 22:39:04.188: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023069103s
Jan  4 22:39:04.188: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  4 22:39:04.192: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-264" to be "running"
Jan  4 22:39:04.195: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.142299ms
Jan  4 22:39:04.195: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  4 22:39:04.198: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan  4 22:39:04.198: INFO: Going to poll 10.42.0.65 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan  4 22:39:04.201: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.65:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:39:04.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:39:04.201: INFO: ExecWithOptions: Clientset creation
Jan  4 22:39:04.201: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.0.65%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 22:39:04.285: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  4 22:39:04.285: INFO: Going to poll 10.42.3.132 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan  4 22:39:04.289: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.3.132:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:39:04.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:39:04.290: INFO: ExecWithOptions: Clientset creation
Jan  4 22:39:04.290: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.3.132%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 22:39:04.387: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  4 22:39:04.387: INFO: Going to poll 10.42.1.61 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan  4 22:39:04.391: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.1.61:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:39:04.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:39:04.392: INFO: ExecWithOptions: Clientset creation
Jan  4 22:39:04.392: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.1.61%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 22:39:04.472: INFO: Found all 1 expected endpoints: [netserver-2]
Jan  4 22:39:04.472: INFO: Going to poll 10.42.2.66 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan  4 22:39:04.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.2.66:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:39:04.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:39:04.476: INFO: ExecWithOptions: Clientset creation
Jan  4 22:39:04.477: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.2.66%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 22:39:04.559: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:04.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-264" for this suite. 01/04/23 22:39:04.564
------------------------------
• [SLOW TEST] [24.542 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:38:40.029
    Jan  4 22:38:40.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pod-network-test 01/04/23 22:38:40.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:38:40.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:38:40.048
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-264 01/04/23 22:38:40.056
    STEP: creating a selector 01/04/23 22:38:40.056
    STEP: Creating the service pods in kubernetes 01/04/23 22:38:40.056
    Jan  4 22:38:40.057: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  4 22:38:40.110: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-264" to be "running and ready"
    Jan  4 22:38:40.121: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.38105ms
    Jan  4 22:38:40.121: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:38:42.124: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013880745s
    Jan  4 22:38:42.124: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:38:44.125: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015098532s
    Jan  4 22:38:44.125: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:38:46.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015054262s
    Jan  4 22:38:46.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:38:48.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014987767s
    Jan  4 22:38:48.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:38:50.124: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013989983s
    Jan  4 22:38:50.124: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:38:52.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015381918s
    Jan  4 22:38:52.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:38:54.124: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01379854s
    Jan  4 22:38:54.124: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:38:56.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015322374s
    Jan  4 22:38:56.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:38:58.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015713151s
    Jan  4 22:38:58.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:39:00.128: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017994957s
    Jan  4 22:39:00.128: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:39:02.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015511256s
    Jan  4 22:39:02.126: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  4 22:39:02.126: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  4 22:39:02.129: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-264" to be "running and ready"
    Jan  4 22:39:02.131: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.633127ms
    Jan  4 22:39:02.132: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  4 22:39:02.132: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  4 22:39:02.134: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-264" to be "running and ready"
    Jan  4 22:39:02.137: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.896998ms
    Jan  4 22:39:02.137: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  4 22:39:02.137: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan  4 22:39:02.140: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-264" to be "running and ready"
    Jan  4 22:39:02.144: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.509442ms
    Jan  4 22:39:02.144: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan  4 22:39:02.144: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 01/04/23 22:39:02.147
    Jan  4 22:39:02.165: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-264" to be "running"
    Jan  4 22:39:02.174: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.886431ms
    Jan  4 22:39:04.188: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023069103s
    Jan  4 22:39:04.188: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  4 22:39:04.192: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-264" to be "running"
    Jan  4 22:39:04.195: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.142299ms
    Jan  4 22:39:04.195: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  4 22:39:04.198: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Jan  4 22:39:04.198: INFO: Going to poll 10.42.0.65 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Jan  4 22:39:04.201: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.65:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:39:04.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:39:04.201: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:39:04.201: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.0.65%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 22:39:04.285: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  4 22:39:04.285: INFO: Going to poll 10.42.3.132 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Jan  4 22:39:04.289: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.3.132:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:39:04.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:39:04.290: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:39:04.290: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.3.132%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 22:39:04.387: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  4 22:39:04.387: INFO: Going to poll 10.42.1.61 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Jan  4 22:39:04.391: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.1.61:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:39:04.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:39:04.392: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:39:04.392: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.1.61%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 22:39:04.472: INFO: Found all 1 expected endpoints: [netserver-2]
    Jan  4 22:39:04.472: INFO: Going to poll 10.42.2.66 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Jan  4 22:39:04.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.2.66:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:39:04.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:39:04.476: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:39:04.477: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.2.66%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 22:39:04.559: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:04.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-264" for this suite. 01/04/23 22:39:04.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:04.575
Jan  4 22:39:04.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:39:04.576
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:04.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:04.595
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-db957c4a-2ba0-458e-b81c-afb50c19d454 01/04/23 22:39:04.598
STEP: Creating a pod to test consume secrets 01/04/23 22:39:04.602
Jan  4 22:39:04.609: INFO: Waiting up to 5m0s for pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea" in namespace "secrets-9483" to be "Succeeded or Failed"
Jan  4 22:39:04.615: INFO: Pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.391975ms
Jan  4 22:39:06.619: INFO: Pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010396559s
Jan  4 22:39:08.620: INFO: Pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011026415s
STEP: Saw pod success 01/04/23 22:39:08.62
Jan  4 22:39:08.620: INFO: Pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea" satisfied condition "Succeeded or Failed"
Jan  4 22:39:08.623: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea container secret-env-test: <nil>
STEP: delete the pod 01/04/23 22:39:08.637
Jan  4 22:39:08.649: INFO: Waiting for pod pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea to disappear
Jan  4 22:39:08.651: INFO: Pod pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:08.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9483" for this suite. 01/04/23 22:39:08.658
------------------------------
• [4.089 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:04.575
    Jan  4 22:39:04.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:39:04.576
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:04.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:04.595
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-db957c4a-2ba0-458e-b81c-afb50c19d454 01/04/23 22:39:04.598
    STEP: Creating a pod to test consume secrets 01/04/23 22:39:04.602
    Jan  4 22:39:04.609: INFO: Waiting up to 5m0s for pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea" in namespace "secrets-9483" to be "Succeeded or Failed"
    Jan  4 22:39:04.615: INFO: Pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.391975ms
    Jan  4 22:39:06.619: INFO: Pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010396559s
    Jan  4 22:39:08.620: INFO: Pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011026415s
    STEP: Saw pod success 01/04/23 22:39:08.62
    Jan  4 22:39:08.620: INFO: Pod "pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea" satisfied condition "Succeeded or Failed"
    Jan  4 22:39:08.623: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea container secret-env-test: <nil>
    STEP: delete the pod 01/04/23 22:39:08.637
    Jan  4 22:39:08.649: INFO: Waiting for pod pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea to disappear
    Jan  4 22:39:08.651: INFO: Pod pod-secrets-9004a50a-9b05-44af-bb6b-3bc8dee408ea no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:08.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9483" for this suite. 01/04/23 22:39:08.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:08.667
Jan  4 22:39:08.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:39:08.668
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:08.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:08.69
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:39:08.708
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:39:09.616
STEP: Deploying the webhook pod 01/04/23 22:39:09.629
STEP: Wait for the deployment to be ready 01/04/23 22:39:09.694
Jan  4 22:39:09.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 22, 39, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 22, 39, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 22, 39, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 22, 39, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/04/23 22:39:11.757
STEP: Verifying the service has paired with the endpoint 01/04/23 22:39:11.768
Jan  4 22:39:12.769: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/04/23 22:39:12.772
STEP: Creating a custom resource definition that should be denied by the webhook 01/04/23 22:39:12.787
Jan  4 22:39:12.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:12.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8740" for this suite. 01/04/23 22:39:12.872
STEP: Destroying namespace "webhook-8740-markers" for this suite. 01/04/23 22:39:12.882
------------------------------
• [4.227 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:08.667
    Jan  4 22:39:08.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:39:08.668
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:08.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:08.69
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:39:08.708
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:39:09.616
    STEP: Deploying the webhook pod 01/04/23 22:39:09.629
    STEP: Wait for the deployment to be ready 01/04/23 22:39:09.694
    Jan  4 22:39:09.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 22, 39, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 22, 39, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 22, 39, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 22, 39, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/04/23 22:39:11.757
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:39:11.768
    Jan  4 22:39:12.769: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/04/23 22:39:12.772
    STEP: Creating a custom resource definition that should be denied by the webhook 01/04/23 22:39:12.787
    Jan  4 22:39:12.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:12.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8740" for this suite. 01/04/23 22:39:12.872
    STEP: Destroying namespace "webhook-8740-markers" for this suite. 01/04/23 22:39:12.882
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:12.894
Jan  4 22:39:12.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:39:12.898
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:12.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:12.915
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-a3785eb9-f600-4782-be76-add161653e38 01/04/23 22:39:12.917
STEP: Creating a pod to test consume configMaps 01/04/23 22:39:12.921
Jan  4 22:39:12.931: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350" in namespace "projected-2792" to be "Succeeded or Failed"
Jan  4 22:39:12.934: INFO: Pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119285ms
Jan  4 22:39:14.939: INFO: Pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008121472s
Jan  4 22:39:16.938: INFO: Pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006511052s
STEP: Saw pod success 01/04/23 22:39:16.938
Jan  4 22:39:16.938: INFO: Pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350" satisfied condition "Succeeded or Failed"
Jan  4 22:39:16.940: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:39:16.946
Jan  4 22:39:16.958: INFO: Waiting for pod pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350 to disappear
Jan  4 22:39:16.967: INFO: Pod pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:16.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2792" for this suite. 01/04/23 22:39:16.975
------------------------------
• [4.089 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:12.894
    Jan  4 22:39:12.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:39:12.898
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:12.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:12.915
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-a3785eb9-f600-4782-be76-add161653e38 01/04/23 22:39:12.917
    STEP: Creating a pod to test consume configMaps 01/04/23 22:39:12.921
    Jan  4 22:39:12.931: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350" in namespace "projected-2792" to be "Succeeded or Failed"
    Jan  4 22:39:12.934: INFO: Pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119285ms
    Jan  4 22:39:14.939: INFO: Pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008121472s
    Jan  4 22:39:16.938: INFO: Pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006511052s
    STEP: Saw pod success 01/04/23 22:39:16.938
    Jan  4 22:39:16.938: INFO: Pod "pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350" satisfied condition "Succeeded or Failed"
    Jan  4 22:39:16.940: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:39:16.946
    Jan  4 22:39:16.958: INFO: Waiting for pod pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350 to disappear
    Jan  4 22:39:16.967: INFO: Pod pod-projected-configmaps-7ec90277-32f0-4d02-ad34-c317b1459350 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:16.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2792" for this suite. 01/04/23 22:39:16.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:16.988
Jan  4 22:39:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:39:16.988
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:17.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:17.014
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/04/23 22:39:17.02
STEP: waiting for available Endpoint 01/04/23 22:39:17.024
STEP: listing all Endpoints 01/04/23 22:39:17.027
STEP: updating the Endpoint 01/04/23 22:39:17.03
STEP: fetching the Endpoint 01/04/23 22:39:17.035
STEP: patching the Endpoint 01/04/23 22:39:17.037
STEP: fetching the Endpoint 01/04/23 22:39:17.044
STEP: deleting the Endpoint by Collection 01/04/23 22:39:17.047
STEP: waiting for Endpoint deletion 01/04/23 22:39:17.054
STEP: fetching the Endpoint 01/04/23 22:39:17.055
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:17.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1447" for this suite. 01/04/23 22:39:17.061
------------------------------
• [0.081 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:16.988
    Jan  4 22:39:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:39:16.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:17.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:17.014
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/04/23 22:39:17.02
    STEP: waiting for available Endpoint 01/04/23 22:39:17.024
    STEP: listing all Endpoints 01/04/23 22:39:17.027
    STEP: updating the Endpoint 01/04/23 22:39:17.03
    STEP: fetching the Endpoint 01/04/23 22:39:17.035
    STEP: patching the Endpoint 01/04/23 22:39:17.037
    STEP: fetching the Endpoint 01/04/23 22:39:17.044
    STEP: deleting the Endpoint by Collection 01/04/23 22:39:17.047
    STEP: waiting for Endpoint deletion 01/04/23 22:39:17.054
    STEP: fetching the Endpoint 01/04/23 22:39:17.055
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:17.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1447" for this suite. 01/04/23 22:39:17.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:17.071
Jan  4 22:39:17.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename containers 01/04/23 22:39:17.072
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:17.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:17.09
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/04/23 22:39:17.092
Jan  4 22:39:17.101: INFO: Waiting up to 5m0s for pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89" in namespace "containers-5948" to be "Succeeded or Failed"
Jan  4 22:39:17.106: INFO: Pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763478ms
Jan  4 22:39:19.111: INFO: Pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008768537s
Jan  4 22:39:21.110: INFO: Pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007566821s
STEP: Saw pod success 01/04/23 22:39:21.11
Jan  4 22:39:21.110: INFO: Pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89" satisfied condition "Succeeded or Failed"
Jan  4 22:39:21.113: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:39:21.119
Jan  4 22:39:21.129: INFO: Waiting for pod client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89 to disappear
Jan  4 22:39:21.131: INFO: Pod client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:21.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5948" for this suite. 01/04/23 22:39:21.136
------------------------------
• [4.075 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:17.071
    Jan  4 22:39:17.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename containers 01/04/23 22:39:17.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:17.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:17.09
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/04/23 22:39:17.092
    Jan  4 22:39:17.101: INFO: Waiting up to 5m0s for pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89" in namespace "containers-5948" to be "Succeeded or Failed"
    Jan  4 22:39:17.106: INFO: Pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763478ms
    Jan  4 22:39:19.111: INFO: Pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008768537s
    Jan  4 22:39:21.110: INFO: Pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007566821s
    STEP: Saw pod success 01/04/23 22:39:21.11
    Jan  4 22:39:21.110: INFO: Pod "client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89" satisfied condition "Succeeded or Failed"
    Jan  4 22:39:21.113: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:39:21.119
    Jan  4 22:39:21.129: INFO: Waiting for pod client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89 to disappear
    Jan  4 22:39:21.131: INFO: Pod client-containers-35ae3075-b8b0-4d0b-b5fc-7171a0cf0c89 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:21.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5948" for this suite. 01/04/23 22:39:21.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:21.148
Jan  4 22:39:21.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:39:21.149
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:21.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:21.184
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:39:21.191
Jan  4 22:39:21.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c" in namespace "projected-8549" to be "Succeeded or Failed"
Jan  4 22:39:21.206: INFO: Pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.475662ms
Jan  4 22:39:23.210: INFO: Pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00958307s
Jan  4 22:39:25.209: INFO: Pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008655626s
STEP: Saw pod success 01/04/23 22:39:25.209
Jan  4 22:39:25.210: INFO: Pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c" satisfied condition "Succeeded or Failed"
Jan  4 22:39:25.212: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c container client-container: <nil>
STEP: delete the pod 01/04/23 22:39:25.218
Jan  4 22:39:25.236: INFO: Waiting for pod downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c to disappear
Jan  4 22:39:25.243: INFO: Pod downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:25.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8549" for this suite. 01/04/23 22:39:25.247
------------------------------
• [4.107 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:21.148
    Jan  4 22:39:21.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:39:21.149
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:21.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:21.184
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:39:21.191
    Jan  4 22:39:21.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c" in namespace "projected-8549" to be "Succeeded or Failed"
    Jan  4 22:39:21.206: INFO: Pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.475662ms
    Jan  4 22:39:23.210: INFO: Pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00958307s
    Jan  4 22:39:25.209: INFO: Pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008655626s
    STEP: Saw pod success 01/04/23 22:39:25.209
    Jan  4 22:39:25.210: INFO: Pod "downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c" satisfied condition "Succeeded or Failed"
    Jan  4 22:39:25.212: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c container client-container: <nil>
    STEP: delete the pod 01/04/23 22:39:25.218
    Jan  4 22:39:25.236: INFO: Waiting for pod downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c to disappear
    Jan  4 22:39:25.243: INFO: Pod downwardapi-volume-b270a149-7b66-428e-ab5e-85953ca3aa5c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:25.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8549" for this suite. 01/04/23 22:39:25.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:25.257
Jan  4 22:39:25.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replicaset 01/04/23 22:39:25.258
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:25.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:25.28
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/04/23 22:39:25.282
STEP: Verify that the required pods have come up 01/04/23 22:39:25.287
Jan  4 22:39:25.290: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan  4 22:39:30.296: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/04/23 22:39:30.296
Jan  4 22:39:30.299: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/04/23 22:39:30.3
STEP: DeleteCollection of the ReplicaSets 01/04/23 22:39:30.304
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/04/23 22:39:30.313
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:30.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4902" for this suite. 01/04/23 22:39:30.323
------------------------------
• [SLOW TEST] [5.110 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:25.257
    Jan  4 22:39:25.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replicaset 01/04/23 22:39:25.258
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:25.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:25.28
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/04/23 22:39:25.282
    STEP: Verify that the required pods have come up 01/04/23 22:39:25.287
    Jan  4 22:39:25.290: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan  4 22:39:30.296: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/04/23 22:39:30.296
    Jan  4 22:39:30.299: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/04/23 22:39:30.3
    STEP: DeleteCollection of the ReplicaSets 01/04/23 22:39:30.304
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/04/23 22:39:30.313
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:30.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4902" for this suite. 01/04/23 22:39:30.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:30.374
Jan  4 22:39:30.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename security-context-test 01/04/23 22:39:30.376
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:30.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:30.398
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan  4 22:39:30.407: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c" in namespace "security-context-test-7271" to be "Succeeded or Failed"
Jan  4 22:39:30.410: INFO: Pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908023ms
Jan  4 22:39:32.416: INFO: Pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008406069s
Jan  4 22:39:34.414: INFO: Pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006892467s
Jan  4 22:39:34.414: INFO: Pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:34.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7271" for this suite. 01/04/23 22:39:34.418
------------------------------
• [4.050 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:30.374
    Jan  4 22:39:30.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename security-context-test 01/04/23 22:39:30.376
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:30.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:30.398
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan  4 22:39:30.407: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c" in namespace "security-context-test-7271" to be "Succeeded or Failed"
    Jan  4 22:39:30.410: INFO: Pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908023ms
    Jan  4 22:39:32.416: INFO: Pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008406069s
    Jan  4 22:39:34.414: INFO: Pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006892467s
    Jan  4 22:39:34.414: INFO: Pod "busybox-user-65534-6ed6a6b4-5881-4d81-8598-9cea234a935c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:34.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7271" for this suite. 01/04/23 22:39:34.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:34.426
Jan  4 22:39:34.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename subpath 01/04/23 22:39:34.427
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:34.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:34.449
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/04/23 22:39:34.451
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-7gcl 01/04/23 22:39:34.459
STEP: Creating a pod to test atomic-volume-subpath 01/04/23 22:39:34.46
Jan  4 22:39:34.466: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7gcl" in namespace "subpath-3256" to be "Succeeded or Failed"
Jan  4 22:39:34.469: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459904ms
Jan  4 22:39:36.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006067502s
Jan  4 22:39:38.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 4.007327615s
Jan  4 22:39:40.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 6.006573764s
Jan  4 22:39:42.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 8.007122526s
Jan  4 22:39:44.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 10.007364554s
Jan  4 22:39:46.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 12.006660461s
Jan  4 22:39:48.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 14.006674307s
Jan  4 22:39:50.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 16.007497124s
Jan  4 22:39:52.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 18.007638765s
Jan  4 22:39:54.476: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 20.008949376s
Jan  4 22:39:56.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=false. Elapsed: 22.006276196s
Jan  4 22:39:58.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007231529s
STEP: Saw pod success 01/04/23 22:39:58.474
Jan  4 22:39:58.474: INFO: Pod "pod-subpath-test-projected-7gcl" satisfied condition "Succeeded or Failed"
Jan  4 22:39:58.476: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-projected-7gcl container test-container-subpath-projected-7gcl: <nil>
STEP: delete the pod 01/04/23 22:39:58.483
Jan  4 22:39:58.494: INFO: Waiting for pod pod-subpath-test-projected-7gcl to disappear
Jan  4 22:39:58.496: INFO: Pod pod-subpath-test-projected-7gcl no longer exists
STEP: Deleting pod pod-subpath-test-projected-7gcl 01/04/23 22:39:58.497
Jan  4 22:39:58.497: INFO: Deleting pod "pod-subpath-test-projected-7gcl" in namespace "subpath-3256"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:58.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3256" for this suite. 01/04/23 22:39:58.503
------------------------------
• [SLOW TEST] [24.083 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:34.426
    Jan  4 22:39:34.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename subpath 01/04/23 22:39:34.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:34.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:34.449
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/04/23 22:39:34.451
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-7gcl 01/04/23 22:39:34.459
    STEP: Creating a pod to test atomic-volume-subpath 01/04/23 22:39:34.46
    Jan  4 22:39:34.466: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7gcl" in namespace "subpath-3256" to be "Succeeded or Failed"
    Jan  4 22:39:34.469: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459904ms
    Jan  4 22:39:36.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006067502s
    Jan  4 22:39:38.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 4.007327615s
    Jan  4 22:39:40.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 6.006573764s
    Jan  4 22:39:42.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 8.007122526s
    Jan  4 22:39:44.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 10.007364554s
    Jan  4 22:39:46.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 12.006660461s
    Jan  4 22:39:48.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 14.006674307s
    Jan  4 22:39:50.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 16.007497124s
    Jan  4 22:39:52.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 18.007638765s
    Jan  4 22:39:54.476: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=true. Elapsed: 20.008949376s
    Jan  4 22:39:56.473: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Running", Reason="", readiness=false. Elapsed: 22.006276196s
    Jan  4 22:39:58.474: INFO: Pod "pod-subpath-test-projected-7gcl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007231529s
    STEP: Saw pod success 01/04/23 22:39:58.474
    Jan  4 22:39:58.474: INFO: Pod "pod-subpath-test-projected-7gcl" satisfied condition "Succeeded or Failed"
    Jan  4 22:39:58.476: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-projected-7gcl container test-container-subpath-projected-7gcl: <nil>
    STEP: delete the pod 01/04/23 22:39:58.483
    Jan  4 22:39:58.494: INFO: Waiting for pod pod-subpath-test-projected-7gcl to disappear
    Jan  4 22:39:58.496: INFO: Pod pod-subpath-test-projected-7gcl no longer exists
    STEP: Deleting pod pod-subpath-test-projected-7gcl 01/04/23 22:39:58.497
    Jan  4 22:39:58.497: INFO: Deleting pod "pod-subpath-test-projected-7gcl" in namespace "subpath-3256"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:58.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3256" for this suite. 01/04/23 22:39:58.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:58.512
Jan  4 22:39:58.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-pred 01/04/23 22:39:58.513
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:58.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:58.531
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  4 22:39:58.533: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  4 22:39:58.540: INFO: Waiting for terminating namespaces to be deleted...
Jan  4 22:39:58.542: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-54.us-east-2.compute.internal before test
Jan  4 22:39:58.551: INFO: cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:39 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 22:39:58.551: INFO: etcd-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:18 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container etcd ready: true, restart count 0
Jan  4 22:39:58.551: INFO: helm-install-rke2-canal-r7b4c from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container helm ready: false, restart count 0
Jan  4 22:39:58.551: INFO: helm-install-rke2-coredns-8ff46 from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container helm ready: false, restart count 0
Jan  4 22:39:58.551: INFO: helm-install-rke2-ingress-nginx-qfntk from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container helm ready: false, restart count 0
Jan  4 22:39:58.551: INFO: helm-install-rke2-metrics-server-q46jz from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container helm ready: false, restart count 0
Jan  4 22:39:58.551: INFO: kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:32 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 22:39:58.551: INFO: kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:38 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 22:39:58.551: INFO: kube-proxy-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:42 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 22:39:58.551: INFO: kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:37 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 22:39:58.551: INFO: rke2-canal-ggwd4 from kube-system started at 2023-01-04 20:05:59 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 22:39:58.551: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 22:39:58.551: INFO: rke2-coredns-rke2-coredns-854779488f-mwkvw from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container coredns ready: true, restart count 0
Jan  4 22:39:58.551: INFO: rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container autoscaler ready: true, restart count 0
Jan  4 22:39:58.551: INFO: rke2-ingress-nginx-controller-97km7 from kube-system started at 2023-01-04 20:06:54 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 22:39:58.551: INFO: rke2-metrics-server-778467dc76-4rtdk from kube-system started at 2023-01-04 20:06:31 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container metrics-server ready: true, restart count 0
Jan  4 22:39:58.551: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:39:58.551: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 22:39:58.551: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-117.us-east-2.compute.internal before test
Jan  4 22:39:58.558: INFO: kube-proxy-ip-172-31-13-117.us-east-2.compute.internal from kube-system started at 2023-01-04 20:10:23 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.558: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 22:39:58.558: INFO: rke2-canal-mprb9 from kube-system started at 2023-01-04 20:10:24 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.558: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 22:39:58.558: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 22:39:58.558: INFO: rke2-ingress-nginx-controller-8mjvf from kube-system started at 2023-01-04 20:10:52 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.558: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 22:39:58.558: INFO: sonobuoy from sonobuoy started at 2023-01-04 22:11:19 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.558: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  4 22:39:58.558: INFO: sonobuoy-e2e-job-6a70417ebe254b91 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.558: INFO: 	Container e2e ready: true, restart count 0
Jan  4 22:39:58.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:39:58.558: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:39:58.558: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 22:39:58.558: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-240.us-east-2.compute.internal before test
Jan  4 22:39:58.568: INFO: cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.568: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 22:39:58.568: INFO: etcd-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:07:58 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.568: INFO: 	Container etcd ready: true, restart count 0
Jan  4 22:39:58.568: INFO: kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:18 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.568: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 22:39:58.568: INFO: kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.568: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 22:39:58.569: INFO: kube-proxy-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:27 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.569: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 22:39:58.569: INFO: kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.569: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 22:39:58.569: INFO: rke2-canal-wspdm from kube-system started at 2023-01-04 20:08:32 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.569: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 22:39:58.569: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 22:39:58.569: INFO: rke2-coredns-rke2-coredns-854779488f-n8z2r from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.569: INFO: 	Container coredns ready: true, restart count 0
Jan  4 22:39:58.569: INFO: rke2-ingress-nginx-controller-rv4dm from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.569: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 22:39:58.569: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.569: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:39:58.569: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 22:39:58.569: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-62.us-east-2.compute.internal before test
Jan  4 22:39:58.578: INFO: cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.578: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 22:39:58.578: INFO: etcd-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:22 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.578: INFO: 	Container etcd ready: true, restart count 0
Jan  4 22:39:58.578: INFO: kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:36 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.578: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 22:39:58.578: INFO: kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.578: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 22:39:58.579: INFO: kube-proxy-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:47 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.579: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 22:39:58.579: INFO: kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.579: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 22:39:58.579: INFO: rke2-canal-44lz9 from kube-system started at 2023-01-04 20:08:42 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.579: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 22:39:58.579: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 22:39:58.579: INFO: rke2-ingress-nginx-controller-glxqt from kube-system started at 2023-01-04 20:08:54 +0000 UTC (1 container statuses recorded)
Jan  4 22:39:58.579: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 22:39:58.579: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 22:39:58.579: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 22:39:58.579: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/04/23 22:39:58.579
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17373c382f61a857], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 01/04/23 22:39:58.611
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:39:59.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8535" for this suite. 01/04/23 22:39:59.611
------------------------------
• [1.107 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:58.512
    Jan  4 22:39:58.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-pred 01/04/23 22:39:58.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:58.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:58.531
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  4 22:39:58.533: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  4 22:39:58.540: INFO: Waiting for terminating namespaces to be deleted...
    Jan  4 22:39:58.542: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-11-54.us-east-2.compute.internal before test
    Jan  4 22:39:58.551: INFO: cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:39 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: etcd-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:18 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: helm-install-rke2-canal-r7b4c from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container helm ready: false, restart count 0
    Jan  4 22:39:58.551: INFO: helm-install-rke2-coredns-8ff46 from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container helm ready: false, restart count 0
    Jan  4 22:39:58.551: INFO: helm-install-rke2-ingress-nginx-qfntk from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container helm ready: false, restart count 0
    Jan  4 22:39:58.551: INFO: helm-install-rke2-metrics-server-q46jz from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container helm ready: false, restart count 0
    Jan  4 22:39:58.551: INFO: kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:32 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:38 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: kube-proxy-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:42 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:37 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: rke2-canal-ggwd4 from kube-system started at 2023-01-04 20:05:59 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: rke2-coredns-rke2-coredns-854779488f-mwkvw from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container coredns ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: rke2-ingress-nginx-controller-97km7 from kube-system started at 2023-01-04 20:06:54 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: rke2-metrics-server-778467dc76-4rtdk from kube-system started at 2023-01-04 20:06:31 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 22:39:58.551: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-13-117.us-east-2.compute.internal before test
    Jan  4 22:39:58.558: INFO: kube-proxy-ip-172-31-13-117.us-east-2.compute.internal from kube-system started at 2023-01-04 20:10:23 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.558: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: rke2-canal-mprb9 from kube-system started at 2023-01-04 20:10:24 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.558: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: rke2-ingress-nginx-controller-8mjvf from kube-system started at 2023-01-04 20:10:52 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.558: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: sonobuoy from sonobuoy started at 2023-01-04 22:11:19 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.558: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: sonobuoy-e2e-job-6a70417ebe254b91 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.558: INFO: 	Container e2e ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 22:39:58.558: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-3-240.us-east-2.compute.internal before test
    Jan  4 22:39:58.568: INFO: cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.568: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 22:39:58.568: INFO: etcd-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:07:58 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.568: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 22:39:58.568: INFO: kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:18 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.568: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 22:39:58.568: INFO: kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.568: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: kube-proxy-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:27 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.569: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.569: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: rke2-canal-wspdm from kube-system started at 2023-01-04 20:08:32 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.569: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: rke2-coredns-rke2-coredns-854779488f-n8z2r from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.569: INFO: 	Container coredns ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: rke2-ingress-nginx-controller-rv4dm from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.569: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.569: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 22:39:58.569: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-9-62.us-east-2.compute.internal before test
    Jan  4 22:39:58.578: INFO: cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.578: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 22:39:58.578: INFO: etcd-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:22 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.578: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 22:39:58.578: INFO: kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:36 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.578: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 22:39:58.578: INFO: kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.578: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 22:39:58.579: INFO: kube-proxy-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:47 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.579: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 22:39:58.579: INFO: kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.579: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 22:39:58.579: INFO: rke2-canal-44lz9 from kube-system started at 2023-01-04 20:08:42 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.579: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 22:39:58.579: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 22:39:58.579: INFO: rke2-ingress-nginx-controller-glxqt from kube-system started at 2023-01-04 20:08:54 +0000 UTC (1 container statuses recorded)
    Jan  4 22:39:58.579: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 22:39:58.579: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 22:39:58.579: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 22:39:58.579: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/04/23 22:39:58.579
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17373c382f61a857], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 01/04/23 22:39:58.611
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:39:59.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8535" for this suite. 01/04/23 22:39:59.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:39:59.623
Jan  4 22:39:59.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:39:59.624
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:59.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:59.65
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/04/23 22:39:59.652
Jan  4 22:39:59.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 create -f -'
Jan  4 22:39:59.964: INFO: stderr: ""
Jan  4 22:39:59.964: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/04/23 22:39:59.964
Jan  4 22:39:59.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  4 22:40:00.104: INFO: stderr: ""
Jan  4 22:40:00.104: INFO: stdout: "update-demo-nautilus-kkl9b update-demo-nautilus-zmrz8 "
Jan  4 22:40:00.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-kkl9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 22:40:00.171: INFO: stderr: ""
Jan  4 22:40:00.171: INFO: stdout: ""
Jan  4 22:40:00.171: INFO: update-demo-nautilus-kkl9b is created but not running
Jan  4 22:40:05.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  4 22:40:05.287: INFO: stderr: ""
Jan  4 22:40:05.287: INFO: stdout: "update-demo-nautilus-kkl9b update-demo-nautilus-zmrz8 "
Jan  4 22:40:05.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-kkl9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 22:40:05.405: INFO: stderr: ""
Jan  4 22:40:05.405: INFO: stdout: ""
Jan  4 22:40:05.405: INFO: update-demo-nautilus-kkl9b is created but not running
Jan  4 22:40:10.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  4 22:40:10.578: INFO: stderr: ""
Jan  4 22:40:10.578: INFO: stdout: "update-demo-nautilus-kkl9b update-demo-nautilus-zmrz8 "
Jan  4 22:40:10.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-kkl9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 22:40:10.726: INFO: stderr: ""
Jan  4 22:40:10.726: INFO: stdout: "true"
Jan  4 22:40:10.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-kkl9b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  4 22:40:10.867: INFO: stderr: ""
Jan  4 22:40:10.867: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  4 22:40:10.867: INFO: validating pod update-demo-nautilus-kkl9b
Jan  4 22:40:10.879: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  4 22:40:10.879: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  4 22:40:10.879: INFO: update-demo-nautilus-kkl9b is verified up and running
Jan  4 22:40:10.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-zmrz8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 22:40:11.005: INFO: stderr: ""
Jan  4 22:40:11.005: INFO: stdout: "true"
Jan  4 22:40:11.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-zmrz8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  4 22:40:11.130: INFO: stderr: ""
Jan  4 22:40:11.130: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  4 22:40:11.130: INFO: validating pod update-demo-nautilus-zmrz8
Jan  4 22:40:11.141: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  4 22:40:11.141: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  4 22:40:11.141: INFO: update-demo-nautilus-zmrz8 is verified up and running
STEP: using delete to clean up resources 01/04/23 22:40:11.141
Jan  4 22:40:11.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 delete --grace-period=0 --force -f -'
Jan  4 22:40:11.216: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 22:40:11.216: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  4 22:40:11.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get rc,svc -l name=update-demo --no-headers'
Jan  4 22:40:11.370: INFO: stderr: "No resources found in kubectl-9698 namespace.\n"
Jan  4 22:40:11.370: INFO: stdout: ""
Jan  4 22:40:11.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  4 22:40:11.462: INFO: stderr: ""
Jan  4 22:40:11.462: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:40:11.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9698" for this suite. 01/04/23 22:40:11.468
------------------------------
• [SLOW TEST] [11.852 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:39:59.623
    Jan  4 22:39:59.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:39:59.624
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:39:59.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:39:59.65
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/04/23 22:39:59.652
    Jan  4 22:39:59.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 create -f -'
    Jan  4 22:39:59.964: INFO: stderr: ""
    Jan  4 22:39:59.964: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/04/23 22:39:59.964
    Jan  4 22:39:59.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  4 22:40:00.104: INFO: stderr: ""
    Jan  4 22:40:00.104: INFO: stdout: "update-demo-nautilus-kkl9b update-demo-nautilus-zmrz8 "
    Jan  4 22:40:00.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-kkl9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 22:40:00.171: INFO: stderr: ""
    Jan  4 22:40:00.171: INFO: stdout: ""
    Jan  4 22:40:00.171: INFO: update-demo-nautilus-kkl9b is created but not running
    Jan  4 22:40:05.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  4 22:40:05.287: INFO: stderr: ""
    Jan  4 22:40:05.287: INFO: stdout: "update-demo-nautilus-kkl9b update-demo-nautilus-zmrz8 "
    Jan  4 22:40:05.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-kkl9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 22:40:05.405: INFO: stderr: ""
    Jan  4 22:40:05.405: INFO: stdout: ""
    Jan  4 22:40:05.405: INFO: update-demo-nautilus-kkl9b is created but not running
    Jan  4 22:40:10.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  4 22:40:10.578: INFO: stderr: ""
    Jan  4 22:40:10.578: INFO: stdout: "update-demo-nautilus-kkl9b update-demo-nautilus-zmrz8 "
    Jan  4 22:40:10.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-kkl9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 22:40:10.726: INFO: stderr: ""
    Jan  4 22:40:10.726: INFO: stdout: "true"
    Jan  4 22:40:10.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-kkl9b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  4 22:40:10.867: INFO: stderr: ""
    Jan  4 22:40:10.867: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  4 22:40:10.867: INFO: validating pod update-demo-nautilus-kkl9b
    Jan  4 22:40:10.879: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  4 22:40:10.879: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  4 22:40:10.879: INFO: update-demo-nautilus-kkl9b is verified up and running
    Jan  4 22:40:10.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-zmrz8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 22:40:11.005: INFO: stderr: ""
    Jan  4 22:40:11.005: INFO: stdout: "true"
    Jan  4 22:40:11.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods update-demo-nautilus-zmrz8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  4 22:40:11.130: INFO: stderr: ""
    Jan  4 22:40:11.130: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  4 22:40:11.130: INFO: validating pod update-demo-nautilus-zmrz8
    Jan  4 22:40:11.141: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  4 22:40:11.141: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  4 22:40:11.141: INFO: update-demo-nautilus-zmrz8 is verified up and running
    STEP: using delete to clean up resources 01/04/23 22:40:11.141
    Jan  4 22:40:11.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 delete --grace-period=0 --force -f -'
    Jan  4 22:40:11.216: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 22:40:11.216: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  4 22:40:11.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get rc,svc -l name=update-demo --no-headers'
    Jan  4 22:40:11.370: INFO: stderr: "No resources found in kubectl-9698 namespace.\n"
    Jan  4 22:40:11.370: INFO: stdout: ""
    Jan  4 22:40:11.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9698 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  4 22:40:11.462: INFO: stderr: ""
    Jan  4 22:40:11.462: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:40:11.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9698" for this suite. 01/04/23 22:40:11.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:40:11.475
Jan  4 22:40:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 22:40:11.476
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:40:11.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:40:11.503
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan  4 22:40:11.513: INFO: Waiting up to 5m0s for pod "server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd" in namespace "pods-9263" to be "running and ready"
Jan  4 22:40:11.518: INFO: Pod "server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.154801ms
Jan  4 22:40:11.518: INFO: The phase of Pod server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:40:13.523: INFO: Pod "server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00982463s
Jan  4 22:40:13.523: INFO: The phase of Pod server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd is Running (Ready = true)
Jan  4 22:40:13.523: INFO: Pod "server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd" satisfied condition "running and ready"
Jan  4 22:40:13.549: INFO: Waiting up to 5m0s for pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd" in namespace "pods-9263" to be "Succeeded or Failed"
Jan  4 22:40:13.561: INFO: Pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.491065ms
Jan  4 22:40:15.566: INFO: Pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017037258s
Jan  4 22:40:17.565: INFO: Pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01624375s
STEP: Saw pod success 01/04/23 22:40:17.565
Jan  4 22:40:17.565: INFO: Pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd" satisfied condition "Succeeded or Failed"
Jan  4 22:40:17.568: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd container env3cont: <nil>
STEP: delete the pod 01/04/23 22:40:17.573
Jan  4 22:40:17.586: INFO: Waiting for pod client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd to disappear
Jan  4 22:40:17.589: INFO: Pod client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 22:40:17.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9263" for this suite. 01/04/23 22:40:17.595
------------------------------
• [SLOW TEST] [6.126 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:40:11.475
    Jan  4 22:40:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 22:40:11.476
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:40:11.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:40:11.503
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan  4 22:40:11.513: INFO: Waiting up to 5m0s for pod "server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd" in namespace "pods-9263" to be "running and ready"
    Jan  4 22:40:11.518: INFO: Pod "server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.154801ms
    Jan  4 22:40:11.518: INFO: The phase of Pod server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:40:13.523: INFO: Pod "server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00982463s
    Jan  4 22:40:13.523: INFO: The phase of Pod server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd is Running (Ready = true)
    Jan  4 22:40:13.523: INFO: Pod "server-envvars-b444cef3-6b20-451c-8093-c41325de8ffd" satisfied condition "running and ready"
    Jan  4 22:40:13.549: INFO: Waiting up to 5m0s for pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd" in namespace "pods-9263" to be "Succeeded or Failed"
    Jan  4 22:40:13.561: INFO: Pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.491065ms
    Jan  4 22:40:15.566: INFO: Pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017037258s
    Jan  4 22:40:17.565: INFO: Pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01624375s
    STEP: Saw pod success 01/04/23 22:40:17.565
    Jan  4 22:40:17.565: INFO: Pod "client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd" satisfied condition "Succeeded or Failed"
    Jan  4 22:40:17.568: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd container env3cont: <nil>
    STEP: delete the pod 01/04/23 22:40:17.573
    Jan  4 22:40:17.586: INFO: Waiting for pod client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd to disappear
    Jan  4 22:40:17.589: INFO: Pod client-envvars-cf86dbc0-f60e-4d0c-b1d6-40cb6ad4c9fd no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:40:17.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9263" for this suite. 01/04/23 22:40:17.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:40:17.603
Jan  4 22:40:17.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename job 01/04/23 22:40:17.604
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:40:17.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:40:17.634
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/04/23 22:40:17.643
STEP: Ensuring active pods == parallelism 01/04/23 22:40:17.651
STEP: Orphaning one of the Job's Pods 01/04/23 22:40:19.655
Jan  4 22:40:20.178: INFO: Successfully updated pod "adopt-release-4bvpg"
STEP: Checking that the Job readopts the Pod 01/04/23 22:40:20.178
Jan  4 22:40:20.180: INFO: Waiting up to 15m0s for pod "adopt-release-4bvpg" in namespace "job-9845" to be "adopted"
Jan  4 22:40:20.184: INFO: Pod "adopt-release-4bvpg": Phase="Running", Reason="", readiness=true. Elapsed: 3.735218ms
Jan  4 22:40:22.188: INFO: Pod "adopt-release-4bvpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.007428166s
Jan  4 22:40:22.188: INFO: Pod "adopt-release-4bvpg" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/04/23 22:40:22.188
Jan  4 22:40:22.701: INFO: Successfully updated pod "adopt-release-4bvpg"
STEP: Checking that the Job releases the Pod 01/04/23 22:40:22.701
Jan  4 22:40:22.701: INFO: Waiting up to 15m0s for pod "adopt-release-4bvpg" in namespace "job-9845" to be "released"
Jan  4 22:40:22.710: INFO: Pod "adopt-release-4bvpg": Phase="Running", Reason="", readiness=true. Elapsed: 8.271041ms
Jan  4 22:40:24.724: INFO: Pod "adopt-release-4bvpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.022632933s
Jan  4 22:40:24.724: INFO: Pod "adopt-release-4bvpg" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  4 22:40:24.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9845" for this suite. 01/04/23 22:40:24.728
------------------------------
• [SLOW TEST] [7.138 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:40:17.603
    Jan  4 22:40:17.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename job 01/04/23 22:40:17.604
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:40:17.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:40:17.634
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/04/23 22:40:17.643
    STEP: Ensuring active pods == parallelism 01/04/23 22:40:17.651
    STEP: Orphaning one of the Job's Pods 01/04/23 22:40:19.655
    Jan  4 22:40:20.178: INFO: Successfully updated pod "adopt-release-4bvpg"
    STEP: Checking that the Job readopts the Pod 01/04/23 22:40:20.178
    Jan  4 22:40:20.180: INFO: Waiting up to 15m0s for pod "adopt-release-4bvpg" in namespace "job-9845" to be "adopted"
    Jan  4 22:40:20.184: INFO: Pod "adopt-release-4bvpg": Phase="Running", Reason="", readiness=true. Elapsed: 3.735218ms
    Jan  4 22:40:22.188: INFO: Pod "adopt-release-4bvpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.007428166s
    Jan  4 22:40:22.188: INFO: Pod "adopt-release-4bvpg" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/04/23 22:40:22.188
    Jan  4 22:40:22.701: INFO: Successfully updated pod "adopt-release-4bvpg"
    STEP: Checking that the Job releases the Pod 01/04/23 22:40:22.701
    Jan  4 22:40:22.701: INFO: Waiting up to 15m0s for pod "adopt-release-4bvpg" in namespace "job-9845" to be "released"
    Jan  4 22:40:22.710: INFO: Pod "adopt-release-4bvpg": Phase="Running", Reason="", readiness=true. Elapsed: 8.271041ms
    Jan  4 22:40:24.724: INFO: Pod "adopt-release-4bvpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.022632933s
    Jan  4 22:40:24.724: INFO: Pod "adopt-release-4bvpg" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:40:24.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9845" for this suite. 01/04/23 22:40:24.728
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:40:24.741
Jan  4 22:40:24.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename daemonsets 01/04/23 22:40:24.742
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:40:24.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:40:24.761
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan  4 22:40:24.786: INFO: Create a RollingUpdate DaemonSet
Jan  4 22:40:24.790: INFO: Check that daemon pods launch on every node of the cluster
Jan  4 22:40:24.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:40:24.798: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:40:25.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:40:25.806: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:40:26.820: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 22:40:26.820: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
Jan  4 22:40:26.820: INFO: Update the DaemonSet to trigger a rollout
Jan  4 22:40:26.856: INFO: Updating DaemonSet daemon-set
Jan  4 22:40:29.900: INFO: Roll back the DaemonSet before rollout is complete
Jan  4 22:40:29.927: INFO: Updating DaemonSet daemon-set
Jan  4 22:40:29.927: INFO: Make sure DaemonSet rollback is complete
Jan  4 22:40:31.966: INFO: Pod daemon-set-m2zms is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:40:31.977
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8510, will wait for the garbage collector to delete the pods 01/04/23 22:40:31.977
Jan  4 22:40:32.037: INFO: Deleting DaemonSet.extensions daemon-set took: 6.765144ms
Jan  4 22:40:32.137: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.261715ms
Jan  4 22:41:03.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:41:03.843: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  4 22:41:03.845: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43257"},"items":null}

Jan  4 22:41:03.847: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43257"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:41:03.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8510" for this suite. 01/04/23 22:41:03.866
------------------------------
• [SLOW TEST] [39.132 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:40:24.741
    Jan  4 22:40:24.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename daemonsets 01/04/23 22:40:24.742
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:40:24.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:40:24.761
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan  4 22:40:24.786: INFO: Create a RollingUpdate DaemonSet
    Jan  4 22:40:24.790: INFO: Check that daemon pods launch on every node of the cluster
    Jan  4 22:40:24.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:40:24.798: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:40:25.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:40:25.806: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:40:26.820: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 22:40:26.820: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    Jan  4 22:40:26.820: INFO: Update the DaemonSet to trigger a rollout
    Jan  4 22:40:26.856: INFO: Updating DaemonSet daemon-set
    Jan  4 22:40:29.900: INFO: Roll back the DaemonSet before rollout is complete
    Jan  4 22:40:29.927: INFO: Updating DaemonSet daemon-set
    Jan  4 22:40:29.927: INFO: Make sure DaemonSet rollback is complete
    Jan  4 22:40:31.966: INFO: Pod daemon-set-m2zms is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:40:31.977
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8510, will wait for the garbage collector to delete the pods 01/04/23 22:40:31.977
    Jan  4 22:40:32.037: INFO: Deleting DaemonSet.extensions daemon-set took: 6.765144ms
    Jan  4 22:40:32.137: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.261715ms
    Jan  4 22:41:03.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:41:03.843: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  4 22:41:03.845: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43257"},"items":null}

    Jan  4 22:41:03.847: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43257"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:41:03.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8510" for this suite. 01/04/23 22:41:03.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:41:03.889
Jan  4 22:41:03.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename init-container 01/04/23 22:41:03.889
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:03.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:03.913
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/04/23 22:41:03.915
Jan  4 22:41:03.916: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:41:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2782" for this suite. 01/04/23 22:41:09.391
------------------------------
• [SLOW TEST] [5.511 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:41:03.889
    Jan  4 22:41:03.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename init-container 01/04/23 22:41:03.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:03.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:03.913
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/04/23 22:41:03.915
    Jan  4 22:41:03.916: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:41:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2782" for this suite. 01/04/23 22:41:09.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:41:09.399
Jan  4 22:41:09.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 22:41:09.4
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:09.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:09.425
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-ade8ae42-447c-4ddc-bc48-4423ef976d66 01/04/23 22:41:09.428
STEP: Creating a pod to test consume configMaps 01/04/23 22:41:09.432
Jan  4 22:41:09.439: INFO: Waiting up to 5m0s for pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753" in namespace "configmap-1607" to be "Succeeded or Failed"
Jan  4 22:41:09.443: INFO: Pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749225ms
Jan  4 22:41:11.447: INFO: Pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753": Phase="Running", Reason="", readiness=false. Elapsed: 2.00769051s
Jan  4 22:41:13.450: INFO: Pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011390437s
STEP: Saw pod success 01/04/23 22:41:13.45
Jan  4 22:41:13.451: INFO: Pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753" satisfied condition "Succeeded or Failed"
Jan  4 22:41:13.454: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:41:13.463
Jan  4 22:41:13.475: INFO: Waiting for pod pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753 to disappear
Jan  4 22:41:13.481: INFO: Pod pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:41:13.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1607" for this suite. 01/04/23 22:41:13.488
------------------------------
• [4.096 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:41:09.399
    Jan  4 22:41:09.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 22:41:09.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:09.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:09.425
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-ade8ae42-447c-4ddc-bc48-4423ef976d66 01/04/23 22:41:09.428
    STEP: Creating a pod to test consume configMaps 01/04/23 22:41:09.432
    Jan  4 22:41:09.439: INFO: Waiting up to 5m0s for pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753" in namespace "configmap-1607" to be "Succeeded or Failed"
    Jan  4 22:41:09.443: INFO: Pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749225ms
    Jan  4 22:41:11.447: INFO: Pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753": Phase="Running", Reason="", readiness=false. Elapsed: 2.00769051s
    Jan  4 22:41:13.450: INFO: Pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011390437s
    STEP: Saw pod success 01/04/23 22:41:13.45
    Jan  4 22:41:13.451: INFO: Pod "pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753" satisfied condition "Succeeded or Failed"
    Jan  4 22:41:13.454: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:41:13.463
    Jan  4 22:41:13.475: INFO: Waiting for pod pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753 to disappear
    Jan  4 22:41:13.481: INFO: Pod pod-configmaps-089a7afc-7a91-425c-80e2-bf983c4f5753 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:41:13.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1607" for this suite. 01/04/23 22:41:13.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:41:13.498
Jan  4 22:41:13.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename events 01/04/23 22:41:13.499
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:13.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:13.52
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/04/23 22:41:13.522
Jan  4 22:41:13.531: INFO: created test-event-1
Jan  4 22:41:13.537: INFO: created test-event-2
Jan  4 22:41:13.542: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/04/23 22:41:13.542
STEP: delete collection of events 01/04/23 22:41:13.545
Jan  4 22:41:13.549: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/04/23 22:41:13.571
Jan  4 22:41:13.572: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan  4 22:41:13.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5205" for this suite. 01/04/23 22:41:13.58
------------------------------
• [0.089 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:41:13.498
    Jan  4 22:41:13.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename events 01/04/23 22:41:13.499
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:13.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:13.52
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/04/23 22:41:13.522
    Jan  4 22:41:13.531: INFO: created test-event-1
    Jan  4 22:41:13.537: INFO: created test-event-2
    Jan  4 22:41:13.542: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/04/23 22:41:13.542
    STEP: delete collection of events 01/04/23 22:41:13.545
    Jan  4 22:41:13.549: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/04/23 22:41:13.571
    Jan  4 22:41:13.572: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:41:13.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5205" for this suite. 01/04/23 22:41:13.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:41:13.59
Jan  4 22:41:13.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename gc 01/04/23 22:41:13.591
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:13.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:13.623
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/04/23 22:41:13.653
STEP: delete the rc 01/04/23 22:41:18.714
STEP: wait for the rc to be deleted 01/04/23 22:41:18.787
Jan  4 22:41:19.831: INFO: 84 pods remaining
Jan  4 22:41:19.831: INFO: 83 pods has nil DeletionTimestamp
Jan  4 22:41:19.832: INFO: 
Jan  4 22:41:20.818: INFO: 80 pods remaining
Jan  4 22:41:20.828: INFO: 80 pods has nil DeletionTimestamp
Jan  4 22:41:20.828: INFO: 
Jan  4 22:41:21.818: INFO: 60 pods remaining
Jan  4 22:41:21.818: INFO: 60 pods has nil DeletionTimestamp
Jan  4 22:41:21.818: INFO: 
Jan  4 22:41:22.806: INFO: 43 pods remaining
Jan  4 22:41:22.806: INFO: 43 pods has nil DeletionTimestamp
Jan  4 22:41:22.806: INFO: 
Jan  4 22:41:23.801: INFO: 40 pods remaining
Jan  4 22:41:23.801: INFO: 40 pods has nil DeletionTimestamp
Jan  4 22:41:23.801: INFO: 
Jan  4 22:41:24.796: INFO: 20 pods remaining
Jan  4 22:41:24.796: INFO: 20 pods has nil DeletionTimestamp
Jan  4 22:41:24.796: INFO: 
Jan  4 22:41:25.808: INFO: 4 pods remaining
Jan  4 22:41:25.808: INFO: 4 pods has nil DeletionTimestamp
Jan  4 22:41:25.808: INFO: 
STEP: Gathering metrics 01/04/23 22:41:26.793
Jan  4 22:41:27.120: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
Jan  4 22:41:27.123: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.827989ms
Jan  4 22:41:27.123: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
Jan  4 22:41:27.123: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
Jan  4 22:41:27.563: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  4 22:41:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1371" for this suite. 01/04/23 22:41:27.568
------------------------------
• [SLOW TEST] [13.984 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:41:13.59
    Jan  4 22:41:13.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename gc 01/04/23 22:41:13.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:13.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:13.623
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/04/23 22:41:13.653
    STEP: delete the rc 01/04/23 22:41:18.714
    STEP: wait for the rc to be deleted 01/04/23 22:41:18.787
    Jan  4 22:41:19.831: INFO: 84 pods remaining
    Jan  4 22:41:19.831: INFO: 83 pods has nil DeletionTimestamp
    Jan  4 22:41:19.832: INFO: 
    Jan  4 22:41:20.818: INFO: 80 pods remaining
    Jan  4 22:41:20.828: INFO: 80 pods has nil DeletionTimestamp
    Jan  4 22:41:20.828: INFO: 
    Jan  4 22:41:21.818: INFO: 60 pods remaining
    Jan  4 22:41:21.818: INFO: 60 pods has nil DeletionTimestamp
    Jan  4 22:41:21.818: INFO: 
    Jan  4 22:41:22.806: INFO: 43 pods remaining
    Jan  4 22:41:22.806: INFO: 43 pods has nil DeletionTimestamp
    Jan  4 22:41:22.806: INFO: 
    Jan  4 22:41:23.801: INFO: 40 pods remaining
    Jan  4 22:41:23.801: INFO: 40 pods has nil DeletionTimestamp
    Jan  4 22:41:23.801: INFO: 
    Jan  4 22:41:24.796: INFO: 20 pods remaining
    Jan  4 22:41:24.796: INFO: 20 pods has nil DeletionTimestamp
    Jan  4 22:41:24.796: INFO: 
    Jan  4 22:41:25.808: INFO: 4 pods remaining
    Jan  4 22:41:25.808: INFO: 4 pods has nil DeletionTimestamp
    Jan  4 22:41:25.808: INFO: 
    STEP: Gathering metrics 01/04/23 22:41:26.793
    Jan  4 22:41:27.120: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Jan  4 22:41:27.123: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.827989ms
    Jan  4 22:41:27.123: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
    Jan  4 22:41:27.123: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
    Jan  4 22:41:27.563: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:41:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1371" for this suite. 01/04/23 22:41:27.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:41:27.576
Jan  4 22:41:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-probe 01/04/23 22:41:27.577
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:27.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:27.598
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 in namespace container-probe-6914 01/04/23 22:41:27.601
Jan  4 22:41:27.609: INFO: Waiting up to 5m0s for pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316" in namespace "container-probe-6914" to be "not pending"
Jan  4 22:41:27.612: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.449902ms
Jan  4 22:41:29.617: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007731908s
Jan  4 22:41:31.618: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009337273s
Jan  4 22:41:33.616: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006768277s
Jan  4 22:41:35.616: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Running", Reason="", readiness=true. Elapsed: 8.006798177s
Jan  4 22:41:35.616: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316" satisfied condition "not pending"
Jan  4 22:41:35.616: INFO: Started pod liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 in namespace container-probe-6914
STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 22:41:35.616
Jan  4 22:41:35.621: INFO: Initial restart count of pod liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is 0
Jan  4 22:41:49.655: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 1 (14.034067062s elapsed)
Jan  4 22:42:09.705: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 2 (34.084579794s elapsed)
Jan  4 22:42:29.752: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 3 (54.131356243s elapsed)
Jan  4 22:42:49.795: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 4 (1m14.174134564s elapsed)
Jan  4 22:44:03.945: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 5 (2m28.324337643s elapsed)
STEP: deleting the pod 01/04/23 22:44:03.945
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  4 22:44:03.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6914" for this suite. 01/04/23 22:44:03.973
------------------------------
• [SLOW TEST] [156.408 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:41:27.576
    Jan  4 22:41:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-probe 01/04/23 22:41:27.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:41:27.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:41:27.598
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 in namespace container-probe-6914 01/04/23 22:41:27.601
    Jan  4 22:41:27.609: INFO: Waiting up to 5m0s for pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316" in namespace "container-probe-6914" to be "not pending"
    Jan  4 22:41:27.612: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.449902ms
    Jan  4 22:41:29.617: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007731908s
    Jan  4 22:41:31.618: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009337273s
    Jan  4 22:41:33.616: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006768277s
    Jan  4 22:41:35.616: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316": Phase="Running", Reason="", readiness=true. Elapsed: 8.006798177s
    Jan  4 22:41:35.616: INFO: Pod "liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316" satisfied condition "not pending"
    Jan  4 22:41:35.616: INFO: Started pod liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 in namespace container-probe-6914
    STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 22:41:35.616
    Jan  4 22:41:35.621: INFO: Initial restart count of pod liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is 0
    Jan  4 22:41:49.655: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 1 (14.034067062s elapsed)
    Jan  4 22:42:09.705: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 2 (34.084579794s elapsed)
    Jan  4 22:42:29.752: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 3 (54.131356243s elapsed)
    Jan  4 22:42:49.795: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 4 (1m14.174134564s elapsed)
    Jan  4 22:44:03.945: INFO: Restart count of pod container-probe-6914/liveness-4c8adbb1-4a73-4ea4-82ea-e623fc2f8316 is now 5 (2m28.324337643s elapsed)
    STEP: deleting the pod 01/04/23 22:44:03.945
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:44:03.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6914" for this suite. 01/04/23 22:44:03.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:44:03.985
Jan  4 22:44:03.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 22:44:03.987
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:04.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:04.015
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/04/23 22:44:04.019
Jan  4 22:44:04.028: INFO: Waiting up to 5m0s for pod "pod-68cqf" in namespace "pods-2235" to be "running"
Jan  4 22:44:04.031: INFO: Pod "pod-68cqf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.329839ms
Jan  4 22:44:06.035: INFO: Pod "pod-68cqf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006912921s
Jan  4 22:44:06.035: INFO: Pod "pod-68cqf" satisfied condition "running"
STEP: patching /status 01/04/23 22:44:06.035
Jan  4 22:44:06.048: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 22:44:06.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2235" for this suite. 01/04/23 22:44:06.054
------------------------------
• [2.075 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:44:03.985
    Jan  4 22:44:03.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 22:44:03.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:04.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:04.015
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/04/23 22:44:04.019
    Jan  4 22:44:04.028: INFO: Waiting up to 5m0s for pod "pod-68cqf" in namespace "pods-2235" to be "running"
    Jan  4 22:44:04.031: INFO: Pod "pod-68cqf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.329839ms
    Jan  4 22:44:06.035: INFO: Pod "pod-68cqf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006912921s
    Jan  4 22:44:06.035: INFO: Pod "pod-68cqf" satisfied condition "running"
    STEP: patching /status 01/04/23 22:44:06.035
    Jan  4 22:44:06.048: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:44:06.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2235" for this suite. 01/04/23 22:44:06.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:44:06.064
Jan  4 22:44:06.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 22:44:06.065
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:06.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:06.084
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1862/configmap-test-e12cd8c9-0a1b-44d6-a2bb-ce26991542e7 01/04/23 22:44:06.087
STEP: Creating a pod to test consume configMaps 01/04/23 22:44:06.098
Jan  4 22:44:06.105: INFO: Waiting up to 5m0s for pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba" in namespace "configmap-1862" to be "Succeeded or Failed"
Jan  4 22:44:06.109: INFO: Pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829301ms
Jan  4 22:44:08.112: INFO: Pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006950428s
Jan  4 22:44:10.113: INFO: Pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007625862s
STEP: Saw pod success 01/04/23 22:44:10.113
Jan  4 22:44:10.113: INFO: Pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba" satisfied condition "Succeeded or Failed"
Jan  4 22:44:10.116: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba container env-test: <nil>
STEP: delete the pod 01/04/23 22:44:10.13
Jan  4 22:44:10.140: INFO: Waiting for pod pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba to disappear
Jan  4 22:44:10.142: INFO: Pod pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:44:10.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1862" for this suite. 01/04/23 22:44:10.147
------------------------------
• [4.089 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:44:06.064
    Jan  4 22:44:06.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 22:44:06.065
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:06.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:06.084
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1862/configmap-test-e12cd8c9-0a1b-44d6-a2bb-ce26991542e7 01/04/23 22:44:06.087
    STEP: Creating a pod to test consume configMaps 01/04/23 22:44:06.098
    Jan  4 22:44:06.105: INFO: Waiting up to 5m0s for pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba" in namespace "configmap-1862" to be "Succeeded or Failed"
    Jan  4 22:44:06.109: INFO: Pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829301ms
    Jan  4 22:44:08.112: INFO: Pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006950428s
    Jan  4 22:44:10.113: INFO: Pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007625862s
    STEP: Saw pod success 01/04/23 22:44:10.113
    Jan  4 22:44:10.113: INFO: Pod "pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba" satisfied condition "Succeeded or Failed"
    Jan  4 22:44:10.116: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba container env-test: <nil>
    STEP: delete the pod 01/04/23 22:44:10.13
    Jan  4 22:44:10.140: INFO: Waiting for pod pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba to disappear
    Jan  4 22:44:10.142: INFO: Pod pod-configmaps-a466fde2-58fe-4544-b911-3209a2a0d8ba no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:44:10.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1862" for this suite. 01/04/23 22:44:10.147
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:44:10.159
Jan  4 22:44:10.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename deployment 01/04/23 22:44:10.16
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:10.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:10.184
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan  4 22:44:10.187: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan  4 22:44:10.194: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  4 22:44:15.198: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/04/23 22:44:15.198
Jan  4 22:44:15.198: INFO: Creating deployment "test-rolling-update-deployment"
Jan  4 22:44:15.204: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan  4 22:44:15.209: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan  4 22:44:17.215: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan  4 22:44:17.217: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  4 22:44:17.230: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1307  d98fb81c-423d-4733-8dc5-cedd17eb9882 45689 1 2023-01-04 22:44:15 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-04 22:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cd5498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-04 22:44:15 +0000 UTC,LastTransitionTime:2023-01-04 22:44:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-04 22:44:16 +0000 UTC,LastTransitionTime:2023-01-04 22:44:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  4 22:44:17.240: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1307  2b636151-6b20-4540-8e6a-31e7592c1271 45679 1 2023-01-04 22:44:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d98fb81c-423d-4733-8dc5-cedd17eb9882 0xc005cd5977 0xc005cd5978}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d98fb81c-423d-4733-8dc5-cedd17eb9882\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cd5a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  4 22:44:17.240: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan  4 22:44:17.241: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1307  841589e8-edab-42b2-abfa-651b59db3cc8 45688 2 2023-01-04 22:44:10 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d98fb81c-423d-4733-8dc5-cedd17eb9882 0xc005cd5847 0xc005cd5848}] [] [{e2e.test Update apps/v1 2023-01-04 22:44:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d98fb81c-423d-4733-8dc5-cedd17eb9882\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005cd5908 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  4 22:44:17.245: INFO: Pod "test-rolling-update-deployment-7549d9f46d-f8kxr" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-f8kxr test-rolling-update-deployment-7549d9f46d- deployment-1307  b718e0bf-bdb3-40a7-b39c-cefc92c961ab 45678 0 2023-01-04 22:44:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:3b2087b5d19b5658144a5089a605d48ba59f1219dfda406d169997dc6a18ce58 cni.projectcalico.org/podIP:10.42.3.180/32 cni.projectcalico.org/podIPs:10.42.3.180/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 2b636151-6b20-4540-8e6a-31e7592c1271 0xc004319317 0xc004319318}] [] [{calico Update v1 2023-01-04 22:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 22:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b636151-6b20-4540-8e6a-31e7592c1271\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skmgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skmgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.180,StartTime:2023-01-04 22:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 22:44:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://77ef36a53695725be8b401995499dfaf4772380faf40e5a9af9cdddee1f85c20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  4 22:44:17.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1307" for this suite. 01/04/23 22:44:17.252
------------------------------
• [SLOW TEST] [7.101 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:44:10.159
    Jan  4 22:44:10.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename deployment 01/04/23 22:44:10.16
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:10.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:10.184
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan  4 22:44:10.187: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan  4 22:44:10.194: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  4 22:44:15.198: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/04/23 22:44:15.198
    Jan  4 22:44:15.198: INFO: Creating deployment "test-rolling-update-deployment"
    Jan  4 22:44:15.204: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan  4 22:44:15.209: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan  4 22:44:17.215: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan  4 22:44:17.217: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  4 22:44:17.230: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1307  d98fb81c-423d-4733-8dc5-cedd17eb9882 45689 1 2023-01-04 22:44:15 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-04 22:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cd5498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-04 22:44:15 +0000 UTC,LastTransitionTime:2023-01-04 22:44:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-04 22:44:16 +0000 UTC,LastTransitionTime:2023-01-04 22:44:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  4 22:44:17.240: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1307  2b636151-6b20-4540-8e6a-31e7592c1271 45679 1 2023-01-04 22:44:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d98fb81c-423d-4733-8dc5-cedd17eb9882 0xc005cd5977 0xc005cd5978}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d98fb81c-423d-4733-8dc5-cedd17eb9882\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cd5a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 22:44:17.240: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan  4 22:44:17.241: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1307  841589e8-edab-42b2-abfa-651b59db3cc8 45688 2 2023-01-04 22:44:10 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d98fb81c-423d-4733-8dc5-cedd17eb9882 0xc005cd5847 0xc005cd5848}] [] [{e2e.test Update apps/v1 2023-01-04 22:44:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d98fb81c-423d-4733-8dc5-cedd17eb9882\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005cd5908 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 22:44:17.245: INFO: Pod "test-rolling-update-deployment-7549d9f46d-f8kxr" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-f8kxr test-rolling-update-deployment-7549d9f46d- deployment-1307  b718e0bf-bdb3-40a7-b39c-cefc92c961ab 45678 0 2023-01-04 22:44:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:3b2087b5d19b5658144a5089a605d48ba59f1219dfda406d169997dc6a18ce58 cni.projectcalico.org/podIP:10.42.3.180/32 cni.projectcalico.org/podIPs:10.42.3.180/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 2b636151-6b20-4540-8e6a-31e7592c1271 0xc004319317 0xc004319318}] [] [{calico Update v1 2023-01-04 22:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 22:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b636151-6b20-4540-8e6a-31e7592c1271\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 22:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skmgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skmgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.180,StartTime:2023-01-04 22:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 22:44:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://77ef36a53695725be8b401995499dfaf4772380faf40e5a9af9cdddee1f85c20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:44:17.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1307" for this suite. 01/04/23 22:44:17.252
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:44:17.26
Jan  4 22:44:17.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:44:17.262
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:17.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:17.303
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5129 01/04/23 22:44:17.305
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/04/23 22:44:17.319
STEP: creating service externalsvc in namespace services-5129 01/04/23 22:44:17.319
STEP: creating replication controller externalsvc in namespace services-5129 01/04/23 22:44:17.362
I0104 22:44:17.379482      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5129, replica count: 2
I0104 22:44:20.430631      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/04/23 22:44:20.434
Jan  4 22:44:20.455: INFO: Creating new exec pod
Jan  4 22:44:20.475: INFO: Waiting up to 5m0s for pod "execpoddpngh" in namespace "services-5129" to be "running"
Jan  4 22:44:20.482: INFO: Pod "execpoddpngh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.904873ms
Jan  4 22:44:22.528: INFO: Pod "execpoddpngh": Phase="Running", Reason="", readiness=true. Elapsed: 2.053336894s
Jan  4 22:44:22.529: INFO: Pod "execpoddpngh" satisfied condition "running"
Jan  4 22:44:22.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5129 exec execpoddpngh -- /bin/sh -x -c nslookup clusterip-service.services-5129.svc.cluster.local'
Jan  4 22:44:22.823: INFO: stderr: "+ nslookup clusterip-service.services-5129.svc.cluster.local\n"
Jan  4 22:44:22.823: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nclusterip-service.services-5129.svc.cluster.local\tcanonical name = externalsvc.services-5129.svc.cluster.local.\nName:\texternalsvc.services-5129.svc.cluster.local\nAddress: 10.43.188.206\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5129, will wait for the garbage collector to delete the pods 01/04/23 22:44:22.823
Jan  4 22:44:22.883: INFO: Deleting ReplicationController externalsvc took: 6.1873ms
Jan  4 22:44:22.984: INFO: Terminating ReplicationController externalsvc pods took: 100.948585ms
Jan  4 22:44:25.029: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:44:25.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5129" for this suite. 01/04/23 22:44:25.058
------------------------------
• [SLOW TEST] [7.804 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:44:17.26
    Jan  4 22:44:17.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:44:17.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:17.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:17.303
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5129 01/04/23 22:44:17.305
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/04/23 22:44:17.319
    STEP: creating service externalsvc in namespace services-5129 01/04/23 22:44:17.319
    STEP: creating replication controller externalsvc in namespace services-5129 01/04/23 22:44:17.362
    I0104 22:44:17.379482      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5129, replica count: 2
    I0104 22:44:20.430631      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/04/23 22:44:20.434
    Jan  4 22:44:20.455: INFO: Creating new exec pod
    Jan  4 22:44:20.475: INFO: Waiting up to 5m0s for pod "execpoddpngh" in namespace "services-5129" to be "running"
    Jan  4 22:44:20.482: INFO: Pod "execpoddpngh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.904873ms
    Jan  4 22:44:22.528: INFO: Pod "execpoddpngh": Phase="Running", Reason="", readiness=true. Elapsed: 2.053336894s
    Jan  4 22:44:22.529: INFO: Pod "execpoddpngh" satisfied condition "running"
    Jan  4 22:44:22.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-5129 exec execpoddpngh -- /bin/sh -x -c nslookup clusterip-service.services-5129.svc.cluster.local'
    Jan  4 22:44:22.823: INFO: stderr: "+ nslookup clusterip-service.services-5129.svc.cluster.local\n"
    Jan  4 22:44:22.823: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nclusterip-service.services-5129.svc.cluster.local\tcanonical name = externalsvc.services-5129.svc.cluster.local.\nName:\texternalsvc.services-5129.svc.cluster.local\nAddress: 10.43.188.206\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5129, will wait for the garbage collector to delete the pods 01/04/23 22:44:22.823
    Jan  4 22:44:22.883: INFO: Deleting ReplicationController externalsvc took: 6.1873ms
    Jan  4 22:44:22.984: INFO: Terminating ReplicationController externalsvc pods took: 100.948585ms
    Jan  4 22:44:25.029: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:44:25.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5129" for this suite. 01/04/23 22:44:25.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:44:25.065
Jan  4 22:44:25.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:44:25.066
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:25.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:25.104
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:44:25.119
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:44:25.57
STEP: Deploying the webhook pod 01/04/23 22:44:25.579
STEP: Wait for the deployment to be ready 01/04/23 22:44:25.594
Jan  4 22:44:25.606: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/04/23 22:44:27.616
STEP: Verifying the service has paired with the endpoint 01/04/23 22:44:27.628
Jan  4 22:44:28.628: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/04/23 22:44:28.631
STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:44:28.65
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/04/23 22:44:28.661
STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:44:28.675
STEP: Patching a validating webhook configuration's rules to include the create operation 01/04/23 22:44:28.69
STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:44:28.699
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:44:28.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6026" for this suite. 01/04/23 22:44:28.78
STEP: Destroying namespace "webhook-6026-markers" for this suite. 01/04/23 22:44:28.792
------------------------------
• [3.739 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:44:25.065
    Jan  4 22:44:25.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:44:25.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:25.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:25.104
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:44:25.119
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:44:25.57
    STEP: Deploying the webhook pod 01/04/23 22:44:25.579
    STEP: Wait for the deployment to be ready 01/04/23 22:44:25.594
    Jan  4 22:44:25.606: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/04/23 22:44:27.616
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:44:27.628
    Jan  4 22:44:28.628: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/04/23 22:44:28.631
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:44:28.65
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/04/23 22:44:28.661
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:44:28.675
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/04/23 22:44:28.69
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/04/23 22:44:28.699
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:44:28.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6026" for this suite. 01/04/23 22:44:28.78
    STEP: Destroying namespace "webhook-6026-markers" for this suite. 01/04/23 22:44:28.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:44:28.808
Jan  4 22:44:28.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename job 01/04/23 22:44:28.809
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:28.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:28.831
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/04/23 22:44:28.833
STEP: Ensuring job reaches completions 01/04/23 22:44:28.838
STEP: Ensuring pods with index for job exist 01/04/23 22:44:38.844
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  4 22:44:38.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8667" for this suite. 01/04/23 22:44:38.86
------------------------------
• [SLOW TEST] [10.063 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:44:28.808
    Jan  4 22:44:28.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename job 01/04/23 22:44:28.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:28.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:28.831
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/04/23 22:44:28.833
    STEP: Ensuring job reaches completions 01/04/23 22:44:28.838
    STEP: Ensuring pods with index for job exist 01/04/23 22:44:38.844
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:44:38.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8667" for this suite. 01/04/23 22:44:38.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:44:38.872
Jan  4 22:44:38.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-webhook 01/04/23 22:44:38.874
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:38.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:38.895
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/04/23 22:44:38.899
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/04/23 22:44:39.476
STEP: Deploying the custom resource conversion webhook pod 01/04/23 22:44:39.481
STEP: Wait for the deployment to be ready 01/04/23 22:44:39.492
Jan  4 22:44:39.498: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 22:44:41.528
STEP: Verifying the service has paired with the endpoint 01/04/23 22:44:41.554
Jan  4 22:44:42.555: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan  4 22:44:42.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Creating a v1 custom resource 01/04/23 22:44:45.148
STEP: v2 custom resource should be converted 01/04/23 22:44:45.161
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:44:45.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8910" for this suite. 01/04/23 22:44:45.797
------------------------------
• [SLOW TEST] [6.935 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:44:38.872
    Jan  4 22:44:38.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-webhook 01/04/23 22:44:38.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:38.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:38.895
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/04/23 22:44:38.899
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/04/23 22:44:39.476
    STEP: Deploying the custom resource conversion webhook pod 01/04/23 22:44:39.481
    STEP: Wait for the deployment to be ready 01/04/23 22:44:39.492
    Jan  4 22:44:39.498: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 22:44:41.528
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:44:41.554
    Jan  4 22:44:42.555: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan  4 22:44:42.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Creating a v1 custom resource 01/04/23 22:44:45.148
    STEP: v2 custom resource should be converted 01/04/23 22:44:45.161
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:44:45.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8910" for this suite. 01/04/23 22:44:45.797
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:44:45.81
Jan  4 22:44:45.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename daemonsets 01/04/23 22:44:45.812
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:45.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:45.853
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan  4 22:44:45.879: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 22:44:45.884
Jan  4 22:44:45.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:44:45.897: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:44:46.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:44:46.908: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:44:47.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 22:44:47.905: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image. 01/04/23 22:44:47.914
STEP: Check that daemon pods images are updated. 01/04/23 22:44:47.93
Jan  4 22:44:47.934: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:47.934: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:47.934: INFO: Wrong image for pod: daemon-set-h7zvp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:47.934: INFO: Wrong image for pod: daemon-set-v9v9g. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:48.942: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:48.942: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:48.942: INFO: Wrong image for pod: daemon-set-h7zvp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:49.943: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:49.943: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:49.943: INFO: Wrong image for pod: daemon-set-h7zvp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:50.943: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:50.943: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:50.943: INFO: Pod daemon-set-7zvc9 is not available
Jan  4 22:44:50.943: INFO: Wrong image for pod: daemon-set-h7zvp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:51.943: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:51.943: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:52.945: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:52.945: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:52.945: INFO: Pod daemon-set-npv2p is not available
Jan  4 22:44:53.942: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:54.946: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  4 22:44:54.946: INFO: Pod daemon-set-ntkxg is not available
Jan  4 22:44:56.945: INFO: Pod daemon-set-scvxn is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/04/23 22:44:56.949
Jan  4 22:44:56.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  4 22:44:56.980: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:44:57.988: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 22:44:57.988: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:44:58.003
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3695, will wait for the garbage collector to delete the pods 01/04/23 22:44:58.003
Jan  4 22:44:58.066: INFO: Deleting DaemonSet.extensions daemon-set took: 10.000498ms
Jan  4 22:44:58.167: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.436896ms
Jan  4 22:45:00.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:45:00.670: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  4 22:45:00.676: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46373"},"items":null}

Jan  4 22:45:00.683: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46373"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:45:00.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3695" for this suite. 01/04/23 22:45:00.733
------------------------------
• [SLOW TEST] [14.936 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:44:45.81
    Jan  4 22:44:45.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename daemonsets 01/04/23 22:44:45.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:44:45.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:44:45.853
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan  4 22:44:45.879: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 22:44:45.884
    Jan  4 22:44:45.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:44:45.897: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:44:46.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:44:46.908: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:44:47.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 22:44:47.905: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Update daemon pods image. 01/04/23 22:44:47.914
    STEP: Check that daemon pods images are updated. 01/04/23 22:44:47.93
    Jan  4 22:44:47.934: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:47.934: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:47.934: INFO: Wrong image for pod: daemon-set-h7zvp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:47.934: INFO: Wrong image for pod: daemon-set-v9v9g. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:48.942: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:48.942: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:48.942: INFO: Wrong image for pod: daemon-set-h7zvp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:49.943: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:49.943: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:49.943: INFO: Wrong image for pod: daemon-set-h7zvp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:50.943: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:50.943: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:50.943: INFO: Pod daemon-set-7zvc9 is not available
    Jan  4 22:44:50.943: INFO: Wrong image for pod: daemon-set-h7zvp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:51.943: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:51.943: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:52.945: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:52.945: INFO: Wrong image for pod: daemon-set-7pj8n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:52.945: INFO: Pod daemon-set-npv2p is not available
    Jan  4 22:44:53.942: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:54.946: INFO: Wrong image for pod: daemon-set-47dh4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  4 22:44:54.946: INFO: Pod daemon-set-ntkxg is not available
    Jan  4 22:44:56.945: INFO: Pod daemon-set-scvxn is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/04/23 22:44:56.949
    Jan  4 22:44:56.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  4 22:44:56.980: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:44:57.988: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 22:44:57.988: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:44:58.003
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3695, will wait for the garbage collector to delete the pods 01/04/23 22:44:58.003
    Jan  4 22:44:58.066: INFO: Deleting DaemonSet.extensions daemon-set took: 10.000498ms
    Jan  4 22:44:58.167: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.436896ms
    Jan  4 22:45:00.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:45:00.670: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  4 22:45:00.676: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46373"},"items":null}

    Jan  4 22:45:00.683: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46373"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:45:00.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3695" for this suite. 01/04/23 22:45:00.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:45:00.754
Jan  4 22:45:00.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename prestop 01/04/23 22:45:00.756
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:00.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:00.802
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2375 01/04/23 22:45:00.808
STEP: Waiting for pods to come up. 01/04/23 22:45:00.83
Jan  4 22:45:00.831: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2375" to be "running"
Jan  4 22:45:00.837: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.276157ms
Jan  4 22:45:02.842: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011294705s
Jan  4 22:45:02.842: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2375 01/04/23 22:45:02.845
Jan  4 22:45:02.852: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2375" to be "running"
Jan  4 22:45:02.859: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.884676ms
Jan  4 22:45:04.863: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.011319666s
Jan  4 22:45:04.863: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/04/23 22:45:04.863
Jan  4 22:45:09.880: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/04/23 22:45:09.881
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan  4 22:45:09.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2375" for this suite. 01/04/23 22:45:09.923
------------------------------
• [SLOW TEST] [9.178 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:45:00.754
    Jan  4 22:45:00.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename prestop 01/04/23 22:45:00.756
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:00.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:00.802
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2375 01/04/23 22:45:00.808
    STEP: Waiting for pods to come up. 01/04/23 22:45:00.83
    Jan  4 22:45:00.831: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2375" to be "running"
    Jan  4 22:45:00.837: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.276157ms
    Jan  4 22:45:02.842: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011294705s
    Jan  4 22:45:02.842: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2375 01/04/23 22:45:02.845
    Jan  4 22:45:02.852: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2375" to be "running"
    Jan  4 22:45:02.859: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.884676ms
    Jan  4 22:45:04.863: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.011319666s
    Jan  4 22:45:04.863: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/04/23 22:45:04.863
    Jan  4 22:45:09.880: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/04/23 22:45:09.881
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:45:09.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2375" for this suite. 01/04/23 22:45:09.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:45:09.936
Jan  4 22:45:09.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:45:09.937
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:09.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:09.991
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-7177 01/04/23 22:45:09.994
STEP: creating service affinity-clusterip-transition in namespace services-7177 01/04/23 22:45:09.995
STEP: creating replication controller affinity-clusterip-transition in namespace services-7177 01/04/23 22:45:10.013
I0104 22:45:10.030030      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7177, replica count: 3
I0104 22:45:13.081407      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 22:45:13.088: INFO: Creating new exec pod
Jan  4 22:45:13.094: INFO: Waiting up to 5m0s for pod "execpod-affinityqnqxj" in namespace "services-7177" to be "running"
Jan  4 22:45:13.097: INFO: Pod "execpod-affinityqnqxj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507089ms
Jan  4 22:45:15.102: INFO: Pod "execpod-affinityqnqxj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007732332s
Jan  4 22:45:15.102: INFO: Pod "execpod-affinityqnqxj" satisfied condition "running"
Jan  4 22:45:16.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-7177 exec execpod-affinityqnqxj -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan  4 22:45:16.388: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan  4 22:45:16.388: INFO: stdout: ""
Jan  4 22:45:16.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-7177 exec execpod-affinityqnqxj -- /bin/sh -x -c nc -v -z -w 2 10.43.100.152 80'
Jan  4 22:45:16.623: INFO: stderr: "+ nc -v -z -w 2 10.43.100.152 80\nConnection to 10.43.100.152 80 port [tcp/http] succeeded!\n"
Jan  4 22:45:16.623: INFO: stdout: ""
Jan  4 22:45:16.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-7177 exec execpod-affinityqnqxj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.100.152:80/ ; done'
Jan  4 22:45:16.972: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n"
Jan  4 22:45:16.972: INFO: stdout: "\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-d5824\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-d5824\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-d5824\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-d5824\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-d5824"
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
Jan  4 22:45:16.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-7177 exec execpod-affinityqnqxj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.100.152:80/ ; done'
Jan  4 22:45:17.204: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n"
Jan  4 22:45:17.204: INFO: stdout: "\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5"
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
Jan  4 22:45:17.205: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7177, will wait for the garbage collector to delete the pods 01/04/23 22:45:17.226
Jan  4 22:45:17.294: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.220565ms
Jan  4 22:45:17.395: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.828915ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:45:19.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7177" for this suite. 01/04/23 22:45:19.752
------------------------------
• [SLOW TEST] [9.843 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:45:09.936
    Jan  4 22:45:09.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:45:09.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:09.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:09.991
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-7177 01/04/23 22:45:09.994
    STEP: creating service affinity-clusterip-transition in namespace services-7177 01/04/23 22:45:09.995
    STEP: creating replication controller affinity-clusterip-transition in namespace services-7177 01/04/23 22:45:10.013
    I0104 22:45:10.030030      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7177, replica count: 3
    I0104 22:45:13.081407      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 22:45:13.088: INFO: Creating new exec pod
    Jan  4 22:45:13.094: INFO: Waiting up to 5m0s for pod "execpod-affinityqnqxj" in namespace "services-7177" to be "running"
    Jan  4 22:45:13.097: INFO: Pod "execpod-affinityqnqxj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507089ms
    Jan  4 22:45:15.102: INFO: Pod "execpod-affinityqnqxj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007732332s
    Jan  4 22:45:15.102: INFO: Pod "execpod-affinityqnqxj" satisfied condition "running"
    Jan  4 22:45:16.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-7177 exec execpod-affinityqnqxj -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan  4 22:45:16.388: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan  4 22:45:16.388: INFO: stdout: ""
    Jan  4 22:45:16.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-7177 exec execpod-affinityqnqxj -- /bin/sh -x -c nc -v -z -w 2 10.43.100.152 80'
    Jan  4 22:45:16.623: INFO: stderr: "+ nc -v -z -w 2 10.43.100.152 80\nConnection to 10.43.100.152 80 port [tcp/http] succeeded!\n"
    Jan  4 22:45:16.623: INFO: stdout: ""
    Jan  4 22:45:16.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-7177 exec execpod-affinityqnqxj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.100.152:80/ ; done'
    Jan  4 22:45:16.972: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n"
    Jan  4 22:45:16.972: INFO: stdout: "\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-d5824\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-d5824\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-d5824\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-d5824\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-92qvc\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-d5824"
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-92qvc
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:16.972: INFO: Received response from host: affinity-clusterip-transition-d5824
    Jan  4 22:45:16.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-7177 exec execpod-affinityqnqxj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.100.152:80/ ; done'
    Jan  4 22:45:17.204: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.100.152:80/\n"
    Jan  4 22:45:17.204: INFO: stdout: "\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5\naffinity-clusterip-transition-lk9m5"
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.204: INFO: Received response from host: affinity-clusterip-transition-lk9m5
    Jan  4 22:45:17.205: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7177, will wait for the garbage collector to delete the pods 01/04/23 22:45:17.226
    Jan  4 22:45:17.294: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.220565ms
    Jan  4 22:45:17.395: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.828915ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:45:19.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7177" for this suite. 01/04/23 22:45:19.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:45:19.785
Jan  4 22:45:19.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 22:45:19.786
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:19.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:19.841
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/04/23 22:45:19.843
STEP: Ensuring ResourceQuota status is calculated 01/04/23 22:45:19.847
STEP: Creating a ResourceQuota with not best effort scope 01/04/23 22:45:21.85
STEP: Ensuring ResourceQuota status is calculated 01/04/23 22:45:21.855
STEP: Creating a best-effort pod 01/04/23 22:45:23.858
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/04/23 22:45:23.869
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/04/23 22:45:25.873
STEP: Deleting the pod 01/04/23 22:45:27.878
STEP: Ensuring resource quota status released the pod usage 01/04/23 22:45:27.894
STEP: Creating a not best-effort pod 01/04/23 22:45:29.898
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/04/23 22:45:29.909
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/04/23 22:45:31.916
STEP: Deleting the pod 01/04/23 22:45:33.92
STEP: Ensuring resource quota status released the pod usage 01/04/23 22:45:33.936
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 22:45:35.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6010" for this suite. 01/04/23 22:45:35.946
------------------------------
• [SLOW TEST] [16.169 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:45:19.785
    Jan  4 22:45:19.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 22:45:19.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:19.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:19.841
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/04/23 22:45:19.843
    STEP: Ensuring ResourceQuota status is calculated 01/04/23 22:45:19.847
    STEP: Creating a ResourceQuota with not best effort scope 01/04/23 22:45:21.85
    STEP: Ensuring ResourceQuota status is calculated 01/04/23 22:45:21.855
    STEP: Creating a best-effort pod 01/04/23 22:45:23.858
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/04/23 22:45:23.869
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/04/23 22:45:25.873
    STEP: Deleting the pod 01/04/23 22:45:27.878
    STEP: Ensuring resource quota status released the pod usage 01/04/23 22:45:27.894
    STEP: Creating a not best-effort pod 01/04/23 22:45:29.898
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/04/23 22:45:29.909
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/04/23 22:45:31.916
    STEP: Deleting the pod 01/04/23 22:45:33.92
    STEP: Ensuring resource quota status released the pod usage 01/04/23 22:45:33.936
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:45:35.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6010" for this suite. 01/04/23 22:45:35.946
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:45:35.955
Jan  4 22:45:35.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 22:45:35.956
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:35.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:35.974
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/04/23 22:45:35.977
Jan  4 22:45:35.984: INFO: Waiting up to 5m0s for pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc" in namespace "downward-api-5106" to be "running and ready"
Jan  4 22:45:35.991: INFO: Pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.51038ms
Jan  4 22:45:35.991: INFO: The phase of Pod annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:45:37.997: INFO: Pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012127215s
Jan  4 22:45:37.997: INFO: The phase of Pod annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc is Running (Ready = true)
Jan  4 22:45:37.997: INFO: Pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc" satisfied condition "running and ready"
Jan  4 22:45:38.519: INFO: Successfully updated pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 22:45:40.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5106" for this suite. 01/04/23 22:45:40.54
------------------------------
• [4.593 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:45:35.955
    Jan  4 22:45:35.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 22:45:35.956
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:35.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:35.974
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/04/23 22:45:35.977
    Jan  4 22:45:35.984: INFO: Waiting up to 5m0s for pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc" in namespace "downward-api-5106" to be "running and ready"
    Jan  4 22:45:35.991: INFO: Pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.51038ms
    Jan  4 22:45:35.991: INFO: The phase of Pod annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:45:37.997: INFO: Pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012127215s
    Jan  4 22:45:37.997: INFO: The phase of Pod annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc is Running (Ready = true)
    Jan  4 22:45:37.997: INFO: Pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc" satisfied condition "running and ready"
    Jan  4 22:45:38.519: INFO: Successfully updated pod "annotationupdatebe1e2753-42b1-4659-8c85-e9a6907a2bbc"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:45:40.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5106" for this suite. 01/04/23 22:45:40.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:45:40.55
Jan  4 22:45:40.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:45:40.551
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:40.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:40.573
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-aedf5f22-6d32-4957-91e8-bb0ceca8eb57 01/04/23 22:45:40.576
STEP: Creating a pod to test consume secrets 01/04/23 22:45:40.586
Jan  4 22:45:40.601: INFO: Waiting up to 5m0s for pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4" in namespace "secrets-6867" to be "Succeeded or Failed"
Jan  4 22:45:40.607: INFO: Pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.726572ms
Jan  4 22:45:42.611: INFO: Pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009032259s
Jan  4 22:45:44.611: INFO: Pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009510573s
STEP: Saw pod success 01/04/23 22:45:44.611
Jan  4 22:45:44.611: INFO: Pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4" satisfied condition "Succeeded or Failed"
Jan  4 22:45:44.613: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4 container secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:45:44.619
Jan  4 22:45:44.630: INFO: Waiting for pod pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4 to disappear
Jan  4 22:45:44.632: INFO: Pod pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:45:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6867" for this suite. 01/04/23 22:45:44.636
------------------------------
• [4.095 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:45:40.55
    Jan  4 22:45:40.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:45:40.551
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:40.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:40.573
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-aedf5f22-6d32-4957-91e8-bb0ceca8eb57 01/04/23 22:45:40.576
    STEP: Creating a pod to test consume secrets 01/04/23 22:45:40.586
    Jan  4 22:45:40.601: INFO: Waiting up to 5m0s for pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4" in namespace "secrets-6867" to be "Succeeded or Failed"
    Jan  4 22:45:40.607: INFO: Pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.726572ms
    Jan  4 22:45:42.611: INFO: Pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009032259s
    Jan  4 22:45:44.611: INFO: Pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009510573s
    STEP: Saw pod success 01/04/23 22:45:44.611
    Jan  4 22:45:44.611: INFO: Pod "pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4" satisfied condition "Succeeded or Failed"
    Jan  4 22:45:44.613: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4 container secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:45:44.619
    Jan  4 22:45:44.630: INFO: Waiting for pod pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4 to disappear
    Jan  4 22:45:44.632: INFO: Pod pod-secrets-51864c3d-493b-4d0d-9aba-6e039510e4b4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:45:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6867" for this suite. 01/04/23 22:45:44.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:45:44.649
Jan  4 22:45:44.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename init-container 01/04/23 22:45:44.65
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:44.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:44.679
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/04/23 22:45:44.682
Jan  4 22:45:44.682: INFO: PodSpec: initContainers in spec.initContainers
Jan  4 22:46:31.893: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-303051bf-1421-418a-8eed-622df1d84bc4", GenerateName:"", Namespace:"init-container-8649", SelfLink:"", UID:"70c6159f-0ddb-4058-b257-922cc82f2389", ResourceVersion:"47057", Generation:0, CreationTimestamp:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"682509640"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"71344bf522ed5edcd75d27722dd9d3af43dff896ecbdde12c2ca9095f21bdc15", "cni.projectcalico.org/podIP":"10.42.3.197/32", "cni.projectcalico.org/podIPs":"10.42.3.197/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ef2d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 4, 22, 45, 45, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ef320), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 4, 22, 46, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ef350), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8xgcf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0030abfc0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8xgcf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8xgcf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8xgcf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002e45318), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-13-117.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0001721c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e45390)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e453b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002e453b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002e453bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0002eeab0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.13.117", PodIP:"10.42.3.197", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.42.3.197"}}, StartTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000172380)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001723f0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://adac0b51505cbd225d87c4cc61407bd97a95e5be9a77da96ff2cac7f92165492", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001314040), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001314020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002e4543f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:31.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8649" for this suite. 01/04/23 22:46:31.9
------------------------------
• [SLOW TEST] [47.259 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:45:44.649
    Jan  4 22:45:44.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename init-container 01/04/23 22:45:44.65
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:45:44.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:45:44.679
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/04/23 22:45:44.682
    Jan  4 22:45:44.682: INFO: PodSpec: initContainers in spec.initContainers
    Jan  4 22:46:31.893: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-303051bf-1421-418a-8eed-622df1d84bc4", GenerateName:"", Namespace:"init-container-8649", SelfLink:"", UID:"70c6159f-0ddb-4058-b257-922cc82f2389", ResourceVersion:"47057", Generation:0, CreationTimestamp:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"682509640"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"71344bf522ed5edcd75d27722dd9d3af43dff896ecbdde12c2ca9095f21bdc15", "cni.projectcalico.org/podIP":"10.42.3.197/32", "cni.projectcalico.org/podIPs":"10.42.3.197/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ef2d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 4, 22, 45, 45, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ef320), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 4, 22, 46, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ef350), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8xgcf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0030abfc0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8xgcf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8xgcf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8xgcf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002e45318), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-13-117.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0001721c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e45390)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e453b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002e453b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002e453bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0002eeab0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.13.117", PodIP:"10.42.3.197", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.42.3.197"}}, StartTime:time.Date(2023, time.January, 4, 22, 45, 44, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000172380)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001723f0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://adac0b51505cbd225d87c4cc61407bd97a95e5be9a77da96ff2cac7f92165492", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001314040), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001314020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002e4543f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:31.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8649" for this suite. 01/04/23 22:46:31.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:31.918
Jan  4 22:46:31.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename watch 01/04/23 22:46:31.92
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:31.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:31.939
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/04/23 22:46:31.941
STEP: modifying the configmap once 01/04/23 22:46:31.949
STEP: modifying the configmap a second time 01/04/23 22:46:31.956
STEP: deleting the configmap 01/04/23 22:46:31.97
STEP: creating a watch on configmaps from the resource version returned by the first update 01/04/23 22:46:31.976
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/04/23 22:46:31.977
Jan  4 22:46:31.978: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1094  7609f610-8f69-4f7c-abd1-84049d7d5f6f 47065 0 2023-01-04 22:46:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-04 22:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:46:31.978: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1094  7609f610-8f69-4f7c-abd1-84049d7d5f6f 47066 0 2023-01-04 22:46:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-04 22:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:31.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1094" for this suite. 01/04/23 22:46:31.983
------------------------------
• [0.071 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:31.918
    Jan  4 22:46:31.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename watch 01/04/23 22:46:31.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:31.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:31.939
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/04/23 22:46:31.941
    STEP: modifying the configmap once 01/04/23 22:46:31.949
    STEP: modifying the configmap a second time 01/04/23 22:46:31.956
    STEP: deleting the configmap 01/04/23 22:46:31.97
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/04/23 22:46:31.976
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/04/23 22:46:31.977
    Jan  4 22:46:31.978: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1094  7609f610-8f69-4f7c-abd1-84049d7d5f6f 47065 0 2023-01-04 22:46:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-04 22:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:46:31.978: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1094  7609f610-8f69-4f7c-abd1-84049d7d5f6f 47066 0 2023-01-04 22:46:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-04 22:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:31.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1094" for this suite. 01/04/23 22:46:31.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:31.994
Jan  4 22:46:31.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename podtemplate 01/04/23 22:46:31.994
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:32.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:32.02
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/04/23 22:46:32.023
Jan  4 22:46:32.030: INFO: created test-podtemplate-1
Jan  4 22:46:32.035: INFO: created test-podtemplate-2
Jan  4 22:46:32.043: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/04/23 22:46:32.043
STEP: delete collection of pod templates 01/04/23 22:46:32.047
Jan  4 22:46:32.047: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/04/23 22:46:32.061
Jan  4 22:46:32.061: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:32.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2008" for this suite. 01/04/23 22:46:32.067
------------------------------
• [0.085 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:31.994
    Jan  4 22:46:31.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename podtemplate 01/04/23 22:46:31.994
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:32.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:32.02
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/04/23 22:46:32.023
    Jan  4 22:46:32.030: INFO: created test-podtemplate-1
    Jan  4 22:46:32.035: INFO: created test-podtemplate-2
    Jan  4 22:46:32.043: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/04/23 22:46:32.043
    STEP: delete collection of pod templates 01/04/23 22:46:32.047
    Jan  4 22:46:32.047: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/04/23 22:46:32.061
    Jan  4 22:46:32.061: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:32.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2008" for this suite. 01/04/23 22:46:32.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:32.081
Jan  4 22:46:32.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:46:32.082
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:32.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:32.099
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-9b6ba5a5-b3cf-4cd2-9907-b4077cce0a09 01/04/23 22:46:32.102
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:32.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-14" for this suite. 01/04/23 22:46:32.11
------------------------------
• [0.035 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:32.081
    Jan  4 22:46:32.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:46:32.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:32.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:32.099
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-9b6ba5a5-b3cf-4cd2-9907-b4077cce0a09 01/04/23 22:46:32.102
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:32.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-14" for this suite. 01/04/23 22:46:32.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:32.118
Jan  4 22:46:32.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:46:32.119
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:32.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:32.137
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:46:32.14
Jan  4 22:46:32.147: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732" in namespace "projected-2841" to be "Succeeded or Failed"
Jan  4 22:46:32.153: INFO: Pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114821ms
Jan  4 22:46:34.157: INFO: Pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009689247s
Jan  4 22:46:36.156: INFO: Pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008616381s
STEP: Saw pod success 01/04/23 22:46:36.156
Jan  4 22:46:36.156: INFO: Pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732" satisfied condition "Succeeded or Failed"
Jan  4 22:46:36.160: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732 container client-container: <nil>
STEP: delete the pod 01/04/23 22:46:36.165
Jan  4 22:46:36.175: INFO: Waiting for pod downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732 to disappear
Jan  4 22:46:36.178: INFO: Pod downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:36.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2841" for this suite. 01/04/23 22:46:36.182
------------------------------
• [4.070 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:32.118
    Jan  4 22:46:32.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:46:32.119
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:32.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:32.137
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:46:32.14
    Jan  4 22:46:32.147: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732" in namespace "projected-2841" to be "Succeeded or Failed"
    Jan  4 22:46:32.153: INFO: Pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114821ms
    Jan  4 22:46:34.157: INFO: Pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009689247s
    Jan  4 22:46:36.156: INFO: Pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008616381s
    STEP: Saw pod success 01/04/23 22:46:36.156
    Jan  4 22:46:36.156: INFO: Pod "downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732" satisfied condition "Succeeded or Failed"
    Jan  4 22:46:36.160: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732 container client-container: <nil>
    STEP: delete the pod 01/04/23 22:46:36.165
    Jan  4 22:46:36.175: INFO: Waiting for pod downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732 to disappear
    Jan  4 22:46:36.178: INFO: Pod downwardapi-volume-d01bd7a2-9365-4974-8c08-98d5bb441732 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:36.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2841" for this suite. 01/04/23 22:46:36.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:36.19
Jan  4 22:46:36.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename var-expansion 01/04/23 22:46:36.191
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:36.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:36.218
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/04/23 22:46:36.22
Jan  4 22:46:36.228: INFO: Waiting up to 5m0s for pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4" in namespace "var-expansion-638" to be "Succeeded or Failed"
Jan  4 22:46:36.234: INFO: Pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.224727ms
Jan  4 22:46:38.238: INFO: Pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009064271s
Jan  4 22:46:40.238: INFO: Pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009328804s
STEP: Saw pod success 01/04/23 22:46:40.238
Jan  4 22:46:40.238: INFO: Pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4" satisfied condition "Succeeded or Failed"
Jan  4 22:46:40.241: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod var-expansion-dceaae65-9851-4464-93ff-ff87336832b4 container dapi-container: <nil>
STEP: delete the pod 01/04/23 22:46:40.247
Jan  4 22:46:40.258: INFO: Waiting for pod var-expansion-dceaae65-9851-4464-93ff-ff87336832b4 to disappear
Jan  4 22:46:40.260: INFO: Pod var-expansion-dceaae65-9851-4464-93ff-ff87336832b4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:40.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-638" for this suite. 01/04/23 22:46:40.266
------------------------------
• [4.083 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:36.19
    Jan  4 22:46:36.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename var-expansion 01/04/23 22:46:36.191
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:36.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:36.218
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/04/23 22:46:36.22
    Jan  4 22:46:36.228: INFO: Waiting up to 5m0s for pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4" in namespace "var-expansion-638" to be "Succeeded or Failed"
    Jan  4 22:46:36.234: INFO: Pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.224727ms
    Jan  4 22:46:38.238: INFO: Pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009064271s
    Jan  4 22:46:40.238: INFO: Pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009328804s
    STEP: Saw pod success 01/04/23 22:46:40.238
    Jan  4 22:46:40.238: INFO: Pod "var-expansion-dceaae65-9851-4464-93ff-ff87336832b4" satisfied condition "Succeeded or Failed"
    Jan  4 22:46:40.241: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod var-expansion-dceaae65-9851-4464-93ff-ff87336832b4 container dapi-container: <nil>
    STEP: delete the pod 01/04/23 22:46:40.247
    Jan  4 22:46:40.258: INFO: Waiting for pod var-expansion-dceaae65-9851-4464-93ff-ff87336832b4 to disappear
    Jan  4 22:46:40.260: INFO: Pod var-expansion-dceaae65-9851-4464-93ff-ff87336832b4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:40.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-638" for this suite. 01/04/23 22:46:40.266
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:40.274
Jan  4 22:46:40.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:46:40.275
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:40.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:40.293
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/04/23 22:46:40.296
Jan  4 22:46:40.296: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-1323 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/04/23 22:46:40.352
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:40.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1323" for this suite. 01/04/23 22:46:40.36
------------------------------
• [0.093 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:40.274
    Jan  4 22:46:40.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:46:40.275
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:40.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:40.293
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/04/23 22:46:40.296
    Jan  4 22:46:40.296: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-1323 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/04/23 22:46:40.352
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:40.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1323" for this suite. 01/04/23 22:46:40.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:40.368
Jan  4 22:46:40.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename security-context-test 01/04/23 22:46:40.369
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:40.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:40.387
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan  4 22:46:40.399: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b" in namespace "security-context-test-6945" to be "Succeeded or Failed"
Jan  4 22:46:40.401: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253251ms
Jan  4 22:46:42.406: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006820012s
Jan  4 22:46:44.406: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006720795s
Jan  4 22:46:46.405: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005738373s
Jan  4 22:46:46.405: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:46.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6945" for this suite. 01/04/23 22:46:46.419
------------------------------
• [SLOW TEST] [6.057 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:40.368
    Jan  4 22:46:40.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename security-context-test 01/04/23 22:46:40.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:40.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:40.387
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan  4 22:46:40.399: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b" in namespace "security-context-test-6945" to be "Succeeded or Failed"
    Jan  4 22:46:40.401: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253251ms
    Jan  4 22:46:42.406: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006820012s
    Jan  4 22:46:44.406: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006720795s
    Jan  4 22:46:46.405: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005738373s
    Jan  4 22:46:46.405: INFO: Pod "alpine-nnp-false-9f6c3e77-9caa-4cd0-996d-0d89e793922b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:46.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6945" for this suite. 01/04/23 22:46:46.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:46.427
Jan  4 22:46:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename dns 01/04/23 22:46:46.428
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:46.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:46.448
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/04/23 22:46:46.45
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/04/23 22:46:46.455
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/04/23 22:46:46.455
STEP: creating a pod to probe DNS 01/04/23 22:46:46.455
STEP: submitting the pod to kubernetes 01/04/23 22:46:46.456
Jan  4 22:46:46.466: INFO: Waiting up to 15m0s for pod "dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86" in namespace "dns-2569" to be "running"
Jan  4 22:46:46.481: INFO: Pod "dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 15.359927ms
Jan  4 22:46:48.485: INFO: Pod "dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86": Phase="Running", Reason="", readiness=true. Elapsed: 2.018801604s
Jan  4 22:46:48.485: INFO: Pod "dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86" satisfied condition "running"
STEP: retrieving the pod 01/04/23 22:46:48.485
STEP: looking for the results for each expected name from probers 01/04/23 22:46:48.488
Jan  4 22:46:48.503: INFO: DNS probes using dns-2569/dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86 succeeded

STEP: deleting the pod 01/04/23 22:46:48.503
STEP: deleting the test headless service 01/04/23 22:46:48.519
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:48.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2569" for this suite. 01/04/23 22:46:48.542
------------------------------
• [2.126 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:46.427
    Jan  4 22:46:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename dns 01/04/23 22:46:46.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:46.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:46.448
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/04/23 22:46:46.45
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/04/23 22:46:46.455
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/04/23 22:46:46.455
    STEP: creating a pod to probe DNS 01/04/23 22:46:46.455
    STEP: submitting the pod to kubernetes 01/04/23 22:46:46.456
    Jan  4 22:46:46.466: INFO: Waiting up to 15m0s for pod "dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86" in namespace "dns-2569" to be "running"
    Jan  4 22:46:46.481: INFO: Pod "dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 15.359927ms
    Jan  4 22:46:48.485: INFO: Pod "dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86": Phase="Running", Reason="", readiness=true. Elapsed: 2.018801604s
    Jan  4 22:46:48.485: INFO: Pod "dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 22:46:48.485
    STEP: looking for the results for each expected name from probers 01/04/23 22:46:48.488
    Jan  4 22:46:48.503: INFO: DNS probes using dns-2569/dns-test-d1147e4a-5c0c-4612-bcfe-aa08e7c6aa86 succeeded

    STEP: deleting the pod 01/04/23 22:46:48.503
    STEP: deleting the test headless service 01/04/23 22:46:48.519
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:48.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2569" for this suite. 01/04/23 22:46:48.542
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:48.554
Jan  4 22:46:48.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:46:48.555
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:48.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:48.579
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:46:48.597
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:46:49.216
STEP: Deploying the webhook pod 01/04/23 22:46:49.224
STEP: Wait for the deployment to be ready 01/04/23 22:46:49.236
Jan  4 22:46:49.249: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 22:46:51.258
STEP: Verifying the service has paired with the endpoint 01/04/23 22:46:51.268
Jan  4 22:46:52.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan  4 22:46:52.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1765-crds.webhook.example.com via the AdmissionRegistration API 01/04/23 22:46:52.784
STEP: Creating a custom resource that should be mutated by the webhook 01/04/23 22:46:52.811
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:46:55.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4640" for this suite. 01/04/23 22:46:55.446
STEP: Destroying namespace "webhook-4640-markers" for this suite. 01/04/23 22:46:55.465
------------------------------
• [SLOW TEST] [6.948 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:48.554
    Jan  4 22:46:48.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:46:48.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:48.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:48.579
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:46:48.597
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:46:49.216
    STEP: Deploying the webhook pod 01/04/23 22:46:49.224
    STEP: Wait for the deployment to be ready 01/04/23 22:46:49.236
    Jan  4 22:46:49.249: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 22:46:51.258
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:46:51.268
    Jan  4 22:46:52.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan  4 22:46:52.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1765-crds.webhook.example.com via the AdmissionRegistration API 01/04/23 22:46:52.784
    STEP: Creating a custom resource that should be mutated by the webhook 01/04/23 22:46:52.811
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:46:55.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4640" for this suite. 01/04/23 22:46:55.446
    STEP: Destroying namespace "webhook-4640-markers" for this suite. 01/04/23 22:46:55.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:46:55.504
Jan  4 22:46:55.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename job 01/04/23 22:46:55.51
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:55.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:55.559
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/04/23 22:46:55.563
STEP: Ensuring job reaches completions 01/04/23 22:46:55.568
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  4 22:47:07.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-120" for this suite. 01/04/23 22:47:07.576
------------------------------
• [SLOW TEST] [12.084 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:46:55.504
    Jan  4 22:46:55.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename job 01/04/23 22:46:55.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:46:55.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:46:55.559
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/04/23 22:46:55.563
    STEP: Ensuring job reaches completions 01/04/23 22:46:55.568
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:47:07.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-120" for this suite. 01/04/23 22:47:07.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:47:07.593
Jan  4 22:47:07.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename namespaces 01/04/23 22:47:07.595
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:07.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:07.614
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/04/23 22:47:07.615
STEP: patching the Namespace 01/04/23 22:47:07.631
STEP: get the Namespace and ensuring it has the label 01/04/23 22:47:07.64
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:47:07.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3773" for this suite. 01/04/23 22:47:07.649
STEP: Destroying namespace "nspatchtest-b21c96fb-f4e1-4e68-8008-e972639cf995-4100" for this suite. 01/04/23 22:47:07.665
------------------------------
• [0.080 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:47:07.593
    Jan  4 22:47:07.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename namespaces 01/04/23 22:47:07.595
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:07.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:07.614
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/04/23 22:47:07.615
    STEP: patching the Namespace 01/04/23 22:47:07.631
    STEP: get the Namespace and ensuring it has the label 01/04/23 22:47:07.64
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:47:07.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3773" for this suite. 01/04/23 22:47:07.649
    STEP: Destroying namespace "nspatchtest-b21c96fb-f4e1-4e68-8008-e972639cf995-4100" for this suite. 01/04/23 22:47:07.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:47:07.674
Jan  4 22:47:07.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename deployment 01/04/23 22:47:07.675
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:07.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:07.703
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/04/23 22:47:07.71
STEP: waiting for Deployment to be created 01/04/23 22:47:07.716
STEP: waiting for all Replicas to be Ready 01/04/23 22:47:07.717
Jan  4 22:47:07.718: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  4 22:47:07.718: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  4 22:47:07.730: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  4 22:47:07.730: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  4 22:47:07.756: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  4 22:47:07.756: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  4 22:47:07.828: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  4 22:47:07.828: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  4 22:47:09.103: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  4 22:47:09.103: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  4 22:47:09.310: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/04/23 22:47:09.31
W0104 22:47:09.320828      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  4 22:47:09.322: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/04/23 22:47:09.322
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:09.325: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:09.325: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:09.325: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:09.325: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:09.330: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:09.330: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:09.350: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:09.350: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:09.369: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:09.369: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:09.374: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:09.374: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:11.131: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:11.131: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:11.169: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
STEP: listing Deployments 01/04/23 22:47:11.169
Jan  4 22:47:11.175: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/04/23 22:47:11.175
Jan  4 22:47:11.190: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/04/23 22:47:11.19
Jan  4 22:47:11.201: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:11.207: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:11.241: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:11.276: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:11.299: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:12.851: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:13.134: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:13.193: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:13.222: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  4 22:47:14.332: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/04/23 22:47:14.354
STEP: fetching the DeploymentStatus 01/04/23 22:47:14.361
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 3
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 3
STEP: deleting the Deployment 01/04/23 22:47:14.366
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.373: INFO: observed event type MODIFIED
Jan  4 22:47:14.374: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  4 22:47:14.378: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  4 22:47:14.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6229" for this suite. 01/04/23 22:47:14.386
------------------------------
• [SLOW TEST] [6.724 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:47:07.674
    Jan  4 22:47:07.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename deployment 01/04/23 22:47:07.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:07.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:07.703
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/04/23 22:47:07.71
    STEP: waiting for Deployment to be created 01/04/23 22:47:07.716
    STEP: waiting for all Replicas to be Ready 01/04/23 22:47:07.717
    Jan  4 22:47:07.718: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  4 22:47:07.718: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  4 22:47:07.730: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  4 22:47:07.730: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  4 22:47:07.756: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  4 22:47:07.756: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  4 22:47:07.828: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  4 22:47:07.828: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  4 22:47:09.103: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  4 22:47:09.103: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  4 22:47:09.310: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/04/23 22:47:09.31
    W0104 22:47:09.320828      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  4 22:47:09.322: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/04/23 22:47:09.322
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 0
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:09.324: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:09.325: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:09.325: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:09.325: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:09.325: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:09.330: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:09.330: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:09.350: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:09.350: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:09.369: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:09.369: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:09.374: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:09.374: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:11.131: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:11.131: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:11.169: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    STEP: listing Deployments 01/04/23 22:47:11.169
    Jan  4 22:47:11.175: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/04/23 22:47:11.175
    Jan  4 22:47:11.190: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/04/23 22:47:11.19
    Jan  4 22:47:11.201: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:11.207: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:11.241: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:11.276: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:11.299: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:12.851: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:13.134: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:13.193: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:13.222: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  4 22:47:14.332: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/04/23 22:47:14.354
    STEP: fetching the DeploymentStatus 01/04/23 22:47:14.361
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 1
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 3
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 2
    Jan  4 22:47:14.365: INFO: observed Deployment test-deployment in namespace deployment-6229 with ReadyReplicas 3
    STEP: deleting the Deployment 01/04/23 22:47:14.366
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.373: INFO: observed event type MODIFIED
    Jan  4 22:47:14.374: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  4 22:47:14.378: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:47:14.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6229" for this suite. 01/04/23 22:47:14.386
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:47:14.399
Jan  4 22:47:14.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pod-network-test 01/04/23 22:47:14.401
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:14.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:14.42
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9226 01/04/23 22:47:14.422
STEP: creating a selector 01/04/23 22:47:14.422
STEP: Creating the service pods in kubernetes 01/04/23 22:47:14.422
Jan  4 22:47:14.422: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  4 22:47:14.464: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9226" to be "running and ready"
Jan  4 22:47:14.471: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232129ms
Jan  4 22:47:14.471: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:47:16.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010643425s
Jan  4 22:47:16.475: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:18.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00994743s
Jan  4 22:47:18.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:20.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011077273s
Jan  4 22:47:20.476: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:22.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010934561s
Jan  4 22:47:22.475: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:24.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020910879s
Jan  4 22:47:24.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:26.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009991565s
Jan  4 22:47:26.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:28.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011011923s
Jan  4 22:47:28.476: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:30.476: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011208057s
Jan  4 22:47:30.476: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:32.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009483108s
Jan  4 22:47:32.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:34.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010344515s
Jan  4 22:47:34.475: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:47:36.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010413613s
Jan  4 22:47:36.475: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  4 22:47:36.475: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  4 22:47:36.478: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9226" to be "running and ready"
Jan  4 22:47:36.480: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.357783ms
Jan  4 22:47:36.480: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  4 22:47:36.480: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  4 22:47:36.488: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9226" to be "running and ready"
Jan  4 22:47:36.491: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.087508ms
Jan  4 22:47:36.491: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  4 22:47:36.491: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan  4 22:47:36.494: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9226" to be "running and ready"
Jan  4 22:47:36.496: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.327055ms
Jan  4 22:47:36.496: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan  4 22:47:36.496: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 01/04/23 22:47:36.498
Jan  4 22:47:36.512: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9226" to be "running"
Jan  4 22:47:36.517: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.058336ms
Jan  4 22:47:38.521: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008772194s
Jan  4 22:47:38.521: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  4 22:47:38.523: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9226" to be "running"
Jan  4 22:47:38.525: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.197805ms
Jan  4 22:47:38.525: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  4 22:47:38.528: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan  4 22:47:38.528: INFO: Going to poll 10.42.0.95 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jan  4 22:47:38.530: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:47:38.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:47:38.530: INFO: ExecWithOptions: Clientset creation
Jan  4 22:47:38.530: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.0.95+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 22:47:39.615: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  4 22:47:39.615: INFO: Going to poll 10.42.3.210 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jan  4 22:47:39.618: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.3.210 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:47:39.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:47:39.619: INFO: ExecWithOptions: Clientset creation
Jan  4 22:47:39.619: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.3.210+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 22:47:40.693: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  4 22:47:40.694: INFO: Going to poll 10.42.1.92 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jan  4 22:47:40.697: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.1.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:47:40.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:47:40.698: INFO: ExecWithOptions: Clientset creation
Jan  4 22:47:40.698: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.1.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 22:47:41.795: INFO: Found all 1 expected endpoints: [netserver-2]
Jan  4 22:47:41.795: INFO: Going to poll 10.42.2.101 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jan  4 22:47:41.800: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.2.101 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:47:41.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:47:41.801: INFO: ExecWithOptions: Clientset creation
Jan  4 22:47:41.801: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.2.101+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 22:47:42.893: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  4 22:47:42.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9226" for this suite. 01/04/23 22:47:42.899
------------------------------
• [SLOW TEST] [28.515 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:47:14.399
    Jan  4 22:47:14.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pod-network-test 01/04/23 22:47:14.401
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:14.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:14.42
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9226 01/04/23 22:47:14.422
    STEP: creating a selector 01/04/23 22:47:14.422
    STEP: Creating the service pods in kubernetes 01/04/23 22:47:14.422
    Jan  4 22:47:14.422: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  4 22:47:14.464: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9226" to be "running and ready"
    Jan  4 22:47:14.471: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232129ms
    Jan  4 22:47:14.471: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:47:16.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010643425s
    Jan  4 22:47:16.475: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:18.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00994743s
    Jan  4 22:47:18.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:20.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011077273s
    Jan  4 22:47:20.476: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:22.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010934561s
    Jan  4 22:47:22.475: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:24.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020910879s
    Jan  4 22:47:24.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:26.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009991565s
    Jan  4 22:47:26.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:28.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011011923s
    Jan  4 22:47:28.476: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:30.476: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011208057s
    Jan  4 22:47:30.476: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:32.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009483108s
    Jan  4 22:47:32.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:34.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010344515s
    Jan  4 22:47:34.475: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:47:36.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010413613s
    Jan  4 22:47:36.475: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  4 22:47:36.475: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  4 22:47:36.478: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9226" to be "running and ready"
    Jan  4 22:47:36.480: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.357783ms
    Jan  4 22:47:36.480: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  4 22:47:36.480: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  4 22:47:36.488: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9226" to be "running and ready"
    Jan  4 22:47:36.491: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.087508ms
    Jan  4 22:47:36.491: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  4 22:47:36.491: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan  4 22:47:36.494: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9226" to be "running and ready"
    Jan  4 22:47:36.496: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.327055ms
    Jan  4 22:47:36.496: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan  4 22:47:36.496: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 01/04/23 22:47:36.498
    Jan  4 22:47:36.512: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9226" to be "running"
    Jan  4 22:47:36.517: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.058336ms
    Jan  4 22:47:38.521: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008772194s
    Jan  4 22:47:38.521: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  4 22:47:38.523: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9226" to be "running"
    Jan  4 22:47:38.525: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.197805ms
    Jan  4 22:47:38.525: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  4 22:47:38.528: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Jan  4 22:47:38.528: INFO: Going to poll 10.42.0.95 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Jan  4 22:47:38.530: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:47:38.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:47:38.530: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:47:38.530: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.0.95+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 22:47:39.615: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  4 22:47:39.615: INFO: Going to poll 10.42.3.210 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Jan  4 22:47:39.618: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.3.210 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:47:39.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:47:39.619: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:47:39.619: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.3.210+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 22:47:40.693: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  4 22:47:40.694: INFO: Going to poll 10.42.1.92 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Jan  4 22:47:40.697: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.1.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:47:40.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:47:40.698: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:47:40.698: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.1.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 22:47:41.795: INFO: Found all 1 expected endpoints: [netserver-2]
    Jan  4 22:47:41.795: INFO: Going to poll 10.42.2.101 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Jan  4 22:47:41.800: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.2.101 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:47:41.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:47:41.801: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:47:41.801: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.2.101+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 22:47:42.893: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:47:42.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9226" for this suite. 01/04/23 22:47:42.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:47:42.916
Jan  4 22:47:42.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 22:47:42.917
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:42.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:42.935
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/04/23 22:47:42.937
Jan  4 22:47:42.944: INFO: Waiting up to 5m0s for pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d" in namespace "downward-api-5753" to be "Succeeded or Failed"
Jan  4 22:47:42.950: INFO: Pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.108936ms
Jan  4 22:47:44.953: INFO: Pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008629778s
Jan  4 22:47:46.959: INFO: Pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014595434s
STEP: Saw pod success 01/04/23 22:47:46.959
Jan  4 22:47:46.959: INFO: Pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d" satisfied condition "Succeeded or Failed"
Jan  4 22:47:46.962: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d container dapi-container: <nil>
STEP: delete the pod 01/04/23 22:47:46.98
Jan  4 22:47:46.994: INFO: Waiting for pod downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d to disappear
Jan  4 22:47:46.996: INFO: Pod downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  4 22:47:46.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5753" for this suite. 01/04/23 22:47:46.999
------------------------------
• [4.089 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:47:42.916
    Jan  4 22:47:42.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 22:47:42.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:42.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:42.935
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/04/23 22:47:42.937
    Jan  4 22:47:42.944: INFO: Waiting up to 5m0s for pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d" in namespace "downward-api-5753" to be "Succeeded or Failed"
    Jan  4 22:47:42.950: INFO: Pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.108936ms
    Jan  4 22:47:44.953: INFO: Pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008629778s
    Jan  4 22:47:46.959: INFO: Pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014595434s
    STEP: Saw pod success 01/04/23 22:47:46.959
    Jan  4 22:47:46.959: INFO: Pod "downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d" satisfied condition "Succeeded or Failed"
    Jan  4 22:47:46.962: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d container dapi-container: <nil>
    STEP: delete the pod 01/04/23 22:47:46.98
    Jan  4 22:47:46.994: INFO: Waiting for pod downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d to disappear
    Jan  4 22:47:46.996: INFO: Pod downward-api-2bd2d96a-bfef-4f6e-a878-1d6ef9e2104d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:47:46.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5753" for this suite. 01/04/23 22:47:46.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:47:47.006
Jan  4 22:47:47.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename subpath 01/04/23 22:47:47.008
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:47.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:47.028
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/04/23 22:47:47.031
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-f69q 01/04/23 22:47:47.04
STEP: Creating a pod to test atomic-volume-subpath 01/04/23 22:47:47.04
Jan  4 22:47:47.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-f69q" in namespace "subpath-1347" to be "Succeeded or Failed"
Jan  4 22:47:47.055: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.105487ms
Jan  4 22:47:49.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 2.01032581s
Jan  4 22:47:51.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 4.010012696s
Jan  4 22:47:53.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 6.010079647s
Jan  4 22:47:55.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 8.010615852s
Jan  4 22:47:57.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 10.009847876s
Jan  4 22:47:59.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 12.011117921s
Jan  4 22:48:01.061: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 14.011795649s
Jan  4 22:48:03.062: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 16.012393826s
Jan  4 22:48:05.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 18.011185445s
Jan  4 22:48:07.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 20.009335104s
Jan  4 22:48:09.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=false. Elapsed: 22.010318863s
Jan  4 22:48:11.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009700516s
STEP: Saw pod success 01/04/23 22:48:11.059
Jan  4 22:48:11.059: INFO: Pod "pod-subpath-test-downwardapi-f69q" satisfied condition "Succeeded or Failed"
Jan  4 22:48:11.062: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-downwardapi-f69q container test-container-subpath-downwardapi-f69q: <nil>
STEP: delete the pod 01/04/23 22:48:11.069
Jan  4 22:48:11.080: INFO: Waiting for pod pod-subpath-test-downwardapi-f69q to disappear
Jan  4 22:48:11.083: INFO: Pod pod-subpath-test-downwardapi-f69q no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-f69q 01/04/23 22:48:11.083
Jan  4 22:48:11.083: INFO: Deleting pod "pod-subpath-test-downwardapi-f69q" in namespace "subpath-1347"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  4 22:48:11.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1347" for this suite. 01/04/23 22:48:11.09
------------------------------
• [SLOW TEST] [24.092 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:47:47.006
    Jan  4 22:47:47.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename subpath 01/04/23 22:47:47.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:47:47.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:47:47.028
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/04/23 22:47:47.031
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-f69q 01/04/23 22:47:47.04
    STEP: Creating a pod to test atomic-volume-subpath 01/04/23 22:47:47.04
    Jan  4 22:47:47.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-f69q" in namespace "subpath-1347" to be "Succeeded or Failed"
    Jan  4 22:47:47.055: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.105487ms
    Jan  4 22:47:49.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 2.01032581s
    Jan  4 22:47:51.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 4.010012696s
    Jan  4 22:47:53.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 6.010079647s
    Jan  4 22:47:55.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 8.010615852s
    Jan  4 22:47:57.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 10.009847876s
    Jan  4 22:47:59.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 12.011117921s
    Jan  4 22:48:01.061: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 14.011795649s
    Jan  4 22:48:03.062: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 16.012393826s
    Jan  4 22:48:05.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 18.011185445s
    Jan  4 22:48:07.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=true. Elapsed: 20.009335104s
    Jan  4 22:48:09.060: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Running", Reason="", readiness=false. Elapsed: 22.010318863s
    Jan  4 22:48:11.059: INFO: Pod "pod-subpath-test-downwardapi-f69q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009700516s
    STEP: Saw pod success 01/04/23 22:48:11.059
    Jan  4 22:48:11.059: INFO: Pod "pod-subpath-test-downwardapi-f69q" satisfied condition "Succeeded or Failed"
    Jan  4 22:48:11.062: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-downwardapi-f69q container test-container-subpath-downwardapi-f69q: <nil>
    STEP: delete the pod 01/04/23 22:48:11.069
    Jan  4 22:48:11.080: INFO: Waiting for pod pod-subpath-test-downwardapi-f69q to disappear
    Jan  4 22:48:11.083: INFO: Pod pod-subpath-test-downwardapi-f69q no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-f69q 01/04/23 22:48:11.083
    Jan  4 22:48:11.083: INFO: Deleting pod "pod-subpath-test-downwardapi-f69q" in namespace "subpath-1347"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:48:11.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1347" for this suite. 01/04/23 22:48:11.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:48:11.102
Jan  4 22:48:11.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename statefulset 01/04/23 22:48:11.103
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:11.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:11.125
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1818 01/04/23 22:48:11.128
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-1818 01/04/23 22:48:11.135
Jan  4 22:48:11.154: INFO: Found 0 stateful pods, waiting for 1
Jan  4 22:48:21.158: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/04/23 22:48:21.163
STEP: updating a scale subresource 01/04/23 22:48:21.167
STEP: verifying the statefulset Spec.Replicas was modified 01/04/23 22:48:21.173
STEP: Patch a scale subresource 01/04/23 22:48:21.18
STEP: verifying the statefulset Spec.Replicas was modified 01/04/23 22:48:21.218
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  4 22:48:21.225: INFO: Deleting all statefulset in ns statefulset-1818
Jan  4 22:48:21.241: INFO: Scaling statefulset ss to 0
Jan  4 22:48:31.274: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 22:48:31.277: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  4 22:48:31.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1818" for this suite. 01/04/23 22:48:31.313
------------------------------
• [SLOW TEST] [20.232 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:48:11.102
    Jan  4 22:48:11.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename statefulset 01/04/23 22:48:11.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:11.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:11.125
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1818 01/04/23 22:48:11.128
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-1818 01/04/23 22:48:11.135
    Jan  4 22:48:11.154: INFO: Found 0 stateful pods, waiting for 1
    Jan  4 22:48:21.158: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/04/23 22:48:21.163
    STEP: updating a scale subresource 01/04/23 22:48:21.167
    STEP: verifying the statefulset Spec.Replicas was modified 01/04/23 22:48:21.173
    STEP: Patch a scale subresource 01/04/23 22:48:21.18
    STEP: verifying the statefulset Spec.Replicas was modified 01/04/23 22:48:21.218
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  4 22:48:21.225: INFO: Deleting all statefulset in ns statefulset-1818
    Jan  4 22:48:21.241: INFO: Scaling statefulset ss to 0
    Jan  4 22:48:31.274: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 22:48:31.277: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:48:31.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1818" for this suite. 01/04/23 22:48:31.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:48:31.336
Jan  4 22:48:31.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replication-controller 01/04/23 22:48:31.337
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:31.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:31.362
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/04/23 22:48:31.368
STEP: waiting for RC to be added 01/04/23 22:48:31.374
STEP: waiting for available Replicas 01/04/23 22:48:31.374
STEP: patching ReplicationController 01/04/23 22:48:32.343
STEP: waiting for RC to be modified 01/04/23 22:48:32.353
STEP: patching ReplicationController status 01/04/23 22:48:32.354
STEP: waiting for RC to be modified 01/04/23 22:48:32.363
STEP: waiting for available Replicas 01/04/23 22:48:32.363
STEP: fetching ReplicationController status 01/04/23 22:48:32.365
STEP: patching ReplicationController scale 01/04/23 22:48:32.368
STEP: waiting for RC to be modified 01/04/23 22:48:32.373
STEP: waiting for ReplicationController's scale to be the max amount 01/04/23 22:48:32.373
STEP: fetching ReplicationController; ensuring that it's patched 01/04/23 22:48:33.496
STEP: updating ReplicationController status 01/04/23 22:48:33.498
STEP: waiting for RC to be modified 01/04/23 22:48:33.504
STEP: listing all ReplicationControllers 01/04/23 22:48:33.504
STEP: checking that ReplicationController has expected values 01/04/23 22:48:33.508
STEP: deleting ReplicationControllers by collection 01/04/23 22:48:33.508
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/04/23 22:48:33.515
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  4 22:48:33.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3964" for this suite. 01/04/23 22:48:33.563
------------------------------
• [2.233 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:48:31.336
    Jan  4 22:48:31.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replication-controller 01/04/23 22:48:31.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:31.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:31.362
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/04/23 22:48:31.368
    STEP: waiting for RC to be added 01/04/23 22:48:31.374
    STEP: waiting for available Replicas 01/04/23 22:48:31.374
    STEP: patching ReplicationController 01/04/23 22:48:32.343
    STEP: waiting for RC to be modified 01/04/23 22:48:32.353
    STEP: patching ReplicationController status 01/04/23 22:48:32.354
    STEP: waiting for RC to be modified 01/04/23 22:48:32.363
    STEP: waiting for available Replicas 01/04/23 22:48:32.363
    STEP: fetching ReplicationController status 01/04/23 22:48:32.365
    STEP: patching ReplicationController scale 01/04/23 22:48:32.368
    STEP: waiting for RC to be modified 01/04/23 22:48:32.373
    STEP: waiting for ReplicationController's scale to be the max amount 01/04/23 22:48:32.373
    STEP: fetching ReplicationController; ensuring that it's patched 01/04/23 22:48:33.496
    STEP: updating ReplicationController status 01/04/23 22:48:33.498
    STEP: waiting for RC to be modified 01/04/23 22:48:33.504
    STEP: listing all ReplicationControllers 01/04/23 22:48:33.504
    STEP: checking that ReplicationController has expected values 01/04/23 22:48:33.508
    STEP: deleting ReplicationControllers by collection 01/04/23 22:48:33.508
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/04/23 22:48:33.515
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:48:33.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3964" for this suite. 01/04/23 22:48:33.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:48:33.569
Jan  4 22:48:33.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:48:33.571
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:33.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:33.59
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/04/23 22:48:33.592
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/04/23 22:48:33.597
STEP: patching the secret 01/04/23 22:48:33.609
STEP: deleting the secret using a LabelSelector 01/04/23 22:48:33.618
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/04/23 22:48:33.624
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:48:33.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9017" for this suite. 01/04/23 22:48:33.633
------------------------------
• [0.070 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:48:33.569
    Jan  4 22:48:33.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:48:33.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:33.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:33.59
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/04/23 22:48:33.592
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/04/23 22:48:33.597
    STEP: patching the secret 01/04/23 22:48:33.609
    STEP: deleting the secret using a LabelSelector 01/04/23 22:48:33.618
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/04/23 22:48:33.624
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:48:33.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9017" for this suite. 01/04/23 22:48:33.633
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:48:33.64
Jan  4 22:48:33.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:48:33.641
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:33.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:33.666
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/04/23 22:48:33.668
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:48:33.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8464" for this suite. 01/04/23 22:48:33.686
------------------------------
• [0.058 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:48:33.64
    Jan  4 22:48:33.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:48:33.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:33.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:33.666
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/04/23 22:48:33.668
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:48:33.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8464" for this suite. 01/04/23 22:48:33.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:48:33.701
Jan  4 22:48:33.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 22:48:33.702
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:33.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:33.73
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:48:33.733
Jan  4 22:48:33.743: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7" in namespace "downward-api-3170" to be "Succeeded or Failed"
Jan  4 22:48:33.749: INFO: Pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.858762ms
Jan  4 22:48:35.754: INFO: Pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010189126s
Jan  4 22:48:37.755: INFO: Pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011305697s
STEP: Saw pod success 01/04/23 22:48:37.755
Jan  4 22:48:37.755: INFO: Pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7" satisfied condition "Succeeded or Failed"
Jan  4 22:48:37.758: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7 container client-container: <nil>
STEP: delete the pod 01/04/23 22:48:37.764
Jan  4 22:48:37.773: INFO: Waiting for pod downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7 to disappear
Jan  4 22:48:37.775: INFO: Pod downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 22:48:37.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3170" for this suite. 01/04/23 22:48:37.779
------------------------------
• [4.083 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:48:33.701
    Jan  4 22:48:33.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 22:48:33.702
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:33.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:33.73
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:48:33.733
    Jan  4 22:48:33.743: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7" in namespace "downward-api-3170" to be "Succeeded or Failed"
    Jan  4 22:48:33.749: INFO: Pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.858762ms
    Jan  4 22:48:35.754: INFO: Pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010189126s
    Jan  4 22:48:37.755: INFO: Pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011305697s
    STEP: Saw pod success 01/04/23 22:48:37.755
    Jan  4 22:48:37.755: INFO: Pod "downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7" satisfied condition "Succeeded or Failed"
    Jan  4 22:48:37.758: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7 container client-container: <nil>
    STEP: delete the pod 01/04/23 22:48:37.764
    Jan  4 22:48:37.773: INFO: Waiting for pod downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7 to disappear
    Jan  4 22:48:37.775: INFO: Pod downwardapi-volume-69bfa85b-cf57-45a9-8bc0-c218585cb8b7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:48:37.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3170" for this suite. 01/04/23 22:48:37.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:48:37.785
Jan  4 22:48:37.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename deployment 01/04/23 22:48:37.786
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:37.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:37.803
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan  4 22:48:37.812: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan  4 22:48:42.816: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/04/23 22:48:42.816
Jan  4 22:48:42.817: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/04/23 22:48:42.825
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  4 22:48:42.836: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8604  3c7f112e-e6b0-4604-b54c-03d478b911e9 48538 1 2023-01-04 22:48:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-04 22:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00395c458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan  4 22:48:42.844: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan  4 22:48:42.844: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan  4 22:48:42.844: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8604  16301ea6-affd-4d02-b828-6d61218bb8dd 48539 1 2023-01-04 22:48:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3c7f112e-e6b0-4604-b54c-03d478b911e9 0xc00395c7a7 0xc00395c7a8}] [] [{e2e.test Update apps/v1 2023-01-04 22:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:48:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-04 22:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"3c7f112e-e6b0-4604-b54c-03d478b911e9\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00395c868 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  4 22:48:42.854: INFO: Pod "test-cleanup-controller-rpfnb" is available:
&Pod{ObjectMeta:{test-cleanup-controller-rpfnb test-cleanup-controller- deployment-8604  c0d3b5cb-4fa2-4961-8ba4-e19e4e45f718 48521 0 2023-01-04 22:48:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:e587cae69e4535afb620fa75b3279cc759781eed6243e99b90456e2663acbe91 cni.projectcalico.org/podIP:10.42.3.217/32 cni.projectcalico.org/podIPs:10.42.3.217/32] [{apps/v1 ReplicaSet test-cleanup-controller 16301ea6-affd-4d02-b828-6d61218bb8dd 0xc00395cce7 0xc00395cce8}] [] [{kube-controller-manager Update v1 2023-01-04 22:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16301ea6-affd-4d02-b828-6d61218bb8dd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-04 22:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-04 22:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsvj2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsvj2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.217,StartTime:2023-01-04 22:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 22:48:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d91b43e9f5d12642ac209eb951f94d659bf4c2b9fcf03edc70ea41e238e18ddc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  4 22:48:42.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8604" for this suite. 01/04/23 22:48:42.859
------------------------------
• [SLOW TEST] [5.084 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:48:37.785
    Jan  4 22:48:37.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename deployment 01/04/23 22:48:37.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:37.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:37.803
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan  4 22:48:37.812: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan  4 22:48:42.816: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/04/23 22:48:42.816
    Jan  4 22:48:42.817: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/04/23 22:48:42.825
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  4 22:48:42.836: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8604  3c7f112e-e6b0-4604-b54c-03d478b911e9 48538 1 2023-01-04 22:48:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-04 22:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00395c458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  4 22:48:42.844: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan  4 22:48:42.844: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan  4 22:48:42.844: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8604  16301ea6-affd-4d02-b828-6d61218bb8dd 48539 1 2023-01-04 22:48:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3c7f112e-e6b0-4604-b54c-03d478b911e9 0xc00395c7a7 0xc00395c7a8}] [] [{e2e.test Update apps/v1 2023-01-04 22:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:48:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-04 22:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"3c7f112e-e6b0-4604-b54c-03d478b911e9\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00395c868 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 22:48:42.854: INFO: Pod "test-cleanup-controller-rpfnb" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-rpfnb test-cleanup-controller- deployment-8604  c0d3b5cb-4fa2-4961-8ba4-e19e4e45f718 48521 0 2023-01-04 22:48:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:e587cae69e4535afb620fa75b3279cc759781eed6243e99b90456e2663acbe91 cni.projectcalico.org/podIP:10.42.3.217/32 cni.projectcalico.org/podIPs:10.42.3.217/32] [{apps/v1 ReplicaSet test-cleanup-controller 16301ea6-affd-4d02-b828-6d61218bb8dd 0xc00395cce7 0xc00395cce8}] [] [{kube-controller-manager Update v1 2023-01-04 22:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16301ea6-affd-4d02-b828-6d61218bb8dd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-04 22:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-04 22:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsvj2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsvj2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.217,StartTime:2023-01-04 22:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 22:48:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d91b43e9f5d12642ac209eb951f94d659bf4c2b9fcf03edc70ea41e238e18ddc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:48:42.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8604" for this suite. 01/04/23 22:48:42.859
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:48:42.874
Jan  4 22:48:42.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename endpointslice 01/04/23 22:48:42.875
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:42.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:42.9
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/04/23 22:48:48.018
STEP: referencing matching pods with named port 01/04/23 22:48:53.025
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/04/23 22:48:58.032
STEP: recreating EndpointSlices after they've been deleted 01/04/23 22:49:03.041
Jan  4 22:49:03.075: INFO: EndpointSlice for Service endpointslice-544/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:13.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-544" for this suite. 01/04/23 22:49:13.086
------------------------------
• [SLOW TEST] [30.218 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:48:42.874
    Jan  4 22:48:42.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename endpointslice 01/04/23 22:48:42.875
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:48:42.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:48:42.9
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/04/23 22:48:48.018
    STEP: referencing matching pods with named port 01/04/23 22:48:53.025
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/04/23 22:48:58.032
    STEP: recreating EndpointSlices after they've been deleted 01/04/23 22:49:03.041
    Jan  4 22:49:03.075: INFO: EndpointSlice for Service endpointslice-544/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:13.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-544" for this suite. 01/04/23 22:49:13.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:13.103
Jan  4 22:49:13.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename deployment 01/04/23 22:49:13.104
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:13.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:13.125
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan  4 22:49:13.129: INFO: Creating deployment "test-recreate-deployment"
Jan  4 22:49:13.134: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan  4 22:49:13.140: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan  4 22:49:15.148: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan  4 22:49:15.150: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan  4 22:49:15.158: INFO: Updating deployment test-recreate-deployment
Jan  4 22:49:15.158: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  4 22:49:15.300: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2203  f6090e84-7c84-408d-b08d-fd723b7d57c0 48787 2 2023-01-04 22:49:13 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e0618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-04 22:49:15 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-04 22:49:15 +0000 UTC,LastTransitionTime:2023-01-04 22:49:13 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan  4 22:49:15.309: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2203  367f9e9d-81d6-48a0-81ea-ebb99856898e 48784 1 2023-01-04 22:49:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f6090e84-7c84-408d-b08d-fd723b7d57c0 0xc0047e0af0 0xc0047e0af1}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6090e84-7c84-408d-b08d-fd723b7d57c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e0b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  4 22:49:15.309: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan  4 22:49:15.309: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2203  e955437b-9532-4c5a-980e-b42352b191dd 48774 2 2023-01-04 22:49:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f6090e84-7c84-408d-b08d-fd723b7d57c0 0xc0047e09d7 0xc0047e09d8}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6090e84-7c84-408d-b08d-fd723b7d57c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e0a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  4 22:49:15.314: INFO: Pod "test-recreate-deployment-cff6dc657-98gzd" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-98gzd test-recreate-deployment-cff6dc657- deployment-2203  98f95edc-44bc-43d8-b825-e9361f700123 48785 0 2023-01-04 22:49:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 367f9e9d-81d6-48a0-81ea-ebb99856898e 0xc003bfe8f0 0xc003bfe8f1}] [] [{kube-controller-manager Update v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"367f9e9d-81d6-48a0-81ea-ebb99856898e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8bqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8bqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:,StartTime:2023-01-04 22:49:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:15.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2203" for this suite. 01/04/23 22:49:15.324
------------------------------
• [2.238 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:13.103
    Jan  4 22:49:13.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename deployment 01/04/23 22:49:13.104
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:13.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:13.125
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan  4 22:49:13.129: INFO: Creating deployment "test-recreate-deployment"
    Jan  4 22:49:13.134: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan  4 22:49:13.140: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan  4 22:49:15.148: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan  4 22:49:15.150: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan  4 22:49:15.158: INFO: Updating deployment test-recreate-deployment
    Jan  4 22:49:15.158: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  4 22:49:15.300: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2203  f6090e84-7c84-408d-b08d-fd723b7d57c0 48787 2 2023-01-04 22:49:13 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e0618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-04 22:49:15 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-04 22:49:15 +0000 UTC,LastTransitionTime:2023-01-04 22:49:13 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  4 22:49:15.309: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2203  367f9e9d-81d6-48a0-81ea-ebb99856898e 48784 1 2023-01-04 22:49:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f6090e84-7c84-408d-b08d-fd723b7d57c0 0xc0047e0af0 0xc0047e0af1}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6090e84-7c84-408d-b08d-fd723b7d57c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e0b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 22:49:15.309: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan  4 22:49:15.309: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2203  e955437b-9532-4c5a-980e-b42352b191dd 48774 2 2023-01-04 22:49:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f6090e84-7c84-408d-b08d-fd723b7d57c0 0xc0047e09d7 0xc0047e09d8}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6090e84-7c84-408d-b08d-fd723b7d57c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e0a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 22:49:15.314: INFO: Pod "test-recreate-deployment-cff6dc657-98gzd" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-98gzd test-recreate-deployment-cff6dc657- deployment-2203  98f95edc-44bc-43d8-b825-e9361f700123 48785 0 2023-01-04 22:49:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 367f9e9d-81d6-48a0-81ea-ebb99856898e 0xc003bfe8f0 0xc003bfe8f1}] [] [{kube-controller-manager Update v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"367f9e9d-81d6-48a0-81ea-ebb99856898e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 22:49:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8bqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8bqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:49:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:,StartTime:2023-01-04 22:49:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:15.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2203" for this suite. 01/04/23 22:49:15.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:15.345
Jan  4 22:49:15.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 22:49:15.346
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:15.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:15.399
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/04/23 22:49:15.401
Jan  4 22:49:15.408: INFO: Waiting up to 5m0s for pod "pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727" in namespace "pods-3446" to be "running and ready"
Jan  4 22:49:15.412: INFO: Pod "pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135868ms
Jan  4 22:49:15.412: INFO: The phase of Pod pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:49:17.415: INFO: Pod "pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727": Phase="Running", Reason="", readiness=true. Elapsed: 2.007561048s
Jan  4 22:49:17.415: INFO: The phase of Pod pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727 is Running (Ready = true)
Jan  4 22:49:17.415: INFO: Pod "pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727" satisfied condition "running and ready"
Jan  4 22:49:17.419: INFO: Pod pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727 has hostIP: 172.31.13.117
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:17.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3446" for this suite. 01/04/23 22:49:17.424
------------------------------
• [2.085 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:15.345
    Jan  4 22:49:15.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 22:49:15.346
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:15.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:15.399
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/04/23 22:49:15.401
    Jan  4 22:49:15.408: INFO: Waiting up to 5m0s for pod "pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727" in namespace "pods-3446" to be "running and ready"
    Jan  4 22:49:15.412: INFO: Pod "pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135868ms
    Jan  4 22:49:15.412: INFO: The phase of Pod pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:49:17.415: INFO: Pod "pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727": Phase="Running", Reason="", readiness=true. Elapsed: 2.007561048s
    Jan  4 22:49:17.415: INFO: The phase of Pod pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727 is Running (Ready = true)
    Jan  4 22:49:17.415: INFO: Pod "pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727" satisfied condition "running and ready"
    Jan  4 22:49:17.419: INFO: Pod pod-hostip-f5f18702-256a-4e78-ba5f-3bec1c46b727 has hostIP: 172.31.13.117
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:17.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3446" for this suite. 01/04/23 22:49:17.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:17.432
Jan  4 22:49:17.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename security-context-test 01/04/23 22:49:17.433
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:17.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:17.459
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan  4 22:49:17.472: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8" in namespace "security-context-test-5832" to be "Succeeded or Failed"
Jan  4 22:49:17.474: INFO: Pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.313203ms
Jan  4 22:49:19.480: INFO: Pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007760462s
Jan  4 22:49:21.478: INFO: Pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006026277s
Jan  4 22:49:21.478: INFO: Pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:21.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5832" for this suite. 01/04/23 22:49:21.482
------------------------------
• [4.055 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:17.432
    Jan  4 22:49:17.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename security-context-test 01/04/23 22:49:17.433
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:17.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:17.459
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan  4 22:49:17.472: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8" in namespace "security-context-test-5832" to be "Succeeded or Failed"
    Jan  4 22:49:17.474: INFO: Pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.313203ms
    Jan  4 22:49:19.480: INFO: Pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007760462s
    Jan  4 22:49:21.478: INFO: Pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006026277s
    Jan  4 22:49:21.478: INFO: Pod "busybox-readonly-false-cdb86b91-6f97-400d-b05f-f7453bef52e8" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:21.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5832" for this suite. 01/04/23 22:49:21.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:21.503
Jan  4 22:49:21.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename containers 01/04/23 22:49:21.504
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:21.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:21.524
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/04/23 22:49:21.526
Jan  4 22:49:21.534: INFO: Waiting up to 5m0s for pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484" in namespace "containers-9051" to be "Succeeded or Failed"
Jan  4 22:49:21.539: INFO: Pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028006ms
Jan  4 22:49:23.546: INFO: Pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484": Phase="Running", Reason="", readiness=false. Elapsed: 2.012143187s
Jan  4 22:49:25.543: INFO: Pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008930551s
STEP: Saw pod success 01/04/23 22:49:25.543
Jan  4 22:49:25.544: INFO: Pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484" satisfied condition "Succeeded or Failed"
Jan  4 22:49:25.546: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:49:25.556
Jan  4 22:49:25.566: INFO: Waiting for pod client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484 to disappear
Jan  4 22:49:25.568: INFO: Pod client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:25.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9051" for this suite. 01/04/23 22:49:25.572
------------------------------
• [4.080 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:21.503
    Jan  4 22:49:21.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename containers 01/04/23 22:49:21.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:21.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:21.524
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/04/23 22:49:21.526
    Jan  4 22:49:21.534: INFO: Waiting up to 5m0s for pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484" in namespace "containers-9051" to be "Succeeded or Failed"
    Jan  4 22:49:21.539: INFO: Pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028006ms
    Jan  4 22:49:23.546: INFO: Pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484": Phase="Running", Reason="", readiness=false. Elapsed: 2.012143187s
    Jan  4 22:49:25.543: INFO: Pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008930551s
    STEP: Saw pod success 01/04/23 22:49:25.543
    Jan  4 22:49:25.544: INFO: Pod "client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484" satisfied condition "Succeeded or Failed"
    Jan  4 22:49:25.546: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:49:25.556
    Jan  4 22:49:25.566: INFO: Waiting for pod client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484 to disappear
    Jan  4 22:49:25.568: INFO: Pod client-containers-d186f89e-3e80-4f3c-9d0b-fa02d9874484 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:25.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9051" for this suite. 01/04/23 22:49:25.572
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:25.584
Jan  4 22:49:25.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 22:49:25.585
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:25.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:25.609
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-b30a7850-bec4-41c4-93e4-b8342e0132c1 01/04/23 22:49:25.611
STEP: Creating a pod to test consume configMaps 01/04/23 22:49:25.615
Jan  4 22:49:25.623: INFO: Waiting up to 5m0s for pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89" in namespace "configmap-1825" to be "Succeeded or Failed"
Jan  4 22:49:25.629: INFO: Pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478881ms
Jan  4 22:49:27.637: INFO: Pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013917862s
Jan  4 22:49:29.633: INFO: Pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010508035s
STEP: Saw pod success 01/04/23 22:49:29.633
Jan  4 22:49:29.634: INFO: Pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89" satisfied condition "Succeeded or Failed"
Jan  4 22:49:29.636: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:49:29.643
Jan  4 22:49:29.654: INFO: Waiting for pod pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89 to disappear
Jan  4 22:49:29.656: INFO: Pod pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:29.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1825" for this suite. 01/04/23 22:49:29.661
------------------------------
• [4.084 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:25.584
    Jan  4 22:49:25.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 22:49:25.585
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:25.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:25.609
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-b30a7850-bec4-41c4-93e4-b8342e0132c1 01/04/23 22:49:25.611
    STEP: Creating a pod to test consume configMaps 01/04/23 22:49:25.615
    Jan  4 22:49:25.623: INFO: Waiting up to 5m0s for pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89" in namespace "configmap-1825" to be "Succeeded or Failed"
    Jan  4 22:49:25.629: INFO: Pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478881ms
    Jan  4 22:49:27.637: INFO: Pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013917862s
    Jan  4 22:49:29.633: INFO: Pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010508035s
    STEP: Saw pod success 01/04/23 22:49:29.633
    Jan  4 22:49:29.634: INFO: Pod "pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89" satisfied condition "Succeeded or Failed"
    Jan  4 22:49:29.636: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:49:29.643
    Jan  4 22:49:29.654: INFO: Waiting for pod pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89 to disappear
    Jan  4 22:49:29.656: INFO: Pod pod-configmaps-e75cafef-62b2-46de-ba2f-f808879bcd89 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:29.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1825" for this suite. 01/04/23 22:49:29.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:29.669
Jan  4 22:49:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:49:29.67
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:29.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:29.685
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:49:29.699
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:49:30.44
STEP: Deploying the webhook pod 01/04/23 22:49:30.447
STEP: Wait for the deployment to be ready 01/04/23 22:49:30.458
Jan  4 22:49:30.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 22:49:32.475
STEP: Verifying the service has paired with the endpoint 01/04/23 22:49:32.484
Jan  4 22:49:33.484: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/04/23 22:49:33.489
STEP: create a pod that should be updated by the webhook 01/04/23 22:49:33.507
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:33.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7177" for this suite. 01/04/23 22:49:33.605
STEP: Destroying namespace "webhook-7177-markers" for this suite. 01/04/23 22:49:33.615
------------------------------
• [3.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:29.669
    Jan  4 22:49:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:49:29.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:29.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:29.685
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:49:29.699
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:49:30.44
    STEP: Deploying the webhook pod 01/04/23 22:49:30.447
    STEP: Wait for the deployment to be ready 01/04/23 22:49:30.458
    Jan  4 22:49:30.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 22:49:32.475
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:49:32.484
    Jan  4 22:49:33.484: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/04/23 22:49:33.489
    STEP: create a pod that should be updated by the webhook 01/04/23 22:49:33.507
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:33.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7177" for this suite. 01/04/23 22:49:33.605
    STEP: Destroying namespace "webhook-7177-markers" for this suite. 01/04/23 22:49:33.615
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:33.633
Jan  4 22:49:33.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 22:49:33.634
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:33.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:33.666
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/04/23 22:49:33.668
Jan  4 22:49:33.681: INFO: Waiting up to 5m0s for pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9" in namespace "downward-api-5295" to be "Succeeded or Failed"
Jan  4 22:49:33.684: INFO: Pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.285076ms
Jan  4 22:49:35.693: INFO: Pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011738557s
Jan  4 22:49:37.689: INFO: Pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007628754s
STEP: Saw pod success 01/04/23 22:49:37.689
Jan  4 22:49:37.689: INFO: Pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9" satisfied condition "Succeeded or Failed"
Jan  4 22:49:37.692: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9 container dapi-container: <nil>
STEP: delete the pod 01/04/23 22:49:37.698
Jan  4 22:49:37.709: INFO: Waiting for pod downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9 to disappear
Jan  4 22:49:37.716: INFO: Pod downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:37.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5295" for this suite. 01/04/23 22:49:37.721
------------------------------
• [4.093 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:33.633
    Jan  4 22:49:33.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 22:49:33.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:33.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:33.666
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/04/23 22:49:33.668
    Jan  4 22:49:33.681: INFO: Waiting up to 5m0s for pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9" in namespace "downward-api-5295" to be "Succeeded or Failed"
    Jan  4 22:49:33.684: INFO: Pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.285076ms
    Jan  4 22:49:35.693: INFO: Pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011738557s
    Jan  4 22:49:37.689: INFO: Pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007628754s
    STEP: Saw pod success 01/04/23 22:49:37.689
    Jan  4 22:49:37.689: INFO: Pod "downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9" satisfied condition "Succeeded or Failed"
    Jan  4 22:49:37.692: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9 container dapi-container: <nil>
    STEP: delete the pod 01/04/23 22:49:37.698
    Jan  4 22:49:37.709: INFO: Waiting for pod downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9 to disappear
    Jan  4 22:49:37.716: INFO: Pod downward-api-a9cf69a4-d66e-4618-9a15-e269cb29aaa9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:37.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5295" for this suite. 01/04/23 22:49:37.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:37.731
Jan  4 22:49:37.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:49:37.734
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:37.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:37.754
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4097 01/04/23 22:49:37.757
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/04/23 22:49:37.771
STEP: creating service externalsvc in namespace services-4097 01/04/23 22:49:37.771
STEP: creating replication controller externalsvc in namespace services-4097 01/04/23 22:49:37.8
I0104 22:49:37.809630      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4097, replica count: 2
I0104 22:49:40.861344      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/04/23 22:49:40.864
Jan  4 22:49:40.878: INFO: Creating new exec pod
Jan  4 22:49:40.892: INFO: Waiting up to 5m0s for pod "execpodt44jd" in namespace "services-4097" to be "running"
Jan  4 22:49:40.898: INFO: Pod "execpodt44jd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.173852ms
Jan  4 22:49:42.906: INFO: Pod "execpodt44jd": Phase="Running", Reason="", readiness=true. Elapsed: 2.014091865s
Jan  4 22:49:42.906: INFO: Pod "execpodt44jd" satisfied condition "running"
Jan  4 22:49:42.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4097 exec execpodt44jd -- /bin/sh -x -c nslookup nodeport-service.services-4097.svc.cluster.local'
Jan  4 22:49:43.142: INFO: stderr: "+ nslookup nodeport-service.services-4097.svc.cluster.local\n"
Jan  4 22:49:43.142: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nnodeport-service.services-4097.svc.cluster.local\tcanonical name = externalsvc.services-4097.svc.cluster.local.\nName:\texternalsvc.services-4097.svc.cluster.local\nAddress: 10.43.42.61\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4097, will wait for the garbage collector to delete the pods 01/04/23 22:49:43.142
Jan  4 22:49:43.202: INFO: Deleting ReplicationController externalsvc took: 6.899148ms
Jan  4 22:49:43.302: INFO: Terminating ReplicationController externalsvc pods took: 100.063602ms
Jan  4 22:49:45.020: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:45.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4097" for this suite. 01/04/23 22:49:45.05
------------------------------
• [SLOW TEST] [7.345 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:37.731
    Jan  4 22:49:37.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:49:37.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:37.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:37.754
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-4097 01/04/23 22:49:37.757
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/04/23 22:49:37.771
    STEP: creating service externalsvc in namespace services-4097 01/04/23 22:49:37.771
    STEP: creating replication controller externalsvc in namespace services-4097 01/04/23 22:49:37.8
    I0104 22:49:37.809630      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4097, replica count: 2
    I0104 22:49:40.861344      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/04/23 22:49:40.864
    Jan  4 22:49:40.878: INFO: Creating new exec pod
    Jan  4 22:49:40.892: INFO: Waiting up to 5m0s for pod "execpodt44jd" in namespace "services-4097" to be "running"
    Jan  4 22:49:40.898: INFO: Pod "execpodt44jd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.173852ms
    Jan  4 22:49:42.906: INFO: Pod "execpodt44jd": Phase="Running", Reason="", readiness=true. Elapsed: 2.014091865s
    Jan  4 22:49:42.906: INFO: Pod "execpodt44jd" satisfied condition "running"
    Jan  4 22:49:42.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-4097 exec execpodt44jd -- /bin/sh -x -c nslookup nodeport-service.services-4097.svc.cluster.local'
    Jan  4 22:49:43.142: INFO: stderr: "+ nslookup nodeport-service.services-4097.svc.cluster.local\n"
    Jan  4 22:49:43.142: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nnodeport-service.services-4097.svc.cluster.local\tcanonical name = externalsvc.services-4097.svc.cluster.local.\nName:\texternalsvc.services-4097.svc.cluster.local\nAddress: 10.43.42.61\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4097, will wait for the garbage collector to delete the pods 01/04/23 22:49:43.142
    Jan  4 22:49:43.202: INFO: Deleting ReplicationController externalsvc took: 6.899148ms
    Jan  4 22:49:43.302: INFO: Terminating ReplicationController externalsvc pods took: 100.063602ms
    Jan  4 22:49:45.020: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:45.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4097" for this suite. 01/04/23 22:49:45.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:45.076
Jan  4 22:49:45.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename watch 01/04/23 22:49:45.077
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:45.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:45.103
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/04/23 22:49:45.105
STEP: creating a new configmap 01/04/23 22:49:45.106
STEP: modifying the configmap once 01/04/23 22:49:45.11
STEP: closing the watch once it receives two notifications 01/04/23 22:49:45.12
Jan  4 22:49:45.120: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5769  944a26b6-0c32-48c7-a76b-88385e54db7e 49243 0 2023-01-04 22:49:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-04 22:49:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:49:45.120: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5769  944a26b6-0c32-48c7-a76b-88385e54db7e 49244 0 2023-01-04 22:49:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-04 22:49:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/04/23 22:49:45.121
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/04/23 22:49:45.127
STEP: deleting the configmap 01/04/23 22:49:45.128
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/04/23 22:49:45.133
Jan  4 22:49:45.134: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5769  944a26b6-0c32-48c7-a76b-88385e54db7e 49245 0 2023-01-04 22:49:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-04 22:49:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 22:49:45.134: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5769  944a26b6-0c32-48c7-a76b-88385e54db7e 49246 0 2023-01-04 22:49:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-04 22:49:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:45.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5769" for this suite. 01/04/23 22:49:45.137
------------------------------
• [0.067 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:45.076
    Jan  4 22:49:45.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename watch 01/04/23 22:49:45.077
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:45.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:45.103
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/04/23 22:49:45.105
    STEP: creating a new configmap 01/04/23 22:49:45.106
    STEP: modifying the configmap once 01/04/23 22:49:45.11
    STEP: closing the watch once it receives two notifications 01/04/23 22:49:45.12
    Jan  4 22:49:45.120: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5769  944a26b6-0c32-48c7-a76b-88385e54db7e 49243 0 2023-01-04 22:49:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-04 22:49:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:49:45.120: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5769  944a26b6-0c32-48c7-a76b-88385e54db7e 49244 0 2023-01-04 22:49:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-04 22:49:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/04/23 22:49:45.121
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/04/23 22:49:45.127
    STEP: deleting the configmap 01/04/23 22:49:45.128
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/04/23 22:49:45.133
    Jan  4 22:49:45.134: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5769  944a26b6-0c32-48c7-a76b-88385e54db7e 49245 0 2023-01-04 22:49:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-04 22:49:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 22:49:45.134: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5769  944a26b6-0c32-48c7-a76b-88385e54db7e 49246 0 2023-01-04 22:49:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-04 22:49:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:45.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5769" for this suite. 01/04/23 22:49:45.137
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:45.144
Jan  4 22:49:45.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 22:49:45.145
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:45.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:45.168
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:49:45.17
Jan  4 22:49:45.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0" in namespace "downward-api-1866" to be "Succeeded or Failed"
Jan  4 22:49:45.188: INFO: Pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820616ms
Jan  4 22:49:47.191: INFO: Pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007927226s
Jan  4 22:49:49.192: INFO: Pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009556805s
STEP: Saw pod success 01/04/23 22:49:49.193
Jan  4 22:49:49.193: INFO: Pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0" satisfied condition "Succeeded or Failed"
Jan  4 22:49:49.196: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0 container client-container: <nil>
STEP: delete the pod 01/04/23 22:49:49.201
Jan  4 22:49:49.215: INFO: Waiting for pod downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0 to disappear
Jan  4 22:49:49.220: INFO: Pod downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:49.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1866" for this suite. 01/04/23 22:49:49.24
------------------------------
• [4.111 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:45.144
    Jan  4 22:49:45.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 22:49:45.145
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:45.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:45.168
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:49:45.17
    Jan  4 22:49:45.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0" in namespace "downward-api-1866" to be "Succeeded or Failed"
    Jan  4 22:49:45.188: INFO: Pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820616ms
    Jan  4 22:49:47.191: INFO: Pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007927226s
    Jan  4 22:49:49.192: INFO: Pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009556805s
    STEP: Saw pod success 01/04/23 22:49:49.193
    Jan  4 22:49:49.193: INFO: Pod "downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0" satisfied condition "Succeeded or Failed"
    Jan  4 22:49:49.196: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0 container client-container: <nil>
    STEP: delete the pod 01/04/23 22:49:49.201
    Jan  4 22:49:49.215: INFO: Waiting for pod downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0 to disappear
    Jan  4 22:49:49.220: INFO: Pod downwardapi-volume-3e259c4d-4f1a-4bbe-8725-a87ea4492fa0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:49.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1866" for this suite. 01/04/23 22:49:49.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:49.257
Jan  4 22:49:49.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:49:49.258
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:49.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:49.282
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/04/23 22:49:49.285
Jan  4 22:49:49.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-1007 create -f -'
Jan  4 22:49:50.544: INFO: stderr: ""
Jan  4 22:49:50.544: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/04/23 22:49:50.544
Jan  4 22:49:50.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-1007 diff -f -'
Jan  4 22:49:51.477: INFO: rc: 1
Jan  4 22:49:51.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-1007 delete -f -'
Jan  4 22:49:51.533: INFO: stderr: ""
Jan  4 22:49:51.533: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:51.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1007" for this suite. 01/04/23 22:49:51.539
------------------------------
• [2.292 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:49.257
    Jan  4 22:49:49.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:49:49.258
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:49.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:49.282
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/04/23 22:49:49.285
    Jan  4 22:49:49.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-1007 create -f -'
    Jan  4 22:49:50.544: INFO: stderr: ""
    Jan  4 22:49:50.544: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/04/23 22:49:50.544
    Jan  4 22:49:50.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-1007 diff -f -'
    Jan  4 22:49:51.477: INFO: rc: 1
    Jan  4 22:49:51.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-1007 delete -f -'
    Jan  4 22:49:51.533: INFO: stderr: ""
    Jan  4 22:49:51.533: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:51.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1007" for this suite. 01/04/23 22:49:51.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:51.55
Jan  4 22:49:51.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:49:51.55
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:51.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:51.566
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-f71db010-d4e4-4783-8fd7-234eecf1f960 01/04/23 22:49:51.568
STEP: Creating a pod to test consume configMaps 01/04/23 22:49:51.572
Jan  4 22:49:51.579: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885" in namespace "projected-3105" to be "Succeeded or Failed"
Jan  4 22:49:51.583: INFO: Pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885": Phase="Pending", Reason="", readiness=false. Elapsed: 4.265586ms
Jan  4 22:49:53.586: INFO: Pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007526302s
Jan  4 22:49:55.587: INFO: Pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008037607s
STEP: Saw pod success 01/04/23 22:49:55.587
Jan  4 22:49:55.587: INFO: Pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885" satisfied condition "Succeeded or Failed"
Jan  4 22:49:55.590: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 22:49:55.597
Jan  4 22:49:55.607: INFO: Waiting for pod pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885 to disappear
Jan  4 22:49:55.610: INFO: Pod pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:55.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3105" for this suite. 01/04/23 22:49:55.617
------------------------------
• [4.073 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:51.55
    Jan  4 22:49:51.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:49:51.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:51.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:51.566
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-f71db010-d4e4-4783-8fd7-234eecf1f960 01/04/23 22:49:51.568
    STEP: Creating a pod to test consume configMaps 01/04/23 22:49:51.572
    Jan  4 22:49:51.579: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885" in namespace "projected-3105" to be "Succeeded or Failed"
    Jan  4 22:49:51.583: INFO: Pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885": Phase="Pending", Reason="", readiness=false. Elapsed: 4.265586ms
    Jan  4 22:49:53.586: INFO: Pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007526302s
    Jan  4 22:49:55.587: INFO: Pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008037607s
    STEP: Saw pod success 01/04/23 22:49:55.587
    Jan  4 22:49:55.587: INFO: Pod "pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885" satisfied condition "Succeeded or Failed"
    Jan  4 22:49:55.590: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 22:49:55.597
    Jan  4 22:49:55.607: INFO: Waiting for pod pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885 to disappear
    Jan  4 22:49:55.610: INFO: Pod pod-projected-configmaps-44d6afc9-4dd9-4648-89e9-d4d7eaf43885 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:55.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3105" for this suite. 01/04/23 22:49:55.617
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:55.624
Jan  4 22:49:55.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 22:49:55.626
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:55.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:55.661
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-b25492b7-195d-4c2d-bc33-905613d5fc43 01/04/23 22:49:55.687
STEP: Creating a pod to test consume secrets 01/04/23 22:49:55.695
Jan  4 22:49:55.705: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521" in namespace "projected-3524" to be "Succeeded or Failed"
Jan  4 22:49:55.724: INFO: Pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521": Phase="Pending", Reason="", readiness=false. Elapsed: 19.302999ms
Jan  4 22:49:57.727: INFO: Pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022635597s
Jan  4 22:49:59.730: INFO: Pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024913641s
STEP: Saw pod success 01/04/23 22:49:59.73
Jan  4 22:49:59.730: INFO: Pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521" satisfied condition "Succeeded or Failed"
Jan  4 22:49:59.735: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:49:59.742
Jan  4 22:49:59.753: INFO: Waiting for pod pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521 to disappear
Jan  4 22:49:59.756: INFO: Pod pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:59.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3524" for this suite. 01/04/23 22:49:59.76
------------------------------
• [4.142 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:55.624
    Jan  4 22:49:55.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 22:49:55.626
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:55.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:55.661
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-b25492b7-195d-4c2d-bc33-905613d5fc43 01/04/23 22:49:55.687
    STEP: Creating a pod to test consume secrets 01/04/23 22:49:55.695
    Jan  4 22:49:55.705: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521" in namespace "projected-3524" to be "Succeeded or Failed"
    Jan  4 22:49:55.724: INFO: Pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521": Phase="Pending", Reason="", readiness=false. Elapsed: 19.302999ms
    Jan  4 22:49:57.727: INFO: Pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022635597s
    Jan  4 22:49:59.730: INFO: Pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024913641s
    STEP: Saw pod success 01/04/23 22:49:59.73
    Jan  4 22:49:59.730: INFO: Pod "pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521" satisfied condition "Succeeded or Failed"
    Jan  4 22:49:59.735: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:49:59.742
    Jan  4 22:49:59.753: INFO: Waiting for pod pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521 to disappear
    Jan  4 22:49:59.756: INFO: Pod pod-projected-secrets-e06b5796-a415-4c3e-910f-1dcdad769521 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:59.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3524" for this suite. 01/04/23 22:49:59.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:59.769
Jan  4 22:49:59.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:49:59.77
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:59.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:59.789
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan  4 22:49:59.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5894 version'
Jan  4 22:49:59.857: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan  4 22:49:59.857: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0+rke2r1\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-09T04:58:37Z\", GoVersion:\"go1.19.4 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:49:59.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5894" for this suite. 01/04/23 22:49:59.864
------------------------------
• [0.101 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:59.769
    Jan  4 22:49:59.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:49:59.77
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:59.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:59.789
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan  4 22:49:59.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5894 version'
    Jan  4 22:49:59.857: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan  4 22:49:59.857: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0+rke2r1\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-09T04:58:37Z\", GoVersion:\"go1.19.4 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:49:59.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5894" for this suite. 01/04/23 22:49:59.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:49:59.872
Jan  4 22:49:59.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 22:49:59.874
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:59.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:59.898
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/04/23 22:49:59.901
Jan  4 22:49:59.909: INFO: Waiting up to 5m0s for pod "pod-0eb85195-340f-4907-8944-5dc2a519f653" in namespace "emptydir-524" to be "Succeeded or Failed"
Jan  4 22:49:59.916: INFO: Pod "pod-0eb85195-340f-4907-8944-5dc2a519f653": Phase="Pending", Reason="", readiness=false. Elapsed: 6.829901ms
Jan  4 22:50:01.919: INFO: Pod "pod-0eb85195-340f-4907-8944-5dc2a519f653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010540072s
Jan  4 22:50:03.920: INFO: Pod "pod-0eb85195-340f-4907-8944-5dc2a519f653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010966679s
STEP: Saw pod success 01/04/23 22:50:03.92
Jan  4 22:50:03.920: INFO: Pod "pod-0eb85195-340f-4907-8944-5dc2a519f653" satisfied condition "Succeeded or Failed"
Jan  4 22:50:03.922: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-0eb85195-340f-4907-8944-5dc2a519f653 container test-container: <nil>
STEP: delete the pod 01/04/23 22:50:03.929
Jan  4 22:50:03.944: INFO: Waiting for pod pod-0eb85195-340f-4907-8944-5dc2a519f653 to disappear
Jan  4 22:50:03.946: INFO: Pod pod-0eb85195-340f-4907-8944-5dc2a519f653 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 22:50:03.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-524" for this suite. 01/04/23 22:50:03.951
------------------------------
• [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:49:59.872
    Jan  4 22:49:59.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 22:49:59.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:49:59.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:49:59.898
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/04/23 22:49:59.901
    Jan  4 22:49:59.909: INFO: Waiting up to 5m0s for pod "pod-0eb85195-340f-4907-8944-5dc2a519f653" in namespace "emptydir-524" to be "Succeeded or Failed"
    Jan  4 22:49:59.916: INFO: Pod "pod-0eb85195-340f-4907-8944-5dc2a519f653": Phase="Pending", Reason="", readiness=false. Elapsed: 6.829901ms
    Jan  4 22:50:01.919: INFO: Pod "pod-0eb85195-340f-4907-8944-5dc2a519f653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010540072s
    Jan  4 22:50:03.920: INFO: Pod "pod-0eb85195-340f-4907-8944-5dc2a519f653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010966679s
    STEP: Saw pod success 01/04/23 22:50:03.92
    Jan  4 22:50:03.920: INFO: Pod "pod-0eb85195-340f-4907-8944-5dc2a519f653" satisfied condition "Succeeded or Failed"
    Jan  4 22:50:03.922: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-0eb85195-340f-4907-8944-5dc2a519f653 container test-container: <nil>
    STEP: delete the pod 01/04/23 22:50:03.929
    Jan  4 22:50:03.944: INFO: Waiting for pod pod-0eb85195-340f-4907-8944-5dc2a519f653 to disappear
    Jan  4 22:50:03.946: INFO: Pod pod-0eb85195-340f-4907-8944-5dc2a519f653 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:50:03.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-524" for this suite. 01/04/23 22:50:03.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:50:03.958
Jan  4 22:50:03.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename daemonsets 01/04/23 22:50:03.959
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:50:03.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:50:03.986
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/04/23 22:50:04.015
STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 22:50:04.021
Jan  4 22:50:04.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:50:04.030: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:50:05.044: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  4 22:50:05.044: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:50:06.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 22:50:06.043: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/04/23 22:50:06.046
Jan  4 22:50:06.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  4 22:50:06.073: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:50:07.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  4 22:50:07.082: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:50:08.088: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  4 22:50:08.088: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:50:09.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  4 22:50:09.094: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:50:10.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  4 22:50:10.085: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 22:50:11.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 22:50:11.082: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:50:11.094
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1513, will wait for the garbage collector to delete the pods 01/04/23 22:50:11.094
Jan  4 22:50:11.155: INFO: Deleting DaemonSet.extensions daemon-set took: 6.565245ms
Jan  4 22:50:11.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.982137ms
Jan  4 22:50:13.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 22:50:13.761: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  4 22:50:13.763: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49626"},"items":null}

Jan  4 22:50:13.766: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49626"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:50:13.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1513" for this suite. 01/04/23 22:50:13.784
------------------------------
• [SLOW TEST] [9.832 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:50:03.958
    Jan  4 22:50:03.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename daemonsets 01/04/23 22:50:03.959
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:50:03.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:50:03.986
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/04/23 22:50:04.015
    STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 22:50:04.021
    Jan  4 22:50:04.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:50:04.030: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:50:05.044: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  4 22:50:05.044: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:50:06.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 22:50:06.043: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/04/23 22:50:06.046
    Jan  4 22:50:06.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  4 22:50:06.073: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:50:07.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  4 22:50:07.082: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:50:08.088: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  4 22:50:08.088: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:50:09.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  4 22:50:09.094: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:50:10.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  4 22:50:10.085: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 22:50:11.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 22:50:11.082: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/04/23 22:50:11.094
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1513, will wait for the garbage collector to delete the pods 01/04/23 22:50:11.094
    Jan  4 22:50:11.155: INFO: Deleting DaemonSet.extensions daemon-set took: 6.565245ms
    Jan  4 22:50:11.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.982137ms
    Jan  4 22:50:13.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 22:50:13.761: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  4 22:50:13.763: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49626"},"items":null}

    Jan  4 22:50:13.766: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49626"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:50:13.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1513" for this suite. 01/04/23 22:50:13.784
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:50:13.791
Jan  4 22:50:13.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename var-expansion 01/04/23 22:50:13.792
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:50:13.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:50:13.829
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/04/23 22:50:13.831
STEP: waiting for pod running 01/04/23 22:50:13.839
Jan  4 22:50:13.839: INFO: Waiting up to 2m0s for pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" in namespace "var-expansion-6570" to be "running"
Jan  4 22:50:13.846: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.632154ms
Jan  4 22:50:15.850: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12": Phase="Running", Reason="", readiness=true. Elapsed: 2.010635157s
Jan  4 22:50:15.850: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" satisfied condition "running"
STEP: creating a file in subpath 01/04/23 22:50:15.85
Jan  4 22:50:15.853: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6570 PodName:var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:50:15.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:50:15.854: INFO: ExecWithOptions: Clientset creation
Jan  4 22:50:15.854: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-6570/pods/var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/04/23 22:50:15.923
Jan  4 22:50:15.926: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6570 PodName:var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:50:15.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:50:15.927: INFO: ExecWithOptions: Clientset creation
Jan  4 22:50:15.927: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-6570/pods/var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/04/23 22:50:16.008
Jan  4 22:50:16.527: INFO: Successfully updated pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12"
STEP: waiting for annotated pod running 01/04/23 22:50:16.528
Jan  4 22:50:16.528: INFO: Waiting up to 2m0s for pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" in namespace "var-expansion-6570" to be "running"
Jan  4 22:50:16.532: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12": Phase="Running", Reason="", readiness=true. Elapsed: 4.166604ms
Jan  4 22:50:16.532: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" satisfied condition "running"
STEP: deleting the pod gracefully 01/04/23 22:50:16.533
Jan  4 22:50:16.534: INFO: Deleting pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" in namespace "var-expansion-6570"
Jan  4 22:50:16.543: INFO: Wait up to 5m0s for pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  4 22:50:50.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6570" for this suite. 01/04/23 22:50:50.556
------------------------------
• [SLOW TEST] [36.771 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:50:13.791
    Jan  4 22:50:13.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename var-expansion 01/04/23 22:50:13.792
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:50:13.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:50:13.829
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/04/23 22:50:13.831
    STEP: waiting for pod running 01/04/23 22:50:13.839
    Jan  4 22:50:13.839: INFO: Waiting up to 2m0s for pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" in namespace "var-expansion-6570" to be "running"
    Jan  4 22:50:13.846: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.632154ms
    Jan  4 22:50:15.850: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12": Phase="Running", Reason="", readiness=true. Elapsed: 2.010635157s
    Jan  4 22:50:15.850: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" satisfied condition "running"
    STEP: creating a file in subpath 01/04/23 22:50:15.85
    Jan  4 22:50:15.853: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6570 PodName:var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:50:15.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:50:15.854: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:50:15.854: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-6570/pods/var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/04/23 22:50:15.923
    Jan  4 22:50:15.926: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6570 PodName:var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:50:15.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:50:15.927: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:50:15.927: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-6570/pods/var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/04/23 22:50:16.008
    Jan  4 22:50:16.527: INFO: Successfully updated pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12"
    STEP: waiting for annotated pod running 01/04/23 22:50:16.528
    Jan  4 22:50:16.528: INFO: Waiting up to 2m0s for pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" in namespace "var-expansion-6570" to be "running"
    Jan  4 22:50:16.532: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12": Phase="Running", Reason="", readiness=true. Elapsed: 4.166604ms
    Jan  4 22:50:16.532: INFO: Pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" satisfied condition "running"
    STEP: deleting the pod gracefully 01/04/23 22:50:16.533
    Jan  4 22:50:16.534: INFO: Deleting pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" in namespace "var-expansion-6570"
    Jan  4 22:50:16.543: INFO: Wait up to 5m0s for pod "var-expansion-cfd247b4-f9f4-435d-a4a1-cf02568b0b12" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:50:50.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6570" for this suite. 01/04/23 22:50:50.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:50:50.563
Jan  4 22:50:50.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 22:50:50.567
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:50:50.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:50:50.592
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-a00e1139-e1df-49ef-bafe-1768dc95fab6 01/04/23 22:50:50.639
STEP: Creating a pod to test consume secrets 01/04/23 22:50:50.645
Jan  4 22:50:50.658: INFO: Waiting up to 5m0s for pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660" in namespace "secrets-771" to be "Succeeded or Failed"
Jan  4 22:50:50.665: INFO: Pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660": Phase="Pending", Reason="", readiness=false. Elapsed: 6.820601ms
Jan  4 22:50:52.670: INFO: Pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011417501s
Jan  4 22:50:54.669: INFO: Pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010868084s
STEP: Saw pod success 01/04/23 22:50:54.669
Jan  4 22:50:54.669: INFO: Pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660" satisfied condition "Succeeded or Failed"
Jan  4 22:50:54.673: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660 container secret-volume-test: <nil>
STEP: delete the pod 01/04/23 22:50:54.678
Jan  4 22:50:54.692: INFO: Waiting for pod pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660 to disappear
Jan  4 22:50:54.698: INFO: Pod pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 22:50:54.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-771" for this suite. 01/04/23 22:50:54.702
STEP: Destroying namespace "secret-namespace-4701" for this suite. 01/04/23 22:50:54.709
------------------------------
• [4.159 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:50:50.563
    Jan  4 22:50:50.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 22:50:50.567
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:50:50.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:50:50.592
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-a00e1139-e1df-49ef-bafe-1768dc95fab6 01/04/23 22:50:50.639
    STEP: Creating a pod to test consume secrets 01/04/23 22:50:50.645
    Jan  4 22:50:50.658: INFO: Waiting up to 5m0s for pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660" in namespace "secrets-771" to be "Succeeded or Failed"
    Jan  4 22:50:50.665: INFO: Pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660": Phase="Pending", Reason="", readiness=false. Elapsed: 6.820601ms
    Jan  4 22:50:52.670: INFO: Pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011417501s
    Jan  4 22:50:54.669: INFO: Pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010868084s
    STEP: Saw pod success 01/04/23 22:50:54.669
    Jan  4 22:50:54.669: INFO: Pod "pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660" satisfied condition "Succeeded or Failed"
    Jan  4 22:50:54.673: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660 container secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 22:50:54.678
    Jan  4 22:50:54.692: INFO: Waiting for pod pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660 to disappear
    Jan  4 22:50:54.698: INFO: Pod pod-secrets-a291b5c7-07ca-4e63-9195-cbe12ed5d660 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:50:54.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-771" for this suite. 01/04/23 22:50:54.702
    STEP: Destroying namespace "secret-namespace-4701" for this suite. 01/04/23 22:50:54.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:50:54.722
Jan  4 22:50:54.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename var-expansion 01/04/23 22:50:54.723
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:50:54.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:50:54.741
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/04/23 22:50:54.746
Jan  4 22:50:54.762: INFO: Waiting up to 2m0s for pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" in namespace "var-expansion-5182" to be "running"
Jan  4 22:50:54.776: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.671772ms
Jan  4 22:50:56.782: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019624176s
Jan  4 22:50:58.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017372659s
Jan  4 22:51:00.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018409113s
Jan  4 22:51:02.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017495443s
Jan  4 22:51:04.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017594476s
Jan  4 22:51:06.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021113114s
Jan  4 22:51:08.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017413771s
Jan  4 22:51:10.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.01744845s
Jan  4 22:51:12.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017764724s
Jan  4 22:51:14.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017346762s
Jan  4 22:51:16.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.018314798s
Jan  4 22:51:18.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.018339244s
Jan  4 22:51:20.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.017887683s
Jan  4 22:51:22.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01773932s
Jan  4 22:51:24.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.018211859s
Jan  4 22:51:26.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021424901s
Jan  4 22:51:28.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01821488s
Jan  4 22:51:30.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.017717811s
Jan  4 22:51:32.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 38.017589913s
Jan  4 22:51:34.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018865678s
Jan  4 22:51:36.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 42.017646079s
Jan  4 22:51:38.779: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016879359s
Jan  4 22:51:40.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018253678s
Jan  4 22:51:42.779: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 48.016888515s
Jan  4 22:51:44.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 50.018378583s
Jan  4 22:51:46.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021912527s
Jan  4 22:51:48.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017190967s
Jan  4 22:51:50.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 56.017285581s
Jan  4 22:51:52.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 58.017783893s
Jan  4 22:51:54.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.017424554s
Jan  4 22:51:56.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021125525s
Jan  4 22:51:58.782: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.019274502s
Jan  4 22:52:00.789: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026108751s
Jan  4 22:52:02.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017586329s
Jan  4 22:52:04.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.018903051s
Jan  4 22:52:06.782: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020068718s
Jan  4 22:52:08.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017726999s
Jan  4 22:52:10.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.017817333s
Jan  4 22:52:12.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.017371185s
Jan  4 22:52:14.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.017240115s
Jan  4 22:52:16.783: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.020129086s
Jan  4 22:52:18.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.017294726s
Jan  4 22:52:20.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018074569s
Jan  4 22:52:22.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017853277s
Jan  4 22:52:24.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018383671s
Jan  4 22:52:26.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017921489s
Jan  4 22:52:28.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.017665229s
Jan  4 22:52:30.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017937419s
Jan  4 22:52:32.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.017588615s
Jan  4 22:52:34.783: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.020529337s
Jan  4 22:52:36.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.021998819s
Jan  4 22:52:38.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017897237s
Jan  4 22:52:40.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.018651765s
Jan  4 22:52:42.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.0172208s
Jan  4 22:52:44.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01731802s
Jan  4 22:52:46.782: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019852301s
Jan  4 22:52:48.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.017273802s
Jan  4 22:52:50.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018463959s
Jan  4 22:52:52.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018235771s
Jan  4 22:52:54.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.021700801s
Jan  4 22:52:54.788: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.025935959s
STEP: updating the pod 01/04/23 22:52:54.789
Jan  4 22:52:55.302: INFO: Successfully updated pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b"
STEP: waiting for pod running 01/04/23 22:52:55.303
Jan  4 22:52:55.303: INFO: Waiting up to 2m0s for pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" in namespace "var-expansion-5182" to be "running"
Jan  4 22:52:55.314: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.199775ms
Jan  4 22:52:57.318: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.01513624s
Jan  4 22:52:57.318: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" satisfied condition "running"
STEP: deleting the pod gracefully 01/04/23 22:52:57.318
Jan  4 22:52:57.318: INFO: Deleting pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" in namespace "var-expansion-5182"
Jan  4 22:52:57.328: INFO: Wait up to 5m0s for pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  4 22:53:29.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5182" for this suite. 01/04/23 22:53:29.345
------------------------------
• [SLOW TEST] [154.629 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:50:54.722
    Jan  4 22:50:54.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename var-expansion 01/04/23 22:50:54.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:50:54.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:50:54.741
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/04/23 22:50:54.746
    Jan  4 22:50:54.762: INFO: Waiting up to 2m0s for pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" in namespace "var-expansion-5182" to be "running"
    Jan  4 22:50:54.776: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.671772ms
    Jan  4 22:50:56.782: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019624176s
    Jan  4 22:50:58.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017372659s
    Jan  4 22:51:00.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018409113s
    Jan  4 22:51:02.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017495443s
    Jan  4 22:51:04.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017594476s
    Jan  4 22:51:06.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021113114s
    Jan  4 22:51:08.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017413771s
    Jan  4 22:51:10.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.01744845s
    Jan  4 22:51:12.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017764724s
    Jan  4 22:51:14.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017346762s
    Jan  4 22:51:16.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.018314798s
    Jan  4 22:51:18.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.018339244s
    Jan  4 22:51:20.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.017887683s
    Jan  4 22:51:22.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01773932s
    Jan  4 22:51:24.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.018211859s
    Jan  4 22:51:26.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021424901s
    Jan  4 22:51:28.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01821488s
    Jan  4 22:51:30.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.017717811s
    Jan  4 22:51:32.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 38.017589913s
    Jan  4 22:51:34.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018865678s
    Jan  4 22:51:36.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 42.017646079s
    Jan  4 22:51:38.779: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016879359s
    Jan  4 22:51:40.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018253678s
    Jan  4 22:51:42.779: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 48.016888515s
    Jan  4 22:51:44.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 50.018378583s
    Jan  4 22:51:46.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021912527s
    Jan  4 22:51:48.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017190967s
    Jan  4 22:51:50.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 56.017285581s
    Jan  4 22:51:52.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 58.017783893s
    Jan  4 22:51:54.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.017424554s
    Jan  4 22:51:56.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021125525s
    Jan  4 22:51:58.782: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.019274502s
    Jan  4 22:52:00.789: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026108751s
    Jan  4 22:52:02.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017586329s
    Jan  4 22:52:04.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.018903051s
    Jan  4 22:52:06.782: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020068718s
    Jan  4 22:52:08.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017726999s
    Jan  4 22:52:10.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.017817333s
    Jan  4 22:52:12.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.017371185s
    Jan  4 22:52:14.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.017240115s
    Jan  4 22:52:16.783: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.020129086s
    Jan  4 22:52:18.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.017294726s
    Jan  4 22:52:20.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018074569s
    Jan  4 22:52:22.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017853277s
    Jan  4 22:52:24.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018383671s
    Jan  4 22:52:26.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017921489s
    Jan  4 22:52:28.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.017665229s
    Jan  4 22:52:30.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017937419s
    Jan  4 22:52:32.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.017588615s
    Jan  4 22:52:34.783: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.020529337s
    Jan  4 22:52:36.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.021998819s
    Jan  4 22:52:38.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017897237s
    Jan  4 22:52:40.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.018651765s
    Jan  4 22:52:42.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.0172208s
    Jan  4 22:52:44.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01731802s
    Jan  4 22:52:46.782: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019852301s
    Jan  4 22:52:48.780: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.017273802s
    Jan  4 22:52:50.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018463959s
    Jan  4 22:52:52.781: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018235771s
    Jan  4 22:52:54.784: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.021700801s
    Jan  4 22:52:54.788: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.025935959s
    STEP: updating the pod 01/04/23 22:52:54.789
    Jan  4 22:52:55.302: INFO: Successfully updated pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b"
    STEP: waiting for pod running 01/04/23 22:52:55.303
    Jan  4 22:52:55.303: INFO: Waiting up to 2m0s for pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" in namespace "var-expansion-5182" to be "running"
    Jan  4 22:52:55.314: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.199775ms
    Jan  4 22:52:57.318: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.01513624s
    Jan  4 22:52:57.318: INFO: Pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" satisfied condition "running"
    STEP: deleting the pod gracefully 01/04/23 22:52:57.318
    Jan  4 22:52:57.318: INFO: Deleting pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" in namespace "var-expansion-5182"
    Jan  4 22:52:57.328: INFO: Wait up to 5m0s for pod "var-expansion-6a0ae51d-017a-4dca-b1a0-322f7805be3b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:53:29.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5182" for this suite. 01/04/23 22:53:29.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:53:29.354
Jan  4 22:53:29.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:53:29.355
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:53:29.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:53:29.376
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/04/23 22:53:29.379
Jan  4 22:53:29.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-8129 create -f -'
Jan  4 22:53:30.495: INFO: stderr: ""
Jan  4 22:53:30.496: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/04/23 22:53:30.496
Jan  4 22:53:31.501: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 22:53:31.501: INFO: Found 0 / 1
Jan  4 22:53:32.499: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 22:53:32.499: INFO: Found 1 / 1
Jan  4 22:53:32.499: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/04/23 22:53:32.499
Jan  4 22:53:32.502: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 22:53:32.502: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  4 22:53:32.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-8129 patch pod agnhost-primary-fbtlc -p {"metadata":{"annotations":{"x":"y"}}}'
Jan  4 22:53:32.596: INFO: stderr: ""
Jan  4 22:53:32.596: INFO: stdout: "pod/agnhost-primary-fbtlc patched\n"
STEP: checking annotations 01/04/23 22:53:32.596
Jan  4 22:53:32.603: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 22:53:32.603: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:53:32.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8129" for this suite. 01/04/23 22:53:32.607
------------------------------
• [3.260 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:53:29.354
    Jan  4 22:53:29.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:53:29.355
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:53:29.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:53:29.376
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/04/23 22:53:29.379
    Jan  4 22:53:29.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-8129 create -f -'
    Jan  4 22:53:30.495: INFO: stderr: ""
    Jan  4 22:53:30.496: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/04/23 22:53:30.496
    Jan  4 22:53:31.501: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 22:53:31.501: INFO: Found 0 / 1
    Jan  4 22:53:32.499: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 22:53:32.499: INFO: Found 1 / 1
    Jan  4 22:53:32.499: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/04/23 22:53:32.499
    Jan  4 22:53:32.502: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 22:53:32.502: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  4 22:53:32.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-8129 patch pod agnhost-primary-fbtlc -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan  4 22:53:32.596: INFO: stderr: ""
    Jan  4 22:53:32.596: INFO: stdout: "pod/agnhost-primary-fbtlc patched\n"
    STEP: checking annotations 01/04/23 22:53:32.596
    Jan  4 22:53:32.603: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 22:53:32.603: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:53:32.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8129" for this suite. 01/04/23 22:53:32.607
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:53:32.614
Jan  4 22:53:32.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 22:53:32.615
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:53:32.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:53:32.633
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6358 01/04/23 22:53:32.635
STEP: changing the ExternalName service to type=ClusterIP 01/04/23 22:53:32.64
STEP: creating replication controller externalname-service in namespace services-6358 01/04/23 22:53:32.661
I0104 22:53:32.669086      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6358, replica count: 2
I0104 22:53:35.719658      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 22:53:35.719: INFO: Creating new exec pod
Jan  4 22:53:35.728: INFO: Waiting up to 5m0s for pod "execpodzqhvw" in namespace "services-6358" to be "running"
Jan  4 22:53:35.731: INFO: Pod "execpodzqhvw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13434ms
Jan  4 22:53:37.735: INFO: Pod "execpodzqhvw": Phase="Running", Reason="", readiness=true. Elapsed: 2.006666293s
Jan  4 22:53:37.735: INFO: Pod "execpodzqhvw" satisfied condition "running"
Jan  4 22:53:38.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-6358 exec execpodzqhvw -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan  4 22:53:38.854: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  4 22:53:38.854: INFO: stdout: ""
Jan  4 22:53:38.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-6358 exec execpodzqhvw -- /bin/sh -x -c nc -v -z -w 2 10.43.38.81 80'
Jan  4 22:53:38.974: INFO: stderr: "+ nc -v -z -w 2 10.43.38.81 80\nConnection to 10.43.38.81 80 port [tcp/http] succeeded!\n"
Jan  4 22:53:38.974: INFO: stdout: ""
Jan  4 22:53:38.974: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 22:53:39.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6358" for this suite. 01/04/23 22:53:39.025
------------------------------
• [SLOW TEST] [6.435 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:53:32.614
    Jan  4 22:53:32.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 22:53:32.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:53:32.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:53:32.633
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6358 01/04/23 22:53:32.635
    STEP: changing the ExternalName service to type=ClusterIP 01/04/23 22:53:32.64
    STEP: creating replication controller externalname-service in namespace services-6358 01/04/23 22:53:32.661
    I0104 22:53:32.669086      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6358, replica count: 2
    I0104 22:53:35.719658      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 22:53:35.719: INFO: Creating new exec pod
    Jan  4 22:53:35.728: INFO: Waiting up to 5m0s for pod "execpodzqhvw" in namespace "services-6358" to be "running"
    Jan  4 22:53:35.731: INFO: Pod "execpodzqhvw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13434ms
    Jan  4 22:53:37.735: INFO: Pod "execpodzqhvw": Phase="Running", Reason="", readiness=true. Elapsed: 2.006666293s
    Jan  4 22:53:37.735: INFO: Pod "execpodzqhvw" satisfied condition "running"
    Jan  4 22:53:38.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-6358 exec execpodzqhvw -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan  4 22:53:38.854: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  4 22:53:38.854: INFO: stdout: ""
    Jan  4 22:53:38.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-6358 exec execpodzqhvw -- /bin/sh -x -c nc -v -z -w 2 10.43.38.81 80'
    Jan  4 22:53:38.974: INFO: stderr: "+ nc -v -z -w 2 10.43.38.81 80\nConnection to 10.43.38.81 80 port [tcp/http] succeeded!\n"
    Jan  4 22:53:38.974: INFO: stdout: ""
    Jan  4 22:53:38.974: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:53:39.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6358" for this suite. 01/04/23 22:53:39.025
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:53:39.048
Jan  4 22:53:39.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 22:53:39.05
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:53:39.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:53:39.082
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 22:53:39.106
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:53:39.486
STEP: Deploying the webhook pod 01/04/23 22:53:39.494
STEP: Wait for the deployment to be ready 01/04/23 22:53:39.509
Jan  4 22:53:39.523: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 22:53:41.532
STEP: Verifying the service has paired with the endpoint 01/04/23 22:53:41.545
Jan  4 22:53:42.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/04/23 22:53:42.549
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/04/23 22:53:42.568
STEP: Creating a configMap that should not be mutated 01/04/23 22:53:42.573
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/04/23 22:53:42.585
STEP: Creating a configMap that should be mutated 01/04/23 22:53:42.592
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 22:53:42.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4799" for this suite. 01/04/23 22:53:42.679
STEP: Destroying namespace "webhook-4799-markers" for this suite. 01/04/23 22:53:42.688
------------------------------
• [3.646 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:53:39.048
    Jan  4 22:53:39.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 22:53:39.05
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:53:39.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:53:39.082
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 22:53:39.106
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 22:53:39.486
    STEP: Deploying the webhook pod 01/04/23 22:53:39.494
    STEP: Wait for the deployment to be ready 01/04/23 22:53:39.509
    Jan  4 22:53:39.523: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 22:53:41.532
    STEP: Verifying the service has paired with the endpoint 01/04/23 22:53:41.545
    Jan  4 22:53:42.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/04/23 22:53:42.549
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/04/23 22:53:42.568
    STEP: Creating a configMap that should not be mutated 01/04/23 22:53:42.573
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/04/23 22:53:42.585
    STEP: Creating a configMap that should be mutated 01/04/23 22:53:42.592
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:53:42.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4799" for this suite. 01/04/23 22:53:42.679
    STEP: Destroying namespace "webhook-4799-markers" for this suite. 01/04/23 22:53:42.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:53:42.696
Jan  4 22:53:42.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename cronjob 01/04/23 22:53:42.701
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:53:42.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:53:42.724
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/04/23 22:53:42.727
STEP: Ensuring a job is scheduled 01/04/23 22:53:42.732
STEP: Ensuring exactly one is scheduled 01/04/23 22:54:00.737
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/04/23 22:54:00.748
STEP: Ensuring no more jobs are scheduled 01/04/23 22:54:00.754
STEP: Removing cronjob 01/04/23 22:59:00.76
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  4 22:59:00.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8820" for this suite. 01/04/23 22:59:00.771
------------------------------
• [SLOW TEST] [318.084 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:53:42.696
    Jan  4 22:53:42.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename cronjob 01/04/23 22:53:42.701
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:53:42.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:53:42.724
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/04/23 22:53:42.727
    STEP: Ensuring a job is scheduled 01/04/23 22:53:42.732
    STEP: Ensuring exactly one is scheduled 01/04/23 22:54:00.737
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/04/23 22:54:00.748
    STEP: Ensuring no more jobs are scheduled 01/04/23 22:54:00.754
    STEP: Removing cronjob 01/04/23 22:59:00.76
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:59:00.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8820" for this suite. 01/04/23 22:59:00.771
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:59:00.783
Jan  4 22:59:00.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 22:59:00.784
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:00.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:00.815
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/04/23 22:59:00.818
Jan  4 22:59:00.827: INFO: Waiting up to 5m0s for pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343" in namespace "emptydir-5637" to be "Succeeded or Failed"
Jan  4 22:59:00.835: INFO: Pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343": Phase="Pending", Reason="", readiness=false. Elapsed: 8.543823ms
Jan  4 22:59:02.846: INFO: Pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01885842s
Jan  4 22:59:04.840: INFO: Pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013042256s
STEP: Saw pod success 01/04/23 22:59:04.84
Jan  4 22:59:04.840: INFO: Pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343" satisfied condition "Succeeded or Failed"
Jan  4 22:59:04.843: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-e54e36c7-19ff-44b2-9a24-70cd44630343 container test-container: <nil>
STEP: delete the pod 01/04/23 22:59:04.858
Jan  4 22:59:04.868: INFO: Waiting for pod pod-e54e36c7-19ff-44b2-9a24-70cd44630343 to disappear
Jan  4 22:59:04.872: INFO: Pod pod-e54e36c7-19ff-44b2-9a24-70cd44630343 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 22:59:04.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5637" for this suite. 01/04/23 22:59:04.875
------------------------------
• [4.097 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:59:00.783
    Jan  4 22:59:00.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 22:59:00.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:00.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:00.815
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/04/23 22:59:00.818
    Jan  4 22:59:00.827: INFO: Waiting up to 5m0s for pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343" in namespace "emptydir-5637" to be "Succeeded or Failed"
    Jan  4 22:59:00.835: INFO: Pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343": Phase="Pending", Reason="", readiness=false. Elapsed: 8.543823ms
    Jan  4 22:59:02.846: INFO: Pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01885842s
    Jan  4 22:59:04.840: INFO: Pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013042256s
    STEP: Saw pod success 01/04/23 22:59:04.84
    Jan  4 22:59:04.840: INFO: Pod "pod-e54e36c7-19ff-44b2-9a24-70cd44630343" satisfied condition "Succeeded or Failed"
    Jan  4 22:59:04.843: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-e54e36c7-19ff-44b2-9a24-70cd44630343 container test-container: <nil>
    STEP: delete the pod 01/04/23 22:59:04.858
    Jan  4 22:59:04.868: INFO: Waiting for pod pod-e54e36c7-19ff-44b2-9a24-70cd44630343 to disappear
    Jan  4 22:59:04.872: INFO: Pod pod-e54e36c7-19ff-44b2-9a24-70cd44630343 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:59:04.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5637" for this suite. 01/04/23 22:59:04.875
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:59:04.881
Jan  4 22:59:04.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:59:04.882
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:04.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:04.896
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/04/23 22:59:04.899
Jan  4 22:59:04.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6754 api-versions'
Jan  4 22:59:05.099: INFO: stderr: ""
Jan  4 22:59:05.099: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.cattle.io/v1\nk3s.cattle.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 22:59:05.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6754" for this suite. 01/04/23 22:59:05.104
------------------------------
• [0.232 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:59:04.881
    Jan  4 22:59:04.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:59:04.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:04.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:04.896
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/04/23 22:59:04.899
    Jan  4 22:59:04.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6754 api-versions'
    Jan  4 22:59:05.099: INFO: stderr: ""
    Jan  4 22:59:05.099: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.cattle.io/v1\nk3s.cattle.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:59:05.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6754" for this suite. 01/04/23 22:59:05.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:59:05.119
Jan  4 22:59:05.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-probe 01/04/23 22:59:05.119
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:05.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:05.147
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-a9eea594-366f-47c7-ac8d-daaee298f150 in namespace container-probe-1690 01/04/23 22:59:05.153
Jan  4 22:59:05.166: INFO: Waiting up to 5m0s for pod "liveness-a9eea594-366f-47c7-ac8d-daaee298f150" in namespace "container-probe-1690" to be "not pending"
Jan  4 22:59:05.174: INFO: Pod "liveness-a9eea594-366f-47c7-ac8d-daaee298f150": Phase="Pending", Reason="", readiness=false. Elapsed: 8.348791ms
Jan  4 22:59:07.178: INFO: Pod "liveness-a9eea594-366f-47c7-ac8d-daaee298f150": Phase="Running", Reason="", readiness=true. Elapsed: 2.011926184s
Jan  4 22:59:07.178: INFO: Pod "liveness-a9eea594-366f-47c7-ac8d-daaee298f150" satisfied condition "not pending"
Jan  4 22:59:07.178: INFO: Started pod liveness-a9eea594-366f-47c7-ac8d-daaee298f150 in namespace container-probe-1690
STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 22:59:07.178
Jan  4 22:59:07.190: INFO: Initial restart count of pod liveness-a9eea594-366f-47c7-ac8d-daaee298f150 is 0
Jan  4 22:59:27.232: INFO: Restart count of pod container-probe-1690/liveness-a9eea594-366f-47c7-ac8d-daaee298f150 is now 1 (20.042001273s elapsed)
STEP: deleting the pod 01/04/23 22:59:27.232
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  4 22:59:27.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1690" for this suite. 01/04/23 22:59:27.246
------------------------------
• [SLOW TEST] [22.139 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:59:05.119
    Jan  4 22:59:05.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-probe 01/04/23 22:59:05.119
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:05.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:05.147
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-a9eea594-366f-47c7-ac8d-daaee298f150 in namespace container-probe-1690 01/04/23 22:59:05.153
    Jan  4 22:59:05.166: INFO: Waiting up to 5m0s for pod "liveness-a9eea594-366f-47c7-ac8d-daaee298f150" in namespace "container-probe-1690" to be "not pending"
    Jan  4 22:59:05.174: INFO: Pod "liveness-a9eea594-366f-47c7-ac8d-daaee298f150": Phase="Pending", Reason="", readiness=false. Elapsed: 8.348791ms
    Jan  4 22:59:07.178: INFO: Pod "liveness-a9eea594-366f-47c7-ac8d-daaee298f150": Phase="Running", Reason="", readiness=true. Elapsed: 2.011926184s
    Jan  4 22:59:07.178: INFO: Pod "liveness-a9eea594-366f-47c7-ac8d-daaee298f150" satisfied condition "not pending"
    Jan  4 22:59:07.178: INFO: Started pod liveness-a9eea594-366f-47c7-ac8d-daaee298f150 in namespace container-probe-1690
    STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 22:59:07.178
    Jan  4 22:59:07.190: INFO: Initial restart count of pod liveness-a9eea594-366f-47c7-ac8d-daaee298f150 is 0
    Jan  4 22:59:27.232: INFO: Restart count of pod container-probe-1690/liveness-a9eea594-366f-47c7-ac8d-daaee298f150 is now 1 (20.042001273s elapsed)
    STEP: deleting the pod 01/04/23 22:59:27.232
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:59:27.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1690" for this suite. 01/04/23 22:59:27.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:59:27.264
Jan  4 22:59:27.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 22:59:27.265
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:27.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:27.297
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/04/23 22:59:27.299
Jan  4 22:59:27.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32" in namespace "downward-api-4420" to be "Succeeded or Failed"
Jan  4 22:59:27.317: INFO: Pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32": Phase="Pending", Reason="", readiness=false. Elapsed: 7.408388ms
Jan  4 22:59:29.321: INFO: Pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011333683s
Jan  4 22:59:31.321: INFO: Pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011582063s
STEP: Saw pod success 01/04/23 22:59:31.321
Jan  4 22:59:31.321: INFO: Pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32" satisfied condition "Succeeded or Failed"
Jan  4 22:59:31.324: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32 container client-container: <nil>
STEP: delete the pod 01/04/23 22:59:31.332
Jan  4 22:59:31.344: INFO: Waiting for pod downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32 to disappear
Jan  4 22:59:31.350: INFO: Pod downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 22:59:31.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4420" for this suite. 01/04/23 22:59:31.356
------------------------------
• [4.101 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:59:27.264
    Jan  4 22:59:27.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 22:59:27.265
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:27.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:27.297
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/04/23 22:59:27.299
    Jan  4 22:59:27.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32" in namespace "downward-api-4420" to be "Succeeded or Failed"
    Jan  4 22:59:27.317: INFO: Pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32": Phase="Pending", Reason="", readiness=false. Elapsed: 7.408388ms
    Jan  4 22:59:29.321: INFO: Pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011333683s
    Jan  4 22:59:31.321: INFO: Pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011582063s
    STEP: Saw pod success 01/04/23 22:59:31.321
    Jan  4 22:59:31.321: INFO: Pod "downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32" satisfied condition "Succeeded or Failed"
    Jan  4 22:59:31.324: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32 container client-container: <nil>
    STEP: delete the pod 01/04/23 22:59:31.332
    Jan  4 22:59:31.344: INFO: Waiting for pod downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32 to disappear
    Jan  4 22:59:31.350: INFO: Pod downwardapi-volume-53f35b29-92b5-4c0e-9a16-274928bb4b32 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:59:31.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4420" for this suite. 01/04/23 22:59:31.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:59:31.367
Jan  4 22:59:31.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pod-network-test 01/04/23 22:59:31.368
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:31.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:31.394
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7221 01/04/23 22:59:31.398
STEP: creating a selector 01/04/23 22:59:31.398
STEP: Creating the service pods in kubernetes 01/04/23 22:59:31.398
Jan  4 22:59:31.398: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  4 22:59:31.459: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7221" to be "running and ready"
Jan  4 22:59:31.477: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.752588ms
Jan  4 22:59:31.477: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 22:59:33.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021685112s
Jan  4 22:59:33.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:35.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023015594s
Jan  4 22:59:35.482: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:37.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020999197s
Jan  4 22:59:37.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:39.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.022809073s
Jan  4 22:59:39.482: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:41.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022304516s
Jan  4 22:59:41.482: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:43.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.021625316s
Jan  4 22:59:43.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:45.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.020886816s
Jan  4 22:59:45.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:47.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021003929s
Jan  4 22:59:47.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:49.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0218413s
Jan  4 22:59:49.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:51.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.021451744s
Jan  4 22:59:51.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 22:59:53.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.021374869s
Jan  4 22:59:53.481: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  4 22:59:53.481: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  4 22:59:53.485: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7221" to be "running and ready"
Jan  4 22:59:53.488: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.953562ms
Jan  4 22:59:53.488: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  4 22:59:53.488: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  4 22:59:53.491: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7221" to be "running and ready"
Jan  4 22:59:53.494: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.780891ms
Jan  4 22:59:53.494: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  4 22:59:53.494: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan  4 22:59:53.497: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7221" to be "running and ready"
Jan  4 22:59:53.505: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 7.997257ms
Jan  4 22:59:53.505: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan  4 22:59:53.505: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 01/04/23 22:59:53.51
Jan  4 22:59:53.517: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7221" to be "running"
Jan  4 22:59:53.524: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.60631ms
Jan  4 22:59:55.528: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011178996s
Jan  4 22:59:55.528: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  4 22:59:55.530: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan  4 22:59:55.530: INFO: Breadth first check of 10.42.0.97 on host 172.31.11.54...
Jan  4 22:59:55.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.248:9080/dial?request=hostname&protocol=http&host=10.42.0.97&port=8083&tries=1'] Namespace:pod-network-test-7221 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:59:55.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:59:55.533: INFO: ExecWithOptions: Clientset creation
Jan  4 22:59:55.533: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7221/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.0.97%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  4 22:59:55.622: INFO: Waiting for responses: map[]
Jan  4 22:59:55.622: INFO: reached 10.42.0.97 after 0/1 tries
Jan  4 22:59:55.622: INFO: Breadth first check of 10.42.3.247 on host 172.31.13.117...
Jan  4 22:59:55.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.248:9080/dial?request=hostname&protocol=http&host=10.42.3.247&port=8083&tries=1'] Namespace:pod-network-test-7221 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:59:55.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:59:55.626: INFO: ExecWithOptions: Clientset creation
Jan  4 22:59:55.626: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7221/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.3.247%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  4 22:59:55.711: INFO: Waiting for responses: map[]
Jan  4 22:59:55.712: INFO: reached 10.42.3.247 after 0/1 tries
Jan  4 22:59:55.712: INFO: Breadth first check of 10.42.1.94 on host 172.31.3.240...
Jan  4 22:59:55.715: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.248:9080/dial?request=hostname&protocol=http&host=10.42.1.94&port=8083&tries=1'] Namespace:pod-network-test-7221 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:59:55.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:59:55.715: INFO: ExecWithOptions: Clientset creation
Jan  4 22:59:55.715: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7221/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.1.94%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  4 22:59:55.783: INFO: Waiting for responses: map[]
Jan  4 22:59:55.783: INFO: reached 10.42.1.94 after 0/1 tries
Jan  4 22:59:55.783: INFO: Breadth first check of 10.42.2.107 on host 172.31.9.62...
Jan  4 22:59:55.787: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.248:9080/dial?request=hostname&protocol=http&host=10.42.2.107&port=8083&tries=1'] Namespace:pod-network-test-7221 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 22:59:55.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 22:59:55.787: INFO: ExecWithOptions: Clientset creation
Jan  4 22:59:55.787: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7221/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.2.107%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  4 22:59:55.863: INFO: Waiting for responses: map[]
Jan  4 22:59:55.863: INFO: reached 10.42.2.107 after 0/1 tries
Jan  4 22:59:55.863: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  4 22:59:55.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7221" for this suite. 01/04/23 22:59:55.871
------------------------------
• [SLOW TEST] [24.510 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:59:31.367
    Jan  4 22:59:31.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pod-network-test 01/04/23 22:59:31.368
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:31.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:31.394
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7221 01/04/23 22:59:31.398
    STEP: creating a selector 01/04/23 22:59:31.398
    STEP: Creating the service pods in kubernetes 01/04/23 22:59:31.398
    Jan  4 22:59:31.398: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  4 22:59:31.459: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7221" to be "running and ready"
    Jan  4 22:59:31.477: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.752588ms
    Jan  4 22:59:31.477: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 22:59:33.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021685112s
    Jan  4 22:59:33.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:35.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023015594s
    Jan  4 22:59:35.482: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:37.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020999197s
    Jan  4 22:59:37.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:39.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.022809073s
    Jan  4 22:59:39.482: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:41.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022304516s
    Jan  4 22:59:41.482: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:43.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.021625316s
    Jan  4 22:59:43.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:45.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.020886816s
    Jan  4 22:59:45.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:47.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021003929s
    Jan  4 22:59:47.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:49.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0218413s
    Jan  4 22:59:49.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:51.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.021451744s
    Jan  4 22:59:51.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 22:59:53.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.021374869s
    Jan  4 22:59:53.481: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  4 22:59:53.481: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  4 22:59:53.485: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7221" to be "running and ready"
    Jan  4 22:59:53.488: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.953562ms
    Jan  4 22:59:53.488: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  4 22:59:53.488: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  4 22:59:53.491: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7221" to be "running and ready"
    Jan  4 22:59:53.494: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.780891ms
    Jan  4 22:59:53.494: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  4 22:59:53.494: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan  4 22:59:53.497: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7221" to be "running and ready"
    Jan  4 22:59:53.505: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 7.997257ms
    Jan  4 22:59:53.505: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan  4 22:59:53.505: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 01/04/23 22:59:53.51
    Jan  4 22:59:53.517: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7221" to be "running"
    Jan  4 22:59:53.524: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.60631ms
    Jan  4 22:59:55.528: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011178996s
    Jan  4 22:59:55.528: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  4 22:59:55.530: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Jan  4 22:59:55.530: INFO: Breadth first check of 10.42.0.97 on host 172.31.11.54...
    Jan  4 22:59:55.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.248:9080/dial?request=hostname&protocol=http&host=10.42.0.97&port=8083&tries=1'] Namespace:pod-network-test-7221 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:59:55.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:59:55.533: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:59:55.533: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7221/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.0.97%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  4 22:59:55.622: INFO: Waiting for responses: map[]
    Jan  4 22:59:55.622: INFO: reached 10.42.0.97 after 0/1 tries
    Jan  4 22:59:55.622: INFO: Breadth first check of 10.42.3.247 on host 172.31.13.117...
    Jan  4 22:59:55.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.248:9080/dial?request=hostname&protocol=http&host=10.42.3.247&port=8083&tries=1'] Namespace:pod-network-test-7221 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:59:55.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:59:55.626: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:59:55.626: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7221/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.3.247%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  4 22:59:55.711: INFO: Waiting for responses: map[]
    Jan  4 22:59:55.712: INFO: reached 10.42.3.247 after 0/1 tries
    Jan  4 22:59:55.712: INFO: Breadth first check of 10.42.1.94 on host 172.31.3.240...
    Jan  4 22:59:55.715: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.248:9080/dial?request=hostname&protocol=http&host=10.42.1.94&port=8083&tries=1'] Namespace:pod-network-test-7221 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:59:55.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:59:55.715: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:59:55.715: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7221/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.1.94%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  4 22:59:55.783: INFO: Waiting for responses: map[]
    Jan  4 22:59:55.783: INFO: reached 10.42.1.94 after 0/1 tries
    Jan  4 22:59:55.783: INFO: Breadth first check of 10.42.2.107 on host 172.31.9.62...
    Jan  4 22:59:55.787: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.248:9080/dial?request=hostname&protocol=http&host=10.42.2.107&port=8083&tries=1'] Namespace:pod-network-test-7221 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 22:59:55.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 22:59:55.787: INFO: ExecWithOptions: Clientset creation
    Jan  4 22:59:55.787: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7221/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.2.107%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  4 22:59:55.863: INFO: Waiting for responses: map[]
    Jan  4 22:59:55.863: INFO: reached 10.42.2.107 after 0/1 tries
    Jan  4 22:59:55.863: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:59:55.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7221" for this suite. 01/04/23 22:59:55.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:59:55.879
Jan  4 22:59:55.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename deployment 01/04/23 22:59:55.881
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:55.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:55.904
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/04/23 22:59:55.91
Jan  4 22:59:55.910: INFO: Creating simple deployment test-deployment-g29d7
Jan  4 22:59:55.924: INFO: deployment "test-deployment-g29d7" doesn't have the required revision set
STEP: Getting /status 01/04/23 22:59:57.943
Jan  4 22:59:57.948: INFO: Deployment test-deployment-g29d7 has Conditions: [{Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/04/23 22:59:57.948
Jan  4 22:59:57.964: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 22, 59, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 22, 59, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 22, 59, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 22, 59, 55, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-g29d7-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/04/23 22:59:57.964
Jan  4 22:59:57.965: INFO: Observed &Deployment event: ADDED
Jan  4 22:59:57.965: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g29d7-54bc444df"}
Jan  4 22:59:57.965: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g29d7-54bc444df"}
Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  4 22:59:57.966: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-g29d7-54bc444df" is progressing.}
Jan  4 22:59:57.966: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}
Jan  4 22:59:57.966: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}
Jan  4 22:59:57.966: INFO: Found Deployment test-deployment-g29d7 in namespace deployment-2888 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  4 22:59:57.966: INFO: Deployment test-deployment-g29d7 has an updated status
STEP: patching the Statefulset Status 01/04/23 22:59:57.966
Jan  4 22:59:57.966: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  4 22:59:57.973: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/04/23 22:59:57.973
Jan  4 22:59:57.975: INFO: Observed &Deployment event: ADDED
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g29d7-54bc444df"}
Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g29d7-54bc444df"}
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-g29d7-54bc444df" is progressing.}
Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}
Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}
Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
Jan  4 22:59:57.975: INFO: Found deployment test-deployment-g29d7 in namespace deployment-2888 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan  4 22:59:57.976: INFO: Deployment test-deployment-g29d7 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  4 22:59:57.980: INFO: Deployment "test-deployment-g29d7":
&Deployment{ObjectMeta:{test-deployment-g29d7  deployment-2888  e6e2f311-2e6e-4b02-ac4d-39d746096608 52310 1 2023-01-04 22:59:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-04 22:59:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-04 22:59:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-04 22:59:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00474f938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-g29d7-54bc444df",LastUpdateTime:2023-01-04 22:59:57 +0000 UTC,LastTransitionTime:2023-01-04 22:59:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  4 22:59:57.983: INFO: New ReplicaSet "test-deployment-g29d7-54bc444df" of Deployment "test-deployment-g29d7":
&ReplicaSet{ObjectMeta:{test-deployment-g29d7-54bc444df  deployment-2888  a7f23aca-8e64-4e09-b120-fccb641b234d 52303 1 2023-01-04 22:59:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-g29d7 e6e2f311-2e6e-4b02-ac4d-39d746096608 0xc004720640 0xc004720641}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:59:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6e2f311-2e6e-4b02-ac4d-39d746096608\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:59:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047207e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  4 22:59:57.986: INFO: Pod "test-deployment-g29d7-54bc444df-mg5d8" is available:
&Pod{ObjectMeta:{test-deployment-g29d7-54bc444df-mg5d8 test-deployment-g29d7-54bc444df- deployment-2888  12318cbc-40db-4e37-8a82-dd3f34dec9ba 52302 0 2023-01-04 22:59:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:6cfe24b705ba16bb88fbe9b2c820633b46b0f5bef39550cdce16a25d43d2ae21 cni.projectcalico.org/podIP:10.42.3.249/32 cni.projectcalico.org/podIPs:10.42.3.249/32] [{apps/v1 ReplicaSet test-deployment-g29d7-54bc444df a7f23aca-8e64-4e09-b120-fccb641b234d 0xc0047216a0 0xc0047216a1}] [] [{kube-controller-manager Update v1 2023-01-04 22:59:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7f23aca-8e64-4e09-b120-fccb641b234d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-04 22:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-04 22:59:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfqvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfqvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:59:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:59:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.249,StartTime:2023-01-04 22:59:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 22:59:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://03cc1499fc4c09256ffec56cf483e89de7b15bd71b9b31576a05652454e6d7ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  4 22:59:57.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2888" for this suite. 01/04/23 22:59:57.99
------------------------------
• [2.117 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:59:55.879
    Jan  4 22:59:55.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename deployment 01/04/23 22:59:55.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:55.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:55.904
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/04/23 22:59:55.91
    Jan  4 22:59:55.910: INFO: Creating simple deployment test-deployment-g29d7
    Jan  4 22:59:55.924: INFO: deployment "test-deployment-g29d7" doesn't have the required revision set
    STEP: Getting /status 01/04/23 22:59:57.943
    Jan  4 22:59:57.948: INFO: Deployment test-deployment-g29d7 has Conditions: [{Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/04/23 22:59:57.948
    Jan  4 22:59:57.964: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 22, 59, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 22, 59, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 22, 59, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 22, 59, 55, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-g29d7-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/04/23 22:59:57.964
    Jan  4 22:59:57.965: INFO: Observed &Deployment event: ADDED
    Jan  4 22:59:57.965: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g29d7-54bc444df"}
    Jan  4 22:59:57.965: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g29d7-54bc444df"}
    Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  4 22:59:57.966: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-g29d7-54bc444df" is progressing.}
    Jan  4 22:59:57.966: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}
    Jan  4 22:59:57.966: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  4 22:59:57.966: INFO: Observed Deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}
    Jan  4 22:59:57.966: INFO: Found Deployment test-deployment-g29d7 in namespace deployment-2888 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  4 22:59:57.966: INFO: Deployment test-deployment-g29d7 has an updated status
    STEP: patching the Statefulset Status 01/04/23 22:59:57.966
    Jan  4 22:59:57.966: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  4 22:59:57.973: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/04/23 22:59:57.973
    Jan  4 22:59:57.975: INFO: Observed &Deployment event: ADDED
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g29d7-54bc444df"}
    Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g29d7-54bc444df"}
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:55 +0000 UTC 2023-01-04 22:59:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-g29d7-54bc444df" is progressing.}
    Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}
    Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-04 22:59:56 +0000 UTC 2023-01-04 22:59:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g29d7-54bc444df" has successfully progressed.}
    Jan  4 22:59:57.975: INFO: Observed deployment test-deployment-g29d7 in namespace deployment-2888 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  4 22:59:57.975: INFO: Observed &Deployment event: MODIFIED
    Jan  4 22:59:57.975: INFO: Found deployment test-deployment-g29d7 in namespace deployment-2888 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan  4 22:59:57.976: INFO: Deployment test-deployment-g29d7 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  4 22:59:57.980: INFO: Deployment "test-deployment-g29d7":
    &Deployment{ObjectMeta:{test-deployment-g29d7  deployment-2888  e6e2f311-2e6e-4b02-ac4d-39d746096608 52310 1 2023-01-04 22:59:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-04 22:59:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-04 22:59:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-04 22:59:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00474f938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-g29d7-54bc444df",LastUpdateTime:2023-01-04 22:59:57 +0000 UTC,LastTransitionTime:2023-01-04 22:59:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  4 22:59:57.983: INFO: New ReplicaSet "test-deployment-g29d7-54bc444df" of Deployment "test-deployment-g29d7":
    &ReplicaSet{ObjectMeta:{test-deployment-g29d7-54bc444df  deployment-2888  a7f23aca-8e64-4e09-b120-fccb641b234d 52303 1 2023-01-04 22:59:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-g29d7 e6e2f311-2e6e-4b02-ac4d-39d746096608 0xc004720640 0xc004720641}] [] [{kube-controller-manager Update apps/v1 2023-01-04 22:59:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6e2f311-2e6e-4b02-ac4d-39d746096608\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 22:59:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047207e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 22:59:57.986: INFO: Pod "test-deployment-g29d7-54bc444df-mg5d8" is available:
    &Pod{ObjectMeta:{test-deployment-g29d7-54bc444df-mg5d8 test-deployment-g29d7-54bc444df- deployment-2888  12318cbc-40db-4e37-8a82-dd3f34dec9ba 52302 0 2023-01-04 22:59:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:6cfe24b705ba16bb88fbe9b2c820633b46b0f5bef39550cdce16a25d43d2ae21 cni.projectcalico.org/podIP:10.42.3.249/32 cni.projectcalico.org/podIPs:10.42.3.249/32] [{apps/v1 ReplicaSet test-deployment-g29d7-54bc444df a7f23aca-8e64-4e09-b120-fccb641b234d 0xc0047216a0 0xc0047216a1}] [] [{kube-controller-manager Update v1 2023-01-04 22:59:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7f23aca-8e64-4e09-b120-fccb641b234d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-04 22:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-04 22:59:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfqvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfqvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:59:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 22:59:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.249,StartTime:2023-01-04 22:59:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 22:59:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://03cc1499fc4c09256ffec56cf483e89de7b15bd71b9b31576a05652454e6d7ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  4 22:59:57.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2888" for this suite. 01/04/23 22:59:57.99
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 22:59:57.997
Jan  4 22:59:57.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 22:59:57.997
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:58.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:58.039
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan  4 22:59:58.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 create -f -'
Jan  4 22:59:58.238: INFO: stderr: ""
Jan  4 22:59:58.238: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan  4 22:59:58.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 create -f -'
Jan  4 22:59:58.494: INFO: stderr: ""
Jan  4 22:59:58.494: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/04/23 22:59:58.494
Jan  4 22:59:59.497: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 22:59:59.497: INFO: Found 0 / 1
Jan  4 23:00:00.497: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 23:00:00.497: INFO: Found 1 / 1
Jan  4 23:00:00.497: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  4 23:00:00.500: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 23:00:00.500: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  4 23:00:00.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe pod agnhost-primary-hkgn7'
Jan  4 23:00:00.583: INFO: stderr: ""
Jan  4 23:00:00.583: INFO: stdout: "Name:             agnhost-primary-hkgn7\nNamespace:        kubectl-5503\nPriority:         0\nService Account:  default\nNode:             ip-172-31-13-117.us-east-2.compute.internal/172.31.13.117\nStart Time:       Wed, 04 Jan 2023 22:59:58 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 41bb7412a2f9ccc22ec9097d71a4dad3fefacff9fefc9dd7b7d2e268b8f621d2\n                  cni.projectcalico.org/podIP: 10.42.3.250/32\n                  cni.projectcalico.org/podIPs: 10.42.3.250/32\nStatus:           Running\nIP:               10.42.3.250\nIPs:\n  IP:           10.42.3.250\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://7a96378b93d9ba52e22740a76db356e5ace62b98f13044fc512e03716ec7a1af\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 04 Jan 2023 22:59:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vbwgz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-vbwgz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5503/agnhost-primary-hkgn7 to ip-172-31-13-117.us-east-2.compute.internal\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan  4 23:00:00.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe rc agnhost-primary'
Jan  4 23:00:00.696: INFO: stderr: ""
Jan  4 23:00:00.696: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5503\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-hkgn7\n"
Jan  4 23:00:00.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe service agnhost-primary'
Jan  4 23:00:00.792: INFO: stderr: ""
Jan  4 23:00:00.792: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5503\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.43.52.180\nIPs:               10.43.52.180\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.3.250:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan  4 23:00:00.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe node ip-172-31-11-54.us-east-2.compute.internal'
Jan  4 23:00:01.392: INFO: stderr: ""
Jan  4 23:00:01.392: INFO: stdout: "Name:               ip-172-31-11-54.us-east-2.compute.internal\nRoles:              control-plane,etcd,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=rke2\n                    beta.kubernetes.io/os=linux\n                    egress.rke2.io/cluster=true\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-11-54.us-east-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/master=true\n                    node.kubernetes.io/instance-type=rke2\nAnnotations:        etcd.rke2.cattle.io/node-address: 172.31.11.54\n                    etcd.rke2.cattle.io/node-name: ip-172-31-11-54.us-east-2.compute.internal-5bf70c88\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"46:b9:c5:c9:36:ad\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.11.54\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.11.54/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.42.0.1\n                    rke2.io/encryption-config-hash: start-aa8081c15d2f4171450722e9a53734238147315b24d9833f2836ce63679b3dd7\n                    rke2.io/external-ip: 18.217.16.178\n                    rke2.io/hostname: ip-172-31-11-54.us-east-2.compute.internal\n                    rke2.io/internal-ip: 172.31.11.54\n                    rke2.io/node-args:\n                      [\"server\",\"--write-kubeconfig-mode\",\"0644\",\"--tls-san\",\"fake.fqdn.value\",\"--node-name\",\"ip-172-31-11-54.us-east-2.compute.internal\",\"--nod...\n                    rke2.io/node-config-hash: GRY46QOYYS52FSFSUDYKGAGT6LU3WO53BVEF56ZQTFUIOGVKXDVA====\n                    rke2.io/node-env: {}\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 04 Jan 2023 20:05:38 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-11-54.us-east-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 04 Jan 2023 23:00:00 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 04 Jan 2023 20:06:21 +0000   Wed, 04 Jan 2023 20:06:21 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 04 Jan 2023 22:59:16 +0000   Wed, 04 Jan 2023 20:05:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 04 Jan 2023 22:59:16 +0000   Wed, 04 Jan 2023 20:05:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 04 Jan 2023 22:59:16 +0000   Wed, 04 Jan 2023 20:05:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 04 Jan 2023 22:59:16 +0000   Wed, 04 Jan 2023 20:06:13 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.11.54\n  ExternalIP:  18.217.16.178\n  Hostname:    ip-172-31-11-54.us-east-2.compute.internal\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      20937708Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3857Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    2\n  ephemeral-storage:      20368202327\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3857Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 ec21b89af69303a68bc5b14142e340b5\n  System UUID:                ec21b89a-f693-03a6-8bc5-b14142e340b5\n  Boot ID:                    b8aa956d-796b-4c72-a2a1-5c22c0a69e96\n  Kernel Version:             5.14.21-150400.22-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12-k3s1\n  Kubelet Version:            v1.26.0+rke2r1\n  Kube-Proxy Version:         v1.26.0+rke2r1\nPodCIDR:                      10.42.0.0/24\nPodCIDRs:                     10.42.0.0/24\nProviderID:                   rke2://ip-172-31-11-54.us-east-2.compute.internal\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal    100m (5%)     0 (0%)      128Mi (3%)       0 (0%)         174m\n  kube-system                 etcd-ip-172-31-11-54.us-east-2.compute.internal                        200m (10%)    0 (0%)      512Mi (13%)      0 (0%)         173m\n  kube-system                 kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal              250m (12%)    0 (0%)      1Gi (26%)        0 (0%)         174m\n  kube-system                 kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal     200m (10%)    0 (0%)      256Mi (6%)       0 (0%)         174m\n  kube-system                 kube-proxy-ip-172-31-11-54.us-east-2.compute.internal                  250m (12%)    0 (0%)      128Mi (3%)       0 (0%)         174m\n  kube-system                 kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal              100m (5%)     0 (0%)      128Mi (3%)       0 (0%)         174m\n  kube-system                 rke2-canal-ggwd4                                                       250m (12%)    0 (0%)      0 (0%)           0 (0%)         174m\n  kube-system                 rke2-coredns-rke2-coredns-854779488f-mwkvw                             100m (5%)     100m (5%)   128Mi (3%)       128Mi (3%)     174m\n  kube-system                 rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq                  25m (1%)      100m (5%)   16Mi (0%)        64Mi (1%)      174m\n  kube-system                 rke2-ingress-nginx-controller-97km7                                    100m (5%)     0 (0%)      90Mi (2%)        0 (0%)         173m\n  kube-system                 rke2-metrics-server-778467dc76-4rtdk                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         173m\n  pod-network-test-7221       netserver-0                                                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         30s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2                0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests      Limits\n  --------               --------      ------\n  cpu                    1575m (78%)   200m (10%)\n  memory                 2410Mi (62%)  192Mi (4%)\n  ephemeral-storage      0 (0%)        0 (0%)\n  hugepages-1Gi          0 (0%)        0 (0%)\n  hugepages-2Mi          0 (0%)        0 (0%)\n  scheduling.k8s.io/foo  0             0\nEvents:                  <none>\n"
Jan  4 23:00:01.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe namespace kubectl-5503'
Jan  4 23:00:01.558: INFO: stderr: ""
Jan  4 23:00:01.558: INFO: stdout: "Name:         kubectl-5503\nLabels:       e2e-framework=kubectl\n              e2e-run=fcc81411-4ecc-4b5e-a2b9-1b71a19b6b05\n              kubernetes.io/metadata.name=kubectl-5503\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:01.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5503" for this suite. 01/04/23 23:00:01.568
------------------------------
• [3.586 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 22:59:57.997
    Jan  4 22:59:57.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 22:59:57.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 22:59:58.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 22:59:58.039
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan  4 22:59:58.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 create -f -'
    Jan  4 22:59:58.238: INFO: stderr: ""
    Jan  4 22:59:58.238: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan  4 22:59:58.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 create -f -'
    Jan  4 22:59:58.494: INFO: stderr: ""
    Jan  4 22:59:58.494: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/04/23 22:59:58.494
    Jan  4 22:59:59.497: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 22:59:59.497: INFO: Found 0 / 1
    Jan  4 23:00:00.497: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 23:00:00.497: INFO: Found 1 / 1
    Jan  4 23:00:00.497: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  4 23:00:00.500: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 23:00:00.500: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  4 23:00:00.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe pod agnhost-primary-hkgn7'
    Jan  4 23:00:00.583: INFO: stderr: ""
    Jan  4 23:00:00.583: INFO: stdout: "Name:             agnhost-primary-hkgn7\nNamespace:        kubectl-5503\nPriority:         0\nService Account:  default\nNode:             ip-172-31-13-117.us-east-2.compute.internal/172.31.13.117\nStart Time:       Wed, 04 Jan 2023 22:59:58 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 41bb7412a2f9ccc22ec9097d71a4dad3fefacff9fefc9dd7b7d2e268b8f621d2\n                  cni.projectcalico.org/podIP: 10.42.3.250/32\n                  cni.projectcalico.org/podIPs: 10.42.3.250/32\nStatus:           Running\nIP:               10.42.3.250\nIPs:\n  IP:           10.42.3.250\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://7a96378b93d9ba52e22740a76db356e5ace62b98f13044fc512e03716ec7a1af\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 04 Jan 2023 22:59:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vbwgz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-vbwgz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5503/agnhost-primary-hkgn7 to ip-172-31-13-117.us-east-2.compute.internal\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan  4 23:00:00.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe rc agnhost-primary'
    Jan  4 23:00:00.696: INFO: stderr: ""
    Jan  4 23:00:00.696: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5503\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-hkgn7\n"
    Jan  4 23:00:00.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe service agnhost-primary'
    Jan  4 23:00:00.792: INFO: stderr: ""
    Jan  4 23:00:00.792: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5503\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.43.52.180\nIPs:               10.43.52.180\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.3.250:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan  4 23:00:00.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe node ip-172-31-11-54.us-east-2.compute.internal'
    Jan  4 23:00:01.392: INFO: stderr: ""
    Jan  4 23:00:01.392: INFO: stdout: "Name:               ip-172-31-11-54.us-east-2.compute.internal\nRoles:              control-plane,etcd,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=rke2\n                    beta.kubernetes.io/os=linux\n                    egress.rke2.io/cluster=true\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-11-54.us-east-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/master=true\n                    node.kubernetes.io/instance-type=rke2\nAnnotations:        etcd.rke2.cattle.io/node-address: 172.31.11.54\n                    etcd.rke2.cattle.io/node-name: ip-172-31-11-54.us-east-2.compute.internal-5bf70c88\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"46:b9:c5:c9:36:ad\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.11.54\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.11.54/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.42.0.1\n                    rke2.io/encryption-config-hash: start-aa8081c15d2f4171450722e9a53734238147315b24d9833f2836ce63679b3dd7\n                    rke2.io/external-ip: 18.217.16.178\n                    rke2.io/hostname: ip-172-31-11-54.us-east-2.compute.internal\n                    rke2.io/internal-ip: 172.31.11.54\n                    rke2.io/node-args:\n                      [\"server\",\"--write-kubeconfig-mode\",\"0644\",\"--tls-san\",\"fake.fqdn.value\",\"--node-name\",\"ip-172-31-11-54.us-east-2.compute.internal\",\"--nod...\n                    rke2.io/node-config-hash: GRY46QOYYS52FSFSUDYKGAGT6LU3WO53BVEF56ZQTFUIOGVKXDVA====\n                    rke2.io/node-env: {}\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 04 Jan 2023 20:05:38 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-11-54.us-east-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 04 Jan 2023 23:00:00 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 04 Jan 2023 20:06:21 +0000   Wed, 04 Jan 2023 20:06:21 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 04 Jan 2023 22:59:16 +0000   Wed, 04 Jan 2023 20:05:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 04 Jan 2023 22:59:16 +0000   Wed, 04 Jan 2023 20:05:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 04 Jan 2023 22:59:16 +0000   Wed, 04 Jan 2023 20:05:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 04 Jan 2023 22:59:16 +0000   Wed, 04 Jan 2023 20:06:13 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.11.54\n  ExternalIP:  18.217.16.178\n  Hostname:    ip-172-31-11-54.us-east-2.compute.internal\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      20937708Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3857Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    2\n  ephemeral-storage:      20368202327\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3857Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 ec21b89af69303a68bc5b14142e340b5\n  System UUID:                ec21b89a-f693-03a6-8bc5-b14142e340b5\n  Boot ID:                    b8aa956d-796b-4c72-a2a1-5c22c0a69e96\n  Kernel Version:             5.14.21-150400.22-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12-k3s1\n  Kubelet Version:            v1.26.0+rke2r1\n  Kube-Proxy Version:         v1.26.0+rke2r1\nPodCIDR:                      10.42.0.0/24\nPodCIDRs:                     10.42.0.0/24\nProviderID:                   rke2://ip-172-31-11-54.us-east-2.compute.internal\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal    100m (5%)     0 (0%)      128Mi (3%)       0 (0%)         174m\n  kube-system                 etcd-ip-172-31-11-54.us-east-2.compute.internal                        200m (10%)    0 (0%)      512Mi (13%)      0 (0%)         173m\n  kube-system                 kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal              250m (12%)    0 (0%)      1Gi (26%)        0 (0%)         174m\n  kube-system                 kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal     200m (10%)    0 (0%)      256Mi (6%)       0 (0%)         174m\n  kube-system                 kube-proxy-ip-172-31-11-54.us-east-2.compute.internal                  250m (12%)    0 (0%)      128Mi (3%)       0 (0%)         174m\n  kube-system                 kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal              100m (5%)     0 (0%)      128Mi (3%)       0 (0%)         174m\n  kube-system                 rke2-canal-ggwd4                                                       250m (12%)    0 (0%)      0 (0%)           0 (0%)         174m\n  kube-system                 rke2-coredns-rke2-coredns-854779488f-mwkvw                             100m (5%)     100m (5%)   128Mi (3%)       128Mi (3%)     174m\n  kube-system                 rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq                  25m (1%)      100m (5%)   16Mi (0%)        64Mi (1%)      174m\n  kube-system                 rke2-ingress-nginx-controller-97km7                                    100m (5%)     0 (0%)      90Mi (2%)        0 (0%)         173m\n  kube-system                 rke2-metrics-server-778467dc76-4rtdk                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         173m\n  pod-network-test-7221       netserver-0                                                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         30s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2                0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests      Limits\n  --------               --------      ------\n  cpu                    1575m (78%)   200m (10%)\n  memory                 2410Mi (62%)  192Mi (4%)\n  ephemeral-storage      0 (0%)        0 (0%)\n  hugepages-1Gi          0 (0%)        0 (0%)\n  hugepages-2Mi          0 (0%)        0 (0%)\n  scheduling.k8s.io/foo  0             0\nEvents:                  <none>\n"
    Jan  4 23:00:01.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-5503 describe namespace kubectl-5503'
    Jan  4 23:00:01.558: INFO: stderr: ""
    Jan  4 23:00:01.558: INFO: stdout: "Name:         kubectl-5503\nLabels:       e2e-framework=kubectl\n              e2e-run=fcc81411-4ecc-4b5e-a2b9-1b71a19b6b05\n              kubernetes.io/metadata.name=kubectl-5503\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:01.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5503" for this suite. 01/04/23 23:00:01.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:01.584
Jan  4 23:00:01.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename daemonsets 01/04/23 23:00:01.585
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:01.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:01.62
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan  4 23:00:01.693: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/04/23 23:00:01.701
Jan  4 23:00:01.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:01.706: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/04/23 23:00:01.706
Jan  4 23:00:01.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:01.846: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:00:02.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:02.850: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:00:03.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  4 23:00:03.856: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/04/23 23:00:03.861
Jan  4 23:00:03.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:03.896: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/04/23 23:00:03.896
Jan  4 23:00:03.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:03.917: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:00:04.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:04.920: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:00:05.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:05.922: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:00:06.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:06.994: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:00:07.921: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  4 23:00:07.921: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/04/23 23:00:07.928
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7118, will wait for the garbage collector to delete the pods 01/04/23 23:00:07.928
Jan  4 23:00:07.988: INFO: Deleting DaemonSet.extensions daemon-set took: 6.608671ms
Jan  4 23:00:08.089: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.606223ms
Jan  4 23:00:10.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:00:10.094: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  4 23:00:10.097: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52523"},"items":null}

Jan  4 23:00:10.101: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52523"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:10.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7118" for this suite. 01/04/23 23:00:10.142
------------------------------
• [SLOW TEST] [8.573 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:01.584
    Jan  4 23:00:01.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename daemonsets 01/04/23 23:00:01.585
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:01.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:01.62
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan  4 23:00:01.693: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/04/23 23:00:01.701
    Jan  4 23:00:01.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:01.706: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/04/23 23:00:01.706
    Jan  4 23:00:01.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:01.846: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:00:02.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:02.850: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:00:03.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  4 23:00:03.856: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/04/23 23:00:03.861
    Jan  4 23:00:03.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:03.896: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/04/23 23:00:03.896
    Jan  4 23:00:03.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:03.917: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:00:04.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:04.920: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:00:05.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:05.922: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:00:06.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:06.994: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:00:07.921: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  4 23:00:07.921: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/04/23 23:00:07.928
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7118, will wait for the garbage collector to delete the pods 01/04/23 23:00:07.928
    Jan  4 23:00:07.988: INFO: Deleting DaemonSet.extensions daemon-set took: 6.608671ms
    Jan  4 23:00:08.089: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.606223ms
    Jan  4 23:00:10.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:00:10.094: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  4 23:00:10.097: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52523"},"items":null}

    Jan  4 23:00:10.101: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52523"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:10.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7118" for this suite. 01/04/23 23:00:10.142
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:10.158
Jan  4 23:00:10.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:00:10.158
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:10.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:10.178
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/04/23 23:00:10.18
Jan  4 23:00:10.189: INFO: Waiting up to 5m0s for pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef" in namespace "projected-4452" to be "running and ready"
Jan  4 23:00:10.192: INFO: Pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915054ms
Jan  4 23:00:10.192: INFO: The phase of Pod annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:00:12.200: INFO: Pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef": Phase="Running", Reason="", readiness=true. Elapsed: 2.010737668s
Jan  4 23:00:12.200: INFO: The phase of Pod annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef is Running (Ready = true)
Jan  4 23:00:12.200: INFO: Pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef" satisfied condition "running and ready"
Jan  4 23:00:12.733: INFO: Successfully updated pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:14.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4452" for this suite. 01/04/23 23:00:14.755
------------------------------
• [4.604 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:10.158
    Jan  4 23:00:10.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:00:10.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:10.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:10.178
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/04/23 23:00:10.18
    Jan  4 23:00:10.189: INFO: Waiting up to 5m0s for pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef" in namespace "projected-4452" to be "running and ready"
    Jan  4 23:00:10.192: INFO: Pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915054ms
    Jan  4 23:00:10.192: INFO: The phase of Pod annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:00:12.200: INFO: Pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef": Phase="Running", Reason="", readiness=true. Elapsed: 2.010737668s
    Jan  4 23:00:12.200: INFO: The phase of Pod annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef is Running (Ready = true)
    Jan  4 23:00:12.200: INFO: Pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef" satisfied condition "running and ready"
    Jan  4 23:00:12.733: INFO: Successfully updated pod "annotationupdate59f232d0-fd18-49f3-8991-2771cd0b3cef"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:14.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4452" for this suite. 01/04/23 23:00:14.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:14.766
Jan  4 23:00:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename discovery 01/04/23 23:00:14.767
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:14.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:14.785
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/04/23 23:00:14.789
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan  4 23:00:15.227: INFO: Checking APIGroup: apiregistration.k8s.io
Jan  4 23:00:15.229: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan  4 23:00:15.229: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan  4 23:00:15.229: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan  4 23:00:15.229: INFO: Checking APIGroup: apps
Jan  4 23:00:15.230: INFO: PreferredVersion.GroupVersion: apps/v1
Jan  4 23:00:15.230: INFO: Versions found [{apps/v1 v1}]
Jan  4 23:00:15.230: INFO: apps/v1 matches apps/v1
Jan  4 23:00:15.230: INFO: Checking APIGroup: events.k8s.io
Jan  4 23:00:15.231: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan  4 23:00:15.231: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan  4 23:00:15.231: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan  4 23:00:15.231: INFO: Checking APIGroup: authentication.k8s.io
Jan  4 23:00:15.232: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan  4 23:00:15.232: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan  4 23:00:15.232: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan  4 23:00:15.232: INFO: Checking APIGroup: authorization.k8s.io
Jan  4 23:00:15.234: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan  4 23:00:15.234: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan  4 23:00:15.234: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan  4 23:00:15.234: INFO: Checking APIGroup: autoscaling
Jan  4 23:00:15.237: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan  4 23:00:15.237: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan  4 23:00:15.237: INFO: autoscaling/v2 matches autoscaling/v2
Jan  4 23:00:15.238: INFO: Checking APIGroup: batch
Jan  4 23:00:15.239: INFO: PreferredVersion.GroupVersion: batch/v1
Jan  4 23:00:15.239: INFO: Versions found [{batch/v1 v1}]
Jan  4 23:00:15.239: INFO: batch/v1 matches batch/v1
Jan  4 23:00:15.239: INFO: Checking APIGroup: certificates.k8s.io
Jan  4 23:00:15.240: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan  4 23:00:15.240: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan  4 23:00:15.240: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan  4 23:00:15.240: INFO: Checking APIGroup: networking.k8s.io
Jan  4 23:00:15.241: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan  4 23:00:15.241: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan  4 23:00:15.241: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan  4 23:00:15.241: INFO: Checking APIGroup: policy
Jan  4 23:00:15.242: INFO: PreferredVersion.GroupVersion: policy/v1
Jan  4 23:00:15.242: INFO: Versions found [{policy/v1 v1}]
Jan  4 23:00:15.242: INFO: policy/v1 matches policy/v1
Jan  4 23:00:15.242: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan  4 23:00:15.243: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan  4 23:00:15.243: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan  4 23:00:15.243: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan  4 23:00:15.243: INFO: Checking APIGroup: storage.k8s.io
Jan  4 23:00:15.244: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan  4 23:00:15.244: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan  4 23:00:15.244: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan  4 23:00:15.244: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan  4 23:00:15.245: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan  4 23:00:15.245: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan  4 23:00:15.245: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan  4 23:00:15.245: INFO: Checking APIGroup: apiextensions.k8s.io
Jan  4 23:00:15.246: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan  4 23:00:15.246: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan  4 23:00:15.246: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan  4 23:00:15.246: INFO: Checking APIGroup: scheduling.k8s.io
Jan  4 23:00:15.247: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan  4 23:00:15.247: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan  4 23:00:15.247: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan  4 23:00:15.247: INFO: Checking APIGroup: coordination.k8s.io
Jan  4 23:00:15.248: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan  4 23:00:15.248: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan  4 23:00:15.248: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan  4 23:00:15.248: INFO: Checking APIGroup: node.k8s.io
Jan  4 23:00:15.250: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan  4 23:00:15.250: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan  4 23:00:15.250: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan  4 23:00:15.250: INFO: Checking APIGroup: discovery.k8s.io
Jan  4 23:00:15.251: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan  4 23:00:15.251: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan  4 23:00:15.251: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan  4 23:00:15.251: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan  4 23:00:15.252: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan  4 23:00:15.252: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan  4 23:00:15.252: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan  4 23:00:15.252: INFO: Checking APIGroup: crd.projectcalico.org
Jan  4 23:00:15.252: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan  4 23:00:15.252: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan  4 23:00:15.252: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan  4 23:00:15.252: INFO: Checking APIGroup: helm.cattle.io
Jan  4 23:00:15.253: INFO: PreferredVersion.GroupVersion: helm.cattle.io/v1
Jan  4 23:00:15.253: INFO: Versions found [{helm.cattle.io/v1 v1}]
Jan  4 23:00:15.253: INFO: helm.cattle.io/v1 matches helm.cattle.io/v1
Jan  4 23:00:15.253: INFO: Checking APIGroup: k3s.cattle.io
Jan  4 23:00:15.254: INFO: PreferredVersion.GroupVersion: k3s.cattle.io/v1
Jan  4 23:00:15.254: INFO: Versions found [{k3s.cattle.io/v1 v1}]
Jan  4 23:00:15.254: INFO: k3s.cattle.io/v1 matches k3s.cattle.io/v1
Jan  4 23:00:15.254: INFO: Checking APIGroup: metrics.k8s.io
Jan  4 23:00:15.255: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan  4 23:00:15.255: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan  4 23:00:15.255: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:15.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-6853" for this suite. 01/04/23 23:00:15.26
------------------------------
• [0.502 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:14.766
    Jan  4 23:00:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename discovery 01/04/23 23:00:14.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:14.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:14.785
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/04/23 23:00:14.789
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan  4 23:00:15.227: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan  4 23:00:15.229: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan  4 23:00:15.229: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan  4 23:00:15.229: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan  4 23:00:15.229: INFO: Checking APIGroup: apps
    Jan  4 23:00:15.230: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan  4 23:00:15.230: INFO: Versions found [{apps/v1 v1}]
    Jan  4 23:00:15.230: INFO: apps/v1 matches apps/v1
    Jan  4 23:00:15.230: INFO: Checking APIGroup: events.k8s.io
    Jan  4 23:00:15.231: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan  4 23:00:15.231: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan  4 23:00:15.231: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan  4 23:00:15.231: INFO: Checking APIGroup: authentication.k8s.io
    Jan  4 23:00:15.232: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan  4 23:00:15.232: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan  4 23:00:15.232: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan  4 23:00:15.232: INFO: Checking APIGroup: authorization.k8s.io
    Jan  4 23:00:15.234: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan  4 23:00:15.234: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan  4 23:00:15.234: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan  4 23:00:15.234: INFO: Checking APIGroup: autoscaling
    Jan  4 23:00:15.237: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan  4 23:00:15.237: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan  4 23:00:15.237: INFO: autoscaling/v2 matches autoscaling/v2
    Jan  4 23:00:15.238: INFO: Checking APIGroup: batch
    Jan  4 23:00:15.239: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan  4 23:00:15.239: INFO: Versions found [{batch/v1 v1}]
    Jan  4 23:00:15.239: INFO: batch/v1 matches batch/v1
    Jan  4 23:00:15.239: INFO: Checking APIGroup: certificates.k8s.io
    Jan  4 23:00:15.240: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan  4 23:00:15.240: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan  4 23:00:15.240: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan  4 23:00:15.240: INFO: Checking APIGroup: networking.k8s.io
    Jan  4 23:00:15.241: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan  4 23:00:15.241: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan  4 23:00:15.241: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan  4 23:00:15.241: INFO: Checking APIGroup: policy
    Jan  4 23:00:15.242: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan  4 23:00:15.242: INFO: Versions found [{policy/v1 v1}]
    Jan  4 23:00:15.242: INFO: policy/v1 matches policy/v1
    Jan  4 23:00:15.242: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan  4 23:00:15.243: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan  4 23:00:15.243: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan  4 23:00:15.243: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan  4 23:00:15.243: INFO: Checking APIGroup: storage.k8s.io
    Jan  4 23:00:15.244: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan  4 23:00:15.244: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan  4 23:00:15.244: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan  4 23:00:15.244: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan  4 23:00:15.245: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan  4 23:00:15.245: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan  4 23:00:15.245: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan  4 23:00:15.245: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan  4 23:00:15.246: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan  4 23:00:15.246: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan  4 23:00:15.246: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan  4 23:00:15.246: INFO: Checking APIGroup: scheduling.k8s.io
    Jan  4 23:00:15.247: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan  4 23:00:15.247: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan  4 23:00:15.247: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan  4 23:00:15.247: INFO: Checking APIGroup: coordination.k8s.io
    Jan  4 23:00:15.248: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan  4 23:00:15.248: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan  4 23:00:15.248: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan  4 23:00:15.248: INFO: Checking APIGroup: node.k8s.io
    Jan  4 23:00:15.250: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan  4 23:00:15.250: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan  4 23:00:15.250: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan  4 23:00:15.250: INFO: Checking APIGroup: discovery.k8s.io
    Jan  4 23:00:15.251: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan  4 23:00:15.251: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan  4 23:00:15.251: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan  4 23:00:15.251: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan  4 23:00:15.252: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan  4 23:00:15.252: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan  4 23:00:15.252: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan  4 23:00:15.252: INFO: Checking APIGroup: crd.projectcalico.org
    Jan  4 23:00:15.252: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan  4 23:00:15.252: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan  4 23:00:15.252: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan  4 23:00:15.252: INFO: Checking APIGroup: helm.cattle.io
    Jan  4 23:00:15.253: INFO: PreferredVersion.GroupVersion: helm.cattle.io/v1
    Jan  4 23:00:15.253: INFO: Versions found [{helm.cattle.io/v1 v1}]
    Jan  4 23:00:15.253: INFO: helm.cattle.io/v1 matches helm.cattle.io/v1
    Jan  4 23:00:15.253: INFO: Checking APIGroup: k3s.cattle.io
    Jan  4 23:00:15.254: INFO: PreferredVersion.GroupVersion: k3s.cattle.io/v1
    Jan  4 23:00:15.254: INFO: Versions found [{k3s.cattle.io/v1 v1}]
    Jan  4 23:00:15.254: INFO: k3s.cattle.io/v1 matches k3s.cattle.io/v1
    Jan  4 23:00:15.254: INFO: Checking APIGroup: metrics.k8s.io
    Jan  4 23:00:15.255: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan  4 23:00:15.255: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan  4 23:00:15.255: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:15.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-6853" for this suite. 01/04/23 23:00:15.26
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:15.274
Jan  4 23:00:15.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename dns 01/04/23 23:00:15.279
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:15.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:15.315
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/04/23 23:00:15.317
Jan  4 23:00:15.329: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6349  4b90c26a-9416-4e67-9931-9eabc60cb525 52583 0 2023-01-04 23:00:15 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:15 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h82h6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h82h6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:00:15.330: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6349" to be "running and ready"
Jan  4 23:00:15.334: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.680033ms
Jan  4 23:00:15.334: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:00:17.339: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009824018s
Jan  4 23:00:17.340: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan  4 23:00:17.340: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/04/23 23:00:17.34
Jan  4 23:00:17.340: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6349 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:00:17.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:00:17.340: INFO: ExecWithOptions: Clientset creation
Jan  4 23:00:17.340: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-6349/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/04/23 23:00:17.421
Jan  4 23:00:17.422: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6349 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:00:17.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:00:17.423: INFO: ExecWithOptions: Clientset creation
Jan  4 23:00:17.423: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-6349/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  4 23:00:17.557: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:17.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6349" for this suite. 01/04/23 23:00:17.582
------------------------------
• [2.315 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:15.274
    Jan  4 23:00:15.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename dns 01/04/23 23:00:15.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:15.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:15.315
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/04/23 23:00:15.317
    Jan  4 23:00:15.329: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6349  4b90c26a-9416-4e67-9931-9eabc60cb525 52583 0 2023-01-04 23:00:15 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:15 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h82h6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h82h6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:00:15.330: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6349" to be "running and ready"
    Jan  4 23:00:15.334: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.680033ms
    Jan  4 23:00:15.334: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:00:17.339: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009824018s
    Jan  4 23:00:17.340: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan  4 23:00:17.340: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/04/23 23:00:17.34
    Jan  4 23:00:17.340: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6349 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:00:17.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:00:17.340: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:00:17.340: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-6349/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/04/23 23:00:17.421
    Jan  4 23:00:17.422: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6349 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:00:17.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:00:17.423: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:00:17.423: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-6349/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  4 23:00:17.557: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:17.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6349" for this suite. 01/04/23 23:00:17.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:17.597
Jan  4 23:00:17.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-runtime 01/04/23 23:00:17.598
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:17.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:17.619
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/04/23 23:00:17.623
STEP: wait for the container to reach Succeeded 01/04/23 23:00:17.631
STEP: get the container status 01/04/23 23:00:21.873
STEP: the container should be terminated 01/04/23 23:00:21.876
STEP: the termination message should be set 01/04/23 23:00:21.876
Jan  4 23:00:21.876: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/04/23 23:00:21.876
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:21.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7740" for this suite. 01/04/23 23:00:21.903
------------------------------
• [4.313 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:17.597
    Jan  4 23:00:17.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-runtime 01/04/23 23:00:17.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:17.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:17.619
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/04/23 23:00:17.623
    STEP: wait for the container to reach Succeeded 01/04/23 23:00:17.631
    STEP: get the container status 01/04/23 23:00:21.873
    STEP: the container should be terminated 01/04/23 23:00:21.876
    STEP: the termination message should be set 01/04/23 23:00:21.876
    Jan  4 23:00:21.876: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/04/23 23:00:21.876
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:21.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7740" for this suite. 01/04/23 23:00:21.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:21.911
Jan  4 23:00:21.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 23:00:21.912
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:22.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:22.556
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 23:00:22.569
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:00:22.994
STEP: Deploying the webhook pod 01/04/23 23:00:23.005
STEP: Wait for the deployment to be ready 01/04/23 23:00:23.017
Jan  4 23:00:23.023: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 23:00:25.035
STEP: Verifying the service has paired with the endpoint 01/04/23 23:00:25.049
Jan  4 23:00:26.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan  4 23:00:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-636-crds.webhook.example.com via the AdmissionRegistration API 01/04/23 23:00:26.568
STEP: Creating a custom resource that should be mutated by the webhook 01/04/23 23:00:26.583
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:29.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7520" for this suite. 01/04/23 23:00:29.25
STEP: Destroying namespace "webhook-7520-markers" for this suite. 01/04/23 23:00:29.27
------------------------------
• [SLOW TEST] [7.385 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:21.911
    Jan  4 23:00:21.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 23:00:21.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:22.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:22.556
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 23:00:22.569
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:00:22.994
    STEP: Deploying the webhook pod 01/04/23 23:00:23.005
    STEP: Wait for the deployment to be ready 01/04/23 23:00:23.017
    Jan  4 23:00:23.023: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 23:00:25.035
    STEP: Verifying the service has paired with the endpoint 01/04/23 23:00:25.049
    Jan  4 23:00:26.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan  4 23:00:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-636-crds.webhook.example.com via the AdmissionRegistration API 01/04/23 23:00:26.568
    STEP: Creating a custom resource that should be mutated by the webhook 01/04/23 23:00:26.583
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:29.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7520" for this suite. 01/04/23 23:00:29.25
    STEP: Destroying namespace "webhook-7520-markers" for this suite. 01/04/23 23:00:29.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:29.298
Jan  4 23:00:29.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename init-container 01/04/23 23:00:29.299
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:29.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:29.349
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/04/23 23:00:29.373
Jan  4 23:00:29.373: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:33.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9768" for this suite. 01/04/23 23:00:33.127
------------------------------
• [3.836 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:29.298
    Jan  4 23:00:29.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename init-container 01/04/23 23:00:29.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:29.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:29.349
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/04/23 23:00:29.373
    Jan  4 23:00:29.373: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:33.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9768" for this suite. 01/04/23 23:00:33.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:33.135
Jan  4 23:00:33.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sysctl 01/04/23 23:00:33.136
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:33.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:33.153
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/04/23 23:00:33.156
STEP: Watching for error events or started pod 01/04/23 23:00:33.164
STEP: Waiting for pod completion 01/04/23 23:00:35.168
Jan  4 23:00:35.169: INFO: Waiting up to 3m0s for pod "sysctl-7e87fdf6-c175-4328-99b4-be32a5eafdf9" in namespace "sysctl-3851" to be "completed"
Jan  4 23:00:35.172: INFO: Pod "sysctl-7e87fdf6-c175-4328-99b4-be32a5eafdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.161175ms
Jan  4 23:00:37.176: INFO: Pod "sysctl-7e87fdf6-c175-4328-99b4-be32a5eafdf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007648997s
Jan  4 23:00:37.176: INFO: Pod "sysctl-7e87fdf6-c175-4328-99b4-be32a5eafdf9" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/04/23 23:00:37.179
STEP: Getting logs from the pod 01/04/23 23:00:37.179
STEP: Checking that the sysctl is actually updated 01/04/23 23:00:37.186
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:37.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3851" for this suite. 01/04/23 23:00:37.191
------------------------------
• [4.074 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:33.135
    Jan  4 23:00:33.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sysctl 01/04/23 23:00:33.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:33.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:33.153
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/04/23 23:00:33.156
    STEP: Watching for error events or started pod 01/04/23 23:00:33.164
    STEP: Waiting for pod completion 01/04/23 23:00:35.168
    Jan  4 23:00:35.169: INFO: Waiting up to 3m0s for pod "sysctl-7e87fdf6-c175-4328-99b4-be32a5eafdf9" in namespace "sysctl-3851" to be "completed"
    Jan  4 23:00:35.172: INFO: Pod "sysctl-7e87fdf6-c175-4328-99b4-be32a5eafdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.161175ms
    Jan  4 23:00:37.176: INFO: Pod "sysctl-7e87fdf6-c175-4328-99b4-be32a5eafdf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007648997s
    Jan  4 23:00:37.176: INFO: Pod "sysctl-7e87fdf6-c175-4328-99b4-be32a5eafdf9" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/04/23 23:00:37.179
    STEP: Getting logs from the pod 01/04/23 23:00:37.179
    STEP: Checking that the sysctl is actually updated 01/04/23 23:00:37.186
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:37.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3851" for this suite. 01/04/23 23:00:37.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:37.225
Jan  4 23:00:37.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 23:00:37.226
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:37.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:37.249
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/04/23 23:00:37.251
Jan  4 23:00:37.252: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6873 proxy --unix-socket=/tmp/kubectl-proxy-unix1778656928/test'
STEP: retrieving proxy /api/ output 01/04/23 23:00:37.321
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:37.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6873" for this suite. 01/04/23 23:00:37.331
------------------------------
• [0.113 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:37.225
    Jan  4 23:00:37.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 23:00:37.226
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:37.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:37.249
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/04/23 23:00:37.251
    Jan  4 23:00:37.252: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6873 proxy --unix-socket=/tmp/kubectl-proxy-unix1778656928/test'
    STEP: retrieving proxy /api/ output 01/04/23 23:00:37.321
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:37.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6873" for this suite. 01/04/23 23:00:37.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:37.344
Jan  4 23:00:37.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 23:00:37.345
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:37.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:37.362
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/04/23 23:00:37.364
STEP: fetching the ConfigMap 01/04/23 23:00:37.368
STEP: patching the ConfigMap 01/04/23 23:00:37.37
STEP: listing all ConfigMaps in all namespaces with a label selector 01/04/23 23:00:37.375
STEP: deleting the ConfigMap by collection with a label selector 01/04/23 23:00:37.378
STEP: listing all ConfigMaps in test namespace 01/04/23 23:00:37.385
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:37.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4690" for this suite. 01/04/23 23:00:37.393
------------------------------
• [0.054 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:37.344
    Jan  4 23:00:37.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 23:00:37.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:37.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:37.362
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/04/23 23:00:37.364
    STEP: fetching the ConfigMap 01/04/23 23:00:37.368
    STEP: patching the ConfigMap 01/04/23 23:00:37.37
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/04/23 23:00:37.375
    STEP: deleting the ConfigMap by collection with a label selector 01/04/23 23:00:37.378
    STEP: listing all ConfigMaps in test namespace 01/04/23 23:00:37.385
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:37.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4690" for this suite. 01/04/23 23:00:37.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:37.404
Jan  4 23:00:37.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename watch 01/04/23 23:00:37.405
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:37.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:37.422
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/04/23 23:00:37.426
STEP: creating a new configmap 01/04/23 23:00:37.427
STEP: modifying the configmap once 01/04/23 23:00:37.432
STEP: changing the label value of the configmap 01/04/23 23:00:37.439
STEP: Expecting to observe a delete notification for the watched object 01/04/23 23:00:37.445
Jan  4 23:00:37.445: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52857 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 23:00:37.446: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52858 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 23:00:37.446: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52859 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/04/23 23:00:37.446
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/04/23 23:00:37.452
STEP: changing the label value of the configmap back 01/04/23 23:00:47.452
STEP: modifying the configmap a third time 01/04/23 23:00:47.461
STEP: deleting the configmap 01/04/23 23:00:48.076
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/04/23 23:00:48.084
Jan  4 23:00:48.084: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52937 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 23:00:48.084: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52938 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  4 23:00:48.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52939 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:48.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7077" for this suite. 01/04/23 23:00:48.088
------------------------------
• [SLOW TEST] [10.694 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:37.404
    Jan  4 23:00:37.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename watch 01/04/23 23:00:37.405
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:37.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:37.422
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/04/23 23:00:37.426
    STEP: creating a new configmap 01/04/23 23:00:37.427
    STEP: modifying the configmap once 01/04/23 23:00:37.432
    STEP: changing the label value of the configmap 01/04/23 23:00:37.439
    STEP: Expecting to observe a delete notification for the watched object 01/04/23 23:00:37.445
    Jan  4 23:00:37.445: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52857 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 23:00:37.446: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52858 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 23:00:37.446: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52859 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/04/23 23:00:37.446
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/04/23 23:00:37.452
    STEP: changing the label value of the configmap back 01/04/23 23:00:47.452
    STEP: modifying the configmap a third time 01/04/23 23:00:47.461
    STEP: deleting the configmap 01/04/23 23:00:48.076
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/04/23 23:00:48.084
    Jan  4 23:00:48.084: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52937 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 23:00:48.084: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52938 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  4 23:00:48.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7077  4a7c9444-91b9-4858-bb88-0111b54e9afa 52939 0 2023-01-04 23:00:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-04 23:00:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:48.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7077" for this suite. 01/04/23 23:00:48.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:48.101
Jan  4 23:00:48.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-pred 01/04/23 23:00:48.102
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:48.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:48.121
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  4 23:00:48.123: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  4 23:00:48.129: INFO: Waiting for terminating namespaces to be deleted...
Jan  4 23:00:48.131: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-54.us-east-2.compute.internal before test
Jan  4 23:00:48.142: INFO: cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:39 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 23:00:48.142: INFO: etcd-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:18 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container etcd ready: true, restart count 0
Jan  4 23:00:48.142: INFO: helm-install-rke2-canal-r7b4c from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container helm ready: false, restart count 0
Jan  4 23:00:48.142: INFO: helm-install-rke2-coredns-8ff46 from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container helm ready: false, restart count 0
Jan  4 23:00:48.142: INFO: helm-install-rke2-ingress-nginx-qfntk from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container helm ready: false, restart count 0
Jan  4 23:00:48.142: INFO: helm-install-rke2-metrics-server-q46jz from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container helm ready: false, restart count 0
Jan  4 23:00:48.142: INFO: kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:32 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 23:00:48.142: INFO: kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:38 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 23:00:48.142: INFO: kube-proxy-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:42 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 23:00:48.142: INFO: kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:37 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 23:00:48.142: INFO: rke2-canal-ggwd4 from kube-system started at 2023-01-04 20:05:59 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 23:00:48.142: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 23:00:48.142: INFO: rke2-coredns-rke2-coredns-854779488f-mwkvw from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container coredns ready: true, restart count 0
Jan  4 23:00:48.142: INFO: rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container autoscaler ready: true, restart count 0
Jan  4 23:00:48.142: INFO: rke2-ingress-nginx-controller-97km7 from kube-system started at 2023-01-04 20:06:54 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 23:00:48.142: INFO: rke2-metrics-server-778467dc76-4rtdk from kube-system started at 2023-01-04 20:06:31 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container metrics-server ready: true, restart count 0
Jan  4 23:00:48.142: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.142: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:00:48.143: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 23:00:48.143: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-117.us-east-2.compute.internal before test
Jan  4 23:00:48.150: INFO: kube-proxy-ip-172-31-13-117.us-east-2.compute.internal from kube-system started at 2023-01-04 20:10:23 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.150: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 23:00:48.150: INFO: rke2-canal-mprb9 from kube-system started at 2023-01-04 20:10:24 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.150: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 23:00:48.150: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 23:00:48.150: INFO: rke2-ingress-nginx-controller-8mjvf from kube-system started at 2023-01-04 20:10:52 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.150: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 23:00:48.150: INFO: sonobuoy from sonobuoy started at 2023-01-04 22:11:19 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.150: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  4 23:00:48.150: INFO: sonobuoy-e2e-job-6a70417ebe254b91 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.150: INFO: 	Container e2e ready: true, restart count 0
Jan  4 23:00:48.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:00:48.150: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:00:48.150: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 23:00:48.151: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-240.us-east-2.compute.internal before test
Jan  4 23:00:48.159: INFO: cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 23:00:48.159: INFO: etcd-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:07:58 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container etcd ready: true, restart count 0
Jan  4 23:00:48.159: INFO: kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:18 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 23:00:48.159: INFO: kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 23:00:48.159: INFO: kube-proxy-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:27 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 23:00:48.159: INFO: kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 23:00:48.159: INFO: rke2-canal-wspdm from kube-system started at 2023-01-04 20:08:32 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 23:00:48.159: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 23:00:48.159: INFO: rke2-coredns-rke2-coredns-854779488f-n8z2r from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container coredns ready: true, restart count 0
Jan  4 23:00:48.159: INFO: rke2-ingress-nginx-controller-rv4dm from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 23:00:48.159: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.159: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:00:48.159: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 23:00:48.159: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-62.us-east-2.compute.internal before test
Jan  4 23:00:48.170: INFO: cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 23:00:48.170: INFO: etcd-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:22 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container etcd ready: true, restart count 0
Jan  4 23:00:48.170: INFO: kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:36 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 23:00:48.170: INFO: kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 23:00:48.170: INFO: kube-proxy-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:47 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 23:00:48.170: INFO: kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 23:00:48.170: INFO: rke2-canal-44lz9 from kube-system started at 2023-01-04 20:08:42 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 23:00:48.170: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 23:00:48.170: INFO: rke2-ingress-nginx-controller-glxqt from kube-system started at 2023-01-04 20:08:54 +0000 UTC (1 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 23:00:48.170: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:00:48.170: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:00:48.170: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ip-172-31-11-54.us-east-2.compute.internal 01/04/23 23:00:48.197
STEP: verifying the node has the label node ip-172-31-13-117.us-east-2.compute.internal 01/04/23 23:00:48.217
STEP: verifying the node has the label node ip-172-31-3-240.us-east-2.compute.internal 01/04/23 23:00:48.241
STEP: verifying the node has the label node ip-172-31-9-62.us-east-2.compute.internal 01/04/23 23:00:48.262
Jan  4 23:00:48.290: INFO: Pod cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.290: INFO: Pod cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.290: INFO: Pod cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.290: INFO: Pod etcd-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.290: INFO: Pod etcd-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.290: INFO: Pod etcd-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.290: INFO: Pod kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.290: INFO: Pod kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-proxy-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-proxy-ip-172-31-13-117.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-13-117.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-proxy-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-proxy-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-canal-44lz9 requesting resource cpu=250m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-canal-ggwd4 requesting resource cpu=250m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-canal-mprb9 requesting resource cpu=250m on Node ip-172-31-13-117.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-canal-wspdm requesting resource cpu=250m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-coredns-rke2-coredns-854779488f-mwkvw requesting resource cpu=100m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-coredns-rke2-coredns-854779488f-n8z2r requesting resource cpu=100m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq requesting resource cpu=25m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-ingress-nginx-controller-8mjvf requesting resource cpu=100m on Node ip-172-31-13-117.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-ingress-nginx-controller-97km7 requesting resource cpu=100m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-ingress-nginx-controller-glxqt requesting resource cpu=100m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-ingress-nginx-controller-rv4dm requesting resource cpu=100m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.291: INFO: Pod rke2-metrics-server-778467dc76-4rtdk requesting resource cpu=0m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.292: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-13-117.us-east-2.compute.internal
Jan  4 23:00:48.292: INFO: Pod sonobuoy-e2e-job-6a70417ebe254b91 requesting resource cpu=0m on Node ip-172-31-13-117.us-east-2.compute.internal
Jan  4 23:00:48.292: INFO: Pod sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 requesting resource cpu=0m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.292: INFO: Pod sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj requesting resource cpu=0m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.292: INFO: Pod sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n requesting resource cpu=0m on Node ip-172-31-13-117.us-east-2.compute.internal
Jan  4 23:00:48.292: INFO: Pod sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls requesting resource cpu=0m on Node ip-172-31-9-62.us-east-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU. 01/04/23 23:00:48.292
Jan  4 23:00:48.292: INFO: Creating a pod which consumes cpu=980m on Node ip-172-31-13-117.us-east-2.compute.internal
Jan  4 23:00:48.305: INFO: Creating a pod which consumes cpu=315m on Node ip-172-31-3-240.us-east-2.compute.internal
Jan  4 23:00:48.316: INFO: Creating a pod which consumes cpu=385m on Node ip-172-31-9-62.us-east-2.compute.internal
Jan  4 23:00:48.329: INFO: Creating a pod which consumes cpu=297m on Node ip-172-31-11-54.us-east-2.compute.internal
Jan  4 23:00:48.339: INFO: Waiting up to 5m0s for pod "filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945" in namespace "sched-pred-7014" to be "running"
Jan  4 23:00:48.354: INFO: Pod "filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945": Phase="Pending", Reason="", readiness=false. Elapsed: 7.001707ms
Jan  4 23:00:50.358: INFO: Pod "filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945": Phase="Running", Reason="", readiness=true. Elapsed: 2.010863107s
Jan  4 23:00:50.358: INFO: Pod "filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945" satisfied condition "running"
Jan  4 23:00:50.358: INFO: Waiting up to 5m0s for pod "filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16" in namespace "sched-pred-7014" to be "running"
Jan  4 23:00:50.361: INFO: Pod "filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16": Phase="Running", Reason="", readiness=true. Elapsed: 3.500128ms
Jan  4 23:00:50.361: INFO: Pod "filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16" satisfied condition "running"
Jan  4 23:00:50.361: INFO: Waiting up to 5m0s for pod "filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6" in namespace "sched-pred-7014" to be "running"
Jan  4 23:00:50.364: INFO: Pod "filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6": Phase="Running", Reason="", readiness=true. Elapsed: 2.465646ms
Jan  4 23:00:50.364: INFO: Pod "filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6" satisfied condition "running"
Jan  4 23:00:50.364: INFO: Waiting up to 5m0s for pod "filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3" in namespace "sched-pred-7014" to be "running"
Jan  4 23:00:50.367: INFO: Pod "filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658792ms
Jan  4 23:00:52.371: INFO: Pod "filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007082666s
Jan  4 23:00:52.371: INFO: Pod "filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/04/23 23:00:52.371
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945.17373d5b2961df57], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7014/filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945 to ip-172-31-13-117.us-east-2.compute.internal] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945.17373d5b50a0c13a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945.17373d5b51e32d87], Reason = [Created], Message = [Created container filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945.17373d5b58858457], Reason = [Started], Message = [Started container filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6.17373d5b2ba20dbf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7014/filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6 to ip-172-31-9-62.us-east-2.compute.internal] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6.17373d5b50985d75], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6.17373d5b51d4e120], Reason = [Created], Message = [Created container filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6.17373d5b58e567b5], Reason = [Started], Message = [Started container filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3.17373d5b2c4b4916], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7014/filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3 to ip-172-31-11-54.us-east-2.compute.internal] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3.17373d5b8871796d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3.17373d5b89f53c50], Reason = [Created], Message = [Created container filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3.17373d5b942ca420], Reason = [Started], Message = [Started container filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3] 01/04/23 23:00:52.375
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16.17373d5b2baf49c4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7014/filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16 to ip-172-31-3-240.us-east-2.compute.internal] 01/04/23 23:00:52.376
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16.17373d5b501e7bcd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/04/23 23:00:52.376
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16.17373d5b5125ae6e], Reason = [Created], Message = [Created container filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16] 01/04/23 23:00:52.376
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16.17373d5b5fff4ac9], Reason = [Started], Message = [Started container filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16] 01/04/23 23:00:52.376
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17373d5c1aa46e6d], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod..] 01/04/23 23:00:52.39
STEP: removing the label node off the node ip-172-31-11-54.us-east-2.compute.internal 01/04/23 23:00:53.393
STEP: verifying the node doesn't have the label node 01/04/23 23:00:53.413
STEP: removing the label node off the node ip-172-31-13-117.us-east-2.compute.internal 01/04/23 23:00:53.425
STEP: verifying the node doesn't have the label node 01/04/23 23:00:53.446
STEP: removing the label node off the node ip-172-31-3-240.us-east-2.compute.internal 01/04/23 23:00:53.45
STEP: verifying the node doesn't have the label node 01/04/23 23:00:53.468
STEP: removing the label node off the node ip-172-31-9-62.us-east-2.compute.internal 01/04/23 23:00:53.479
STEP: verifying the node doesn't have the label node 01/04/23 23:00:53.509
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:00:53.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7014" for this suite. 01/04/23 23:00:53.522
------------------------------
• [SLOW TEST] [5.439 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:48.101
    Jan  4 23:00:48.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-pred 01/04/23 23:00:48.102
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:48.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:48.121
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  4 23:00:48.123: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  4 23:00:48.129: INFO: Waiting for terminating namespaces to be deleted...
    Jan  4 23:00:48.131: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-11-54.us-east-2.compute.internal before test
    Jan  4 23:00:48.142: INFO: cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:39 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: etcd-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:18 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: helm-install-rke2-canal-r7b4c from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container helm ready: false, restart count 0
    Jan  4 23:00:48.142: INFO: helm-install-rke2-coredns-8ff46 from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container helm ready: false, restart count 0
    Jan  4 23:00:48.142: INFO: helm-install-rke2-ingress-nginx-qfntk from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container helm ready: false, restart count 0
    Jan  4 23:00:48.142: INFO: helm-install-rke2-metrics-server-q46jz from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container helm ready: false, restart count 0
    Jan  4 23:00:48.142: INFO: kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:32 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:38 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: kube-proxy-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:42 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:37 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: rke2-canal-ggwd4 from kube-system started at 2023-01-04 20:05:59 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: rke2-coredns-rke2-coredns-854779488f-mwkvw from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container coredns ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: rke2-ingress-nginx-controller-97km7 from kube-system started at 2023-01-04 20:06:54 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: rke2-metrics-server-778467dc76-4rtdk from kube-system started at 2023-01-04 20:06:31 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  4 23:00:48.142: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.142: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:00:48.143: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 23:00:48.143: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-13-117.us-east-2.compute.internal before test
    Jan  4 23:00:48.150: INFO: kube-proxy-ip-172-31-13-117.us-east-2.compute.internal from kube-system started at 2023-01-04 20:10:23 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.150: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 23:00:48.150: INFO: rke2-canal-mprb9 from kube-system started at 2023-01-04 20:10:24 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.150: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 23:00:48.150: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 23:00:48.150: INFO: rke2-ingress-nginx-controller-8mjvf from kube-system started at 2023-01-04 20:10:52 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.150: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 23:00:48.150: INFO: sonobuoy from sonobuoy started at 2023-01-04 22:11:19 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.150: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  4 23:00:48.150: INFO: sonobuoy-e2e-job-6a70417ebe254b91 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.150: INFO: 	Container e2e ready: true, restart count 0
    Jan  4 23:00:48.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:00:48.150: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:00:48.150: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 23:00:48.151: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-3-240.us-east-2.compute.internal before test
    Jan  4 23:00:48.159: INFO: cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: etcd-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:07:58 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:18 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: kube-proxy-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:27 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: rke2-canal-wspdm from kube-system started at 2023-01-04 20:08:32 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: rke2-coredns-rke2-coredns-854779488f-n8z2r from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container coredns ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: rke2-ingress-nginx-controller-rv4dm from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.159: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 23:00:48.159: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-9-62.us-east-2.compute.internal before test
    Jan  4 23:00:48.170: INFO: cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: etcd-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:22 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:36 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: kube-proxy-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:47 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: rke2-canal-44lz9 from kube-system started at 2023-01-04 20:08:42 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: rke2-ingress-nginx-controller-glxqt from kube-system started at 2023-01-04 20:08:54 +0000 UTC (1 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:00:48.170: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:00:48.170: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ip-172-31-11-54.us-east-2.compute.internal 01/04/23 23:00:48.197
    STEP: verifying the node has the label node ip-172-31-13-117.us-east-2.compute.internal 01/04/23 23:00:48.217
    STEP: verifying the node has the label node ip-172-31-3-240.us-east-2.compute.internal 01/04/23 23:00:48.241
    STEP: verifying the node has the label node ip-172-31-9-62.us-east-2.compute.internal 01/04/23 23:00:48.262
    Jan  4 23:00:48.290: INFO: Pod cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.290: INFO: Pod cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.290: INFO: Pod cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.290: INFO: Pod etcd-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.290: INFO: Pod etcd-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.290: INFO: Pod etcd-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.290: INFO: Pod kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.290: INFO: Pod kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-proxy-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-proxy-ip-172-31-13-117.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-13-117.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-proxy-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-proxy-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-canal-44lz9 requesting resource cpu=250m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-canal-ggwd4 requesting resource cpu=250m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-canal-mprb9 requesting resource cpu=250m on Node ip-172-31-13-117.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-canal-wspdm requesting resource cpu=250m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-coredns-rke2-coredns-854779488f-mwkvw requesting resource cpu=100m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-coredns-rke2-coredns-854779488f-n8z2r requesting resource cpu=100m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq requesting resource cpu=25m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-ingress-nginx-controller-8mjvf requesting resource cpu=100m on Node ip-172-31-13-117.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-ingress-nginx-controller-97km7 requesting resource cpu=100m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-ingress-nginx-controller-glxqt requesting resource cpu=100m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-ingress-nginx-controller-rv4dm requesting resource cpu=100m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.291: INFO: Pod rke2-metrics-server-778467dc76-4rtdk requesting resource cpu=0m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.292: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-13-117.us-east-2.compute.internal
    Jan  4 23:00:48.292: INFO: Pod sonobuoy-e2e-job-6a70417ebe254b91 requesting resource cpu=0m on Node ip-172-31-13-117.us-east-2.compute.internal
    Jan  4 23:00:48.292: INFO: Pod sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 requesting resource cpu=0m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.292: INFO: Pod sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj requesting resource cpu=0m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.292: INFO: Pod sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n requesting resource cpu=0m on Node ip-172-31-13-117.us-east-2.compute.internal
    Jan  4 23:00:48.292: INFO: Pod sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls requesting resource cpu=0m on Node ip-172-31-9-62.us-east-2.compute.internal
    STEP: Starting Pods to consume most of the cluster CPU. 01/04/23 23:00:48.292
    Jan  4 23:00:48.292: INFO: Creating a pod which consumes cpu=980m on Node ip-172-31-13-117.us-east-2.compute.internal
    Jan  4 23:00:48.305: INFO: Creating a pod which consumes cpu=315m on Node ip-172-31-3-240.us-east-2.compute.internal
    Jan  4 23:00:48.316: INFO: Creating a pod which consumes cpu=385m on Node ip-172-31-9-62.us-east-2.compute.internal
    Jan  4 23:00:48.329: INFO: Creating a pod which consumes cpu=297m on Node ip-172-31-11-54.us-east-2.compute.internal
    Jan  4 23:00:48.339: INFO: Waiting up to 5m0s for pod "filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945" in namespace "sched-pred-7014" to be "running"
    Jan  4 23:00:48.354: INFO: Pod "filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945": Phase="Pending", Reason="", readiness=false. Elapsed: 7.001707ms
    Jan  4 23:00:50.358: INFO: Pod "filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945": Phase="Running", Reason="", readiness=true. Elapsed: 2.010863107s
    Jan  4 23:00:50.358: INFO: Pod "filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945" satisfied condition "running"
    Jan  4 23:00:50.358: INFO: Waiting up to 5m0s for pod "filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16" in namespace "sched-pred-7014" to be "running"
    Jan  4 23:00:50.361: INFO: Pod "filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16": Phase="Running", Reason="", readiness=true. Elapsed: 3.500128ms
    Jan  4 23:00:50.361: INFO: Pod "filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16" satisfied condition "running"
    Jan  4 23:00:50.361: INFO: Waiting up to 5m0s for pod "filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6" in namespace "sched-pred-7014" to be "running"
    Jan  4 23:00:50.364: INFO: Pod "filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6": Phase="Running", Reason="", readiness=true. Elapsed: 2.465646ms
    Jan  4 23:00:50.364: INFO: Pod "filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6" satisfied condition "running"
    Jan  4 23:00:50.364: INFO: Waiting up to 5m0s for pod "filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3" in namespace "sched-pred-7014" to be "running"
    Jan  4 23:00:50.367: INFO: Pod "filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658792ms
    Jan  4 23:00:52.371: INFO: Pod "filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007082666s
    Jan  4 23:00:52.371: INFO: Pod "filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/04/23 23:00:52.371
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945.17373d5b2961df57], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7014/filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945 to ip-172-31-13-117.us-east-2.compute.internal] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945.17373d5b50a0c13a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945.17373d5b51e32d87], Reason = [Created], Message = [Created container filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945.17373d5b58858457], Reason = [Started], Message = [Started container filler-pod-3bcfd21f-65a3-4990-ac2f-787b56379945] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6.17373d5b2ba20dbf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7014/filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6 to ip-172-31-9-62.us-east-2.compute.internal] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6.17373d5b50985d75], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6.17373d5b51d4e120], Reason = [Created], Message = [Created container filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6.17373d5b58e567b5], Reason = [Started], Message = [Started container filler-pod-87f23bca-c12c-4bf7-89f1-34a18420afb6] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3.17373d5b2c4b4916], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7014/filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3 to ip-172-31-11-54.us-east-2.compute.internal] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3.17373d5b8871796d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3.17373d5b89f53c50], Reason = [Created], Message = [Created container filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3.17373d5b942ca420], Reason = [Started], Message = [Started container filler-pod-c1c39b3d-faa9-46ec-b985-801965277db3] 01/04/23 23:00:52.375
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16.17373d5b2baf49c4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7014/filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16 to ip-172-31-3-240.us-east-2.compute.internal] 01/04/23 23:00:52.376
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16.17373d5b501e7bcd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/04/23 23:00:52.376
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16.17373d5b5125ae6e], Reason = [Created], Message = [Created container filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16] 01/04/23 23:00:52.376
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16.17373d5b5fff4ac9], Reason = [Started], Message = [Started container filler-pod-db9d4ccd-9076-4898-8c2b-082fc1c38c16] 01/04/23 23:00:52.376
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17373d5c1aa46e6d], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod..] 01/04/23 23:00:52.39
    STEP: removing the label node off the node ip-172-31-11-54.us-east-2.compute.internal 01/04/23 23:00:53.393
    STEP: verifying the node doesn't have the label node 01/04/23 23:00:53.413
    STEP: removing the label node off the node ip-172-31-13-117.us-east-2.compute.internal 01/04/23 23:00:53.425
    STEP: verifying the node doesn't have the label node 01/04/23 23:00:53.446
    STEP: removing the label node off the node ip-172-31-3-240.us-east-2.compute.internal 01/04/23 23:00:53.45
    STEP: verifying the node doesn't have the label node 01/04/23 23:00:53.468
    STEP: removing the label node off the node ip-172-31-9-62.us-east-2.compute.internal 01/04/23 23:00:53.479
    STEP: verifying the node doesn't have the label node 01/04/23 23:00:53.509
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:00:53.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7014" for this suite. 01/04/23 23:00:53.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:00:53.54
Jan  4 23:00:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 23:00:53.542
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:53.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:53.574
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-3995 01/04/23 23:00:53.578
STEP: creating replication controller nodeport-test in namespace services-3995 01/04/23 23:00:53.593
I0104 23:00:53.610898      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3995, replica count: 2
I0104 23:00:56.661757      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 23:00:56.661: INFO: Creating new exec pod
Jan  4 23:00:56.673: INFO: Waiting up to 5m0s for pod "execpodhfvkv" in namespace "services-3995" to be "running"
Jan  4 23:00:56.695: INFO: Pod "execpodhfvkv": Phase="Pending", Reason="", readiness=false. Elapsed: 21.418292ms
Jan  4 23:00:58.700: INFO: Pod "execpodhfvkv": Phase="Running", Reason="", readiness=true. Elapsed: 2.026597189s
Jan  4 23:00:58.700: INFO: Pod "execpodhfvkv" satisfied condition "running"
Jan  4 23:00:59.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-3995 exec execpodhfvkv -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan  4 23:00:59.850: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan  4 23:00:59.850: INFO: stdout: ""
Jan  4 23:00:59.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-3995 exec execpodhfvkv -- /bin/sh -x -c nc -v -z -w 2 10.43.47.177 80'
Jan  4 23:01:00.015: INFO: stderr: "+ nc -v -z -w 2 10.43.47.177 80\nConnection to 10.43.47.177 80 port [tcp/http] succeeded!\n"
Jan  4 23:01:00.015: INFO: stdout: ""
Jan  4 23:01:00.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-3995 exec execpodhfvkv -- /bin/sh -x -c nc -v -z -w 2 172.31.9.62 32264'
Jan  4 23:01:00.154: INFO: stderr: "+ nc -v -z -w 2 172.31.9.62 32264\nConnection to 172.31.9.62 32264 port [tcp/*] succeeded!\n"
Jan  4 23:01:00.154: INFO: stdout: ""
Jan  4 23:01:00.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-3995 exec execpodhfvkv -- /bin/sh -x -c nc -v -z -w 2 172.31.11.54 32264'
Jan  4 23:01:00.320: INFO: stderr: "+ nc -v -z -w 2 172.31.11.54 32264\nConnection to 172.31.11.54 32264 port [tcp/*] succeeded!\n"
Jan  4 23:01:00.320: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 23:01:00.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3995" for this suite. 01/04/23 23:01:00.325
------------------------------
• [SLOW TEST] [6.791 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:00:53.54
    Jan  4 23:00:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 23:00:53.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:00:53.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:00:53.574
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-3995 01/04/23 23:00:53.578
    STEP: creating replication controller nodeport-test in namespace services-3995 01/04/23 23:00:53.593
    I0104 23:00:53.610898      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3995, replica count: 2
    I0104 23:00:56.661757      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 23:00:56.661: INFO: Creating new exec pod
    Jan  4 23:00:56.673: INFO: Waiting up to 5m0s for pod "execpodhfvkv" in namespace "services-3995" to be "running"
    Jan  4 23:00:56.695: INFO: Pod "execpodhfvkv": Phase="Pending", Reason="", readiness=false. Elapsed: 21.418292ms
    Jan  4 23:00:58.700: INFO: Pod "execpodhfvkv": Phase="Running", Reason="", readiness=true. Elapsed: 2.026597189s
    Jan  4 23:00:58.700: INFO: Pod "execpodhfvkv" satisfied condition "running"
    Jan  4 23:00:59.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-3995 exec execpodhfvkv -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan  4 23:00:59.850: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan  4 23:00:59.850: INFO: stdout: ""
    Jan  4 23:00:59.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-3995 exec execpodhfvkv -- /bin/sh -x -c nc -v -z -w 2 10.43.47.177 80'
    Jan  4 23:01:00.015: INFO: stderr: "+ nc -v -z -w 2 10.43.47.177 80\nConnection to 10.43.47.177 80 port [tcp/http] succeeded!\n"
    Jan  4 23:01:00.015: INFO: stdout: ""
    Jan  4 23:01:00.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-3995 exec execpodhfvkv -- /bin/sh -x -c nc -v -z -w 2 172.31.9.62 32264'
    Jan  4 23:01:00.154: INFO: stderr: "+ nc -v -z -w 2 172.31.9.62 32264\nConnection to 172.31.9.62 32264 port [tcp/*] succeeded!\n"
    Jan  4 23:01:00.154: INFO: stdout: ""
    Jan  4 23:01:00.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-3995 exec execpodhfvkv -- /bin/sh -x -c nc -v -z -w 2 172.31.11.54 32264'
    Jan  4 23:01:00.320: INFO: stderr: "+ nc -v -z -w 2 172.31.11.54 32264\nConnection to 172.31.11.54 32264 port [tcp/*] succeeded!\n"
    Jan  4 23:01:00.320: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:01:00.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3995" for this suite. 01/04/23 23:01:00.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:01:00.332
Jan  4 23:01:00.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 23:01:00.333
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:00.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:00.35
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/04/23 23:01:00.352
Jan  4 23:01:00.352: INFO: namespace kubectl-6723
Jan  4 23:01:00.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6723 create -f -'
Jan  4 23:01:01.177: INFO: stderr: ""
Jan  4 23:01:01.177: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/04/23 23:01:01.177
Jan  4 23:01:02.182: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 23:01:02.182: INFO: Found 0 / 1
Jan  4 23:01:03.186: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 23:01:03.186: INFO: Found 0 / 1
Jan  4 23:01:04.182: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 23:01:04.182: INFO: Found 1 / 1
Jan  4 23:01:04.182: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  4 23:01:04.184: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  4 23:01:04.184: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  4 23:01:04.184: INFO: wait on agnhost-primary startup in kubectl-6723 
Jan  4 23:01:04.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6723 logs agnhost-primary-5rvxn agnhost-primary'
Jan  4 23:01:04.285: INFO: stderr: ""
Jan  4 23:01:04.285: INFO: stdout: "Paused\n"
STEP: exposing RC 01/04/23 23:01:04.285
Jan  4 23:01:04.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6723 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan  4 23:01:04.421: INFO: stderr: ""
Jan  4 23:01:04.422: INFO: stdout: "service/rm2 exposed\n"
Jan  4 23:01:04.428: INFO: Service rm2 in namespace kubectl-6723 found.
STEP: exposing service 01/04/23 23:01:06.435
Jan  4 23:01:06.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6723 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan  4 23:01:06.570: INFO: stderr: ""
Jan  4 23:01:06.570: INFO: stdout: "service/rm3 exposed\n"
Jan  4 23:01:06.576: INFO: Service rm3 in namespace kubectl-6723 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 23:01:08.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6723" for this suite. 01/04/23 23:01:08.586
------------------------------
• [SLOW TEST] [8.260 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:01:00.332
    Jan  4 23:01:00.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 23:01:00.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:00.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:00.35
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/04/23 23:01:00.352
    Jan  4 23:01:00.352: INFO: namespace kubectl-6723
    Jan  4 23:01:00.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6723 create -f -'
    Jan  4 23:01:01.177: INFO: stderr: ""
    Jan  4 23:01:01.177: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/04/23 23:01:01.177
    Jan  4 23:01:02.182: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 23:01:02.182: INFO: Found 0 / 1
    Jan  4 23:01:03.186: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 23:01:03.186: INFO: Found 0 / 1
    Jan  4 23:01:04.182: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 23:01:04.182: INFO: Found 1 / 1
    Jan  4 23:01:04.182: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  4 23:01:04.184: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  4 23:01:04.184: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  4 23:01:04.184: INFO: wait on agnhost-primary startup in kubectl-6723 
    Jan  4 23:01:04.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6723 logs agnhost-primary-5rvxn agnhost-primary'
    Jan  4 23:01:04.285: INFO: stderr: ""
    Jan  4 23:01:04.285: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/04/23 23:01:04.285
    Jan  4 23:01:04.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6723 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan  4 23:01:04.421: INFO: stderr: ""
    Jan  4 23:01:04.422: INFO: stdout: "service/rm2 exposed\n"
    Jan  4 23:01:04.428: INFO: Service rm2 in namespace kubectl-6723 found.
    STEP: exposing service 01/04/23 23:01:06.435
    Jan  4 23:01:06.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6723 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan  4 23:01:06.570: INFO: stderr: ""
    Jan  4 23:01:06.570: INFO: stdout: "service/rm3 exposed\n"
    Jan  4 23:01:06.576: INFO: Service rm3 in namespace kubectl-6723 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:01:08.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6723" for this suite. 01/04/23 23:01:08.586
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:01:08.593
Jan  4 23:01:08.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 23:01:08.594
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:08.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:08.611
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-80eb3f6b-4066-4021-b8cd-cc1aaa2b1ac1 01/04/23 23:01:08.613
STEP: Creating a pod to test consume configMaps 01/04/23 23:01:08.617
Jan  4 23:01:08.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d" in namespace "configmap-8244" to be "Succeeded or Failed"
Jan  4 23:01:08.626: INFO: Pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67758ms
Jan  4 23:01:10.631: INFO: Pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007526189s
Jan  4 23:01:12.631: INFO: Pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006988498s
STEP: Saw pod success 01/04/23 23:01:12.631
Jan  4 23:01:12.631: INFO: Pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d" satisfied condition "Succeeded or Failed"
Jan  4 23:01:12.635: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d container agnhost-container: <nil>
STEP: delete the pod 01/04/23 23:01:12.64
Jan  4 23:01:12.651: INFO: Waiting for pod pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d to disappear
Jan  4 23:01:12.653: INFO: Pod pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:01:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8244" for this suite. 01/04/23 23:01:12.657
------------------------------
• [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:01:08.593
    Jan  4 23:01:08.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 23:01:08.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:08.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:08.611
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-80eb3f6b-4066-4021-b8cd-cc1aaa2b1ac1 01/04/23 23:01:08.613
    STEP: Creating a pod to test consume configMaps 01/04/23 23:01:08.617
    Jan  4 23:01:08.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d" in namespace "configmap-8244" to be "Succeeded or Failed"
    Jan  4 23:01:08.626: INFO: Pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67758ms
    Jan  4 23:01:10.631: INFO: Pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007526189s
    Jan  4 23:01:12.631: INFO: Pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006988498s
    STEP: Saw pod success 01/04/23 23:01:12.631
    Jan  4 23:01:12.631: INFO: Pod "pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d" satisfied condition "Succeeded or Failed"
    Jan  4 23:01:12.635: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 23:01:12.64
    Jan  4 23:01:12.651: INFO: Waiting for pod pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d to disappear
    Jan  4 23:01:12.653: INFO: Pod pod-configmaps-da0e3836-a10d-44dc-9f0c-2c1b1f1e938d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:01:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8244" for this suite. 01/04/23 23:01:12.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:01:12.664
Jan  4 23:01:12.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:01:12.665
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:12.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:12.682
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/04/23 23:01:12.684
Jan  4 23:01:12.692: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb" in namespace "downward-api-1216" to be "Succeeded or Failed"
Jan  4 23:01:12.697: INFO: Pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.96645ms
Jan  4 23:01:14.701: INFO: Pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00880548s
Jan  4 23:01:16.705: INFO: Pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01279816s
STEP: Saw pod success 01/04/23 23:01:16.705
Jan  4 23:01:16.705: INFO: Pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb" satisfied condition "Succeeded or Failed"
Jan  4 23:01:16.709: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb container client-container: <nil>
STEP: delete the pod 01/04/23 23:01:16.72
Jan  4 23:01:16.748: INFO: Waiting for pod downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb to disappear
Jan  4 23:01:16.757: INFO: Pod downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 23:01:16.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1216" for this suite. 01/04/23 23:01:16.764
------------------------------
• [4.122 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:01:12.664
    Jan  4 23:01:12.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:01:12.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:12.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:12.682
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/04/23 23:01:12.684
    Jan  4 23:01:12.692: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb" in namespace "downward-api-1216" to be "Succeeded or Failed"
    Jan  4 23:01:12.697: INFO: Pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.96645ms
    Jan  4 23:01:14.701: INFO: Pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00880548s
    Jan  4 23:01:16.705: INFO: Pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01279816s
    STEP: Saw pod success 01/04/23 23:01:16.705
    Jan  4 23:01:16.705: INFO: Pod "downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb" satisfied condition "Succeeded or Failed"
    Jan  4 23:01:16.709: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb container client-container: <nil>
    STEP: delete the pod 01/04/23 23:01:16.72
    Jan  4 23:01:16.748: INFO: Waiting for pod downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb to disappear
    Jan  4 23:01:16.757: INFO: Pod downwardapi-volume-7738b7ed-2b23-4112-8f3b-a62286df38fb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:01:16.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1216" for this suite. 01/04/23 23:01:16.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:01:16.79
Jan  4 23:01:16.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replication-controller 01/04/23 23:01:16.797
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:16.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:16.823
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-rh6rz" 01/04/23 23:01:16.825
Jan  4 23:01:16.832: INFO: Get Replication Controller "e2e-rc-rh6rz" to confirm replicas
Jan  4 23:01:17.836: INFO: Get Replication Controller "e2e-rc-rh6rz" to confirm replicas
Jan  4 23:01:17.845: INFO: Found 1 replicas for "e2e-rc-rh6rz" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-rh6rz" 01/04/23 23:01:17.845
STEP: Updating a scale subresource 01/04/23 23:01:17.861
STEP: Verifying replicas where modified for replication controller "e2e-rc-rh6rz" 01/04/23 23:01:17.877
Jan  4 23:01:17.877: INFO: Get Replication Controller "e2e-rc-rh6rz" to confirm replicas
Jan  4 23:01:17.898: INFO: Found 2 replicas for "e2e-rc-rh6rz" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  4 23:01:17.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1068" for this suite. 01/04/23 23:01:17.909
------------------------------
• [1.129 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:01:16.79
    Jan  4 23:01:16.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replication-controller 01/04/23 23:01:16.797
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:16.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:16.823
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-rh6rz" 01/04/23 23:01:16.825
    Jan  4 23:01:16.832: INFO: Get Replication Controller "e2e-rc-rh6rz" to confirm replicas
    Jan  4 23:01:17.836: INFO: Get Replication Controller "e2e-rc-rh6rz" to confirm replicas
    Jan  4 23:01:17.845: INFO: Found 1 replicas for "e2e-rc-rh6rz" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-rh6rz" 01/04/23 23:01:17.845
    STEP: Updating a scale subresource 01/04/23 23:01:17.861
    STEP: Verifying replicas where modified for replication controller "e2e-rc-rh6rz" 01/04/23 23:01:17.877
    Jan  4 23:01:17.877: INFO: Get Replication Controller "e2e-rc-rh6rz" to confirm replicas
    Jan  4 23:01:17.898: INFO: Found 2 replicas for "e2e-rc-rh6rz" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:01:17.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1068" for this suite. 01/04/23 23:01:17.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:01:17.924
Jan  4 23:01:17.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename statefulset 01/04/23 23:01:17.925
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:17.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:17.943
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3847 01/04/23 23:01:17.946
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-3847 01/04/23 23:01:17.962
Jan  4 23:01:17.971: INFO: Found 0 stateful pods, waiting for 1
Jan  4 23:01:27.976: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/04/23 23:01:27.98
STEP: Getting /status 01/04/23 23:01:27.991
Jan  4 23:01:27.995: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/04/23 23:01:27.995
Jan  4 23:01:28.010: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/04/23 23:01:28.01
Jan  4 23:01:28.011: INFO: Observed &StatefulSet event: ADDED
Jan  4 23:01:28.011: INFO: Found Statefulset ss in namespace statefulset-3847 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  4 23:01:28.012: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/04/23 23:01:28.012
Jan  4 23:01:28.012: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  4 23:01:28.018: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/04/23 23:01:28.018
Jan  4 23:01:28.020: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  4 23:01:28.020: INFO: Deleting all statefulset in ns statefulset-3847
Jan  4 23:01:28.022: INFO: Scaling statefulset ss to 0
Jan  4 23:01:38.038: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:01:38.040: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:01:38.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3847" for this suite. 01/04/23 23:01:38.064
------------------------------
• [SLOW TEST] [20.151 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:01:17.924
    Jan  4 23:01:17.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename statefulset 01/04/23 23:01:17.925
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:17.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:17.943
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3847 01/04/23 23:01:17.946
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-3847 01/04/23 23:01:17.962
    Jan  4 23:01:17.971: INFO: Found 0 stateful pods, waiting for 1
    Jan  4 23:01:27.976: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/04/23 23:01:27.98
    STEP: Getting /status 01/04/23 23:01:27.991
    Jan  4 23:01:27.995: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/04/23 23:01:27.995
    Jan  4 23:01:28.010: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/04/23 23:01:28.01
    Jan  4 23:01:28.011: INFO: Observed &StatefulSet event: ADDED
    Jan  4 23:01:28.011: INFO: Found Statefulset ss in namespace statefulset-3847 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  4 23:01:28.012: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/04/23 23:01:28.012
    Jan  4 23:01:28.012: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  4 23:01:28.018: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/04/23 23:01:28.018
    Jan  4 23:01:28.020: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  4 23:01:28.020: INFO: Deleting all statefulset in ns statefulset-3847
    Jan  4 23:01:28.022: INFO: Scaling statefulset ss to 0
    Jan  4 23:01:38.038: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:01:38.040: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:01:38.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3847" for this suite. 01/04/23 23:01:38.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:01:38.078
Jan  4 23:01:38.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replicaset 01/04/23 23:01:38.079
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:38.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:38.105
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan  4 23:01:38.107: INFO: Creating ReplicaSet my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1
Jan  4 23:01:38.116: INFO: Pod name my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1: Found 0 pods out of 1
Jan  4 23:01:43.120: INFO: Pod name my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1: Found 1 pods out of 1
Jan  4 23:01:43.120: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1" is running
Jan  4 23:01:43.120: INFO: Waiting up to 5m0s for pod "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc" in namespace "replicaset-5235" to be "running"
Jan  4 23:01:43.124: INFO: Pod "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc": Phase="Running", Reason="", readiness=true. Elapsed: 3.370611ms
Jan  4 23:01:43.124: INFO: Pod "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc" satisfied condition "running"
Jan  4 23:01:43.124: INFO: Pod "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:01:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:01:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:01:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:01:38 +0000 UTC Reason: Message:}])
Jan  4 23:01:43.124: INFO: Trying to dial the pod
Jan  4 23:01:48.136: INFO: Controller my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1: Got expected result from replica 1 [my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc]: "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:01:48.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5235" for this suite. 01/04/23 23:01:48.139
------------------------------
• [SLOW TEST] [10.066 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:01:38.078
    Jan  4 23:01:38.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replicaset 01/04/23 23:01:38.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:38.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:38.105
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan  4 23:01:38.107: INFO: Creating ReplicaSet my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1
    Jan  4 23:01:38.116: INFO: Pod name my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1: Found 0 pods out of 1
    Jan  4 23:01:43.120: INFO: Pod name my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1: Found 1 pods out of 1
    Jan  4 23:01:43.120: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1" is running
    Jan  4 23:01:43.120: INFO: Waiting up to 5m0s for pod "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc" in namespace "replicaset-5235" to be "running"
    Jan  4 23:01:43.124: INFO: Pod "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc": Phase="Running", Reason="", readiness=true. Elapsed: 3.370611ms
    Jan  4 23:01:43.124: INFO: Pod "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc" satisfied condition "running"
    Jan  4 23:01:43.124: INFO: Pod "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:01:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:01:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:01:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:01:38 +0000 UTC Reason: Message:}])
    Jan  4 23:01:43.124: INFO: Trying to dial the pod
    Jan  4 23:01:48.136: INFO: Controller my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1: Got expected result from replica 1 [my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc]: "my-hostname-basic-10eb5dd1-e504-4d63-aa5f-6d707ad706f1-4mxlc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:01:48.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5235" for this suite. 01/04/23 23:01:48.139
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:01:48.145
Jan  4 23:01:48.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename taint-single-pod 01/04/23 23:01:48.146
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:48.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:48.17
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan  4 23:01:48.172: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  4 23:02:48.203: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan  4 23:02:48.206: INFO: Starting informer...
STEP: Starting pod... 01/04/23 23:02:48.206
Jan  4 23:02:48.418: INFO: Pod is running on ip-172-31-13-117.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 01/04/23 23:02:48.418
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/04/23 23:02:48.559
STEP: Waiting short time to make sure Pod is queued for deletion 01/04/23 23:02:48.572
Jan  4 23:02:48.572: INFO: Pod wasn't evicted. Proceeding
Jan  4 23:02:48.572: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/04/23 23:02:48.602
STEP: Waiting some time to make sure that toleration time passed. 01/04/23 23:02:48.614
Jan  4 23:04:03.617: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:03.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-8273" for this suite. 01/04/23 23:04:03.634
------------------------------
• [SLOW TEST] [135.507 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:01:48.145
    Jan  4 23:01:48.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename taint-single-pod 01/04/23 23:01:48.146
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:01:48.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:01:48.17
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan  4 23:01:48.172: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  4 23:02:48.203: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan  4 23:02:48.206: INFO: Starting informer...
    STEP: Starting pod... 01/04/23 23:02:48.206
    Jan  4 23:02:48.418: INFO: Pod is running on ip-172-31-13-117.us-east-2.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 01/04/23 23:02:48.418
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/04/23 23:02:48.559
    STEP: Waiting short time to make sure Pod is queued for deletion 01/04/23 23:02:48.572
    Jan  4 23:02:48.572: INFO: Pod wasn't evicted. Proceeding
    Jan  4 23:02:48.572: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/04/23 23:02:48.602
    STEP: Waiting some time to make sure that toleration time passed. 01/04/23 23:02:48.614
    Jan  4 23:04:03.617: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:03.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-8273" for this suite. 01/04/23 23:04:03.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:03.657
Jan  4 23:04:03.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename statefulset 01/04/23 23:04:03.658
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:03.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:03.698
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-805 01/04/23 23:04:03.703
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/04/23 23:04:03.712
STEP: Creating pod with conflicting port in namespace statefulset-805 01/04/23 23:04:03.724
STEP: Waiting until pod test-pod will start running in namespace statefulset-805 01/04/23 23:04:03.743
Jan  4 23:04:03.743: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-805" to be "running"
Jan  4 23:04:03.753: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.862079ms
Jan  4 23:04:05.884: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.140399091s
Jan  4 23:04:05.884: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-805 01/04/23 23:04:05.884
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-805 01/04/23 23:04:05.889
Jan  4 23:04:05.903: INFO: Observed stateful pod in namespace: statefulset-805, name: ss-0, uid: 36b70af3-9c5e-40f1-bc21-c6553e57c4b1, status phase: Pending. Waiting for statefulset controller to delete.
Jan  4 23:04:05.918: INFO: Observed stateful pod in namespace: statefulset-805, name: ss-0, uid: 36b70af3-9c5e-40f1-bc21-c6553e57c4b1, status phase: Failed. Waiting for statefulset controller to delete.
Jan  4 23:04:05.935: INFO: Observed stateful pod in namespace: statefulset-805, name: ss-0, uid: 36b70af3-9c5e-40f1-bc21-c6553e57c4b1, status phase: Failed. Waiting for statefulset controller to delete.
Jan  4 23:04:05.938: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-805
STEP: Removing pod with conflicting port in namespace statefulset-805 01/04/23 23:04:05.939
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-805 and will be in running state 01/04/23 23:04:05.958
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  4 23:04:07.967: INFO: Deleting all statefulset in ns statefulset-805
Jan  4 23:04:07.970: INFO: Scaling statefulset ss to 0
Jan  4 23:04:17.990: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:04:17.993: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:18.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-805" for this suite. 01/04/23 23:04:18.027
------------------------------
• [SLOW TEST] [14.392 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:03.657
    Jan  4 23:04:03.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename statefulset 01/04/23 23:04:03.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:03.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:03.698
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-805 01/04/23 23:04:03.703
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/04/23 23:04:03.712
    STEP: Creating pod with conflicting port in namespace statefulset-805 01/04/23 23:04:03.724
    STEP: Waiting until pod test-pod will start running in namespace statefulset-805 01/04/23 23:04:03.743
    Jan  4 23:04:03.743: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-805" to be "running"
    Jan  4 23:04:03.753: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.862079ms
    Jan  4 23:04:05.884: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.140399091s
    Jan  4 23:04:05.884: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-805 01/04/23 23:04:05.884
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-805 01/04/23 23:04:05.889
    Jan  4 23:04:05.903: INFO: Observed stateful pod in namespace: statefulset-805, name: ss-0, uid: 36b70af3-9c5e-40f1-bc21-c6553e57c4b1, status phase: Pending. Waiting for statefulset controller to delete.
    Jan  4 23:04:05.918: INFO: Observed stateful pod in namespace: statefulset-805, name: ss-0, uid: 36b70af3-9c5e-40f1-bc21-c6553e57c4b1, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  4 23:04:05.935: INFO: Observed stateful pod in namespace: statefulset-805, name: ss-0, uid: 36b70af3-9c5e-40f1-bc21-c6553e57c4b1, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  4 23:04:05.938: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-805
    STEP: Removing pod with conflicting port in namespace statefulset-805 01/04/23 23:04:05.939
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-805 and will be in running state 01/04/23 23:04:05.958
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  4 23:04:07.967: INFO: Deleting all statefulset in ns statefulset-805
    Jan  4 23:04:07.970: INFO: Scaling statefulset ss to 0
    Jan  4 23:04:17.990: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:04:17.993: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:18.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-805" for this suite. 01/04/23 23:04:18.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:18.05
Jan  4 23:04:18.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename limitrange 01/04/23 23:04:18.055
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:18.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:18.11
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-zpbtx" in namespace "limitrange-6421" 01/04/23 23:04:18.115
STEP: Creating another limitRange in another namespace 01/04/23 23:04:18.126
Jan  4 23:04:18.155: INFO: Namespace "e2e-limitrange-zpbtx-2870" created
Jan  4 23:04:18.155: INFO: Creating LimitRange "e2e-limitrange-zpbtx" in namespace "e2e-limitrange-zpbtx-2870"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-zpbtx" 01/04/23 23:04:18.16
Jan  4 23:04:18.163: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-zpbtx" in "limitrange-6421" namespace 01/04/23 23:04:18.163
Jan  4 23:04:18.170: INFO: LimitRange "e2e-limitrange-zpbtx" has been patched
STEP: Delete LimitRange "e2e-limitrange-zpbtx" by Collection with labelSelector: "e2e-limitrange-zpbtx=patched" 01/04/23 23:04:18.17
STEP: Confirm that the limitRange "e2e-limitrange-zpbtx" has been deleted 01/04/23 23:04:18.177
Jan  4 23:04:18.177: INFO: Requesting list of LimitRange to confirm quantity
Jan  4 23:04:18.180: INFO: Found 0 LimitRange with label "e2e-limitrange-zpbtx=patched"
Jan  4 23:04:18.180: INFO: LimitRange "e2e-limitrange-zpbtx" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-zpbtx" 01/04/23 23:04:18.18
Jan  4 23:04:18.184: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:18.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6421" for this suite. 01/04/23 23:04:18.187
STEP: Destroying namespace "e2e-limitrange-zpbtx-2870" for this suite. 01/04/23 23:04:18.193
------------------------------
• [0.148 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:18.05
    Jan  4 23:04:18.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename limitrange 01/04/23 23:04:18.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:18.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:18.11
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-zpbtx" in namespace "limitrange-6421" 01/04/23 23:04:18.115
    STEP: Creating another limitRange in another namespace 01/04/23 23:04:18.126
    Jan  4 23:04:18.155: INFO: Namespace "e2e-limitrange-zpbtx-2870" created
    Jan  4 23:04:18.155: INFO: Creating LimitRange "e2e-limitrange-zpbtx" in namespace "e2e-limitrange-zpbtx-2870"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-zpbtx" 01/04/23 23:04:18.16
    Jan  4 23:04:18.163: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-zpbtx" in "limitrange-6421" namespace 01/04/23 23:04:18.163
    Jan  4 23:04:18.170: INFO: LimitRange "e2e-limitrange-zpbtx" has been patched
    STEP: Delete LimitRange "e2e-limitrange-zpbtx" by Collection with labelSelector: "e2e-limitrange-zpbtx=patched" 01/04/23 23:04:18.17
    STEP: Confirm that the limitRange "e2e-limitrange-zpbtx" has been deleted 01/04/23 23:04:18.177
    Jan  4 23:04:18.177: INFO: Requesting list of LimitRange to confirm quantity
    Jan  4 23:04:18.180: INFO: Found 0 LimitRange with label "e2e-limitrange-zpbtx=patched"
    Jan  4 23:04:18.180: INFO: LimitRange "e2e-limitrange-zpbtx" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-zpbtx" 01/04/23 23:04:18.18
    Jan  4 23:04:18.184: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:18.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6421" for this suite. 01/04/23 23:04:18.187
    STEP: Destroying namespace "e2e-limitrange-zpbtx-2870" for this suite. 01/04/23 23:04:18.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:18.205
Jan  4 23:04:18.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:04:18.206
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:18.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:18.231
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/04/23 23:04:18.236
Jan  4 23:04:18.246: INFO: Waiting up to 5m0s for pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d" in namespace "emptydir-1042" to be "Succeeded or Failed"
Jan  4 23:04:18.249: INFO: Pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281271ms
Jan  4 23:04:20.253: INFO: Pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006725932s
Jan  4 23:04:22.451: INFO: Pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.205116009s
STEP: Saw pod success 01/04/23 23:04:22.451
Jan  4 23:04:22.451: INFO: Pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d" satisfied condition "Succeeded or Failed"
Jan  4 23:04:22.456: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-e83915d9-16d1-411e-bd84-94cdcd83049d container test-container: <nil>
STEP: delete the pod 01/04/23 23:04:22.471
Jan  4 23:04:22.903: INFO: Waiting for pod pod-e83915d9-16d1-411e-bd84-94cdcd83049d to disappear
Jan  4 23:04:22.906: INFO: Pod pod-e83915d9-16d1-411e-bd84-94cdcd83049d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:22.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1042" for this suite. 01/04/23 23:04:22.909
------------------------------
• [4.710 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:18.205
    Jan  4 23:04:18.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:04:18.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:18.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:18.231
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/04/23 23:04:18.236
    Jan  4 23:04:18.246: INFO: Waiting up to 5m0s for pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d" in namespace "emptydir-1042" to be "Succeeded or Failed"
    Jan  4 23:04:18.249: INFO: Pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281271ms
    Jan  4 23:04:20.253: INFO: Pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006725932s
    Jan  4 23:04:22.451: INFO: Pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.205116009s
    STEP: Saw pod success 01/04/23 23:04:22.451
    Jan  4 23:04:22.451: INFO: Pod "pod-e83915d9-16d1-411e-bd84-94cdcd83049d" satisfied condition "Succeeded or Failed"
    Jan  4 23:04:22.456: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-e83915d9-16d1-411e-bd84-94cdcd83049d container test-container: <nil>
    STEP: delete the pod 01/04/23 23:04:22.471
    Jan  4 23:04:22.903: INFO: Waiting for pod pod-e83915d9-16d1-411e-bd84-94cdcd83049d to disappear
    Jan  4 23:04:22.906: INFO: Pod pod-e83915d9-16d1-411e-bd84-94cdcd83049d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:22.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1042" for this suite. 01/04/23 23:04:22.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:22.923
Jan  4 23:04:22.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 23:04:22.923
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:22.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:22.95
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-147e3beb-9b2c-4ab3-a5a0-5f5439f27127 01/04/23 23:04:22.962
STEP: Creating secret with name s-test-opt-upd-e7e59273-df1e-4495-83e2-09a80cd03e6c 01/04/23 23:04:22.969
STEP: Creating the pod 01/04/23 23:04:22.973
Jan  4 23:04:22.989: INFO: Waiting up to 5m0s for pod "pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743" in namespace "secrets-3662" to be "running and ready"
Jan  4 23:04:23.015: INFO: Pod "pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743": Phase="Pending", Reason="", readiness=false. Elapsed: 25.247829ms
Jan  4 23:04:23.016: INFO: The phase of Pod pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:04:25.233: INFO: Pod "pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743": Phase="Running", Reason="", readiness=true. Elapsed: 2.243645429s
Jan  4 23:04:25.233: INFO: The phase of Pod pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743 is Running (Ready = true)
Jan  4 23:04:25.233: INFO: Pod "pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-147e3beb-9b2c-4ab3-a5a0-5f5439f27127 01/04/23 23:04:25.255
STEP: Updating secret s-test-opt-upd-e7e59273-df1e-4495-83e2-09a80cd03e6c 01/04/23 23:04:25.261
STEP: Creating secret with name s-test-opt-create-8cc25271-9364-4e36-a64d-863579cb43a3 01/04/23 23:04:25.266
STEP: waiting to observe update in volume 01/04/23 23:04:25.27
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:27.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3662" for this suite. 01/04/23 23:04:27.303
------------------------------
• [4.402 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:22.923
    Jan  4 23:04:22.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 23:04:22.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:22.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:22.95
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-147e3beb-9b2c-4ab3-a5a0-5f5439f27127 01/04/23 23:04:22.962
    STEP: Creating secret with name s-test-opt-upd-e7e59273-df1e-4495-83e2-09a80cd03e6c 01/04/23 23:04:22.969
    STEP: Creating the pod 01/04/23 23:04:22.973
    Jan  4 23:04:22.989: INFO: Waiting up to 5m0s for pod "pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743" in namespace "secrets-3662" to be "running and ready"
    Jan  4 23:04:23.015: INFO: Pod "pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743": Phase="Pending", Reason="", readiness=false. Elapsed: 25.247829ms
    Jan  4 23:04:23.016: INFO: The phase of Pod pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:04:25.233: INFO: Pod "pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743": Phase="Running", Reason="", readiness=true. Elapsed: 2.243645429s
    Jan  4 23:04:25.233: INFO: The phase of Pod pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743 is Running (Ready = true)
    Jan  4 23:04:25.233: INFO: Pod "pod-secrets-72d82f93-d278-4f47-adb8-d4ac5ba68743" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-147e3beb-9b2c-4ab3-a5a0-5f5439f27127 01/04/23 23:04:25.255
    STEP: Updating secret s-test-opt-upd-e7e59273-df1e-4495-83e2-09a80cd03e6c 01/04/23 23:04:25.261
    STEP: Creating secret with name s-test-opt-create-8cc25271-9364-4e36-a64d-863579cb43a3 01/04/23 23:04:25.266
    STEP: waiting to observe update in volume 01/04/23 23:04:25.27
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:27.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3662" for this suite. 01/04/23 23:04:27.303
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:27.323
Jan  4 23:04:27.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename csiinlinevolumes 01/04/23 23:04:27.324
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:27.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:27.365
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/04/23 23:04:27.368
STEP: getting 01/04/23 23:04:27.413
STEP: listing in namespace 01/04/23 23:04:27.418
STEP: patching 01/04/23 23:04:27.423
STEP: deleting 01/04/23 23:04:27.431
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:27.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3767" for this suite. 01/04/23 23:04:27.446
------------------------------
• [0.128 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:27.323
    Jan  4 23:04:27.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename csiinlinevolumes 01/04/23 23:04:27.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:27.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:27.365
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/04/23 23:04:27.368
    STEP: getting 01/04/23 23:04:27.413
    STEP: listing in namespace 01/04/23 23:04:27.418
    STEP: patching 01/04/23 23:04:27.423
    STEP: deleting 01/04/23 23:04:27.431
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:27.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3767" for this suite. 01/04/23 23:04:27.446
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:27.453
Jan  4 23:04:27.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 23:04:27.454
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:27.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:27.47
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/04/23 23:04:27.473
Jan  4 23:04:27.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 create -f -'
Jan  4 23:04:28.305: INFO: stderr: ""
Jan  4 23:04:28.305: INFO: stdout: "pod/pause created\n"
Jan  4 23:04:28.305: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan  4 23:04:28.305: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6297" to be "running and ready"
Jan  4 23:04:28.308: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562142ms
Jan  4 23:04:28.309: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-13-117.us-east-2.compute.internal' to be 'Running' but was 'Pending'
Jan  4 23:04:30.314: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009084128s
Jan  4 23:04:30.314: INFO: Pod "pause" satisfied condition "running and ready"
Jan  4 23:04:30.314: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/04/23 23:04:30.314
Jan  4 23:04:30.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 label pods pause testing-label=testing-label-value'
Jan  4 23:04:30.391: INFO: stderr: ""
Jan  4 23:04:30.391: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/04/23 23:04:30.391
Jan  4 23:04:30.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 get pod pause -L testing-label'
Jan  4 23:04:30.453: INFO: stderr: ""
Jan  4 23:04:30.453: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/04/23 23:04:30.453
Jan  4 23:04:30.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 label pods pause testing-label-'
Jan  4 23:04:30.533: INFO: stderr: ""
Jan  4 23:04:30.533: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/04/23 23:04:30.533
Jan  4 23:04:30.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 get pod pause -L testing-label'
Jan  4 23:04:30.597: INFO: stderr: ""
Jan  4 23:04:30.597: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/04/23 23:04:30.597
Jan  4 23:04:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 delete --grace-period=0 --force -f -'
Jan  4 23:04:30.680: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 23:04:30.680: INFO: stdout: "pod \"pause\" force deleted\n"
Jan  4 23:04:30.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 get rc,svc -l name=pause --no-headers'
Jan  4 23:04:30.781: INFO: stderr: "No resources found in kubectl-6297 namespace.\n"
Jan  4 23:04:30.781: INFO: stdout: ""
Jan  4 23:04:30.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  4 23:04:30.855: INFO: stderr: ""
Jan  4 23:04:30.855: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:30.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6297" for this suite. 01/04/23 23:04:30.861
------------------------------
• [3.414 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:27.453
    Jan  4 23:04:27.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 23:04:27.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:27.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:27.47
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/04/23 23:04:27.473
    Jan  4 23:04:27.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 create -f -'
    Jan  4 23:04:28.305: INFO: stderr: ""
    Jan  4 23:04:28.305: INFO: stdout: "pod/pause created\n"
    Jan  4 23:04:28.305: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan  4 23:04:28.305: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6297" to be "running and ready"
    Jan  4 23:04:28.308: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562142ms
    Jan  4 23:04:28.309: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-13-117.us-east-2.compute.internal' to be 'Running' but was 'Pending'
    Jan  4 23:04:30.314: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009084128s
    Jan  4 23:04:30.314: INFO: Pod "pause" satisfied condition "running and ready"
    Jan  4 23:04:30.314: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/04/23 23:04:30.314
    Jan  4 23:04:30.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 label pods pause testing-label=testing-label-value'
    Jan  4 23:04:30.391: INFO: stderr: ""
    Jan  4 23:04:30.391: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/04/23 23:04:30.391
    Jan  4 23:04:30.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 get pod pause -L testing-label'
    Jan  4 23:04:30.453: INFO: stderr: ""
    Jan  4 23:04:30.453: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/04/23 23:04:30.453
    Jan  4 23:04:30.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 label pods pause testing-label-'
    Jan  4 23:04:30.533: INFO: stderr: ""
    Jan  4 23:04:30.533: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/04/23 23:04:30.533
    Jan  4 23:04:30.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 get pod pause -L testing-label'
    Jan  4 23:04:30.597: INFO: stderr: ""
    Jan  4 23:04:30.597: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/04/23 23:04:30.597
    Jan  4 23:04:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 delete --grace-period=0 --force -f -'
    Jan  4 23:04:30.680: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 23:04:30.680: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan  4 23:04:30.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 get rc,svc -l name=pause --no-headers'
    Jan  4 23:04:30.781: INFO: stderr: "No resources found in kubectl-6297 namespace.\n"
    Jan  4 23:04:30.781: INFO: stdout: ""
    Jan  4 23:04:30.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-6297 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  4 23:04:30.855: INFO: stderr: ""
    Jan  4 23:04:30.855: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:30.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6297" for this suite. 01/04/23 23:04:30.861
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:30.867
Jan  4 23:04:30.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 23:04:30.868
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:30.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:30.896
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/04/23 23:04:30.899
STEP: setting up watch 01/04/23 23:04:30.899
STEP: submitting the pod to kubernetes 01/04/23 23:04:31.004
STEP: verifying the pod is in kubernetes 01/04/23 23:04:31.013
STEP: verifying pod creation was observed 01/04/23 23:04:31.017
Jan  4 23:04:31.017: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ee288d75-e164-4f10-89af-b08a3d9691e0" in namespace "pods-8908" to be "running"
Jan  4 23:04:31.029: INFO: Pod "pod-submit-remove-ee288d75-e164-4f10-89af-b08a3d9691e0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.687378ms
Jan  4 23:04:33.061: INFO: Pod "pod-submit-remove-ee288d75-e164-4f10-89af-b08a3d9691e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.043641007s
Jan  4 23:04:33.061: INFO: Pod "pod-submit-remove-ee288d75-e164-4f10-89af-b08a3d9691e0" satisfied condition "running"
STEP: deleting the pod gracefully 01/04/23 23:04:33.074
STEP: verifying pod deletion was observed 01/04/23 23:04:33.09
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:35.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8908" for this suite. 01/04/23 23:04:35.699
------------------------------
• [4.837 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:30.867
    Jan  4 23:04:30.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 23:04:30.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:30.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:30.896
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/04/23 23:04:30.899
    STEP: setting up watch 01/04/23 23:04:30.899
    STEP: submitting the pod to kubernetes 01/04/23 23:04:31.004
    STEP: verifying the pod is in kubernetes 01/04/23 23:04:31.013
    STEP: verifying pod creation was observed 01/04/23 23:04:31.017
    Jan  4 23:04:31.017: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ee288d75-e164-4f10-89af-b08a3d9691e0" in namespace "pods-8908" to be "running"
    Jan  4 23:04:31.029: INFO: Pod "pod-submit-remove-ee288d75-e164-4f10-89af-b08a3d9691e0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.687378ms
    Jan  4 23:04:33.061: INFO: Pod "pod-submit-remove-ee288d75-e164-4f10-89af-b08a3d9691e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.043641007s
    Jan  4 23:04:33.061: INFO: Pod "pod-submit-remove-ee288d75-e164-4f10-89af-b08a3d9691e0" satisfied condition "running"
    STEP: deleting the pod gracefully 01/04/23 23:04:33.074
    STEP: verifying pod deletion was observed 01/04/23 23:04:33.09
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:35.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8908" for this suite. 01/04/23 23:04:35.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:35.705
Jan  4 23:04:35.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:04:35.706
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:35.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:35.724
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-74153748-428f-4361-a01d-c51b70b8789c 01/04/23 23:04:35.727
STEP: Creating a pod to test consume configMaps 01/04/23 23:04:35.731
Jan  4 23:04:35.739: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650" in namespace "projected-9117" to be "Succeeded or Failed"
Jan  4 23:04:35.751: INFO: Pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650": Phase="Pending", Reason="", readiness=false. Elapsed: 11.8975ms
Jan  4 23:04:37.755: INFO: Pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015708423s
Jan  4 23:04:39.756: INFO: Pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016580424s
STEP: Saw pod success 01/04/23 23:04:39.756
Jan  4 23:04:39.756: INFO: Pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650" satisfied condition "Succeeded or Failed"
Jan  4 23:04:39.760: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 23:04:39.767
Jan  4 23:04:39.797: INFO: Waiting for pod pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650 to disappear
Jan  4 23:04:39.804: INFO: Pod pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:39.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9117" for this suite. 01/04/23 23:04:39.814
------------------------------
• [4.120 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:35.705
    Jan  4 23:04:35.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:04:35.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:35.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:35.724
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-74153748-428f-4361-a01d-c51b70b8789c 01/04/23 23:04:35.727
    STEP: Creating a pod to test consume configMaps 01/04/23 23:04:35.731
    Jan  4 23:04:35.739: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650" in namespace "projected-9117" to be "Succeeded or Failed"
    Jan  4 23:04:35.751: INFO: Pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650": Phase="Pending", Reason="", readiness=false. Elapsed: 11.8975ms
    Jan  4 23:04:37.755: INFO: Pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015708423s
    Jan  4 23:04:39.756: INFO: Pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016580424s
    STEP: Saw pod success 01/04/23 23:04:39.756
    Jan  4 23:04:39.756: INFO: Pod "pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650" satisfied condition "Succeeded or Failed"
    Jan  4 23:04:39.760: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 23:04:39.767
    Jan  4 23:04:39.797: INFO: Waiting for pod pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650 to disappear
    Jan  4 23:04:39.804: INFO: Pod pod-projected-configmaps-3eba8a7a-9415-4cb4-ab84-d25f5b9fe650 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:39.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9117" for this suite. 01/04/23 23:04:39.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:39.825
Jan  4 23:04:39.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:04:39.826
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:39.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:39.844
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/04/23 23:04:39.847
Jan  4 23:04:39.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295" in namespace "projected-6803" to be "Succeeded or Failed"
Jan  4 23:04:39.859: INFO: Pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.771199ms
Jan  4 23:04:41.862: INFO: Pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006134025s
Jan  4 23:04:43.863: INFO: Pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007043554s
STEP: Saw pod success 01/04/23 23:04:43.863
Jan  4 23:04:43.863: INFO: Pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295" satisfied condition "Succeeded or Failed"
Jan  4 23:04:43.867: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295 container client-container: <nil>
STEP: delete the pod 01/04/23 23:04:43.873
Jan  4 23:04:43.887: INFO: Waiting for pod downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295 to disappear
Jan  4 23:04:43.892: INFO: Pod downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:43.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6803" for this suite. 01/04/23 23:04:43.897
------------------------------
• [4.079 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:39.825
    Jan  4 23:04:39.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:04:39.826
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:39.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:39.844
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/04/23 23:04:39.847
    Jan  4 23:04:39.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295" in namespace "projected-6803" to be "Succeeded or Failed"
    Jan  4 23:04:39.859: INFO: Pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.771199ms
    Jan  4 23:04:41.862: INFO: Pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006134025s
    Jan  4 23:04:43.863: INFO: Pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007043554s
    STEP: Saw pod success 01/04/23 23:04:43.863
    Jan  4 23:04:43.863: INFO: Pod "downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295" satisfied condition "Succeeded or Failed"
    Jan  4 23:04:43.867: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295 container client-container: <nil>
    STEP: delete the pod 01/04/23 23:04:43.873
    Jan  4 23:04:43.887: INFO: Waiting for pod downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295 to disappear
    Jan  4 23:04:43.892: INFO: Pod downwardapi-volume-e6053196-4bd8-4063-a2b6-67a0b4bb7295 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:43.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6803" for this suite. 01/04/23 23:04:43.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:43.905
Jan  4 23:04:43.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename namespaces 01/04/23 23:04:43.906
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:43.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:43.925
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/04/23 23:04:43.927
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:43.941
STEP: Creating a pod in the namespace 01/04/23 23:04:43.944
STEP: Waiting for the pod to have running status 01/04/23 23:04:43.951
Jan  4 23:04:43.952: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2690" to be "running"
Jan  4 23:04:43.958: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.363911ms
Jan  4 23:04:45.962: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01000682s
Jan  4 23:04:45.962: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/04/23 23:04:45.962
STEP: Waiting for the namespace to be removed. 01/04/23 23:04:45.968
STEP: Recreating the namespace 01/04/23 23:04:56.971
STEP: Verifying there are no pods in the namespace 01/04/23 23:04:56.985
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:04:56.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3990" for this suite. 01/04/23 23:04:56.992
STEP: Destroying namespace "nsdeletetest-2690" for this suite. 01/04/23 23:04:57.005
Jan  4 23:04:57.008: INFO: Namespace nsdeletetest-2690 was already deleted
STEP: Destroying namespace "nsdeletetest-5114" for this suite. 01/04/23 23:04:57.008
------------------------------
• [SLOW TEST] [13.110 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:43.905
    Jan  4 23:04:43.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename namespaces 01/04/23 23:04:43.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:43.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:43.925
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/04/23 23:04:43.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:43.941
    STEP: Creating a pod in the namespace 01/04/23 23:04:43.944
    STEP: Waiting for the pod to have running status 01/04/23 23:04:43.951
    Jan  4 23:04:43.952: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2690" to be "running"
    Jan  4 23:04:43.958: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.363911ms
    Jan  4 23:04:45.962: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01000682s
    Jan  4 23:04:45.962: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/04/23 23:04:45.962
    STEP: Waiting for the namespace to be removed. 01/04/23 23:04:45.968
    STEP: Recreating the namespace 01/04/23 23:04:56.971
    STEP: Verifying there are no pods in the namespace 01/04/23 23:04:56.985
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:04:56.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3990" for this suite. 01/04/23 23:04:56.992
    STEP: Destroying namespace "nsdeletetest-2690" for this suite. 01/04/23 23:04:57.005
    Jan  4 23:04:57.008: INFO: Namespace nsdeletetest-2690 was already deleted
    STEP: Destroying namespace "nsdeletetest-5114" for this suite. 01/04/23 23:04:57.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:04:57.016
Jan  4 23:04:57.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:04:57.018
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:57.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:57.049
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/04/23 23:04:57.051
Jan  4 23:04:57.058: INFO: Waiting up to 5m0s for pod "pod-ce567f2d-1376-4331-aa5f-c71706374082" in namespace "emptydir-199" to be "Succeeded or Failed"
Jan  4 23:04:57.070: INFO: Pod "pod-ce567f2d-1376-4331-aa5f-c71706374082": Phase="Pending", Reason="", readiness=false. Elapsed: 11.443577ms
Jan  4 23:04:59.073: INFO: Pod "pod-ce567f2d-1376-4331-aa5f-c71706374082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014638979s
Jan  4 23:05:01.076: INFO: Pod "pod-ce567f2d-1376-4331-aa5f-c71706374082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017401846s
STEP: Saw pod success 01/04/23 23:05:01.076
Jan  4 23:05:01.076: INFO: Pod "pod-ce567f2d-1376-4331-aa5f-c71706374082" satisfied condition "Succeeded or Failed"
Jan  4 23:05:01.079: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-ce567f2d-1376-4331-aa5f-c71706374082 container test-container: <nil>
STEP: delete the pod 01/04/23 23:05:01.09
Jan  4 23:05:01.125: INFO: Waiting for pod pod-ce567f2d-1376-4331-aa5f-c71706374082 to disappear
Jan  4 23:05:01.139: INFO: Pod pod-ce567f2d-1376-4331-aa5f-c71706374082 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:05:01.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-199" for this suite. 01/04/23 23:05:01.148
------------------------------
• [4.157 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:04:57.016
    Jan  4 23:04:57.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:04:57.018
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:04:57.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:04:57.049
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/04/23 23:04:57.051
    Jan  4 23:04:57.058: INFO: Waiting up to 5m0s for pod "pod-ce567f2d-1376-4331-aa5f-c71706374082" in namespace "emptydir-199" to be "Succeeded or Failed"
    Jan  4 23:04:57.070: INFO: Pod "pod-ce567f2d-1376-4331-aa5f-c71706374082": Phase="Pending", Reason="", readiness=false. Elapsed: 11.443577ms
    Jan  4 23:04:59.073: INFO: Pod "pod-ce567f2d-1376-4331-aa5f-c71706374082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014638979s
    Jan  4 23:05:01.076: INFO: Pod "pod-ce567f2d-1376-4331-aa5f-c71706374082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017401846s
    STEP: Saw pod success 01/04/23 23:05:01.076
    Jan  4 23:05:01.076: INFO: Pod "pod-ce567f2d-1376-4331-aa5f-c71706374082" satisfied condition "Succeeded or Failed"
    Jan  4 23:05:01.079: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-ce567f2d-1376-4331-aa5f-c71706374082 container test-container: <nil>
    STEP: delete the pod 01/04/23 23:05:01.09
    Jan  4 23:05:01.125: INFO: Waiting for pod pod-ce567f2d-1376-4331-aa5f-c71706374082 to disappear
    Jan  4 23:05:01.139: INFO: Pod pod-ce567f2d-1376-4331-aa5f-c71706374082 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:05:01.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-199" for this suite. 01/04/23 23:05:01.148
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:05:01.174
Jan  4 23:05:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:05:01.175
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:01.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:01.211
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/04/23 23:05:01.217
Jan  4 23:05:01.229: INFO: Waiting up to 5m0s for pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f" in namespace "svcaccounts-39" to be "Succeeded or Failed"
Jan  4 23:05:01.249: INFO: Pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.245259ms
Jan  4 23:05:03.264: INFO: Pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034964168s
Jan  4 23:05:05.263: INFO: Pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033532923s
STEP: Saw pod success 01/04/23 23:05:05.263
Jan  4 23:05:05.263: INFO: Pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f" satisfied condition "Succeeded or Failed"
Jan  4 23:05:05.266: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod test-pod-94529a84-069e-4156-b82b-a273dd62bb3f container agnhost-container: <nil>
STEP: delete the pod 01/04/23 23:05:05.273
Jan  4 23:05:05.286: INFO: Waiting for pod test-pod-94529a84-069e-4156-b82b-a273dd62bb3f to disappear
Jan  4 23:05:05.290: INFO: Pod test-pod-94529a84-069e-4156-b82b-a273dd62bb3f no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  4 23:05:05.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-39" for this suite. 01/04/23 23:05:05.294
------------------------------
• [4.127 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:05:01.174
    Jan  4 23:05:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:05:01.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:01.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:01.211
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/04/23 23:05:01.217
    Jan  4 23:05:01.229: INFO: Waiting up to 5m0s for pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f" in namespace "svcaccounts-39" to be "Succeeded or Failed"
    Jan  4 23:05:01.249: INFO: Pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.245259ms
    Jan  4 23:05:03.264: INFO: Pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034964168s
    Jan  4 23:05:05.263: INFO: Pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033532923s
    STEP: Saw pod success 01/04/23 23:05:05.263
    Jan  4 23:05:05.263: INFO: Pod "test-pod-94529a84-069e-4156-b82b-a273dd62bb3f" satisfied condition "Succeeded or Failed"
    Jan  4 23:05:05.266: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod test-pod-94529a84-069e-4156-b82b-a273dd62bb3f container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 23:05:05.273
    Jan  4 23:05:05.286: INFO: Waiting for pod test-pod-94529a84-069e-4156-b82b-a273dd62bb3f to disappear
    Jan  4 23:05:05.290: INFO: Pod test-pod-94529a84-069e-4156-b82b-a273dd62bb3f no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:05:05.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-39" for this suite. 01/04/23 23:05:05.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:05:05.308
Jan  4 23:05:05.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename controllerrevisions 01/04/23 23:05:05.313
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:05.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:05.341
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-x7bqj-daemon-set" 01/04/23 23:05:05.368
STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 23:05:05.372
Jan  4 23:05:05.381: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 0
Jan  4 23:05:05.381: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:05:06.402: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 0
Jan  4 23:05:06.403: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:05:07.388: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 2
Jan  4 23:05:07.388: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:05:08.389: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 3
Jan  4 23:05:08.389: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:05:09.389: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 4
Jan  4 23:05:09.389: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-x7bqj-daemon-set
STEP: Confirm DaemonSet "e2e-x7bqj-daemon-set" successfully created with "daemonset-name=e2e-x7bqj-daemon-set" label 01/04/23 23:05:09.392
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-x7bqj-daemon-set" 01/04/23 23:05:09.398
Jan  4 23:05:09.400: INFO: Located ControllerRevision: "e2e-x7bqj-daemon-set-6dcdd4f8b7"
STEP: Patching ControllerRevision "e2e-x7bqj-daemon-set-6dcdd4f8b7" 01/04/23 23:05:09.403
Jan  4 23:05:09.410: INFO: e2e-x7bqj-daemon-set-6dcdd4f8b7 has been patched
STEP: Create a new ControllerRevision 01/04/23 23:05:09.41
Jan  4 23:05:09.417: INFO: Created ControllerRevision: e2e-x7bqj-daemon-set-57bbc4b859
STEP: Confirm that there are two ControllerRevisions 01/04/23 23:05:09.417
Jan  4 23:05:09.417: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  4 23:05:09.419: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-x7bqj-daemon-set-6dcdd4f8b7" 01/04/23 23:05:09.419
STEP: Confirm that there is only one ControllerRevision 01/04/23 23:05:09.426
Jan  4 23:05:09.426: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  4 23:05:09.429: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-x7bqj-daemon-set-57bbc4b859" 01/04/23 23:05:09.432
Jan  4 23:05:09.440: INFO: e2e-x7bqj-daemon-set-57bbc4b859 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/04/23 23:05:09.44
W0104 23:05:09.446431      18 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/04/23 23:05:09.446
Jan  4 23:05:09.446: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  4 23:05:10.450: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  4 23:05:10.454: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-x7bqj-daemon-set-57bbc4b859=updated" 01/04/23 23:05:10.455
STEP: Confirm that there is only one ControllerRevision 01/04/23 23:05:10.464
Jan  4 23:05:10.464: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  4 23:05:10.469: INFO: Found 1 ControllerRevisions
Jan  4 23:05:10.474: INFO: ControllerRevision "e2e-x7bqj-daemon-set-65497f56cd" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-x7bqj-daemon-set" 01/04/23 23:05:10.482
STEP: deleting DaemonSet.extensions e2e-x7bqj-daemon-set in namespace controllerrevisions-7267, will wait for the garbage collector to delete the pods 01/04/23 23:05:10.482
Jan  4 23:05:10.547: INFO: Deleting DaemonSet.extensions e2e-x7bqj-daemon-set took: 7.476638ms
Jan  4 23:05:10.747: INFO: Terminating DaemonSet.extensions e2e-x7bqj-daemon-set pods took: 200.445075ms
Jan  4 23:05:12.250: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 0
Jan  4 23:05:12.251: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-x7bqj-daemon-set
Jan  4 23:05:12.253: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54903"},"items":null}

Jan  4 23:05:12.255: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54903"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:05:12.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-7267" for this suite. 01/04/23 23:05:12.279
------------------------------
• [SLOW TEST] [6.976 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:05:05.308
    Jan  4 23:05:05.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename controllerrevisions 01/04/23 23:05:05.313
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:05.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:05.341
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-x7bqj-daemon-set" 01/04/23 23:05:05.368
    STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 23:05:05.372
    Jan  4 23:05:05.381: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 0
    Jan  4 23:05:05.381: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:05:06.402: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 0
    Jan  4 23:05:06.403: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:05:07.388: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 2
    Jan  4 23:05:07.388: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:05:08.389: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 3
    Jan  4 23:05:08.389: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:05:09.389: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 4
    Jan  4 23:05:09.389: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-x7bqj-daemon-set
    STEP: Confirm DaemonSet "e2e-x7bqj-daemon-set" successfully created with "daemonset-name=e2e-x7bqj-daemon-set" label 01/04/23 23:05:09.392
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-x7bqj-daemon-set" 01/04/23 23:05:09.398
    Jan  4 23:05:09.400: INFO: Located ControllerRevision: "e2e-x7bqj-daemon-set-6dcdd4f8b7"
    STEP: Patching ControllerRevision "e2e-x7bqj-daemon-set-6dcdd4f8b7" 01/04/23 23:05:09.403
    Jan  4 23:05:09.410: INFO: e2e-x7bqj-daemon-set-6dcdd4f8b7 has been patched
    STEP: Create a new ControllerRevision 01/04/23 23:05:09.41
    Jan  4 23:05:09.417: INFO: Created ControllerRevision: e2e-x7bqj-daemon-set-57bbc4b859
    STEP: Confirm that there are two ControllerRevisions 01/04/23 23:05:09.417
    Jan  4 23:05:09.417: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  4 23:05:09.419: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-x7bqj-daemon-set-6dcdd4f8b7" 01/04/23 23:05:09.419
    STEP: Confirm that there is only one ControllerRevision 01/04/23 23:05:09.426
    Jan  4 23:05:09.426: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  4 23:05:09.429: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-x7bqj-daemon-set-57bbc4b859" 01/04/23 23:05:09.432
    Jan  4 23:05:09.440: INFO: e2e-x7bqj-daemon-set-57bbc4b859 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/04/23 23:05:09.44
    W0104 23:05:09.446431      18 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/04/23 23:05:09.446
    Jan  4 23:05:09.446: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  4 23:05:10.450: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  4 23:05:10.454: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-x7bqj-daemon-set-57bbc4b859=updated" 01/04/23 23:05:10.455
    STEP: Confirm that there is only one ControllerRevision 01/04/23 23:05:10.464
    Jan  4 23:05:10.464: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  4 23:05:10.469: INFO: Found 1 ControllerRevisions
    Jan  4 23:05:10.474: INFO: ControllerRevision "e2e-x7bqj-daemon-set-65497f56cd" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-x7bqj-daemon-set" 01/04/23 23:05:10.482
    STEP: deleting DaemonSet.extensions e2e-x7bqj-daemon-set in namespace controllerrevisions-7267, will wait for the garbage collector to delete the pods 01/04/23 23:05:10.482
    Jan  4 23:05:10.547: INFO: Deleting DaemonSet.extensions e2e-x7bqj-daemon-set took: 7.476638ms
    Jan  4 23:05:10.747: INFO: Terminating DaemonSet.extensions e2e-x7bqj-daemon-set pods took: 200.445075ms
    Jan  4 23:05:12.250: INFO: Number of nodes with available pods controlled by daemonset e2e-x7bqj-daemon-set: 0
    Jan  4 23:05:12.251: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-x7bqj-daemon-set
    Jan  4 23:05:12.253: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54903"},"items":null}

    Jan  4 23:05:12.255: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54903"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:05:12.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-7267" for this suite. 01/04/23 23:05:12.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:05:12.287
Jan  4 23:05:12.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename disruption 01/04/23 23:05:12.287
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:12.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:12.305
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/04/23 23:05:12.308
STEP: Waiting for the pdb to be processed 01/04/23 23:05:12.312
STEP: First trying to evict a pod which shouldn't be evictable 01/04/23 23:05:14.323
STEP: Waiting for all pods to be running 01/04/23 23:05:14.323
Jan  4 23:05:14.326: INFO: pods: 0 < 3
STEP: locating a running pod 01/04/23 23:05:16.33
STEP: Updating the pdb to allow a pod to be evicted 01/04/23 23:05:16.337
STEP: Waiting for the pdb to be processed 01/04/23 23:05:16.344
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/04/23 23:05:16.349
STEP: Waiting for all pods to be running 01/04/23 23:05:16.349
STEP: Waiting for the pdb to observed all healthy pods 01/04/23 23:05:16.352
STEP: Patching the pdb to disallow a pod to be evicted 01/04/23 23:05:16.38
STEP: Waiting for the pdb to be processed 01/04/23 23:05:16.393
STEP: Waiting for all pods to be running 01/04/23 23:05:18.407
STEP: locating a running pod 01/04/23 23:05:18.411
STEP: Deleting the pdb to allow a pod to be evicted 01/04/23 23:05:18.418
STEP: Waiting for the pdb to be deleted 01/04/23 23:05:18.424
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/04/23 23:05:18.426
STEP: Waiting for all pods to be running 01/04/23 23:05:18.426
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  4 23:05:18.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1570" for this suite. 01/04/23 23:05:18.449
------------------------------
• [SLOW TEST] [6.187 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:05:12.287
    Jan  4 23:05:12.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename disruption 01/04/23 23:05:12.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:12.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:12.305
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/04/23 23:05:12.308
    STEP: Waiting for the pdb to be processed 01/04/23 23:05:12.312
    STEP: First trying to evict a pod which shouldn't be evictable 01/04/23 23:05:14.323
    STEP: Waiting for all pods to be running 01/04/23 23:05:14.323
    Jan  4 23:05:14.326: INFO: pods: 0 < 3
    STEP: locating a running pod 01/04/23 23:05:16.33
    STEP: Updating the pdb to allow a pod to be evicted 01/04/23 23:05:16.337
    STEP: Waiting for the pdb to be processed 01/04/23 23:05:16.344
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/04/23 23:05:16.349
    STEP: Waiting for all pods to be running 01/04/23 23:05:16.349
    STEP: Waiting for the pdb to observed all healthy pods 01/04/23 23:05:16.352
    STEP: Patching the pdb to disallow a pod to be evicted 01/04/23 23:05:16.38
    STEP: Waiting for the pdb to be processed 01/04/23 23:05:16.393
    STEP: Waiting for all pods to be running 01/04/23 23:05:18.407
    STEP: locating a running pod 01/04/23 23:05:18.411
    STEP: Deleting the pdb to allow a pod to be evicted 01/04/23 23:05:18.418
    STEP: Waiting for the pdb to be deleted 01/04/23 23:05:18.424
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/04/23 23:05:18.426
    STEP: Waiting for all pods to be running 01/04/23 23:05:18.426
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:05:18.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1570" for this suite. 01/04/23 23:05:18.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:05:18.48
Jan  4 23:05:18.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 23:05:18.48
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:18.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:18.507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 23:05:18.525
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:05:19.091
STEP: Deploying the webhook pod 01/04/23 23:05:19.101
STEP: Wait for the deployment to be ready 01/04/23 23:05:19.119
Jan  4 23:05:19.133: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 23:05:21.143
STEP: Verifying the service has paired with the endpoint 01/04/23 23:05:21.155
Jan  4 23:05:22.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/04/23 23:05:22.159
STEP: Registering slow webhook via the AdmissionRegistration API 01/04/23 23:05:22.159
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/04/23 23:05:22.174
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/04/23 23:05:23.187
STEP: Registering slow webhook via the AdmissionRegistration API 01/04/23 23:05:23.187
STEP: Having no error when timeout is longer than webhook latency 01/04/23 23:05:24.215
STEP: Registering slow webhook via the AdmissionRegistration API 01/04/23 23:05:24.215
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/04/23 23:05:29.245
STEP: Registering slow webhook via the AdmissionRegistration API 01/04/23 23:05:29.245
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:05:34.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7617" for this suite. 01/04/23 23:05:34.416
STEP: Destroying namespace "webhook-7617-markers" for this suite. 01/04/23 23:05:34.429
------------------------------
• [SLOW TEST] [15.974 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:05:18.48
    Jan  4 23:05:18.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 23:05:18.48
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:18.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:18.507
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 23:05:18.525
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:05:19.091
    STEP: Deploying the webhook pod 01/04/23 23:05:19.101
    STEP: Wait for the deployment to be ready 01/04/23 23:05:19.119
    Jan  4 23:05:19.133: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 23:05:21.143
    STEP: Verifying the service has paired with the endpoint 01/04/23 23:05:21.155
    Jan  4 23:05:22.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/04/23 23:05:22.159
    STEP: Registering slow webhook via the AdmissionRegistration API 01/04/23 23:05:22.159
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/04/23 23:05:22.174
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/04/23 23:05:23.187
    STEP: Registering slow webhook via the AdmissionRegistration API 01/04/23 23:05:23.187
    STEP: Having no error when timeout is longer than webhook latency 01/04/23 23:05:24.215
    STEP: Registering slow webhook via the AdmissionRegistration API 01/04/23 23:05:24.215
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/04/23 23:05:29.245
    STEP: Registering slow webhook via the AdmissionRegistration API 01/04/23 23:05:29.245
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:05:34.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7617" for this suite. 01/04/23 23:05:34.416
    STEP: Destroying namespace "webhook-7617-markers" for this suite. 01/04/23 23:05:34.429
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:05:34.453
Jan  4 23:05:34.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 23:05:34.455
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:34.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:34.472
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan  4 23:05:34.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:05:40.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7002" for this suite. 01/04/23 23:05:40.728
------------------------------
• [SLOW TEST] [6.281 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:05:34.453
    Jan  4 23:05:34.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 23:05:34.455
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:34.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:34.472
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan  4 23:05:34.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:05:40.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7002" for this suite. 01/04/23 23:05:40.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:05:40.736
Jan  4 23:05:40.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename dns 01/04/23 23:05:40.737
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:40.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:40.774
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/04/23 23:05:40.777
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
 01/04/23 23:05:40.782
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
 01/04/23 23:05:40.783
STEP: creating a pod to probe DNS 01/04/23 23:05:40.783
STEP: submitting the pod to kubernetes 01/04/23 23:05:40.783
Jan  4 23:05:40.789: INFO: Waiting up to 15m0s for pod "dns-test-c53d9b01-2602-4dab-b009-d428fe83087c" in namespace "dns-5961" to be "running"
Jan  4 23:05:40.796: INFO: Pod "dns-test-c53d9b01-2602-4dab-b009-d428fe83087c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230631ms
Jan  4 23:05:42.800: INFO: Pod "dns-test-c53d9b01-2602-4dab-b009-d428fe83087c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010239844s
Jan  4 23:05:42.800: INFO: Pod "dns-test-c53d9b01-2602-4dab-b009-d428fe83087c" satisfied condition "running"
STEP: retrieving the pod 01/04/23 23:05:42.8
STEP: looking for the results for each expected name from probers 01/04/23 23:05:42.803
Jan  4 23:05:42.813: INFO: DNS probes using dns-test-c53d9b01-2602-4dab-b009-d428fe83087c succeeded

STEP: deleting the pod 01/04/23 23:05:42.813
STEP: changing the externalName to bar.example.com 01/04/23 23:05:42.838
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
 01/04/23 23:05:42.851
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
 01/04/23 23:05:42.851
STEP: creating a second pod to probe DNS 01/04/23 23:05:42.851
STEP: submitting the pod to kubernetes 01/04/23 23:05:42.851
Jan  4 23:05:42.858: INFO: Waiting up to 15m0s for pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2" in namespace "dns-5961" to be "running"
Jan  4 23:05:42.863: INFO: Pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144243ms
Jan  4 23:05:44.867: INFO: Pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008657297s
Jan  4 23:05:46.878: INFO: Pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.020277603s
Jan  4 23:05:46.878: INFO: Pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2" satisfied condition "running"
STEP: retrieving the pod 01/04/23 23:05:46.878
STEP: looking for the results for each expected name from probers 01/04/23 23:05:46.883
Jan  4 23:05:46.919: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:05:46.952: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:05:46.952: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

Jan  4 23:05:51.957: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:05:51.960: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:05:51.960: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

Jan  4 23:05:56.956: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:05:56.959: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:05:56.959: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

Jan  4 23:06:01.956: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:06:01.963: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:06:01.963: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

Jan  4 23:06:06.956: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:06:06.959: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:06:06.959: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

Jan  4 23:06:11.957: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:06:11.960: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  4 23:06:11.960: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

Jan  4 23:06:16.960: INFO: DNS probes using dns-test-182fee5a-e777-4028-9001-aca242e025e2 succeeded

STEP: deleting the pod 01/04/23 23:06:16.96
STEP: changing the service to type=ClusterIP 01/04/23 23:06:16.971
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
 01/04/23 23:06:16.998
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
 01/04/23 23:06:16.998
STEP: creating a third pod to probe DNS 01/04/23 23:06:16.998
STEP: submitting the pod to kubernetes 01/04/23 23:06:17.004
Jan  4 23:06:17.015: INFO: Waiting up to 15m0s for pod "dns-test-6074411e-7562-49d9-b726-48d68a290d1e" in namespace "dns-5961" to be "running"
Jan  4 23:06:17.019: INFO: Pod "dns-test-6074411e-7562-49d9-b726-48d68a290d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.911941ms
Jan  4 23:06:19.023: INFO: Pod "dns-test-6074411e-7562-49d9-b726-48d68a290d1e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008466123s
Jan  4 23:06:19.023: INFO: Pod "dns-test-6074411e-7562-49d9-b726-48d68a290d1e" satisfied condition "running"
STEP: retrieving the pod 01/04/23 23:06:19.023
STEP: looking for the results for each expected name from probers 01/04/23 23:06:19.026
Jan  4 23:06:19.041: INFO: DNS probes using dns-test-6074411e-7562-49d9-b726-48d68a290d1e succeeded

STEP: deleting the pod 01/04/23 23:06:19.041
STEP: deleting the test externalName service 01/04/23 23:06:19.057
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  4 23:06:19.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5961" for this suite. 01/04/23 23:06:19.09
------------------------------
• [SLOW TEST] [38.368 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:05:40.736
    Jan  4 23:05:40.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename dns 01/04/23 23:05:40.737
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:05:40.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:05:40.774
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/04/23 23:05:40.777
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
     01/04/23 23:05:40.782
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
     01/04/23 23:05:40.783
    STEP: creating a pod to probe DNS 01/04/23 23:05:40.783
    STEP: submitting the pod to kubernetes 01/04/23 23:05:40.783
    Jan  4 23:05:40.789: INFO: Waiting up to 15m0s for pod "dns-test-c53d9b01-2602-4dab-b009-d428fe83087c" in namespace "dns-5961" to be "running"
    Jan  4 23:05:40.796: INFO: Pod "dns-test-c53d9b01-2602-4dab-b009-d428fe83087c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230631ms
    Jan  4 23:05:42.800: INFO: Pod "dns-test-c53d9b01-2602-4dab-b009-d428fe83087c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010239844s
    Jan  4 23:05:42.800: INFO: Pod "dns-test-c53d9b01-2602-4dab-b009-d428fe83087c" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 23:05:42.8
    STEP: looking for the results for each expected name from probers 01/04/23 23:05:42.803
    Jan  4 23:05:42.813: INFO: DNS probes using dns-test-c53d9b01-2602-4dab-b009-d428fe83087c succeeded

    STEP: deleting the pod 01/04/23 23:05:42.813
    STEP: changing the externalName to bar.example.com 01/04/23 23:05:42.838
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
     01/04/23 23:05:42.851
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
     01/04/23 23:05:42.851
    STEP: creating a second pod to probe DNS 01/04/23 23:05:42.851
    STEP: submitting the pod to kubernetes 01/04/23 23:05:42.851
    Jan  4 23:05:42.858: INFO: Waiting up to 15m0s for pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2" in namespace "dns-5961" to be "running"
    Jan  4 23:05:42.863: INFO: Pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144243ms
    Jan  4 23:05:44.867: INFO: Pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008657297s
    Jan  4 23:05:46.878: INFO: Pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.020277603s
    Jan  4 23:05:46.878: INFO: Pod "dns-test-182fee5a-e777-4028-9001-aca242e025e2" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 23:05:46.878
    STEP: looking for the results for each expected name from probers 01/04/23 23:05:46.883
    Jan  4 23:05:46.919: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:05:46.952: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:05:46.952: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

    Jan  4 23:05:51.957: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:05:51.960: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:05:51.960: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

    Jan  4 23:05:56.956: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:05:56.959: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:05:56.959: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

    Jan  4 23:06:01.956: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:06:01.963: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:06:01.963: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

    Jan  4 23:06:06.956: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:06:06.959: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:06:06.959: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

    Jan  4 23:06:11.957: INFO: File wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:06:11.960: INFO: File jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local from pod  dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  4 23:06:11.960: INFO: Lookups using dns-5961/dns-test-182fee5a-e777-4028-9001-aca242e025e2 failed for: [wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local]

    Jan  4 23:06:16.960: INFO: DNS probes using dns-test-182fee5a-e777-4028-9001-aca242e025e2 succeeded

    STEP: deleting the pod 01/04/23 23:06:16.96
    STEP: changing the service to type=ClusterIP 01/04/23 23:06:16.971
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
     01/04/23 23:06:16.998
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5961.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5961.svc.cluster.local; sleep 1; done
     01/04/23 23:06:16.998
    STEP: creating a third pod to probe DNS 01/04/23 23:06:16.998
    STEP: submitting the pod to kubernetes 01/04/23 23:06:17.004
    Jan  4 23:06:17.015: INFO: Waiting up to 15m0s for pod "dns-test-6074411e-7562-49d9-b726-48d68a290d1e" in namespace "dns-5961" to be "running"
    Jan  4 23:06:17.019: INFO: Pod "dns-test-6074411e-7562-49d9-b726-48d68a290d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.911941ms
    Jan  4 23:06:19.023: INFO: Pod "dns-test-6074411e-7562-49d9-b726-48d68a290d1e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008466123s
    Jan  4 23:06:19.023: INFO: Pod "dns-test-6074411e-7562-49d9-b726-48d68a290d1e" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 23:06:19.023
    STEP: looking for the results for each expected name from probers 01/04/23 23:06:19.026
    Jan  4 23:06:19.041: INFO: DNS probes using dns-test-6074411e-7562-49d9-b726-48d68a290d1e succeeded

    STEP: deleting the pod 01/04/23 23:06:19.041
    STEP: deleting the test externalName service 01/04/23 23:06:19.057
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:06:19.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5961" for this suite. 01/04/23 23:06:19.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:06:19.111
Jan  4 23:06:19.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 23:06:19.112
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:06:19.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:06:19.141
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/04/23 23:06:19.152
STEP: watching for the Service to be added 01/04/23 23:06:19.162
Jan  4 23:06:19.164: INFO: Found Service test-service-cpdfs in namespace services-46 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan  4 23:06:19.164: INFO: Service test-service-cpdfs created
STEP: Getting /status 01/04/23 23:06:19.164
Jan  4 23:06:19.169: INFO: Service test-service-cpdfs has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/04/23 23:06:19.169
STEP: watching for the Service to be patched 01/04/23 23:06:19.98
Jan  4 23:06:19.983: INFO: observed Service test-service-cpdfs in namespace services-46 with annotations: map[] & LoadBalancer: {[]}
Jan  4 23:06:19.983: INFO: Found Service test-service-cpdfs in namespace services-46 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan  4 23:06:19.983: INFO: Service test-service-cpdfs has service status patched
STEP: updating the ServiceStatus 01/04/23 23:06:19.983
Jan  4 23:06:19.998: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/04/23 23:06:19.998
Jan  4 23:06:19.999: INFO: Observed Service test-service-cpdfs in namespace services-46 with annotations: map[] & Conditions: {[]}
Jan  4 23:06:19.999: INFO: Observed event: &Service{ObjectMeta:{test-service-cpdfs  services-46  630d509b-a591-4604-9358-b22a01b964cb 55541 0 2023-01-04 23:06:19 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-04 23:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-04 23:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.68.125,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.68.125],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan  4 23:06:20.000: INFO: Found Service test-service-cpdfs in namespace services-46 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  4 23:06:20.000: INFO: Service test-service-cpdfs has service status updated
STEP: patching the service 01/04/23 23:06:20
STEP: watching for the Service to be patched 01/04/23 23:06:20.038
Jan  4 23:06:20.043: INFO: observed Service test-service-cpdfs in namespace services-46 with labels: map[test-service-static:true]
Jan  4 23:06:20.043: INFO: observed Service test-service-cpdfs in namespace services-46 with labels: map[test-service-static:true]
Jan  4 23:06:20.043: INFO: observed Service test-service-cpdfs in namespace services-46 with labels: map[test-service-static:true]
Jan  4 23:06:20.043: INFO: Found Service test-service-cpdfs in namespace services-46 with labels: map[test-service:patched test-service-static:true]
Jan  4 23:06:20.043: INFO: Service test-service-cpdfs patched
STEP: deleting the service 01/04/23 23:06:20.043
STEP: watching for the Service to be deleted 01/04/23 23:06:20.076
Jan  4 23:06:20.077: INFO: Observed event: ADDED
Jan  4 23:06:20.077: INFO: Observed event: MODIFIED
Jan  4 23:06:20.077: INFO: Observed event: MODIFIED
Jan  4 23:06:20.077: INFO: Observed event: MODIFIED
Jan  4 23:06:20.077: INFO: Found Service test-service-cpdfs in namespace services-46 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan  4 23:06:20.077: INFO: Service test-service-cpdfs deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 23:06:20.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-46" for this suite. 01/04/23 23:06:20.087
------------------------------
• [0.993 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:06:19.111
    Jan  4 23:06:19.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 23:06:19.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:06:19.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:06:19.141
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/04/23 23:06:19.152
    STEP: watching for the Service to be added 01/04/23 23:06:19.162
    Jan  4 23:06:19.164: INFO: Found Service test-service-cpdfs in namespace services-46 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan  4 23:06:19.164: INFO: Service test-service-cpdfs created
    STEP: Getting /status 01/04/23 23:06:19.164
    Jan  4 23:06:19.169: INFO: Service test-service-cpdfs has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/04/23 23:06:19.169
    STEP: watching for the Service to be patched 01/04/23 23:06:19.98
    Jan  4 23:06:19.983: INFO: observed Service test-service-cpdfs in namespace services-46 with annotations: map[] & LoadBalancer: {[]}
    Jan  4 23:06:19.983: INFO: Found Service test-service-cpdfs in namespace services-46 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan  4 23:06:19.983: INFO: Service test-service-cpdfs has service status patched
    STEP: updating the ServiceStatus 01/04/23 23:06:19.983
    Jan  4 23:06:19.998: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/04/23 23:06:19.998
    Jan  4 23:06:19.999: INFO: Observed Service test-service-cpdfs in namespace services-46 with annotations: map[] & Conditions: {[]}
    Jan  4 23:06:19.999: INFO: Observed event: &Service{ObjectMeta:{test-service-cpdfs  services-46  630d509b-a591-4604-9358-b22a01b964cb 55541 0 2023-01-04 23:06:19 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-04 23:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-04 23:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.68.125,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.68.125],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan  4 23:06:20.000: INFO: Found Service test-service-cpdfs in namespace services-46 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  4 23:06:20.000: INFO: Service test-service-cpdfs has service status updated
    STEP: patching the service 01/04/23 23:06:20
    STEP: watching for the Service to be patched 01/04/23 23:06:20.038
    Jan  4 23:06:20.043: INFO: observed Service test-service-cpdfs in namespace services-46 with labels: map[test-service-static:true]
    Jan  4 23:06:20.043: INFO: observed Service test-service-cpdfs in namespace services-46 with labels: map[test-service-static:true]
    Jan  4 23:06:20.043: INFO: observed Service test-service-cpdfs in namespace services-46 with labels: map[test-service-static:true]
    Jan  4 23:06:20.043: INFO: Found Service test-service-cpdfs in namespace services-46 with labels: map[test-service:patched test-service-static:true]
    Jan  4 23:06:20.043: INFO: Service test-service-cpdfs patched
    STEP: deleting the service 01/04/23 23:06:20.043
    STEP: watching for the Service to be deleted 01/04/23 23:06:20.076
    Jan  4 23:06:20.077: INFO: Observed event: ADDED
    Jan  4 23:06:20.077: INFO: Observed event: MODIFIED
    Jan  4 23:06:20.077: INFO: Observed event: MODIFIED
    Jan  4 23:06:20.077: INFO: Observed event: MODIFIED
    Jan  4 23:06:20.077: INFO: Found Service test-service-cpdfs in namespace services-46 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan  4 23:06:20.077: INFO: Service test-service-cpdfs deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:06:20.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-46" for this suite. 01/04/23 23:06:20.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:06:20.107
Jan  4 23:06:20.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename init-container 01/04/23 23:06:20.108
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:06:20.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:06:20.14
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/04/23 23:06:20.144
Jan  4 23:06:20.144: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:06:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3247" for this suite. 01/04/23 23:06:23.739
------------------------------
• [3.637 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:06:20.107
    Jan  4 23:06:20.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename init-container 01/04/23 23:06:20.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:06:20.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:06:20.14
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/04/23 23:06:20.144
    Jan  4 23:06:20.144: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:06:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3247" for this suite. 01/04/23 23:06:23.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:06:23.754
Jan  4 23:06:23.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 23:06:23.755
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:06:23.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:06:23.771
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/04/23 23:06:23.774
Jan  4 23:06:23.774: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan  4 23:06:23.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
Jan  4 23:06:24.847: INFO: stderr: ""
Jan  4 23:06:24.847: INFO: stdout: "service/agnhost-replica created\n"
Jan  4 23:06:24.848: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan  4 23:06:24.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
Jan  4 23:06:25.066: INFO: stderr: ""
Jan  4 23:06:25.066: INFO: stdout: "service/agnhost-primary created\n"
Jan  4 23:06:25.066: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan  4 23:06:25.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
Jan  4 23:06:25.892: INFO: stderr: ""
Jan  4 23:06:25.892: INFO: stdout: "service/frontend created\n"
Jan  4 23:06:25.892: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan  4 23:06:25.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
Jan  4 23:06:26.097: INFO: stderr: ""
Jan  4 23:06:26.097: INFO: stdout: "deployment.apps/frontend created\n"
Jan  4 23:06:26.097: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  4 23:06:26.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
Jan  4 23:06:27.008: INFO: stderr: ""
Jan  4 23:06:27.008: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan  4 23:06:27.008: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  4 23:06:27.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
Jan  4 23:06:27.196: INFO: stderr: ""
Jan  4 23:06:27.196: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/04/23 23:06:27.196
Jan  4 23:06:27.196: INFO: Waiting for all frontend pods to be Running.
Jan  4 23:06:32.247: INFO: Waiting for frontend to serve content.
Jan  4 23:06:32.260: INFO: Trying to add a new entry to the guestbook.
Jan  4 23:06:32.272: INFO: Verifying that added entry can be retrieved.
Jan  4 23:06:32.280: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 01/04/23 23:06:37.287
Jan  4 23:06:37.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
Jan  4 23:06:37.444: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 23:06:37.444: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/04/23 23:06:37.444
Jan  4 23:06:37.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
Jan  4 23:06:37.643: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 23:06:37.643: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/04/23 23:06:37.643
Jan  4 23:06:37.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
Jan  4 23:06:37.746: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 23:06:37.746: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/04/23 23:06:37.746
Jan  4 23:06:37.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
Jan  4 23:06:37.845: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 23:06:37.845: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/04/23 23:06:37.846
Jan  4 23:06:37.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
Jan  4 23:06:38.034: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 23:06:38.034: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/04/23 23:06:38.034
Jan  4 23:06:38.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
Jan  4 23:06:38.212: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 23:06:38.212: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 23:06:38.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4118" for this suite. 01/04/23 23:06:38.216
------------------------------
• [SLOW TEST] [14.468 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:06:23.754
    Jan  4 23:06:23.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 23:06:23.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:06:23.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:06:23.771
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/04/23 23:06:23.774
    Jan  4 23:06:23.774: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan  4 23:06:23.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
    Jan  4 23:06:24.847: INFO: stderr: ""
    Jan  4 23:06:24.847: INFO: stdout: "service/agnhost-replica created\n"
    Jan  4 23:06:24.848: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan  4 23:06:24.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
    Jan  4 23:06:25.066: INFO: stderr: ""
    Jan  4 23:06:25.066: INFO: stdout: "service/agnhost-primary created\n"
    Jan  4 23:06:25.066: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan  4 23:06:25.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
    Jan  4 23:06:25.892: INFO: stderr: ""
    Jan  4 23:06:25.892: INFO: stdout: "service/frontend created\n"
    Jan  4 23:06:25.892: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan  4 23:06:25.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
    Jan  4 23:06:26.097: INFO: stderr: ""
    Jan  4 23:06:26.097: INFO: stdout: "deployment.apps/frontend created\n"
    Jan  4 23:06:26.097: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  4 23:06:26.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
    Jan  4 23:06:27.008: INFO: stderr: ""
    Jan  4 23:06:27.008: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan  4 23:06:27.008: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  4 23:06:27.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 create -f -'
    Jan  4 23:06:27.196: INFO: stderr: ""
    Jan  4 23:06:27.196: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/04/23 23:06:27.196
    Jan  4 23:06:27.196: INFO: Waiting for all frontend pods to be Running.
    Jan  4 23:06:32.247: INFO: Waiting for frontend to serve content.
    Jan  4 23:06:32.260: INFO: Trying to add a new entry to the guestbook.
    Jan  4 23:06:32.272: INFO: Verifying that added entry can be retrieved.
    Jan  4 23:06:32.280: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 01/04/23 23:06:37.287
    Jan  4 23:06:37.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
    Jan  4 23:06:37.444: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 23:06:37.444: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/04/23 23:06:37.444
    Jan  4 23:06:37.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
    Jan  4 23:06:37.643: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 23:06:37.643: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/04/23 23:06:37.643
    Jan  4 23:06:37.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
    Jan  4 23:06:37.746: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 23:06:37.746: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/04/23 23:06:37.746
    Jan  4 23:06:37.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
    Jan  4 23:06:37.845: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 23:06:37.845: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/04/23 23:06:37.846
    Jan  4 23:06:37.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
    Jan  4 23:06:38.034: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 23:06:38.034: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/04/23 23:06:38.034
    Jan  4 23:06:38.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-4118 delete --grace-period=0 --force -f -'
    Jan  4 23:06:38.212: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 23:06:38.212: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:06:38.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4118" for this suite. 01/04/23 23:06:38.216
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:06:38.224
Jan  4 23:06:38.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename taint-multiple-pods 01/04/23 23:06:38.225
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:06:38.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:06:38.266
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan  4 23:06:38.281: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  4 23:07:38.346: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan  4 23:07:38.355: INFO: Starting informer...
STEP: Starting pods... 01/04/23 23:07:38.355
Jan  4 23:07:38.572: INFO: Pod1 is running on ip-172-31-13-117.us-east-2.compute.internal. Tainting Node
Jan  4 23:07:38.781: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8011" to be "running"
Jan  4 23:07:38.784: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876711ms
Jan  4 23:07:40.788: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007069034s
Jan  4 23:07:40.788: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan  4 23:07:40.789: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8011" to be "running"
Jan  4 23:07:40.791: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.802867ms
Jan  4 23:07:40.791: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan  4 23:07:40.791: INFO: Pod2 is running on ip-172-31-13-117.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 01/04/23 23:07:40.791
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/04/23 23:07:40.813
STEP: Waiting for Pod1 and Pod2 to be deleted 01/04/23 23:07:40.825
Jan  4 23:07:46.762: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan  4 23:08:06.531: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/04/23 23:08:06.55
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:06.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-8011" for this suite. 01/04/23 23:08:06.562
------------------------------
• [SLOW TEST] [88.355 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:06:38.224
    Jan  4 23:06:38.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename taint-multiple-pods 01/04/23 23:06:38.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:06:38.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:06:38.266
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan  4 23:06:38.281: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  4 23:07:38.346: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan  4 23:07:38.355: INFO: Starting informer...
    STEP: Starting pods... 01/04/23 23:07:38.355
    Jan  4 23:07:38.572: INFO: Pod1 is running on ip-172-31-13-117.us-east-2.compute.internal. Tainting Node
    Jan  4 23:07:38.781: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8011" to be "running"
    Jan  4 23:07:38.784: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876711ms
    Jan  4 23:07:40.788: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007069034s
    Jan  4 23:07:40.788: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan  4 23:07:40.789: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8011" to be "running"
    Jan  4 23:07:40.791: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.802867ms
    Jan  4 23:07:40.791: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan  4 23:07:40.791: INFO: Pod2 is running on ip-172-31-13-117.us-east-2.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 01/04/23 23:07:40.791
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/04/23 23:07:40.813
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/04/23 23:07:40.825
    Jan  4 23:07:46.762: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan  4 23:08:06.531: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/04/23 23:08:06.55
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:06.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-8011" for this suite. 01/04/23 23:08:06.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:06.584
Jan  4 23:08:06.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-probe 01/04/23 23:08:06.585
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:06.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:06.621
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan  4 23:08:06.650: INFO: Waiting up to 5m0s for pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508" in namespace "container-probe-7505" to be "running and ready"
Jan  4 23:08:06.660: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Pending", Reason="", readiness=false. Elapsed: 10.064379ms
Jan  4 23:08:06.661: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:08:08.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 2.014246207s
Jan  4 23:08:08.665: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:11.127: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 4.476913619s
Jan  4 23:08:11.127: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:12.894: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 6.243151362s
Jan  4 23:08:12.895: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:14.664: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 8.013349152s
Jan  4 23:08:14.664: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:16.937: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 10.286162498s
Jan  4 23:08:16.937: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:18.666: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 12.015123591s
Jan  4 23:08:18.666: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:20.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 14.015013893s
Jan  4 23:08:20.665: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:22.668: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 16.017123709s
Jan  4 23:08:22.668: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:24.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 18.01412884s
Jan  4 23:08:24.665: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:26.670: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 20.019232153s
Jan  4 23:08:26.670: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
Jan  4 23:08:28.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=true. Elapsed: 22.014703197s
Jan  4 23:08:28.665: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = true)
Jan  4 23:08:28.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508" satisfied condition "running and ready"
Jan  4 23:08:28.668: INFO: Container started at 2023-01-04 23:08:07 +0000 UTC, pod became ready at 2023-01-04 23:08:26 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:28.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7505" for this suite. 01/04/23 23:08:28.673
------------------------------
• [SLOW TEST] [22.094 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:06.584
    Jan  4 23:08:06.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-probe 01/04/23 23:08:06.585
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:06.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:06.621
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan  4 23:08:06.650: INFO: Waiting up to 5m0s for pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508" in namespace "container-probe-7505" to be "running and ready"
    Jan  4 23:08:06.660: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Pending", Reason="", readiness=false. Elapsed: 10.064379ms
    Jan  4 23:08:06.661: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:08:08.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 2.014246207s
    Jan  4 23:08:08.665: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:11.127: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 4.476913619s
    Jan  4 23:08:11.127: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:12.894: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 6.243151362s
    Jan  4 23:08:12.895: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:14.664: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 8.013349152s
    Jan  4 23:08:14.664: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:16.937: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 10.286162498s
    Jan  4 23:08:16.937: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:18.666: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 12.015123591s
    Jan  4 23:08:18.666: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:20.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 14.015013893s
    Jan  4 23:08:20.665: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:22.668: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 16.017123709s
    Jan  4 23:08:22.668: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:24.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 18.01412884s
    Jan  4 23:08:24.665: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:26.670: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=false. Elapsed: 20.019232153s
    Jan  4 23:08:26.670: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = false)
    Jan  4 23:08:28.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508": Phase="Running", Reason="", readiness=true. Elapsed: 22.014703197s
    Jan  4 23:08:28.665: INFO: The phase of Pod test-webserver-14827c5a-ba1c-407f-9a12-00881de66508 is Running (Ready = true)
    Jan  4 23:08:28.665: INFO: Pod "test-webserver-14827c5a-ba1c-407f-9a12-00881de66508" satisfied condition "running and ready"
    Jan  4 23:08:28.668: INFO: Container started at 2023-01-04 23:08:07 +0000 UTC, pod became ready at 2023-01-04 23:08:26 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:28.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7505" for this suite. 01/04/23 23:08:28.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:28.678
Jan  4 23:08:28.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 23:08:28.679
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:28.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:28.703
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/04/23 23:08:28.705
STEP: submitting the pod to kubernetes 01/04/23 23:08:28.706
STEP: verifying QOS class is set on the pod 01/04/23 23:08:28.715
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:28.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6496" for this suite. 01/04/23 23:08:28.727
------------------------------
• [0.063 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:28.678
    Jan  4 23:08:28.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 23:08:28.679
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:28.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:28.703
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/04/23 23:08:28.705
    STEP: submitting the pod to kubernetes 01/04/23 23:08:28.706
    STEP: verifying QOS class is set on the pod 01/04/23 23:08:28.715
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:28.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6496" for this suite. 01/04/23 23:08:28.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:28.745
Jan  4 23:08:28.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pod-network-test 01/04/23 23:08:28.746
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:28.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:28.769
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9129 01/04/23 23:08:28.773
STEP: creating a selector 01/04/23 23:08:28.773
STEP: Creating the service pods in kubernetes 01/04/23 23:08:28.773
Jan  4 23:08:28.773: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  4 23:08:28.811: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9129" to be "running and ready"
Jan  4 23:08:28.816: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.097856ms
Jan  4 23:08:28.817: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:08:30.821: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009387748s
Jan  4 23:08:30.821: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 23:08:32.820: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008715382s
Jan  4 23:08:32.820: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 23:08:34.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.160786108s
Jan  4 23:08:34.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 23:08:36.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023659175s
Jan  4 23:08:36.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 23:08:38.825: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013774243s
Jan  4 23:08:38.825: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  4 23:08:40.822: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010982091s
Jan  4 23:08:40.822: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  4 23:08:40.822: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  4 23:08:40.825: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9129" to be "running and ready"
Jan  4 23:08:40.832: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.097523ms
Jan  4 23:08:40.832: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  4 23:08:40.832: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  4 23:08:40.835: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9129" to be "running and ready"
Jan  4 23:08:40.838: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.468271ms
Jan  4 23:08:40.838: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  4 23:08:40.838: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan  4 23:08:40.840: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9129" to be "running and ready"
Jan  4 23:08:40.843: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.726749ms
Jan  4 23:08:40.843: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jan  4 23:08:43.436: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.595973404s
Jan  4 23:08:43.436: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jan  4 23:08:44.846: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.005945433s
Jan  4 23:08:44.846: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jan  4 23:08:46.848: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 6.007949792s
Jan  4 23:08:46.848: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jan  4 23:08:48.850: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 8.009411657s
Jan  4 23:08:48.850: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jan  4 23:08:50.849: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 10.008521912s
Jan  4 23:08:50.849: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan  4 23:08:50.849: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 01/04/23 23:08:50.852
Jan  4 23:08:50.858: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9129" to be "running"
Jan  4 23:08:50.870: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.295604ms
Jan  4 23:08:52.874: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015634251s
Jan  4 23:08:52.874: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  4 23:08:52.876: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan  4 23:08:52.876: INFO: Breadth first check of 10.42.0.101 on host 172.31.11.54...
Jan  4 23:08:52.878: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.41:9080/dial?request=hostname&protocol=udp&host=10.42.0.101&port=8081&tries=1'] Namespace:pod-network-test-9129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:08:52.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:08:52.879: INFO: ExecWithOptions: Clientset creation
Jan  4 23:08:52.879: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.0.101%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  4 23:08:52.965: INFO: Waiting for responses: map[]
Jan  4 23:08:52.965: INFO: reached 10.42.0.101 after 0/1 tries
Jan  4 23:08:52.965: INFO: Breadth first check of 10.42.3.40 on host 172.31.13.117...
Jan  4 23:08:52.969: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.41:9080/dial?request=hostname&protocol=udp&host=10.42.3.40&port=8081&tries=1'] Namespace:pod-network-test-9129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:08:52.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:08:52.970: INFO: ExecWithOptions: Clientset creation
Jan  4 23:08:52.970: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.3.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  4 23:08:53.069: INFO: Waiting for responses: map[]
Jan  4 23:08:53.069: INFO: reached 10.42.3.40 after 0/1 tries
Jan  4 23:08:53.069: INFO: Breadth first check of 10.42.1.102 on host 172.31.3.240...
Jan  4 23:08:53.072: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.41:9080/dial?request=hostname&protocol=udp&host=10.42.1.102&port=8081&tries=1'] Namespace:pod-network-test-9129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:08:53.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:08:53.073: INFO: ExecWithOptions: Clientset creation
Jan  4 23:08:53.073: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.1.102%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  4 23:08:53.153: INFO: Waiting for responses: map[]
Jan  4 23:08:53.153: INFO: reached 10.42.1.102 after 0/1 tries
Jan  4 23:08:53.153: INFO: Breadth first check of 10.42.2.117 on host 172.31.9.62...
Jan  4 23:08:53.156: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.41:9080/dial?request=hostname&protocol=udp&host=10.42.2.117&port=8081&tries=1'] Namespace:pod-network-test-9129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:08:53.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:08:53.157: INFO: ExecWithOptions: Clientset creation
Jan  4 23:08:53.157: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.2.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  4 23:08:53.226: INFO: Waiting for responses: map[]
Jan  4 23:08:53.226: INFO: reached 10.42.2.117 after 0/1 tries
Jan  4 23:08:53.226: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:53.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9129" for this suite. 01/04/23 23:08:53.23
------------------------------
• [SLOW TEST] [24.493 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:28.745
    Jan  4 23:08:28.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pod-network-test 01/04/23 23:08:28.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:28.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:28.769
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9129 01/04/23 23:08:28.773
    STEP: creating a selector 01/04/23 23:08:28.773
    STEP: Creating the service pods in kubernetes 01/04/23 23:08:28.773
    Jan  4 23:08:28.773: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  4 23:08:28.811: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9129" to be "running and ready"
    Jan  4 23:08:28.816: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.097856ms
    Jan  4 23:08:28.817: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:08:30.821: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009387748s
    Jan  4 23:08:30.821: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 23:08:32.820: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008715382s
    Jan  4 23:08:32.820: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 23:08:34.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.160786108s
    Jan  4 23:08:34.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 23:08:36.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023659175s
    Jan  4 23:08:36.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 23:08:38.825: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013774243s
    Jan  4 23:08:38.825: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  4 23:08:40.822: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010982091s
    Jan  4 23:08:40.822: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  4 23:08:40.822: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  4 23:08:40.825: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9129" to be "running and ready"
    Jan  4 23:08:40.832: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.097523ms
    Jan  4 23:08:40.832: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  4 23:08:40.832: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  4 23:08:40.835: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9129" to be "running and ready"
    Jan  4 23:08:40.838: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.468271ms
    Jan  4 23:08:40.838: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  4 23:08:40.838: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan  4 23:08:40.840: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9129" to be "running and ready"
    Jan  4 23:08:40.843: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.726749ms
    Jan  4 23:08:40.843: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jan  4 23:08:43.436: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.595973404s
    Jan  4 23:08:43.436: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jan  4 23:08:44.846: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.005945433s
    Jan  4 23:08:44.846: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jan  4 23:08:46.848: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 6.007949792s
    Jan  4 23:08:46.848: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jan  4 23:08:48.850: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 8.009411657s
    Jan  4 23:08:48.850: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jan  4 23:08:50.849: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 10.008521912s
    Jan  4 23:08:50.849: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan  4 23:08:50.849: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 01/04/23 23:08:50.852
    Jan  4 23:08:50.858: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9129" to be "running"
    Jan  4 23:08:50.870: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.295604ms
    Jan  4 23:08:52.874: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015634251s
    Jan  4 23:08:52.874: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  4 23:08:52.876: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Jan  4 23:08:52.876: INFO: Breadth first check of 10.42.0.101 on host 172.31.11.54...
    Jan  4 23:08:52.878: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.41:9080/dial?request=hostname&protocol=udp&host=10.42.0.101&port=8081&tries=1'] Namespace:pod-network-test-9129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:08:52.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:08:52.879: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:08:52.879: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.0.101%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  4 23:08:52.965: INFO: Waiting for responses: map[]
    Jan  4 23:08:52.965: INFO: reached 10.42.0.101 after 0/1 tries
    Jan  4 23:08:52.965: INFO: Breadth first check of 10.42.3.40 on host 172.31.13.117...
    Jan  4 23:08:52.969: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.41:9080/dial?request=hostname&protocol=udp&host=10.42.3.40&port=8081&tries=1'] Namespace:pod-network-test-9129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:08:52.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:08:52.970: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:08:52.970: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.3.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  4 23:08:53.069: INFO: Waiting for responses: map[]
    Jan  4 23:08:53.069: INFO: reached 10.42.3.40 after 0/1 tries
    Jan  4 23:08:53.069: INFO: Breadth first check of 10.42.1.102 on host 172.31.3.240...
    Jan  4 23:08:53.072: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.41:9080/dial?request=hostname&protocol=udp&host=10.42.1.102&port=8081&tries=1'] Namespace:pod-network-test-9129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:08:53.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:08:53.073: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:08:53.073: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.1.102%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  4 23:08:53.153: INFO: Waiting for responses: map[]
    Jan  4 23:08:53.153: INFO: reached 10.42.1.102 after 0/1 tries
    Jan  4 23:08:53.153: INFO: Breadth first check of 10.42.2.117 on host 172.31.9.62...
    Jan  4 23:08:53.156: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.41:9080/dial?request=hostname&protocol=udp&host=10.42.2.117&port=8081&tries=1'] Namespace:pod-network-test-9129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:08:53.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:08:53.157: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:08:53.157: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.2.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  4 23:08:53.226: INFO: Waiting for responses: map[]
    Jan  4 23:08:53.226: INFO: reached 10.42.2.117 after 0/1 tries
    Jan  4 23:08:53.226: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:53.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9129" for this suite. 01/04/23 23:08:53.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:53.24
Jan  4 23:08:53.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename runtimeclass 01/04/23 23:08:53.241
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.265
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan  4 23:08:53.285: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6256 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:53.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6256" for this suite. 01/04/23 23:08:53.306
------------------------------
• [0.083 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:53.24
    Jan  4 23:08:53.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename runtimeclass 01/04/23 23:08:53.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.265
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan  4 23:08:53.285: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6256 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:53.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6256" for this suite. 01/04/23 23:08:53.306
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:53.324
Jan  4 23:08:53.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename namespaces 01/04/23 23:08:53.325
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.357
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-2dsfv" 01/04/23 23:08:53.359
Jan  4 23:08:53.389: INFO: Namespace "e2e-ns-2dsfv-2280" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-2dsfv-2280" 01/04/23 23:08:53.389
Jan  4 23:08:53.409: INFO: Namespace "e2e-ns-2dsfv-2280" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-2dsfv-2280" 01/04/23 23:08:53.409
Jan  4 23:08:53.432: INFO: Namespace "e2e-ns-2dsfv-2280" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:53.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7436" for this suite. 01/04/23 23:08:53.436
STEP: Destroying namespace "e2e-ns-2dsfv-2280" for this suite. 01/04/23 23:08:53.447
------------------------------
• [0.137 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:53.324
    Jan  4 23:08:53.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename namespaces 01/04/23 23:08:53.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.357
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-2dsfv" 01/04/23 23:08:53.359
    Jan  4 23:08:53.389: INFO: Namespace "e2e-ns-2dsfv-2280" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-2dsfv-2280" 01/04/23 23:08:53.389
    Jan  4 23:08:53.409: INFO: Namespace "e2e-ns-2dsfv-2280" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-2dsfv-2280" 01/04/23 23:08:53.409
    Jan  4 23:08:53.432: INFO: Namespace "e2e-ns-2dsfv-2280" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:53.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7436" for this suite. 01/04/23 23:08:53.436
    STEP: Destroying namespace "e2e-ns-2dsfv-2280" for this suite. 01/04/23 23:08:53.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:53.465
Jan  4 23:08:53.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 23:08:53.466
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.502
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-1ded8e64-c13b-4902-83d4-01c2804a7215 01/04/23 23:08:53.504
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:53.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-497" for this suite. 01/04/23 23:08:53.512
------------------------------
• [0.058 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:53.465
    Jan  4 23:08:53.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 23:08:53.466
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.502
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-1ded8e64-c13b-4902-83d4-01c2804a7215 01/04/23 23:08:53.504
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:53.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-497" for this suite. 01/04/23 23:08:53.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:53.528
Jan  4 23:08:53.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename runtimeclass 01/04/23 23:08:53.529
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.545
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/04/23 23:08:53.548
STEP: getting /apis/node.k8s.io 01/04/23 23:08:53.55
STEP: getting /apis/node.k8s.io/v1 01/04/23 23:08:53.555
STEP: creating 01/04/23 23:08:53.556
STEP: watching 01/04/23 23:08:53.57
Jan  4 23:08:53.570: INFO: starting watch
STEP: getting 01/04/23 23:08:53.574
STEP: listing 01/04/23 23:08:53.577
STEP: patching 01/04/23 23:08:53.58
STEP: updating 01/04/23 23:08:53.584
Jan  4 23:08:53.589: INFO: waiting for watch events with expected annotations
STEP: deleting 01/04/23 23:08:53.589
STEP: deleting a collection 01/04/23 23:08:53.598
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:53.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7166" for this suite. 01/04/23 23:08:53.621
------------------------------
• [0.102 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:53.528
    Jan  4 23:08:53.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename runtimeclass 01/04/23 23:08:53.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.545
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/04/23 23:08:53.548
    STEP: getting /apis/node.k8s.io 01/04/23 23:08:53.55
    STEP: getting /apis/node.k8s.io/v1 01/04/23 23:08:53.555
    STEP: creating 01/04/23 23:08:53.556
    STEP: watching 01/04/23 23:08:53.57
    Jan  4 23:08:53.570: INFO: starting watch
    STEP: getting 01/04/23 23:08:53.574
    STEP: listing 01/04/23 23:08:53.577
    STEP: patching 01/04/23 23:08:53.58
    STEP: updating 01/04/23 23:08:53.584
    Jan  4 23:08:53.589: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/04/23 23:08:53.589
    STEP: deleting a collection 01/04/23 23:08:53.598
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:53.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7166" for this suite. 01/04/23 23:08:53.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:53.638
Jan  4 23:08:53.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:08:53.639
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.658
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-ed8a0797-ee0f-4302-b963-f74802bc6484 01/04/23 23:08:53.661
STEP: Creating a pod to test consume secrets 01/04/23 23:08:53.667
Jan  4 23:08:53.678: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0" in namespace "projected-7614" to be "Succeeded or Failed"
Jan  4 23:08:53.683: INFO: Pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.556179ms
Jan  4 23:08:55.687: INFO: Pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009187324s
Jan  4 23:08:57.687: INFO: Pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009823863s
STEP: Saw pod success 01/04/23 23:08:57.687
Jan  4 23:08:57.687: INFO: Pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0" satisfied condition "Succeeded or Failed"
Jan  4 23:08:57.690: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0 container secret-volume-test: <nil>
STEP: delete the pod 01/04/23 23:08:57.699
Jan  4 23:08:57.708: INFO: Waiting for pod pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0 to disappear
Jan  4 23:08:57.710: INFO: Pod pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  4 23:08:57.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7614" for this suite. 01/04/23 23:08:57.714
------------------------------
• [4.084 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:53.638
    Jan  4 23:08:53.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:08:53.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:53.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:53.658
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-ed8a0797-ee0f-4302-b963-f74802bc6484 01/04/23 23:08:53.661
    STEP: Creating a pod to test consume secrets 01/04/23 23:08:53.667
    Jan  4 23:08:53.678: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0" in namespace "projected-7614" to be "Succeeded or Failed"
    Jan  4 23:08:53.683: INFO: Pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.556179ms
    Jan  4 23:08:55.687: INFO: Pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009187324s
    Jan  4 23:08:57.687: INFO: Pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009823863s
    STEP: Saw pod success 01/04/23 23:08:57.687
    Jan  4 23:08:57.687: INFO: Pod "pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0" satisfied condition "Succeeded or Failed"
    Jan  4 23:08:57.690: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0 container secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 23:08:57.699
    Jan  4 23:08:57.708: INFO: Waiting for pod pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0 to disappear
    Jan  4 23:08:57.710: INFO: Pod pod-projected-secrets-f9c80049-da8c-44f0-b18e-eb07a7ccaba0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:08:57.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7614" for this suite. 01/04/23 23:08:57.714
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:08:57.722
Jan  4 23:08:57.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:08:57.724
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:57.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:57.74
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/04/23 23:08:57.742
Jan  4 23:08:57.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480" in namespace "downward-api-2843" to be "Succeeded or Failed"
Jan  4 23:08:57.756: INFO: Pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480": Phase="Pending", Reason="", readiness=false. Elapsed: 5.472788ms
Jan  4 23:08:59.764: INFO: Pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013559629s
Jan  4 23:09:01.761: INFO: Pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010467831s
STEP: Saw pod success 01/04/23 23:09:01.761
Jan  4 23:09:01.761: INFO: Pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480" satisfied condition "Succeeded or Failed"
Jan  4 23:09:01.766: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480 container client-container: <nil>
STEP: delete the pod 01/04/23 23:09:01.775
Jan  4 23:09:01.792: INFO: Waiting for pod downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480 to disappear
Jan  4 23:09:01.799: INFO: Pod downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 23:09:01.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2843" for this suite. 01/04/23 23:09:01.818
------------------------------
• [4.105 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:08:57.722
    Jan  4 23:08:57.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:08:57.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:08:57.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:08:57.74
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/04/23 23:08:57.742
    Jan  4 23:08:57.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480" in namespace "downward-api-2843" to be "Succeeded or Failed"
    Jan  4 23:08:57.756: INFO: Pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480": Phase="Pending", Reason="", readiness=false. Elapsed: 5.472788ms
    Jan  4 23:08:59.764: INFO: Pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013559629s
    Jan  4 23:09:01.761: INFO: Pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010467831s
    STEP: Saw pod success 01/04/23 23:09:01.761
    Jan  4 23:09:01.761: INFO: Pod "downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480" satisfied condition "Succeeded or Failed"
    Jan  4 23:09:01.766: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480 container client-container: <nil>
    STEP: delete the pod 01/04/23 23:09:01.775
    Jan  4 23:09:01.792: INFO: Waiting for pod downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480 to disappear
    Jan  4 23:09:01.799: INFO: Pod downwardapi-volume-e3bb691c-0739-424b-a95d-3c3967ef7480 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:09:01.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2843" for this suite. 01/04/23 23:09:01.818
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:09:01.828
Jan  4 23:09:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 23:09:01.83
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:09:01.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:09:01.852
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-4235/secret-test-b2a3f90b-55b5-46b3-b932-ce0dd3e78f69 01/04/23 23:09:01.855
STEP: Creating a pod to test consume secrets 01/04/23 23:09:01.865
Jan  4 23:09:01.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191" in namespace "secrets-4235" to be "Succeeded or Failed"
Jan  4 23:09:01.889: INFO: Pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191": Phase="Pending", Reason="", readiness=false. Elapsed: 13.54664ms
Jan  4 23:09:03.892: INFO: Pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016712438s
Jan  4 23:09:05.892: INFO: Pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016679117s
STEP: Saw pod success 01/04/23 23:09:05.892
Jan  4 23:09:05.893: INFO: Pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191" satisfied condition "Succeeded or Failed"
Jan  4 23:09:05.895: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191 container env-test: <nil>
STEP: delete the pod 01/04/23 23:09:05.902
Jan  4 23:09:05.911: INFO: Waiting for pod pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191 to disappear
Jan  4 23:09:05.913: INFO: Pod pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 23:09:05.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4235" for this suite. 01/04/23 23:09:05.917
------------------------------
• [4.094 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:09:01.828
    Jan  4 23:09:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 23:09:01.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:09:01.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:09:01.852
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-4235/secret-test-b2a3f90b-55b5-46b3-b932-ce0dd3e78f69 01/04/23 23:09:01.855
    STEP: Creating a pod to test consume secrets 01/04/23 23:09:01.865
    Jan  4 23:09:01.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191" in namespace "secrets-4235" to be "Succeeded or Failed"
    Jan  4 23:09:01.889: INFO: Pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191": Phase="Pending", Reason="", readiness=false. Elapsed: 13.54664ms
    Jan  4 23:09:03.892: INFO: Pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016712438s
    Jan  4 23:09:05.892: INFO: Pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016679117s
    STEP: Saw pod success 01/04/23 23:09:05.892
    Jan  4 23:09:05.893: INFO: Pod "pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191" satisfied condition "Succeeded or Failed"
    Jan  4 23:09:05.895: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191 container env-test: <nil>
    STEP: delete the pod 01/04/23 23:09:05.902
    Jan  4 23:09:05.911: INFO: Waiting for pod pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191 to disappear
    Jan  4 23:09:05.913: INFO: Pod pod-configmaps-3556abac-d91c-482b-a705-a4484e62c191 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:09:05.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4235" for this suite. 01/04/23 23:09:05.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:09:05.925
Jan  4 23:09:05.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:09:05.926
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:09:05.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:09:05.942
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/04/23 23:09:05.947
STEP: watching for the ServiceAccount to be added 01/04/23 23:09:05.954
STEP: patching the ServiceAccount 01/04/23 23:09:05.958
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/04/23 23:09:05.964
STEP: deleting the ServiceAccount 01/04/23 23:09:05.968
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  4 23:09:05.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3000" for this suite. 01/04/23 23:09:05.986
------------------------------
• [0.066 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:09:05.925
    Jan  4 23:09:05.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:09:05.926
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:09:05.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:09:05.942
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/04/23 23:09:05.947
    STEP: watching for the ServiceAccount to be added 01/04/23 23:09:05.954
    STEP: patching the ServiceAccount 01/04/23 23:09:05.958
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/04/23 23:09:05.964
    STEP: deleting the ServiceAccount 01/04/23 23:09:05.968
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:09:05.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3000" for this suite. 01/04/23 23:09:05.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:09:05.995
Jan  4 23:09:05.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename statefulset 01/04/23 23:09:05.996
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:09:06.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:09:06.028
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-107 01/04/23 23:09:06.031
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-107 01/04/23 23:09:06.037
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-107 01/04/23 23:09:06.044
Jan  4 23:09:06.048: INFO: Found 0 stateful pods, waiting for 1
Jan  4 23:09:16.052: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/04/23 23:09:16.052
Jan  4 23:09:16.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 23:09:16.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 23:09:16.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 23:09:16.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 23:09:16.239: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  4 23:09:26.245: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  4 23:09:26.245: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:09:26.264: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan  4 23:09:26.264: INFO: ss-0  ip-172-31-13-117.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:06 +0000 UTC  }]
Jan  4 23:09:26.264: INFO: 
Jan  4 23:09:26.264: INFO: StatefulSet ss has not reached scale 3, at 1
Jan  4 23:09:27.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992680336s
Jan  4 23:09:28.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987246137s
Jan  4 23:09:29.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982302639s
Jan  4 23:09:30.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978986423s
Jan  4 23:09:31.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973955741s
Jan  4 23:09:32.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970586442s
Jan  4 23:09:33.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967057071s
Jan  4 23:09:34.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96338443s
Jan  4 23:09:35.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.435646ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-107 01/04/23 23:09:36.302
Jan  4 23:09:36.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 23:09:36.493: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  4 23:09:36.493: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 23:09:36.493: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  4 23:09:36.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 23:09:36.687: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  4 23:09:36.687: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 23:09:36.687: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  4 23:09:36.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 23:09:37.093: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  4 23:09:37.093: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 23:09:37.093: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  4 23:09:37.096: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan  4 23:09:47.100: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:09:47.100: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:09:47.100: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/04/23 23:09:47.1
Jan  4 23:09:47.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 23:09:47.238: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 23:09:47.238: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 23:09:47.238: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 23:09:47.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 23:09:47.372: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 23:09:47.372: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 23:09:47.372: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 23:09:47.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 23:09:47.514: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 23:09:47.514: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 23:09:47.514: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 23:09:47.514: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:09:47.517: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan  4 23:09:57.524: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  4 23:09:57.524: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  4 23:09:57.524: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  4 23:09:57.534: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan  4 23:09:57.534: INFO: ss-0  ip-172-31-13-117.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:06 +0000 UTC  }]
Jan  4 23:09:57.534: INFO: ss-1  ip-172-31-9-62.us-east-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  }]
Jan  4 23:09:57.534: INFO: ss-2  ip-172-31-3-240.us-east-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  }]
Jan  4 23:09:57.534: INFO: 
Jan  4 23:09:57.534: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  4 23:09:58.537: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jan  4 23:09:58.537: INFO: ss-2  ip-172-31-3-240.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  }]
Jan  4 23:09:58.537: INFO: 
Jan  4 23:09:58.537: INFO: StatefulSet ss has not reached scale 0, at 1
Jan  4 23:09:59.541: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993030769s
Jan  4 23:10:00.545: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989946483s
Jan  4 23:10:02.081: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985433638s
Jan  4 23:10:03.088: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.447935012s
Jan  4 23:10:04.092: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.443337206s
Jan  4 23:10:05.095: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.438944314s
Jan  4 23:10:06.100: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.4349233s
Jan  4 23:10:07.103: INFO: Verifying statefulset ss doesn't scale past 0 for another 431.337654ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-107 01/04/23 23:10:08.103
Jan  4 23:10:08.107: INFO: Scaling statefulset ss to 0
Jan  4 23:10:08.123: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  4 23:10:08.127: INFO: Deleting all statefulset in ns statefulset-107
Jan  4 23:10:08.130: INFO: Scaling statefulset ss to 0
Jan  4 23:10:08.146: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:10:08.150: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:08.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-107" for this suite. 01/04/23 23:10:08.181
------------------------------
• [SLOW TEST] [62.206 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:09:05.995
    Jan  4 23:09:05.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename statefulset 01/04/23 23:09:05.996
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:09:06.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:09:06.028
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-107 01/04/23 23:09:06.031
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-107 01/04/23 23:09:06.037
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-107 01/04/23 23:09:06.044
    Jan  4 23:09:06.048: INFO: Found 0 stateful pods, waiting for 1
    Jan  4 23:09:16.052: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/04/23 23:09:16.052
    Jan  4 23:09:16.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 23:09:16.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 23:09:16.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 23:09:16.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 23:09:16.239: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  4 23:09:26.245: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  4 23:09:26.245: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:09:26.264: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Jan  4 23:09:26.264: INFO: ss-0  ip-172-31-13-117.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:06 +0000 UTC  }]
    Jan  4 23:09:26.264: INFO: 
    Jan  4 23:09:26.264: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan  4 23:09:27.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992680336s
    Jan  4 23:09:28.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987246137s
    Jan  4 23:09:29.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982302639s
    Jan  4 23:09:30.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978986423s
    Jan  4 23:09:31.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973955741s
    Jan  4 23:09:32.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970586442s
    Jan  4 23:09:33.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967057071s
    Jan  4 23:09:34.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96338443s
    Jan  4 23:09:35.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.435646ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-107 01/04/23 23:09:36.302
    Jan  4 23:09:36.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 23:09:36.493: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  4 23:09:36.493: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 23:09:36.493: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  4 23:09:36.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 23:09:36.687: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  4 23:09:36.687: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 23:09:36.687: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  4 23:09:36.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 23:09:37.093: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  4 23:09:37.093: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 23:09:37.093: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  4 23:09:37.096: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan  4 23:09:47.100: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:09:47.100: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:09:47.100: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/04/23 23:09:47.1
    Jan  4 23:09:47.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 23:09:47.238: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 23:09:47.238: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 23:09:47.238: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 23:09:47.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 23:09:47.372: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 23:09:47.372: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 23:09:47.372: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 23:09:47.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-107 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 23:09:47.514: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 23:09:47.514: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 23:09:47.514: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 23:09:47.514: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:09:47.517: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan  4 23:09:57.524: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  4 23:09:57.524: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  4 23:09:57.524: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  4 23:09:57.534: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Jan  4 23:09:57.534: INFO: ss-0  ip-172-31-13-117.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:06 +0000 UTC  }]
    Jan  4 23:09:57.534: INFO: ss-1  ip-172-31-9-62.us-east-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  }]
    Jan  4 23:09:57.534: INFO: ss-2  ip-172-31-3-240.us-east-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  }]
    Jan  4 23:09:57.534: INFO: 
    Jan  4 23:09:57.534: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan  4 23:09:58.537: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
    Jan  4 23:09:58.537: INFO: ss-2  ip-172-31-3-240.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:09:26 +0000 UTC  }]
    Jan  4 23:09:58.537: INFO: 
    Jan  4 23:09:58.537: INFO: StatefulSet ss has not reached scale 0, at 1
    Jan  4 23:09:59.541: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993030769s
    Jan  4 23:10:00.545: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989946483s
    Jan  4 23:10:02.081: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985433638s
    Jan  4 23:10:03.088: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.447935012s
    Jan  4 23:10:04.092: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.443337206s
    Jan  4 23:10:05.095: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.438944314s
    Jan  4 23:10:06.100: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.4349233s
    Jan  4 23:10:07.103: INFO: Verifying statefulset ss doesn't scale past 0 for another 431.337654ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-107 01/04/23 23:10:08.103
    Jan  4 23:10:08.107: INFO: Scaling statefulset ss to 0
    Jan  4 23:10:08.123: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  4 23:10:08.127: INFO: Deleting all statefulset in ns statefulset-107
    Jan  4 23:10:08.130: INFO: Scaling statefulset ss to 0
    Jan  4 23:10:08.146: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:10:08.150: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:08.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-107" for this suite. 01/04/23 23:10:08.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:08.206
Jan  4 23:10:08.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 23:10:08.21
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:08.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:08.251
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/04/23 23:10:08.255
STEP: Getting a ResourceQuota 01/04/23 23:10:08.262
STEP: Listing all ResourceQuotas with LabelSelector 01/04/23 23:10:08.266
STEP: Patching the ResourceQuota 01/04/23 23:10:08.277
STEP: Deleting a Collection of ResourceQuotas 01/04/23 23:10:08.287
STEP: Verifying the deleted ResourceQuota 01/04/23 23:10:08.305
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:08.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9204" for this suite. 01/04/23 23:10:08.315
------------------------------
• [0.119 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:08.206
    Jan  4 23:10:08.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 23:10:08.21
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:08.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:08.251
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/04/23 23:10:08.255
    STEP: Getting a ResourceQuota 01/04/23 23:10:08.262
    STEP: Listing all ResourceQuotas with LabelSelector 01/04/23 23:10:08.266
    STEP: Patching the ResourceQuota 01/04/23 23:10:08.277
    STEP: Deleting a Collection of ResourceQuotas 01/04/23 23:10:08.287
    STEP: Verifying the deleted ResourceQuota 01/04/23 23:10:08.305
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:08.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9204" for this suite. 01/04/23 23:10:08.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:08.331
Jan  4 23:10:08.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:10:08.338
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:08.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:08.396
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/04/23 23:10:08.398
Jan  4 23:10:08.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981" in namespace "downward-api-8886" to be "Succeeded or Failed"
Jan  4 23:10:08.428: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948663ms
Jan  4 23:10:10.432: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981": Phase="Running", Reason="", readiness=true. Elapsed: 2.010717782s
Jan  4 23:10:12.432: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981": Phase="Running", Reason="", readiness=false. Elapsed: 4.010974752s
Jan  4 23:10:14.432: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010224953s
STEP: Saw pod success 01/04/23 23:10:14.432
Jan  4 23:10:14.432: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981" satisfied condition "Succeeded or Failed"
Jan  4 23:10:14.434: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981 container client-container: <nil>
STEP: delete the pod 01/04/23 23:10:14.443
Jan  4 23:10:14.452: INFO: Waiting for pod downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981 to disappear
Jan  4 23:10:14.455: INFO: Pod downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:14.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8886" for this suite. 01/04/23 23:10:14.459
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:08.331
    Jan  4 23:10:08.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:10:08.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:08.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:08.396
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/04/23 23:10:08.398
    Jan  4 23:10:08.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981" in namespace "downward-api-8886" to be "Succeeded or Failed"
    Jan  4 23:10:08.428: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948663ms
    Jan  4 23:10:10.432: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981": Phase="Running", Reason="", readiness=true. Elapsed: 2.010717782s
    Jan  4 23:10:12.432: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981": Phase="Running", Reason="", readiness=false. Elapsed: 4.010974752s
    Jan  4 23:10:14.432: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010224953s
    STEP: Saw pod success 01/04/23 23:10:14.432
    Jan  4 23:10:14.432: INFO: Pod "downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981" satisfied condition "Succeeded or Failed"
    Jan  4 23:10:14.434: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981 container client-container: <nil>
    STEP: delete the pod 01/04/23 23:10:14.443
    Jan  4 23:10:14.452: INFO: Waiting for pod downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981 to disappear
    Jan  4 23:10:14.455: INFO: Pod downwardapi-volume-0152034b-eed9-4861-af62-3a44239d9981 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:14.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8886" for this suite. 01/04/23 23:10:14.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:14.466
Jan  4 23:10:14.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 23:10:14.467
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:14.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:14.485
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-16aec6cf-2bd0-4099-9ba5-745f17179761 01/04/23 23:10:14.487
STEP: Creating a pod to test consume secrets 01/04/23 23:10:14.492
Jan  4 23:10:14.504: INFO: Waiting up to 5m0s for pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b" in namespace "secrets-1949" to be "Succeeded or Failed"
Jan  4 23:10:14.509: INFO: Pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233942ms
Jan  4 23:10:16.513: INFO: Pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007431899s
Jan  4 23:10:18.516: INFO: Pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01067662s
STEP: Saw pod success 01/04/23 23:10:18.516
Jan  4 23:10:18.516: INFO: Pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b" satisfied condition "Succeeded or Failed"
Jan  4 23:10:18.519: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b container secret-volume-test: <nil>
STEP: delete the pod 01/04/23 23:10:18.525
Jan  4 23:10:18.534: INFO: Waiting for pod pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b to disappear
Jan  4 23:10:18.536: INFO: Pod pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:18.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1949" for this suite. 01/04/23 23:10:18.54
------------------------------
• [4.079 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:14.466
    Jan  4 23:10:14.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 23:10:14.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:14.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:14.485
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-16aec6cf-2bd0-4099-9ba5-745f17179761 01/04/23 23:10:14.487
    STEP: Creating a pod to test consume secrets 01/04/23 23:10:14.492
    Jan  4 23:10:14.504: INFO: Waiting up to 5m0s for pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b" in namespace "secrets-1949" to be "Succeeded or Failed"
    Jan  4 23:10:14.509: INFO: Pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233942ms
    Jan  4 23:10:16.513: INFO: Pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007431899s
    Jan  4 23:10:18.516: INFO: Pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01067662s
    STEP: Saw pod success 01/04/23 23:10:18.516
    Jan  4 23:10:18.516: INFO: Pod "pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b" satisfied condition "Succeeded or Failed"
    Jan  4 23:10:18.519: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b container secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 23:10:18.525
    Jan  4 23:10:18.534: INFO: Waiting for pod pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b to disappear
    Jan  4 23:10:18.536: INFO: Pod pod-secrets-fc3f7ad5-39c6-4682-94ff-0474b2ff3a2b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:18.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1949" for this suite. 01/04/23 23:10:18.54
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:18.547
Jan  4 23:10:18.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:10:18.547
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:18.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:18.564
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-a9cba0bd-ebc5-475f-b037-7398ed21b5af 01/04/23 23:10:18.58
STEP: Creating secret with name s-test-opt-upd-f08978f0-9e25-4f77-a76d-cc7a3df3ccf1 01/04/23 23:10:18.585
STEP: Creating the pod 01/04/23 23:10:18.597
Jan  4 23:10:18.609: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5" in namespace "projected-1866" to be "running and ready"
Jan  4 23:10:18.615: INFO: Pod "pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605795ms
Jan  4 23:10:18.615: INFO: The phase of Pod pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:10:20.621: INFO: Pod "pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011103505s
Jan  4 23:10:20.621: INFO: The phase of Pod pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5 is Running (Ready = true)
Jan  4 23:10:20.621: INFO: Pod "pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-a9cba0bd-ebc5-475f-b037-7398ed21b5af 01/04/23 23:10:20.64
STEP: Updating secret s-test-opt-upd-f08978f0-9e25-4f77-a76d-cc7a3df3ccf1 01/04/23 23:10:20.645
STEP: Creating secret with name s-test-opt-create-7769d030-e204-4f81-bf53-4e664e63f382 01/04/23 23:10:20.651
STEP: waiting to observe update in volume 01/04/23 23:10:20.655
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:22.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1866" for this suite. 01/04/23 23:10:22.686
------------------------------
• [4.149 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:18.547
    Jan  4 23:10:18.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:10:18.547
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:18.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:18.564
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-a9cba0bd-ebc5-475f-b037-7398ed21b5af 01/04/23 23:10:18.58
    STEP: Creating secret with name s-test-opt-upd-f08978f0-9e25-4f77-a76d-cc7a3df3ccf1 01/04/23 23:10:18.585
    STEP: Creating the pod 01/04/23 23:10:18.597
    Jan  4 23:10:18.609: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5" in namespace "projected-1866" to be "running and ready"
    Jan  4 23:10:18.615: INFO: Pod "pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605795ms
    Jan  4 23:10:18.615: INFO: The phase of Pod pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:10:20.621: INFO: Pod "pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011103505s
    Jan  4 23:10:20.621: INFO: The phase of Pod pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5 is Running (Ready = true)
    Jan  4 23:10:20.621: INFO: Pod "pod-projected-secrets-86687b28-3925-4b59-99ad-82d0ba3f97c5" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-a9cba0bd-ebc5-475f-b037-7398ed21b5af 01/04/23 23:10:20.64
    STEP: Updating secret s-test-opt-upd-f08978f0-9e25-4f77-a76d-cc7a3df3ccf1 01/04/23 23:10:20.645
    STEP: Creating secret with name s-test-opt-create-7769d030-e204-4f81-bf53-4e664e63f382 01/04/23 23:10:20.651
    STEP: waiting to observe update in volume 01/04/23 23:10:20.655
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:22.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1866" for this suite. 01/04/23 23:10:22.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:22.698
Jan  4 23:10:22.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 23:10:22.699
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:22.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:22.724
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan  4 23:10:22.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: creating the pod 01/04/23 23:10:22.727
STEP: submitting the pod to kubernetes 01/04/23 23:10:22.727
Jan  4 23:10:22.735: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834" in namespace "pods-1983" to be "running and ready"
Jan  4 23:10:22.741: INFO: Pod "pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834": Phase="Pending", Reason="", readiness=false. Elapsed: 5.8985ms
Jan  4 23:10:22.741: INFO: The phase of Pod pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:10:24.745: INFO: Pod "pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834": Phase="Running", Reason="", readiness=true. Elapsed: 2.009948949s
Jan  4 23:10:24.745: INFO: The phase of Pod pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834 is Running (Ready = true)
Jan  4 23:10:24.745: INFO: Pod "pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:24.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1983" for this suite. 01/04/23 23:10:24.841
------------------------------
• [2.149 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:22.698
    Jan  4 23:10:22.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 23:10:22.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:22.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:22.724
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan  4 23:10:22.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: creating the pod 01/04/23 23:10:22.727
    STEP: submitting the pod to kubernetes 01/04/23 23:10:22.727
    Jan  4 23:10:22.735: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834" in namespace "pods-1983" to be "running and ready"
    Jan  4 23:10:22.741: INFO: Pod "pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834": Phase="Pending", Reason="", readiness=false. Elapsed: 5.8985ms
    Jan  4 23:10:22.741: INFO: The phase of Pod pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:10:24.745: INFO: Pod "pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834": Phase="Running", Reason="", readiness=true. Elapsed: 2.009948949s
    Jan  4 23:10:24.745: INFO: The phase of Pod pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834 is Running (Ready = true)
    Jan  4 23:10:24.745: INFO: Pod "pod-exec-websocket-29602779-a06b-4303-9730-d5b313118834" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:24.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1983" for this suite. 01/04/23 23:10:24.841
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:24.848
Jan  4 23:10:24.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:10:24.849
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:24.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:24.866
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/04/23 23:10:24.869
Jan  4 23:10:24.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/04/23 23:10:32.684
Jan  4 23:10:32.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:10:34.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:43.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-473" for this suite. 01/04/23 23:10:43.201
------------------------------
• [SLOW TEST] [18.358 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:24.848
    Jan  4 23:10:24.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:10:24.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:24.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:24.866
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/04/23 23:10:24.869
    Jan  4 23:10:24.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/04/23 23:10:32.684
    Jan  4 23:10:32.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:10:34.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:43.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-473" for this suite. 01/04/23 23:10:43.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:43.207
Jan  4 23:10:43.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:10:43.209
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:43.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:43.227
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan  4 23:10:43.245: INFO: Waiting up to 5m0s for pod "pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba" in namespace "svcaccounts-3534" to be "running"
Jan  4 23:10:43.248: INFO: Pod "pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.367434ms
Jan  4 23:10:45.252: INFO: Pod "pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.006401817s
Jan  4 23:10:45.252: INFO: Pod "pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba" satisfied condition "running"
STEP: reading a file in the container 01/04/23 23:10:45.252
Jan  4 23:10:45.252: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3534 pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/04/23 23:10:45.381
Jan  4 23:10:45.381: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3534 pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/04/23 23:10:45.557
Jan  4 23:10:45.557: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3534 pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan  4 23:10:45.728: INFO: Got root ca configmap in namespace "svcaccounts-3534"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:45.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3534" for this suite. 01/04/23 23:10:45.733
------------------------------
• [2.531 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:43.207
    Jan  4 23:10:43.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:10:43.209
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:43.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:43.227
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan  4 23:10:43.245: INFO: Waiting up to 5m0s for pod "pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba" in namespace "svcaccounts-3534" to be "running"
    Jan  4 23:10:43.248: INFO: Pod "pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.367434ms
    Jan  4 23:10:45.252: INFO: Pod "pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.006401817s
    Jan  4 23:10:45.252: INFO: Pod "pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba" satisfied condition "running"
    STEP: reading a file in the container 01/04/23 23:10:45.252
    Jan  4 23:10:45.252: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3534 pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/04/23 23:10:45.381
    Jan  4 23:10:45.381: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3534 pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/04/23 23:10:45.557
    Jan  4 23:10:45.557: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3534 pod-service-account-c590f4b3-f553-4957-8883-766bd5cdc0ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan  4 23:10:45.728: INFO: Got root ca configmap in namespace "svcaccounts-3534"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:45.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3534" for this suite. 01/04/23 23:10:45.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:45.739
Jan  4 23:10:45.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 23:10:45.74
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:45.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:45.765
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-9674 01/04/23 23:10:45.767
STEP: creating service affinity-nodeport in namespace services-9674 01/04/23 23:10:45.767
STEP: creating replication controller affinity-nodeport in namespace services-9674 01/04/23 23:10:45.783
I0104 23:10:45.791909      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9674, replica count: 3
I0104 23:10:48.842839      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 23:10:48.854: INFO: Creating new exec pod
Jan  4 23:10:48.863: INFO: Waiting up to 5m0s for pod "execpod-affinity98tnp" in namespace "services-9674" to be "running"
Jan  4 23:10:48.868: INFO: Pod "execpod-affinity98tnp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.780041ms
Jan  4 23:10:50.873: INFO: Pod "execpod-affinity98tnp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009866973s
Jan  4 23:10:50.873: INFO: Pod "execpod-affinity98tnp" satisfied condition "running"
Jan  4 23:10:51.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan  4 23:10:52.240: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan  4 23:10:52.240: INFO: stdout: ""
Jan  4 23:10:52.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c nc -v -z -w 2 10.43.225.9 80'
Jan  4 23:10:52.514: INFO: stderr: "+ nc -v -z -w 2 10.43.225.9 80\nConnection to 10.43.225.9 80 port [tcp/http] succeeded!\n"
Jan  4 23:10:52.514: INFO: stdout: ""
Jan  4 23:10:52.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c nc -v -z -w 2 172.31.3.240 30237'
Jan  4 23:10:52.642: INFO: stderr: "+ nc -v -z -w 2 172.31.3.240 30237\nConnection to 172.31.3.240 30237 port [tcp/*] succeeded!\n"
Jan  4 23:10:52.642: INFO: stdout: ""
Jan  4 23:10:52.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c nc -v -z -w 2 172.31.11.54 30237'
Jan  4 23:10:52.781: INFO: stderr: "+ nc -v -z -w 2 172.31.11.54 30237\nConnection to 172.31.11.54 30237 port [tcp/*] succeeded!\n"
Jan  4 23:10:52.781: INFO: stdout: ""
Jan  4 23:10:52.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.54:30237/ ; done'
Jan  4 23:10:53.080: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n"
Jan  4 23:10:53.080: INFO: stdout: "\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz"
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
Jan  4 23:10:53.080: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9674, will wait for the garbage collector to delete the pods 01/04/23 23:10:53.094
Jan  4 23:10:53.162: INFO: Deleting ReplicationController affinity-nodeport took: 7.038826ms
Jan  4 23:10:53.263: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.898603ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 23:10:55.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9674" for this suite. 01/04/23 23:10:55.553
------------------------------
• [SLOW TEST] [9.823 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:45.739
    Jan  4 23:10:45.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 23:10:45.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:45.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:45.765
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-9674 01/04/23 23:10:45.767
    STEP: creating service affinity-nodeport in namespace services-9674 01/04/23 23:10:45.767
    STEP: creating replication controller affinity-nodeport in namespace services-9674 01/04/23 23:10:45.783
    I0104 23:10:45.791909      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9674, replica count: 3
    I0104 23:10:48.842839      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 23:10:48.854: INFO: Creating new exec pod
    Jan  4 23:10:48.863: INFO: Waiting up to 5m0s for pod "execpod-affinity98tnp" in namespace "services-9674" to be "running"
    Jan  4 23:10:48.868: INFO: Pod "execpod-affinity98tnp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.780041ms
    Jan  4 23:10:50.873: INFO: Pod "execpod-affinity98tnp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009866973s
    Jan  4 23:10:50.873: INFO: Pod "execpod-affinity98tnp" satisfied condition "running"
    Jan  4 23:10:51.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan  4 23:10:52.240: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan  4 23:10:52.240: INFO: stdout: ""
    Jan  4 23:10:52.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c nc -v -z -w 2 10.43.225.9 80'
    Jan  4 23:10:52.514: INFO: stderr: "+ nc -v -z -w 2 10.43.225.9 80\nConnection to 10.43.225.9 80 port [tcp/http] succeeded!\n"
    Jan  4 23:10:52.514: INFO: stdout: ""
    Jan  4 23:10:52.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c nc -v -z -w 2 172.31.3.240 30237'
    Jan  4 23:10:52.642: INFO: stderr: "+ nc -v -z -w 2 172.31.3.240 30237\nConnection to 172.31.3.240 30237 port [tcp/*] succeeded!\n"
    Jan  4 23:10:52.642: INFO: stdout: ""
    Jan  4 23:10:52.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c nc -v -z -w 2 172.31.11.54 30237'
    Jan  4 23:10:52.781: INFO: stderr: "+ nc -v -z -w 2 172.31.11.54 30237\nConnection to 172.31.11.54 30237 port [tcp/*] succeeded!\n"
    Jan  4 23:10:52.781: INFO: stdout: ""
    Jan  4 23:10:52.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9674 exec execpod-affinity98tnp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.54:30237/ ; done'
    Jan  4 23:10:53.080: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.54:30237/\n"
    Jan  4 23:10:53.080: INFO: stdout: "\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz\naffinity-nodeport-f2dfz"
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Received response from host: affinity-nodeport-f2dfz
    Jan  4 23:10:53.080: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9674, will wait for the garbage collector to delete the pods 01/04/23 23:10:53.094
    Jan  4 23:10:53.162: INFO: Deleting ReplicationController affinity-nodeport took: 7.038826ms
    Jan  4 23:10:53.263: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.898603ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:10:55.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9674" for this suite. 01/04/23 23:10:55.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:10:55.564
Jan  4 23:10:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:10:55.569
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:55.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:55.605
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan  4 23:10:55.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/04/23 23:10:57.44
Jan  4 23:10:57.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 --namespace=crd-publish-openapi-5122 create -f -'
Jan  4 23:10:58.137: INFO: stderr: ""
Jan  4 23:10:58.137: INFO: stdout: "e2e-test-crd-publish-openapi-2141-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  4 23:10:58.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 --namespace=crd-publish-openapi-5122 delete e2e-test-crd-publish-openapi-2141-crds test-cr'
Jan  4 23:10:58.216: INFO: stderr: ""
Jan  4 23:10:58.216: INFO: stdout: "e2e-test-crd-publish-openapi-2141-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan  4 23:10:58.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 --namespace=crd-publish-openapi-5122 apply -f -'
Jan  4 23:10:58.790: INFO: stderr: ""
Jan  4 23:10:58.790: INFO: stdout: "e2e-test-crd-publish-openapi-2141-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  4 23:10:58.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 --namespace=crd-publish-openapi-5122 delete e2e-test-crd-publish-openapi-2141-crds test-cr'
Jan  4 23:10:58.895: INFO: stderr: ""
Jan  4 23:10:58.895: INFO: stdout: "e2e-test-crd-publish-openapi-2141-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/04/23 23:10:58.895
Jan  4 23:10:58.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 explain e2e-test-crd-publish-openapi-2141-crds'
Jan  4 23:10:59.096: INFO: stderr: ""
Jan  4 23:10:59.096: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2141-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:11:01.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5122" for this suite. 01/04/23 23:11:01.612
------------------------------
• [SLOW TEST] [6.063 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:10:55.564
    Jan  4 23:10:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:10:55.569
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:10:55.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:10:55.605
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan  4 23:10:55.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/04/23 23:10:57.44
    Jan  4 23:10:57.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 --namespace=crd-publish-openapi-5122 create -f -'
    Jan  4 23:10:58.137: INFO: stderr: ""
    Jan  4 23:10:58.137: INFO: stdout: "e2e-test-crd-publish-openapi-2141-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  4 23:10:58.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 --namespace=crd-publish-openapi-5122 delete e2e-test-crd-publish-openapi-2141-crds test-cr'
    Jan  4 23:10:58.216: INFO: stderr: ""
    Jan  4 23:10:58.216: INFO: stdout: "e2e-test-crd-publish-openapi-2141-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan  4 23:10:58.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 --namespace=crd-publish-openapi-5122 apply -f -'
    Jan  4 23:10:58.790: INFO: stderr: ""
    Jan  4 23:10:58.790: INFO: stdout: "e2e-test-crd-publish-openapi-2141-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  4 23:10:58.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 --namespace=crd-publish-openapi-5122 delete e2e-test-crd-publish-openapi-2141-crds test-cr'
    Jan  4 23:10:58.895: INFO: stderr: ""
    Jan  4 23:10:58.895: INFO: stdout: "e2e-test-crd-publish-openapi-2141-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/04/23 23:10:58.895
    Jan  4 23:10:58.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5122 explain e2e-test-crd-publish-openapi-2141-crds'
    Jan  4 23:10:59.096: INFO: stderr: ""
    Jan  4 23:10:59.096: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2141-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:11:01.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5122" for this suite. 01/04/23 23:11:01.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:11:01.628
Jan  4 23:11:01.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 23:11:01.629
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:01.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:01.654
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-ed423141-969b-4c32-ba78-3f113ddc22dd 01/04/23 23:11:01.667
STEP: Creating the pod 01/04/23 23:11:01.673
Jan  4 23:11:01.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ee0400c-8da9-43c4-b41b-1a32f139512c" in namespace "configmap-9502" to be "running"
Jan  4 23:11:01.690: INFO: Pod "pod-configmaps-6ee0400c-8da9-43c4-b41b-1a32f139512c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.927739ms
Jan  4 23:11:03.696: INFO: Pod "pod-configmaps-6ee0400c-8da9-43c4-b41b-1a32f139512c": Phase="Running", Reason="", readiness=false. Elapsed: 2.012523054s
Jan  4 23:11:03.696: INFO: Pod "pod-configmaps-6ee0400c-8da9-43c4-b41b-1a32f139512c" satisfied condition "running"
STEP: Waiting for pod with text data 01/04/23 23:11:03.696
STEP: Waiting for pod with binary data 01/04/23 23:11:03.704
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:11:03.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9502" for this suite. 01/04/23 23:11:03.715
------------------------------
• [2.108 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:11:01.628
    Jan  4 23:11:01.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 23:11:01.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:01.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:01.654
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-ed423141-969b-4c32-ba78-3f113ddc22dd 01/04/23 23:11:01.667
    STEP: Creating the pod 01/04/23 23:11:01.673
    Jan  4 23:11:01.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ee0400c-8da9-43c4-b41b-1a32f139512c" in namespace "configmap-9502" to be "running"
    Jan  4 23:11:01.690: INFO: Pod "pod-configmaps-6ee0400c-8da9-43c4-b41b-1a32f139512c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.927739ms
    Jan  4 23:11:03.696: INFO: Pod "pod-configmaps-6ee0400c-8da9-43c4-b41b-1a32f139512c": Phase="Running", Reason="", readiness=false. Elapsed: 2.012523054s
    Jan  4 23:11:03.696: INFO: Pod "pod-configmaps-6ee0400c-8da9-43c4-b41b-1a32f139512c" satisfied condition "running"
    STEP: Waiting for pod with text data 01/04/23 23:11:03.696
    STEP: Waiting for pod with binary data 01/04/23 23:11:03.704
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:11:03.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9502" for this suite. 01/04/23 23:11:03.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:11:03.738
Jan  4 23:11:03.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sysctl 01/04/23 23:11:03.739
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:03.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:03.761
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/04/23 23:11:03.763
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:11:03.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5305" for this suite. 01/04/23 23:11:03.776
------------------------------
• [0.047 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:11:03.738
    Jan  4 23:11:03.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sysctl 01/04/23 23:11:03.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:03.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:03.761
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/04/23 23:11:03.763
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:11:03.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5305" for this suite. 01/04/23 23:11:03.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:11:03.785
Jan  4 23:11:03.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 23:11:03.786
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:03.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:03.807
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/04/23 23:11:03.809
STEP: Creating a ResourceQuota 01/04/23 23:11:08.815
STEP: Ensuring resource quota status is calculated 01/04/23 23:11:08.823
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 23:11:10.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1703" for this suite. 01/04/23 23:11:10.832
------------------------------
• [SLOW TEST] [7.056 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:11:03.785
    Jan  4 23:11:03.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 23:11:03.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:03.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:03.807
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/04/23 23:11:03.809
    STEP: Creating a ResourceQuota 01/04/23 23:11:08.815
    STEP: Ensuring resource quota status is calculated 01/04/23 23:11:08.823
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:11:10.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1703" for this suite. 01/04/23 23:11:10.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:11:10.85
Jan  4 23:11:10.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename aggregator 01/04/23 23:11:10.851
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:10.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:10.884
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan  4 23:11:10.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/04/23 23:11:10.889
Jan  4 23:11:11.471: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan  4 23:11:13.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:15.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:17.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:19.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:21.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:23.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:25.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:27.535: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:29.535: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:31.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:33.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:11:35.669: INFO: Waited 122.204247ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/04/23 23:11:35.723
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/04/23 23:11:35.727
STEP: List APIServices 01/04/23 23:11:35.733
Jan  4 23:11:35.740: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan  4 23:11:36.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2111" for this suite. 01/04/23 23:11:36.318
------------------------------
• [SLOW TEST] [25.473 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:11:10.85
    Jan  4 23:11:10.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename aggregator 01/04/23 23:11:10.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:10.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:10.884
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan  4 23:11:10.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/04/23 23:11:10.889
    Jan  4 23:11:11.471: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Jan  4 23:11:13.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:15.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:17.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:19.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:21.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:23.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:25.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:27.535: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:29.535: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:31.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:33.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 11, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:11:35.669: INFO: Waited 122.204247ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/04/23 23:11:35.723
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/04/23 23:11:35.727
    STEP: List APIServices 01/04/23 23:11:35.733
    Jan  4 23:11:35.740: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:11:36.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2111" for this suite. 01/04/23 23:11:36.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:11:36.324
Jan  4 23:11:36.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:11:36.325
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:36.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:36.357
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/04/23 23:11:36.363
Jan  4 23:11:36.372: INFO: Waiting up to 5m0s for pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22" in namespace "emptydir-970" to be "Succeeded or Failed"
Jan  4 23:11:36.381: INFO: Pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22": Phase="Pending", Reason="", readiness=false. Elapsed: 8.549186ms
Jan  4 23:11:38.391: INFO: Pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018381955s
Jan  4 23:11:40.385: INFO: Pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012776415s
STEP: Saw pod success 01/04/23 23:11:40.385
Jan  4 23:11:40.385: INFO: Pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22" satisfied condition "Succeeded or Failed"
Jan  4 23:11:40.389: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-c05dce18-c9f7-43f8-a783-318c6a70fd22 container test-container: <nil>
STEP: delete the pod 01/04/23 23:11:40.401
Jan  4 23:11:40.413: INFO: Waiting for pod pod-c05dce18-c9f7-43f8-a783-318c6a70fd22 to disappear
Jan  4 23:11:40.417: INFO: Pod pod-c05dce18-c9f7-43f8-a783-318c6a70fd22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:11:40.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-970" for this suite. 01/04/23 23:11:40.421
------------------------------
• [4.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:11:36.324
    Jan  4 23:11:36.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:11:36.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:36.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:36.357
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/04/23 23:11:36.363
    Jan  4 23:11:36.372: INFO: Waiting up to 5m0s for pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22" in namespace "emptydir-970" to be "Succeeded or Failed"
    Jan  4 23:11:36.381: INFO: Pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22": Phase="Pending", Reason="", readiness=false. Elapsed: 8.549186ms
    Jan  4 23:11:38.391: INFO: Pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018381955s
    Jan  4 23:11:40.385: INFO: Pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012776415s
    STEP: Saw pod success 01/04/23 23:11:40.385
    Jan  4 23:11:40.385: INFO: Pod "pod-c05dce18-c9f7-43f8-a783-318c6a70fd22" satisfied condition "Succeeded or Failed"
    Jan  4 23:11:40.389: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-c05dce18-c9f7-43f8-a783-318c6a70fd22 container test-container: <nil>
    STEP: delete the pod 01/04/23 23:11:40.401
    Jan  4 23:11:40.413: INFO: Waiting for pod pod-c05dce18-c9f7-43f8-a783-318c6a70fd22 to disappear
    Jan  4 23:11:40.417: INFO: Pod pod-c05dce18-c9f7-43f8-a783-318c6a70fd22 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:11:40.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-970" for this suite. 01/04/23 23:11:40.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:11:40.43
Jan  4 23:11:40.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename subpath 01/04/23 23:11:40.431
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:40.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:40.465
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/04/23 23:11:40.473
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-69g8 01/04/23 23:11:40.485
STEP: Creating a pod to test atomic-volume-subpath 01/04/23 23:11:40.485
Jan  4 23:11:40.500: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-69g8" in namespace "subpath-4741" to be "Succeeded or Failed"
Jan  4 23:11:40.507: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.532495ms
Jan  4 23:11:42.510: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010639221s
Jan  4 23:11:44.510: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 4.01069101s
Jan  4 23:11:46.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 6.011133556s
Jan  4 23:11:48.512: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 8.012619244s
Jan  4 23:11:50.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 10.011166722s
Jan  4 23:11:52.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 12.0112084s
Jan  4 23:11:54.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 14.010923131s
Jan  4 23:11:56.512: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 16.012126609s
Jan  4 23:11:58.512: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 18.012234083s
Jan  4 23:12:00.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 20.011520646s
Jan  4 23:12:02.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=false. Elapsed: 22.01091175s
Jan  4 23:12:04.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011509544s
STEP: Saw pod success 01/04/23 23:12:04.511
Jan  4 23:12:04.512: INFO: Pod "pod-subpath-test-secret-69g8" satisfied condition "Succeeded or Failed"
Jan  4 23:12:04.514: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-secret-69g8 container test-container-subpath-secret-69g8: <nil>
STEP: delete the pod 01/04/23 23:12:04.52
Jan  4 23:12:04.529: INFO: Waiting for pod pod-subpath-test-secret-69g8 to disappear
Jan  4 23:12:04.531: INFO: Pod pod-subpath-test-secret-69g8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-69g8 01/04/23 23:12:04.531
Jan  4 23:12:04.531: INFO: Deleting pod "pod-subpath-test-secret-69g8" in namespace "subpath-4741"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  4 23:12:04.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4741" for this suite. 01/04/23 23:12:04.536
------------------------------
• [SLOW TEST] [24.114 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:11:40.43
    Jan  4 23:11:40.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename subpath 01/04/23 23:11:40.431
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:11:40.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:11:40.465
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/04/23 23:11:40.473
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-69g8 01/04/23 23:11:40.485
    STEP: Creating a pod to test atomic-volume-subpath 01/04/23 23:11:40.485
    Jan  4 23:11:40.500: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-69g8" in namespace "subpath-4741" to be "Succeeded or Failed"
    Jan  4 23:11:40.507: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.532495ms
    Jan  4 23:11:42.510: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010639221s
    Jan  4 23:11:44.510: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 4.01069101s
    Jan  4 23:11:46.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 6.011133556s
    Jan  4 23:11:48.512: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 8.012619244s
    Jan  4 23:11:50.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 10.011166722s
    Jan  4 23:11:52.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 12.0112084s
    Jan  4 23:11:54.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 14.010923131s
    Jan  4 23:11:56.512: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 16.012126609s
    Jan  4 23:11:58.512: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 18.012234083s
    Jan  4 23:12:00.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=true. Elapsed: 20.011520646s
    Jan  4 23:12:02.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Running", Reason="", readiness=false. Elapsed: 22.01091175s
    Jan  4 23:12:04.511: INFO: Pod "pod-subpath-test-secret-69g8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011509544s
    STEP: Saw pod success 01/04/23 23:12:04.511
    Jan  4 23:12:04.512: INFO: Pod "pod-subpath-test-secret-69g8" satisfied condition "Succeeded or Failed"
    Jan  4 23:12:04.514: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-secret-69g8 container test-container-subpath-secret-69g8: <nil>
    STEP: delete the pod 01/04/23 23:12:04.52
    Jan  4 23:12:04.529: INFO: Waiting for pod pod-subpath-test-secret-69g8 to disappear
    Jan  4 23:12:04.531: INFO: Pod pod-subpath-test-secret-69g8 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-69g8 01/04/23 23:12:04.531
    Jan  4 23:12:04.531: INFO: Deleting pod "pod-subpath-test-secret-69g8" in namespace "subpath-4741"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:12:04.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4741" for this suite. 01/04/23 23:12:04.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:12:04.548
Jan  4 23:12:04.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename secrets 01/04/23 23:12:04.549
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:12:04.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:12:04.565
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-a361911a-9e43-4a78-b822-3d17e098eb48 01/04/23 23:12:04.568
STEP: Creating a pod to test consume secrets 01/04/23 23:12:04.572
Jan  4 23:12:04.579: INFO: Waiting up to 5m0s for pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb" in namespace "secrets-9837" to be "Succeeded or Failed"
Jan  4 23:12:04.582: INFO: Pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288596ms
Jan  4 23:12:06.586: INFO: Pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006435752s
Jan  4 23:12:08.587: INFO: Pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007313996s
STEP: Saw pod success 01/04/23 23:12:08.587
Jan  4 23:12:08.587: INFO: Pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb" satisfied condition "Succeeded or Failed"
Jan  4 23:12:08.590: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb container secret-volume-test: <nil>
STEP: delete the pod 01/04/23 23:12:08.596
Jan  4 23:12:08.605: INFO: Waiting for pod pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb to disappear
Jan  4 23:12:08.607: INFO: Pod pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  4 23:12:08.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9837" for this suite. 01/04/23 23:12:08.612
------------------------------
• [4.073 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:12:04.548
    Jan  4 23:12:04.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename secrets 01/04/23 23:12:04.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:12:04.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:12:04.565
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-a361911a-9e43-4a78-b822-3d17e098eb48 01/04/23 23:12:04.568
    STEP: Creating a pod to test consume secrets 01/04/23 23:12:04.572
    Jan  4 23:12:04.579: INFO: Waiting up to 5m0s for pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb" in namespace "secrets-9837" to be "Succeeded or Failed"
    Jan  4 23:12:04.582: INFO: Pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288596ms
    Jan  4 23:12:06.586: INFO: Pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006435752s
    Jan  4 23:12:08.587: INFO: Pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007313996s
    STEP: Saw pod success 01/04/23 23:12:08.587
    Jan  4 23:12:08.587: INFO: Pod "pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb" satisfied condition "Succeeded or Failed"
    Jan  4 23:12:08.590: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb container secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 23:12:08.596
    Jan  4 23:12:08.605: INFO: Waiting for pod pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb to disappear
    Jan  4 23:12:08.607: INFO: Pod pod-secrets-5fad0a75-496d-4f18-ad7f-8cfe5d6478fb no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:12:08.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9837" for this suite. 01/04/23 23:12:08.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:12:08.631
Jan  4 23:12:08.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-pred 01/04/23 23:12:08.632
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:12:08.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:12:08.663
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  4 23:12:08.668: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  4 23:12:08.675: INFO: Waiting for terminating namespaces to be deleted...
Jan  4 23:12:08.678: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-54.us-east-2.compute.internal before test
Jan  4 23:12:08.685: INFO: cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:39 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.685: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 23:12:08.685: INFO: etcd-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:18 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.685: INFO: 	Container etcd ready: true, restart count 0
Jan  4 23:12:08.686: INFO: helm-install-rke2-canal-r7b4c from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.686: INFO: 	Container helm ready: false, restart count 0
Jan  4 23:12:08.686: INFO: helm-install-rke2-coredns-8ff46 from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.686: INFO: 	Container helm ready: false, restart count 0
Jan  4 23:12:08.686: INFO: helm-install-rke2-ingress-nginx-qfntk from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.686: INFO: 	Container helm ready: false, restart count 0
Jan  4 23:12:08.686: INFO: helm-install-rke2-metrics-server-q46jz from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.686: INFO: 	Container helm ready: false, restart count 0
Jan  4 23:12:08.686: INFO: kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:32 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.686: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 23:12:08.686: INFO: kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:38 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.686: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 23:12:08.686: INFO: kube-proxy-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:42 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.687: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 23:12:08.687: INFO: kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:37 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.687: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 23:12:08.687: INFO: rke2-canal-ggwd4 from kube-system started at 2023-01-04 20:05:59 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.687: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 23:12:08.687: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 23:12:08.687: INFO: rke2-coredns-rke2-coredns-854779488f-mwkvw from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.687: INFO: 	Container coredns ready: true, restart count 0
Jan  4 23:12:08.688: INFO: rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.688: INFO: 	Container autoscaler ready: true, restart count 0
Jan  4 23:12:08.688: INFO: rke2-ingress-nginx-controller-97km7 from kube-system started at 2023-01-04 20:06:54 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.688: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 23:12:08.688: INFO: rke2-metrics-server-778467dc76-4rtdk from kube-system started at 2023-01-04 20:06:31 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.688: INFO: 	Container metrics-server ready: true, restart count 0
Jan  4 23:12:08.688: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.688: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:12:08.688: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 23:12:08.688: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-117.us-east-2.compute.internal before test
Jan  4 23:12:08.695: INFO: kube-proxy-ip-172-31-13-117.us-east-2.compute.internal from kube-system started at 2023-01-04 20:10:23 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.695: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 23:12:08.695: INFO: rke2-canal-mprb9 from kube-system started at 2023-01-04 20:10:24 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.696: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 23:12:08.696: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 23:12:08.696: INFO: rke2-ingress-nginx-controller-jpg2c from kube-system started at 2023-01-04 23:08:06 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.696: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 23:12:08.696: INFO: sonobuoy from sonobuoy started at 2023-01-04 22:11:19 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.696: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  4 23:12:08.696: INFO: sonobuoy-e2e-job-6a70417ebe254b91 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.696: INFO: 	Container e2e ready: true, restart count 0
Jan  4 23:12:08.696: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:12:08.696: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.696: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:12:08.696: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 23:12:08.697: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-240.us-east-2.compute.internal before test
Jan  4 23:12:08.704: INFO: cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.704: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 23:12:08.705: INFO: etcd-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:07:58 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.705: INFO: 	Container etcd ready: true, restart count 0
Jan  4 23:12:08.705: INFO: kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:18 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.705: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 23:12:08.705: INFO: kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.705: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 23:12:08.705: INFO: kube-proxy-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:27 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.705: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 23:12:08.705: INFO: kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.705: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 23:12:08.705: INFO: rke2-canal-wspdm from kube-system started at 2023-01-04 20:08:32 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.705: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 23:12:08.705: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 23:12:08.705: INFO: rke2-coredns-rke2-coredns-854779488f-n8z2r from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.705: INFO: 	Container coredns ready: true, restart count 0
Jan  4 23:12:08.705: INFO: rke2-ingress-nginx-controller-rv4dm from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.705: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 23:12:08.706: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:12:08.706: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  4 23:12:08.706: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-62.us-east-2.compute.internal before test
Jan  4 23:12:08.714: INFO: cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.714: INFO: 	Container cloud-controller-manager ready: true, restart count 0
Jan  4 23:12:08.714: INFO: etcd-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:22 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.714: INFO: 	Container etcd ready: true, restart count 0
Jan  4 23:12:08.714: INFO: kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:36 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.714: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  4 23:12:08.714: INFO: kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.714: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  4 23:12:08.714: INFO: kube-proxy-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:47 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.714: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  4 23:12:08.714: INFO: kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.714: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  4 23:12:08.714: INFO: rke2-canal-44lz9 from kube-system started at 2023-01-04 20:08:42 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.714: INFO: 	Container calico-node ready: true, restart count 0
Jan  4 23:12:08.715: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  4 23:12:08.715: INFO: rke2-ingress-nginx-controller-glxqt from kube-system started at 2023-01-04 20:08:54 +0000 UTC (1 container statuses recorded)
Jan  4 23:12:08.715: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
Jan  4 23:12:08.715: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
Jan  4 23:12:08.715: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  4 23:12:08.715: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/04/23 23:12:08.715
Jan  4 23:12:08.722: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9735" to be "running"
Jan  4 23:12:08.727: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026267ms
Jan  4 23:12:10.730: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007732601s
Jan  4 23:12:10.730: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/04/23 23:12:10.733
STEP: Trying to apply a random label on the found node. 01/04/23 23:12:10.753
STEP: verifying the node has the label kubernetes.io/e2e-f63d1e1c-bb91-465e-bc7c-10f52efca425 95 01/04/23 23:12:10.771
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/04/23 23:12:10.776
Jan  4 23:12:10.782: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9735" to be "not pending"
Jan  4 23:12:10.789: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.970001ms
Jan  4 23:12:12.841: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05879557s
Jan  4 23:12:14.801: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.019101288s
Jan  4 23:12:14.801: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.13.117 on the node which pod4 resides and expect not scheduled 01/04/23 23:12:14.801
Jan  4 23:12:14.806: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9735" to be "not pending"
Jan  4 23:12:14.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289756ms
Jan  4 23:12:16.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009640268s
Jan  4 23:12:18.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00599646s
Jan  4 23:12:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005695958s
Jan  4 23:12:22.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005800283s
Jan  4 23:12:24.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005830715s
Jan  4 23:12:26.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006117488s
Jan  4 23:12:28.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007192622s
Jan  4 23:12:30.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005997871s
Jan  4 23:12:32.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006493435s
Jan  4 23:12:34.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006825589s
Jan  4 23:12:36.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013633604s
Jan  4 23:12:38.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005203943s
Jan  4 23:12:40.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007007509s
Jan  4 23:12:42.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006727747s
Jan  4 23:12:44.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006891193s
Jan  4 23:12:46.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012720482s
Jan  4 23:12:48.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006325104s
Jan  4 23:12:50.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006467086s
Jan  4 23:12:52.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006974172s
Jan  4 23:12:54.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005375176s
Jan  4 23:12:56.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008375798s
Jan  4 23:12:58.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.0052946s
Jan  4 23:13:00.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005899884s
Jan  4 23:13:02.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005663655s
Jan  4 23:13:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005840783s
Jan  4 23:13:06.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006016018s
Jan  4 23:13:08.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006875101s
Jan  4 23:13:10.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00651753s
Jan  4 23:13:12.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005776459s
Jan  4 23:13:14.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011029406s
Jan  4 23:13:16.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007871518s
Jan  4 23:13:18.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00674394s
Jan  4 23:13:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005569277s
Jan  4 23:13:22.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005662345s
Jan  4 23:13:24.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006746044s
Jan  4 23:13:26.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009534722s
Jan  4 23:13:28.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006172235s
Jan  4 23:13:30.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006435657s
Jan  4 23:13:32.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006766733s
Jan  4 23:13:34.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005768581s
Jan  4 23:13:36.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009299782s
Jan  4 23:13:38.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006891034s
Jan  4 23:13:40.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005762799s
Jan  4 23:13:42.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005416605s
Jan  4 23:13:44.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00704229s
Jan  4 23:13:46.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005788359s
Jan  4 23:13:48.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005699666s
Jan  4 23:13:50.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006453402s
Jan  4 23:13:52.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006633923s
Jan  4 23:13:54.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005788947s
Jan  4 23:13:56.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.0066262s
Jan  4 23:13:58.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005630301s
Jan  4 23:14:00.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005421071s
Jan  4 23:14:02.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006815377s
Jan  4 23:14:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005999017s
Jan  4 23:14:06.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009970196s
Jan  4 23:14:08.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00764059s
Jan  4 23:14:10.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005603976s
Jan  4 23:14:12.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00724164s
Jan  4 23:14:14.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006816385s
Jan  4 23:14:16.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007451451s
Jan  4 23:14:18.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006986076s
Jan  4 23:14:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006290694s
Jan  4 23:14:22.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.006391489s
Jan  4 23:14:24.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.005836459s
Jan  4 23:14:26.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005848944s
Jan  4 23:14:28.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005415795s
Jan  4 23:14:30.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005686833s
Jan  4 23:14:32.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007557347s
Jan  4 23:14:34.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006878933s
Jan  4 23:14:36.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.176481189s
Jan  4 23:14:38.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.006208988s
Jan  4 23:14:40.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.00586716s
Jan  4 23:14:42.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007103838s
Jan  4 23:14:44.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005853056s
Jan  4 23:14:46.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007929293s
Jan  4 23:14:48.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005509354s
Jan  4 23:14:50.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007256962s
Jan  4 23:14:52.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005487242s
Jan  4 23:14:54.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005939164s
Jan  4 23:14:56.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006127827s
Jan  4 23:14:58.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005574094s
Jan  4 23:15:00.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.006683982s
Jan  4 23:15:02.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.006555966s
Jan  4 23:15:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.00608293s
Jan  4 23:15:06.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006671615s
Jan  4 23:15:08.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007144827s
Jan  4 23:15:10.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.006774931s
Jan  4 23:15:12.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005546991s
Jan  4 23:15:14.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007378952s
Jan  4 23:15:16.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.011706297s
Jan  4 23:15:18.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008254005s
Jan  4 23:15:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005569569s
Jan  4 23:15:22.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005539782s
Jan  4 23:15:24.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.006539691s
Jan  4 23:15:26.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.006671046s
Jan  4 23:15:28.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006152813s
Jan  4 23:15:30.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006182716s
Jan  4 23:15:32.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007127896s
Jan  4 23:15:34.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005632287s
Jan  4 23:15:36.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006778264s
Jan  4 23:15:38.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005749843s
Jan  4 23:15:40.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.006889803s
Jan  4 23:15:42.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005483292s
Jan  4 23:15:44.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00544647s
Jan  4 23:15:46.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009582695s
Jan  4 23:15:48.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007139375s
Jan  4 23:15:50.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.00734181s
Jan  4 23:15:52.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009769186s
Jan  4 23:15:54.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.005613157s
Jan  4 23:15:56.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011898036s
Jan  4 23:15:58.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00578132s
Jan  4 23:16:00.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005782577s
Jan  4 23:16:02.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005815687s
Jan  4 23:16:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006252094s
Jan  4 23:16:06.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011482142s
Jan  4 23:16:08.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005467945s
Jan  4 23:16:10.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009484978s
Jan  4 23:16:12.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006382753s
Jan  4 23:16:14.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005311682s
Jan  4 23:16:16.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.011948067s
Jan  4 23:16:18.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.014943468s
Jan  4 23:16:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005469544s
Jan  4 23:16:22.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006863698s
Jan  4 23:16:24.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.005307233s
Jan  4 23:16:26.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008492822s
Jan  4 23:16:28.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006196351s
Jan  4 23:16:30.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006928626s
Jan  4 23:16:32.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005519948s
Jan  4 23:16:34.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00603687s
Jan  4 23:16:36.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007148896s
Jan  4 23:16:38.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005981844s
Jan  4 23:16:40.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006975605s
Jan  4 23:16:42.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006316811s
Jan  4 23:16:44.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.006797655s
Jan  4 23:16:46.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00713059s
Jan  4 23:16:48.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007408588s
Jan  4 23:16:50.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.006048231s
Jan  4 23:16:52.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005580573s
Jan  4 23:16:54.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005380784s
Jan  4 23:16:56.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009475794s
Jan  4 23:16:58.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007483402s
Jan  4 23:17:00.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006967476s
Jan  4 23:17:02.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006625331s
Jan  4 23:17:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005903787s
Jan  4 23:17:06.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009541462s
Jan  4 23:17:08.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007094907s
Jan  4 23:17:10.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005856051s
Jan  4 23:17:12.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.006340653s
Jan  4 23:17:14.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006304877s
Jan  4 23:17:14.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009468724s
STEP: removing the label kubernetes.io/e2e-f63d1e1c-bb91-465e-bc7c-10f52efca425 off the node ip-172-31-13-117.us-east-2.compute.internal 01/04/23 23:17:14.816
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f63d1e1c-bb91-465e-bc7c-10f52efca425 01/04/23 23:17:14.83
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:17:14.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9735" for this suite. 01/04/23 23:17:14.859
------------------------------
• [SLOW TEST] [306.239 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:12:08.631
    Jan  4 23:12:08.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-pred 01/04/23 23:12:08.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:12:08.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:12:08.663
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  4 23:12:08.668: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  4 23:12:08.675: INFO: Waiting for terminating namespaces to be deleted...
    Jan  4 23:12:08.678: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-11-54.us-east-2.compute.internal before test
    Jan  4 23:12:08.685: INFO: cloud-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:39 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.685: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 23:12:08.685: INFO: etcd-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:18 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.685: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 23:12:08.686: INFO: helm-install-rke2-canal-r7b4c from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.686: INFO: 	Container helm ready: false, restart count 0
    Jan  4 23:12:08.686: INFO: helm-install-rke2-coredns-8ff46 from kube-system started at 2023-01-04 20:05:53 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.686: INFO: 	Container helm ready: false, restart count 0
    Jan  4 23:12:08.686: INFO: helm-install-rke2-ingress-nginx-qfntk from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.686: INFO: 	Container helm ready: false, restart count 0
    Jan  4 23:12:08.686: INFO: helm-install-rke2-metrics-server-q46jz from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.686: INFO: 	Container helm ready: false, restart count 0
    Jan  4 23:12:08.686: INFO: kube-apiserver-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:32 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.686: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 23:12:08.686: INFO: kube-controller-manager-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:38 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.686: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 23:12:08.686: INFO: kube-proxy-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:42 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.687: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 23:12:08.687: INFO: kube-scheduler-ip-172-31-11-54.us-east-2.compute.internal from kube-system started at 2023-01-04 20:05:37 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.687: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 23:12:08.687: INFO: rke2-canal-ggwd4 from kube-system started at 2023-01-04 20:05:59 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.687: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 23:12:08.687: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 23:12:08.687: INFO: rke2-coredns-rke2-coredns-854779488f-mwkvw from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.687: INFO: 	Container coredns ready: true, restart count 0
    Jan  4 23:12:08.688: INFO: rke2-coredns-rke2-coredns-autoscaler-75b5699cf4-rhjtq from kube-system started at 2023-01-04 20:06:13 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.688: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  4 23:12:08.688: INFO: rke2-ingress-nginx-controller-97km7 from kube-system started at 2023-01-04 20:06:54 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.688: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 23:12:08.688: INFO: rke2-metrics-server-778467dc76-4rtdk from kube-system started at 2023-01-04 20:06:31 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.688: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  4 23:12:08.688: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-5ffk2 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.688: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:12:08.688: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 23:12:08.688: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-13-117.us-east-2.compute.internal before test
    Jan  4 23:12:08.695: INFO: kube-proxy-ip-172-31-13-117.us-east-2.compute.internal from kube-system started at 2023-01-04 20:10:23 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.695: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 23:12:08.695: INFO: rke2-canal-mprb9 from kube-system started at 2023-01-04 20:10:24 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.696: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 23:12:08.696: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 23:12:08.696: INFO: rke2-ingress-nginx-controller-jpg2c from kube-system started at 2023-01-04 23:08:06 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.696: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 23:12:08.696: INFO: sonobuoy from sonobuoy started at 2023-01-04 22:11:19 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.696: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  4 23:12:08.696: INFO: sonobuoy-e2e-job-6a70417ebe254b91 from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.696: INFO: 	Container e2e ready: true, restart count 0
    Jan  4 23:12:08.696: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:12:08.696: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-c7x7n from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.696: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:12:08.696: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 23:12:08.697: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-3-240.us-east-2.compute.internal before test
    Jan  4 23:12:08.704: INFO: cloud-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.704: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: etcd-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:07:58 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.705: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: kube-apiserver-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:18 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.705: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: kube-controller-manager-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.705: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: kube-proxy-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:27 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.705: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: kube-scheduler-ip-172-31-3-240.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:24 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.705: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: rke2-canal-wspdm from kube-system started at 2023-01-04 20:08:32 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.705: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: rke2-coredns-rke2-coredns-854779488f-n8z2r from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.705: INFO: 	Container coredns ready: true, restart count 0
    Jan  4 23:12:08.705: INFO: rke2-ingress-nginx-controller-rv4dm from kube-system started at 2023-01-04 20:08:43 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.705: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 23:12:08.706: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-8kqkj from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:12:08.706: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  4 23:12:08.706: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-9-62.us-east-2.compute.internal before test
    Jan  4 23:12:08.714: INFO: cloud-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.714: INFO: 	Container cloud-controller-manager ready: true, restart count 0
    Jan  4 23:12:08.714: INFO: etcd-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:22 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.714: INFO: 	Container etcd ready: true, restart count 0
    Jan  4 23:12:08.714: INFO: kube-apiserver-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:36 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.714: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  4 23:12:08.714: INFO: kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.714: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan  4 23:12:08.714: INFO: kube-proxy-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:47 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.714: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  4 23:12:08.714: INFO: kube-scheduler-ip-172-31-9-62.us-east-2.compute.internal from kube-system started at 2023-01-04 20:08:41 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.714: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan  4 23:12:08.714: INFO: rke2-canal-44lz9 from kube-system started at 2023-01-04 20:08:42 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.714: INFO: 	Container calico-node ready: true, restart count 0
    Jan  4 23:12:08.715: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  4 23:12:08.715: INFO: rke2-ingress-nginx-controller-glxqt from kube-system started at 2023-01-04 20:08:54 +0000 UTC (1 container statuses recorded)
    Jan  4 23:12:08.715: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
    Jan  4 23:12:08.715: INFO: sonobuoy-systemd-logs-daemon-set-96a1d187dc3c46da-lj9ls from sonobuoy started at 2023-01-04 22:11:22 +0000 UTC (2 container statuses recorded)
    Jan  4 23:12:08.715: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  4 23:12:08.715: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/04/23 23:12:08.715
    Jan  4 23:12:08.722: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9735" to be "running"
    Jan  4 23:12:08.727: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026267ms
    Jan  4 23:12:10.730: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007732601s
    Jan  4 23:12:10.730: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/04/23 23:12:10.733
    STEP: Trying to apply a random label on the found node. 01/04/23 23:12:10.753
    STEP: verifying the node has the label kubernetes.io/e2e-f63d1e1c-bb91-465e-bc7c-10f52efca425 95 01/04/23 23:12:10.771
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/04/23 23:12:10.776
    Jan  4 23:12:10.782: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9735" to be "not pending"
    Jan  4 23:12:10.789: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.970001ms
    Jan  4 23:12:12.841: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05879557s
    Jan  4 23:12:14.801: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.019101288s
    Jan  4 23:12:14.801: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.13.117 on the node which pod4 resides and expect not scheduled 01/04/23 23:12:14.801
    Jan  4 23:12:14.806: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9735" to be "not pending"
    Jan  4 23:12:14.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289756ms
    Jan  4 23:12:16.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009640268s
    Jan  4 23:12:18.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00599646s
    Jan  4 23:12:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005695958s
    Jan  4 23:12:22.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005800283s
    Jan  4 23:12:24.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005830715s
    Jan  4 23:12:26.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006117488s
    Jan  4 23:12:28.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007192622s
    Jan  4 23:12:30.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005997871s
    Jan  4 23:12:32.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006493435s
    Jan  4 23:12:34.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006825589s
    Jan  4 23:12:36.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013633604s
    Jan  4 23:12:38.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005203943s
    Jan  4 23:12:40.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007007509s
    Jan  4 23:12:42.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006727747s
    Jan  4 23:12:44.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006891193s
    Jan  4 23:12:46.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012720482s
    Jan  4 23:12:48.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006325104s
    Jan  4 23:12:50.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006467086s
    Jan  4 23:12:52.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006974172s
    Jan  4 23:12:54.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005375176s
    Jan  4 23:12:56.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008375798s
    Jan  4 23:12:58.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.0052946s
    Jan  4 23:13:00.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005899884s
    Jan  4 23:13:02.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005663655s
    Jan  4 23:13:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005840783s
    Jan  4 23:13:06.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006016018s
    Jan  4 23:13:08.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006875101s
    Jan  4 23:13:10.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00651753s
    Jan  4 23:13:12.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005776459s
    Jan  4 23:13:14.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011029406s
    Jan  4 23:13:16.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007871518s
    Jan  4 23:13:18.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00674394s
    Jan  4 23:13:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005569277s
    Jan  4 23:13:22.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005662345s
    Jan  4 23:13:24.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006746044s
    Jan  4 23:13:26.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009534722s
    Jan  4 23:13:28.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006172235s
    Jan  4 23:13:30.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006435657s
    Jan  4 23:13:32.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006766733s
    Jan  4 23:13:34.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005768581s
    Jan  4 23:13:36.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009299782s
    Jan  4 23:13:38.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006891034s
    Jan  4 23:13:40.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005762799s
    Jan  4 23:13:42.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005416605s
    Jan  4 23:13:44.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00704229s
    Jan  4 23:13:46.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005788359s
    Jan  4 23:13:48.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005699666s
    Jan  4 23:13:50.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006453402s
    Jan  4 23:13:52.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006633923s
    Jan  4 23:13:54.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005788947s
    Jan  4 23:13:56.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.0066262s
    Jan  4 23:13:58.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005630301s
    Jan  4 23:14:00.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005421071s
    Jan  4 23:14:02.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006815377s
    Jan  4 23:14:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005999017s
    Jan  4 23:14:06.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009970196s
    Jan  4 23:14:08.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00764059s
    Jan  4 23:14:10.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005603976s
    Jan  4 23:14:12.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00724164s
    Jan  4 23:14:14.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006816385s
    Jan  4 23:14:16.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007451451s
    Jan  4 23:14:18.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006986076s
    Jan  4 23:14:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006290694s
    Jan  4 23:14:22.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.006391489s
    Jan  4 23:14:24.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.005836459s
    Jan  4 23:14:26.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005848944s
    Jan  4 23:14:28.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005415795s
    Jan  4 23:14:30.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005686833s
    Jan  4 23:14:32.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007557347s
    Jan  4 23:14:34.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006878933s
    Jan  4 23:14:36.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.176481189s
    Jan  4 23:14:38.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.006208988s
    Jan  4 23:14:40.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.00586716s
    Jan  4 23:14:42.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007103838s
    Jan  4 23:14:44.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005853056s
    Jan  4 23:14:46.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007929293s
    Jan  4 23:14:48.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005509354s
    Jan  4 23:14:50.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007256962s
    Jan  4 23:14:52.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005487242s
    Jan  4 23:14:54.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005939164s
    Jan  4 23:14:56.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006127827s
    Jan  4 23:14:58.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005574094s
    Jan  4 23:15:00.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.006683982s
    Jan  4 23:15:02.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.006555966s
    Jan  4 23:15:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.00608293s
    Jan  4 23:15:06.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006671615s
    Jan  4 23:15:08.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007144827s
    Jan  4 23:15:10.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.006774931s
    Jan  4 23:15:12.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005546991s
    Jan  4 23:15:14.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007378952s
    Jan  4 23:15:16.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.011706297s
    Jan  4 23:15:18.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008254005s
    Jan  4 23:15:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005569569s
    Jan  4 23:15:22.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005539782s
    Jan  4 23:15:24.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.006539691s
    Jan  4 23:15:26.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.006671046s
    Jan  4 23:15:28.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006152813s
    Jan  4 23:15:30.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006182716s
    Jan  4 23:15:32.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007127896s
    Jan  4 23:15:34.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005632287s
    Jan  4 23:15:36.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006778264s
    Jan  4 23:15:38.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005749843s
    Jan  4 23:15:40.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.006889803s
    Jan  4 23:15:42.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005483292s
    Jan  4 23:15:44.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00544647s
    Jan  4 23:15:46.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009582695s
    Jan  4 23:15:48.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007139375s
    Jan  4 23:15:50.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.00734181s
    Jan  4 23:15:52.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009769186s
    Jan  4 23:15:54.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.005613157s
    Jan  4 23:15:56.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011898036s
    Jan  4 23:15:58.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00578132s
    Jan  4 23:16:00.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005782577s
    Jan  4 23:16:02.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005815687s
    Jan  4 23:16:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006252094s
    Jan  4 23:16:06.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011482142s
    Jan  4 23:16:08.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005467945s
    Jan  4 23:16:10.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009484978s
    Jan  4 23:16:12.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006382753s
    Jan  4 23:16:14.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005311682s
    Jan  4 23:16:16.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.011948067s
    Jan  4 23:16:18.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.014943468s
    Jan  4 23:16:20.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005469544s
    Jan  4 23:16:22.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006863698s
    Jan  4 23:16:24.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.005307233s
    Jan  4 23:16:26.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008492822s
    Jan  4 23:16:28.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006196351s
    Jan  4 23:16:30.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006928626s
    Jan  4 23:16:32.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005519948s
    Jan  4 23:16:34.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00603687s
    Jan  4 23:16:36.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007148896s
    Jan  4 23:16:38.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005981844s
    Jan  4 23:16:40.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006975605s
    Jan  4 23:16:42.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006316811s
    Jan  4 23:16:44.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.006797655s
    Jan  4 23:16:46.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00713059s
    Jan  4 23:16:48.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007408588s
    Jan  4 23:16:50.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.006048231s
    Jan  4 23:16:52.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005580573s
    Jan  4 23:16:54.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005380784s
    Jan  4 23:16:56.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009475794s
    Jan  4 23:16:58.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007483402s
    Jan  4 23:17:00.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006967476s
    Jan  4 23:17:02.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006625331s
    Jan  4 23:17:04.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005903787s
    Jan  4 23:17:06.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009541462s
    Jan  4 23:17:08.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007094907s
    Jan  4 23:17:10.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005856051s
    Jan  4 23:17:12.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.006340653s
    Jan  4 23:17:14.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006304877s
    Jan  4 23:17:14.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009468724s
    STEP: removing the label kubernetes.io/e2e-f63d1e1c-bb91-465e-bc7c-10f52efca425 off the node ip-172-31-13-117.us-east-2.compute.internal 01/04/23 23:17:14.816
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f63d1e1c-bb91-465e-bc7c-10f52efca425 01/04/23 23:17:14.83
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:17:14.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9735" for this suite. 01/04/23 23:17:14.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:17:14.872
Jan  4 23:17:14.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-preemption 01/04/23 23:17:14.882
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:17:14.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:17:14.908
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan  4 23:17:14.926: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  4 23:18:14.967: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:14.97
Jan  4 23:18:14.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-preemption-path 01/04/23 23:18:14.972
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:14.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:14.99
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Jan  4 23:18:15.014: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan  4 23:18:15.019: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:15.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:15.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8154" for this suite. 01/04/23 23:18:15.15
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9743" for this suite. 01/04/23 23:18:15.165
------------------------------
• [SLOW TEST] [60.307 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:17:14.872
    Jan  4 23:17:14.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-preemption 01/04/23 23:17:14.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:17:14.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:17:14.908
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan  4 23:17:14.926: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  4 23:18:14.967: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:14.97
    Jan  4 23:18:14.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-preemption-path 01/04/23 23:18:14.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:14.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:14.99
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Jan  4 23:18:15.014: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan  4 23:18:15.019: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:15.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:15.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8154" for this suite. 01/04/23 23:18:15.15
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9743" for this suite. 01/04/23 23:18:15.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:15.18
Jan  4 23:18:15.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-lifecycle-hook 01/04/23 23:18:15.183
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:15.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:15.203
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/04/23 23:18:15.211
Jan  4 23:18:15.219: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4781" to be "running and ready"
Jan  4 23:18:15.221: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604513ms
Jan  4 23:18:15.222: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:18:17.226: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007229851s
Jan  4 23:18:17.226: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  4 23:18:17.226: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/04/23 23:18:17.229
Jan  4 23:18:17.235: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4781" to be "running and ready"
Jan  4 23:18:17.238: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841723ms
Jan  4 23:18:17.238: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:18:19.243: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008822154s
Jan  4 23:18:19.244: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan  4 23:18:19.244: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/04/23 23:18:19.248
STEP: delete the pod with lifecycle hook 01/04/23 23:18:19.264
Jan  4 23:18:19.272: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  4 23:18:19.281: INFO: Pod pod-with-poststart-http-hook still exists
Jan  4 23:18:21.281: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  4 23:18:21.284: INFO: Pod pod-with-poststart-http-hook still exists
Jan  4 23:18:23.282: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  4 23:18:23.285: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:23.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4781" for this suite. 01/04/23 23:18:23.29
------------------------------
• [SLOW TEST] [8.119 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:15.18
    Jan  4 23:18:15.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/04/23 23:18:15.183
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:15.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:15.203
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/04/23 23:18:15.211
    Jan  4 23:18:15.219: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4781" to be "running and ready"
    Jan  4 23:18:15.221: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604513ms
    Jan  4 23:18:15.222: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:18:17.226: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007229851s
    Jan  4 23:18:17.226: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  4 23:18:17.226: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/04/23 23:18:17.229
    Jan  4 23:18:17.235: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4781" to be "running and ready"
    Jan  4 23:18:17.238: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841723ms
    Jan  4 23:18:17.238: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:18:19.243: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008822154s
    Jan  4 23:18:19.244: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan  4 23:18:19.244: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/04/23 23:18:19.248
    STEP: delete the pod with lifecycle hook 01/04/23 23:18:19.264
    Jan  4 23:18:19.272: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  4 23:18:19.281: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  4 23:18:21.281: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  4 23:18:21.284: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  4 23:18:23.282: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  4 23:18:23.285: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:23.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4781" for this suite. 01/04/23 23:18:23.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:23.303
Jan  4 23:18:23.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename var-expansion 01/04/23 23:18:23.304
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:23.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:23.346
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan  4 23:18:23.367: INFO: Waiting up to 2m0s for pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11" in namespace "var-expansion-5683" to be "container 0 failed with reason CreateContainerConfigError"
Jan  4 23:18:23.371: INFO: Pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140923ms
Jan  4 23:18:25.375: INFO: Pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007374913s
Jan  4 23:18:25.375: INFO: Pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  4 23:18:25.375: INFO: Deleting pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11" in namespace "var-expansion-5683"
Jan  4 23:18:25.392: INFO: Wait up to 5m0s for pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:29.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5683" for this suite. 01/04/23 23:18:29.407
------------------------------
• [SLOW TEST] [6.110 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:23.303
    Jan  4 23:18:23.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename var-expansion 01/04/23 23:18:23.304
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:23.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:23.346
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan  4 23:18:23.367: INFO: Waiting up to 2m0s for pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11" in namespace "var-expansion-5683" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  4 23:18:23.371: INFO: Pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140923ms
    Jan  4 23:18:25.375: INFO: Pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007374913s
    Jan  4 23:18:25.375: INFO: Pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  4 23:18:25.375: INFO: Deleting pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11" in namespace "var-expansion-5683"
    Jan  4 23:18:25.392: INFO: Wait up to 5m0s for pod "var-expansion-11cc77a1-3508-4ab7-9161-d0b08ac84b11" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:29.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5683" for this suite. 01/04/23 23:18:29.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:29.413
Jan  4 23:18:29.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 23:18:29.414
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:29.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:29.43
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 23:18:29.444
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:18:30.027
STEP: Deploying the webhook pod 01/04/23 23:18:30.036
STEP: Wait for the deployment to be ready 01/04/23 23:18:30.051
Jan  4 23:18:30.070: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 23:18:32.079
STEP: Verifying the service has paired with the endpoint 01/04/23 23:18:32.09
Jan  4 23:18:33.090: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan  4 23:18:33.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9058-crds.webhook.example.com via the AdmissionRegistration API 01/04/23 23:18:33.611
STEP: Creating a custom resource while v1 is storage version 01/04/23 23:18:33.636
STEP: Patching Custom Resource Definition to set v2 as storage 01/04/23 23:18:35.694
STEP: Patching the custom resource while v2 is storage version 01/04/23 23:18:35.714
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:36.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6353" for this suite. 01/04/23 23:18:36.358
STEP: Destroying namespace "webhook-6353-markers" for this suite. 01/04/23 23:18:36.373
------------------------------
• [SLOW TEST] [6.970 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:29.413
    Jan  4 23:18:29.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 23:18:29.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:29.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:29.43
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 23:18:29.444
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:18:30.027
    STEP: Deploying the webhook pod 01/04/23 23:18:30.036
    STEP: Wait for the deployment to be ready 01/04/23 23:18:30.051
    Jan  4 23:18:30.070: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 23:18:32.079
    STEP: Verifying the service has paired with the endpoint 01/04/23 23:18:32.09
    Jan  4 23:18:33.090: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan  4 23:18:33.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9058-crds.webhook.example.com via the AdmissionRegistration API 01/04/23 23:18:33.611
    STEP: Creating a custom resource while v1 is storage version 01/04/23 23:18:33.636
    STEP: Patching Custom Resource Definition to set v2 as storage 01/04/23 23:18:35.694
    STEP: Patching the custom resource while v2 is storage version 01/04/23 23:18:35.714
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:36.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6353" for this suite. 01/04/23 23:18:36.358
    STEP: Destroying namespace "webhook-6353-markers" for this suite. 01/04/23 23:18:36.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:36.393
Jan  4 23:18:36.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename cronjob 01/04/23 23:18:36.402
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:36.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:36.486
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/04/23 23:18:36.492
STEP: creating 01/04/23 23:18:36.497
STEP: getting 01/04/23 23:18:36.511
STEP: listing 01/04/23 23:18:36.533
STEP: watching 01/04/23 23:18:36.547
Jan  4 23:18:36.553: INFO: starting watch
STEP: cluster-wide listing 01/04/23 23:18:36.556
STEP: cluster-wide watching 01/04/23 23:18:36.591
Jan  4 23:18:36.592: INFO: starting watch
STEP: patching 01/04/23 23:18:36.593
STEP: updating 01/04/23 23:18:36.605
Jan  4 23:18:36.636: INFO: waiting for watch events with expected annotations
Jan  4 23:18:36.636: INFO: saw patched and updated annotations
STEP: patching /status 01/04/23 23:18:36.636
STEP: updating /status 01/04/23 23:18:36.645
STEP: get /status 01/04/23 23:18:36.657
STEP: deleting 01/04/23 23:18:36.662
STEP: deleting a collection 01/04/23 23:18:36.697
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:36.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1929" for this suite. 01/04/23 23:18:36.73
------------------------------
• [0.346 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:36.393
    Jan  4 23:18:36.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename cronjob 01/04/23 23:18:36.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:36.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:36.486
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/04/23 23:18:36.492
    STEP: creating 01/04/23 23:18:36.497
    STEP: getting 01/04/23 23:18:36.511
    STEP: listing 01/04/23 23:18:36.533
    STEP: watching 01/04/23 23:18:36.547
    Jan  4 23:18:36.553: INFO: starting watch
    STEP: cluster-wide listing 01/04/23 23:18:36.556
    STEP: cluster-wide watching 01/04/23 23:18:36.591
    Jan  4 23:18:36.592: INFO: starting watch
    STEP: patching 01/04/23 23:18:36.593
    STEP: updating 01/04/23 23:18:36.605
    Jan  4 23:18:36.636: INFO: waiting for watch events with expected annotations
    Jan  4 23:18:36.636: INFO: saw patched and updated annotations
    STEP: patching /status 01/04/23 23:18:36.636
    STEP: updating /status 01/04/23 23:18:36.645
    STEP: get /status 01/04/23 23:18:36.657
    STEP: deleting 01/04/23 23:18:36.662
    STEP: deleting a collection 01/04/23 23:18:36.697
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:36.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1929" for this suite. 01/04/23 23:18:36.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:36.751
Jan  4 23:18:36.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir-wrapper 01/04/23 23:18:36.754
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:36.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:36.786
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan  4 23:18:36.821: INFO: Waiting up to 5m0s for pod "pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de" in namespace "emptydir-wrapper-5407" to be "running and ready"
Jan  4 23:18:36.826: INFO: Pod "pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.737089ms
Jan  4 23:18:36.826: INFO: The phase of Pod pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:18:38.830: INFO: Pod "pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de": Phase="Running", Reason="", readiness=true. Elapsed: 2.009180917s
Jan  4 23:18:38.830: INFO: The phase of Pod pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de is Running (Ready = true)
Jan  4 23:18:38.830: INFO: Pod "pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/04/23 23:18:38.832
STEP: Cleaning up the configmap 01/04/23 23:18:38.838
STEP: Cleaning up the pod 01/04/23 23:18:38.845
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:38.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5407" for this suite. 01/04/23 23:18:38.861
------------------------------
• [2.116 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:36.751
    Jan  4 23:18:36.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir-wrapper 01/04/23 23:18:36.754
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:36.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:36.786
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan  4 23:18:36.821: INFO: Waiting up to 5m0s for pod "pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de" in namespace "emptydir-wrapper-5407" to be "running and ready"
    Jan  4 23:18:36.826: INFO: Pod "pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.737089ms
    Jan  4 23:18:36.826: INFO: The phase of Pod pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:18:38.830: INFO: Pod "pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de": Phase="Running", Reason="", readiness=true. Elapsed: 2.009180917s
    Jan  4 23:18:38.830: INFO: The phase of Pod pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de is Running (Ready = true)
    Jan  4 23:18:38.830: INFO: Pod "pod-secrets-cb0be28f-b971-4f7d-8621-da42dddc81de" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/04/23 23:18:38.832
    STEP: Cleaning up the configmap 01/04/23 23:18:38.838
    STEP: Cleaning up the pod 01/04/23 23:18:38.845
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:38.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5407" for this suite. 01/04/23 23:18:38.861
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:38.868
Jan  4 23:18:38.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename podtemplate 01/04/23 23:18:38.869
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:38.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:38.896
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/04/23 23:18:38.898
STEP: Replace a pod template 01/04/23 23:18:38.904
Jan  4 23:18:38.910: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:38.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8394" for this suite. 01/04/23 23:18:38.914
------------------------------
• [0.053 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:38.868
    Jan  4 23:18:38.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename podtemplate 01/04/23 23:18:38.869
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:38.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:38.896
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/04/23 23:18:38.898
    STEP: Replace a pod template 01/04/23 23:18:38.904
    Jan  4 23:18:38.910: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:38.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8394" for this suite. 01/04/23 23:18:38.914
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:38.922
Jan  4 23:18:38.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename server-version 01/04/23 23:18:38.923
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:38.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:38.943
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/04/23 23:18:38.947
STEP: Confirm major version 01/04/23 23:18:38.948
Jan  4 23:18:38.949: INFO: Major version: 1
STEP: Confirm minor version 01/04/23 23:18:38.949
Jan  4 23:18:38.949: INFO: cleanMinorVersion: 26
Jan  4 23:18:38.949: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:38.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-8526" for this suite. 01/04/23 23:18:38.954
------------------------------
• [0.041 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:38.922
    Jan  4 23:18:38.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename server-version 01/04/23 23:18:38.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:38.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:38.943
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/04/23 23:18:38.947
    STEP: Confirm major version 01/04/23 23:18:38.948
    Jan  4 23:18:38.949: INFO: Major version: 1
    STEP: Confirm minor version 01/04/23 23:18:38.949
    Jan  4 23:18:38.949: INFO: cleanMinorVersion: 26
    Jan  4 23:18:38.949: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:38.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-8526" for this suite. 01/04/23 23:18:38.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:38.965
Jan  4 23:18:38.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-runtime 01/04/23 23:18:38.966
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:38.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:38.985
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/04/23 23:18:38.988
STEP: wait for the container to reach Succeeded 01/04/23 23:18:38.994
STEP: get the container status 01/04/23 23:18:43.022
STEP: the container should be terminated 01/04/23 23:18:43.025
STEP: the termination message should be set 01/04/23 23:18:43.025
Jan  4 23:18:43.025: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/04/23 23:18:43.025
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:43.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3309" for this suite. 01/04/23 23:18:43.043
------------------------------
• [4.086 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:38.965
    Jan  4 23:18:38.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-runtime 01/04/23 23:18:38.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:38.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:38.985
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/04/23 23:18:38.988
    STEP: wait for the container to reach Succeeded 01/04/23 23:18:38.994
    STEP: get the container status 01/04/23 23:18:43.022
    STEP: the container should be terminated 01/04/23 23:18:43.025
    STEP: the termination message should be set 01/04/23 23:18:43.025
    Jan  4 23:18:43.025: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/04/23 23:18:43.025
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:43.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3309" for this suite. 01/04/23 23:18:43.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:43.052
Jan  4 23:18:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-webhook 01/04/23 23:18:43.053
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:43.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:43.069
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/04/23 23:18:43.071
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/04/23 23:18:43.779
STEP: Deploying the custom resource conversion webhook pod 01/04/23 23:18:43.784
STEP: Wait for the deployment to be ready 01/04/23 23:18:43.795
Jan  4 23:18:43.802: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 23:18:45.936
STEP: Verifying the service has paired with the endpoint 01/04/23 23:18:45.945
Jan  4 23:18:46.946: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan  4 23:18:46.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Creating a v1 custom resource 01/04/23 23:18:49.6
STEP: Create a v2 custom resource 01/04/23 23:18:49.615
STEP: List CRs in v1 01/04/23 23:18:49.662
STEP: List CRs in v2 01/04/23 23:18:49.669
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:50.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7947" for this suite. 01/04/23 23:18:50.269
------------------------------
• [SLOW TEST] [7.226 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:43.052
    Jan  4 23:18:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-webhook 01/04/23 23:18:43.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:43.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:43.069
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/04/23 23:18:43.071
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/04/23 23:18:43.779
    STEP: Deploying the custom resource conversion webhook pod 01/04/23 23:18:43.784
    STEP: Wait for the deployment to be ready 01/04/23 23:18:43.795
    Jan  4 23:18:43.802: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 23:18:45.936
    STEP: Verifying the service has paired with the endpoint 01/04/23 23:18:45.945
    Jan  4 23:18:46.946: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan  4 23:18:46.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Creating a v1 custom resource 01/04/23 23:18:49.6
    STEP: Create a v2 custom resource 01/04/23 23:18:49.615
    STEP: List CRs in v1 01/04/23 23:18:49.662
    STEP: List CRs in v2 01/04/23 23:18:49.669
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:50.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7947" for this suite. 01/04/23 23:18:50.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:50.279
Jan  4 23:18:50.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replication-controller 01/04/23 23:18:50.28
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:50.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:50.347
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/04/23 23:18:50.353
Jan  4 23:18:50.363: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9175" to be "running and ready"
Jan  4 23:18:50.373: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.342063ms
Jan  4 23:18:50.373: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:18:52.377: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008327558s
Jan  4 23:18:52.377: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan  4 23:18:52.377: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/04/23 23:18:52.379
STEP: Then the orphan pod is adopted 01/04/23 23:18:52.384
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  4 23:18:53.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9175" for this suite. 01/04/23 23:18:53.402
------------------------------
• [3.148 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:50.279
    Jan  4 23:18:50.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replication-controller 01/04/23 23:18:50.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:50.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:50.347
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/04/23 23:18:50.353
    Jan  4 23:18:50.363: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9175" to be "running and ready"
    Jan  4 23:18:50.373: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.342063ms
    Jan  4 23:18:50.373: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:18:52.377: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008327558s
    Jan  4 23:18:52.377: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan  4 23:18:52.377: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/04/23 23:18:52.379
    STEP: Then the orphan pod is adopted 01/04/23 23:18:52.384
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:18:53.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9175" for this suite. 01/04/23 23:18:53.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:18:53.429
Jan  4 23:18:53.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:18:53.43
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:53.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:53.471
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/04/23 23:18:53.474
Jan  4 23:18:53.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:18:55.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:19:03.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6411" for this suite. 01/04/23 23:19:03.853
------------------------------
• [SLOW TEST] [10.430 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:18:53.429
    Jan  4 23:18:53.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:18:53.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:18:53.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:18:53.471
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/04/23 23:18:53.474
    Jan  4 23:18:53.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:18:55.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:19:03.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6411" for this suite. 01/04/23 23:19:03.853
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:19:03.86
Jan  4 23:19:03.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-runtime 01/04/23 23:19:03.861
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:19:03.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:19:03.882
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/04/23 23:19:03.885
STEP: wait for the container to reach Failed 01/04/23 23:19:03.893
STEP: get the container status 01/04/23 23:19:06.912
STEP: the container should be terminated 01/04/23 23:19:06.915
STEP: the termination message should be set 01/04/23 23:19:06.915
Jan  4 23:19:06.915: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/04/23 23:19:06.915
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  4 23:19:06.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2194" for this suite. 01/04/23 23:19:06.936
------------------------------
• [3.084 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:19:03.86
    Jan  4 23:19:03.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-runtime 01/04/23 23:19:03.861
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:19:03.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:19:03.882
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/04/23 23:19:03.885
    STEP: wait for the container to reach Failed 01/04/23 23:19:03.893
    STEP: get the container status 01/04/23 23:19:06.912
    STEP: the container should be terminated 01/04/23 23:19:06.915
    STEP: the termination message should be set 01/04/23 23:19:06.915
    Jan  4 23:19:06.915: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/04/23 23:19:06.915
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:19:06.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2194" for this suite. 01/04/23 23:19:06.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:19:06.948
Jan  4 23:19:06.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename disruption 01/04/23 23:19:06.952
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:19:06.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:19:06.973
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/04/23 23:19:06.982
STEP: Updating PodDisruptionBudget status 01/04/23 23:19:08.992
STEP: Waiting for all pods to be running 01/04/23 23:19:09.004
Jan  4 23:19:09.009: INFO: running pods: 0 < 1
STEP: locating a running pod 01/04/23 23:19:11.2
STEP: Waiting for the pdb to be processed 01/04/23 23:19:11.211
STEP: Patching PodDisruptionBudget status 01/04/23 23:19:11.219
STEP: Waiting for the pdb to be processed 01/04/23 23:19:11.228
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  4 23:19:11.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5985" for this suite. 01/04/23 23:19:11.237
------------------------------
• [4.296 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:19:06.948
    Jan  4 23:19:06.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename disruption 01/04/23 23:19:06.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:19:06.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:19:06.973
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/04/23 23:19:06.982
    STEP: Updating PodDisruptionBudget status 01/04/23 23:19:08.992
    STEP: Waiting for all pods to be running 01/04/23 23:19:09.004
    Jan  4 23:19:09.009: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/04/23 23:19:11.2
    STEP: Waiting for the pdb to be processed 01/04/23 23:19:11.211
    STEP: Patching PodDisruptionBudget status 01/04/23 23:19:11.219
    STEP: Waiting for the pdb to be processed 01/04/23 23:19:11.228
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:19:11.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5985" for this suite. 01/04/23 23:19:11.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:19:11.248
Jan  4 23:19:11.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename cronjob 01/04/23 23:19:11.249
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:19:11.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:19:11.272
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/04/23 23:19:11.274
STEP: Ensuring a job is scheduled 01/04/23 23:19:11.281
STEP: Ensuring exactly one is scheduled 01/04/23 23:20:01.286
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/04/23 23:20:01.288
STEP: Ensuring the job is replaced with a new one 01/04/23 23:20:01.291
STEP: Removing cronjob 01/04/23 23:21:01.295
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  4 23:21:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5295" for this suite. 01/04/23 23:21:01.32
------------------------------
• [SLOW TEST] [110.088 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:19:11.248
    Jan  4 23:19:11.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename cronjob 01/04/23 23:19:11.249
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:19:11.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:19:11.272
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/04/23 23:19:11.274
    STEP: Ensuring a job is scheduled 01/04/23 23:19:11.281
    STEP: Ensuring exactly one is scheduled 01/04/23 23:20:01.286
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/04/23 23:20:01.288
    STEP: Ensuring the job is replaced with a new one 01/04/23 23:20:01.291
    STEP: Removing cronjob 01/04/23 23:21:01.295
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:21:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5295" for this suite. 01/04/23 23:21:01.32
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:21:01.336
Jan  4 23:21:01.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:21:01.342
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:21:01.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:21:01.378
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1c254453-67ce-48c3-a6cd-c24536db9269 01/04/23 23:21:01.397
STEP: Creating the pod 01/04/23 23:21:01.404
Jan  4 23:21:01.416: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0" in namespace "projected-6443" to be "running and ready"
Jan  4 23:21:01.424: INFO: Pod "pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.831091ms
Jan  4 23:21:01.424: INFO: The phase of Pod pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:21:03.440: INFO: Pod "pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.023607611s
Jan  4 23:21:03.440: INFO: The phase of Pod pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0 is Running (Ready = true)
Jan  4 23:21:03.440: INFO: Pod "pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-1c254453-67ce-48c3-a6cd-c24536db9269 01/04/23 23:21:03.512
STEP: waiting to observe update in volume 01/04/23 23:21:03.53
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:21:05.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6443" for this suite. 01/04/23 23:21:05.563
------------------------------
• [4.235 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:21:01.336
    Jan  4 23:21:01.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:21:01.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:21:01.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:21:01.378
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-1c254453-67ce-48c3-a6cd-c24536db9269 01/04/23 23:21:01.397
    STEP: Creating the pod 01/04/23 23:21:01.404
    Jan  4 23:21:01.416: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0" in namespace "projected-6443" to be "running and ready"
    Jan  4 23:21:01.424: INFO: Pod "pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.831091ms
    Jan  4 23:21:01.424: INFO: The phase of Pod pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:21:03.440: INFO: Pod "pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.023607611s
    Jan  4 23:21:03.440: INFO: The phase of Pod pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0 is Running (Ready = true)
    Jan  4 23:21:03.440: INFO: Pod "pod-projected-configmaps-28506844-070c-4502-a516-bdf692ccd8d0" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-1c254453-67ce-48c3-a6cd-c24536db9269 01/04/23 23:21:03.512
    STEP: waiting to observe update in volume 01/04/23 23:21:03.53
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:21:05.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6443" for this suite. 01/04/23 23:21:05.563
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:21:05.576
Jan  4 23:21:05.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:21:05.583
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:21:05.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:21:05.616
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/04/23 23:21:05.619
Jan  4 23:21:05.627: INFO: Waiting up to 5m0s for pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb" in namespace "emptydir-7394" to be "Succeeded or Failed"
Jan  4 23:21:05.630: INFO: Pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372551ms
Jan  4 23:21:07.633: INFO: Pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00648035s
Jan  4 23:21:09.635: INFO: Pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008013036s
STEP: Saw pod success 01/04/23 23:21:09.635
Jan  4 23:21:09.635: INFO: Pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb" satisfied condition "Succeeded or Failed"
Jan  4 23:21:09.638: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb container test-container: <nil>
STEP: delete the pod 01/04/23 23:21:09.645
Jan  4 23:21:09.656: INFO: Waiting for pod pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb to disappear
Jan  4 23:21:09.658: INFO: Pod pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:21:09.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7394" for this suite. 01/04/23 23:21:09.662
------------------------------
• [4.098 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:21:05.576
    Jan  4 23:21:05.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:21:05.583
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:21:05.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:21:05.616
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/04/23 23:21:05.619
    Jan  4 23:21:05.627: INFO: Waiting up to 5m0s for pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb" in namespace "emptydir-7394" to be "Succeeded or Failed"
    Jan  4 23:21:05.630: INFO: Pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372551ms
    Jan  4 23:21:07.633: INFO: Pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00648035s
    Jan  4 23:21:09.635: INFO: Pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008013036s
    STEP: Saw pod success 01/04/23 23:21:09.635
    Jan  4 23:21:09.635: INFO: Pod "pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb" satisfied condition "Succeeded or Failed"
    Jan  4 23:21:09.638: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb container test-container: <nil>
    STEP: delete the pod 01/04/23 23:21:09.645
    Jan  4 23:21:09.656: INFO: Waiting for pod pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb to disappear
    Jan  4 23:21:09.658: INFO: Pod pod-2fcf381d-2284-40ff-956e-7910ae8cf0eb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:21:09.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7394" for this suite. 01/04/23 23:21:09.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:21:09.673
Jan  4 23:21:09.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 23:21:09.675
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:21:09.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:21:09.693
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-d8knt" 01/04/23 23:21:09.698
Jan  4 23:21:09.707: INFO: Resource quota "e2e-rq-status-d8knt" reports spec: hard cpu limit of 500m
Jan  4 23:21:09.707: INFO: Resource quota "e2e-rq-status-d8knt" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-d8knt" /status 01/04/23 23:21:09.707
STEP: Confirm /status for "e2e-rq-status-d8knt" resourceQuota via watch 01/04/23 23:21:09.72
Jan  4 23:21:09.722: INFO: observed resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList(nil)
Jan  4 23:21:09.722: INFO: Found resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan  4 23:21:09.722: INFO: ResourceQuota "e2e-rq-status-d8knt" /status was updated
STEP: Patching hard spec values for cpu & memory 01/04/23 23:21:09.724
Jan  4 23:21:09.729: INFO: Resource quota "e2e-rq-status-d8knt" reports spec: hard cpu limit of 1
Jan  4 23:21:09.729: INFO: Resource quota "e2e-rq-status-d8knt" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-d8knt" /status 01/04/23 23:21:09.729
STEP: Confirm /status for "e2e-rq-status-d8knt" resourceQuota via watch 01/04/23 23:21:09.735
Jan  4 23:21:09.736: INFO: observed resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan  4 23:21:09.736: INFO: Found resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan  4 23:21:09.736: INFO: ResourceQuota "e2e-rq-status-d8knt" /status was patched
STEP: Get "e2e-rq-status-d8knt" /status 01/04/23 23:21:09.736
Jan  4 23:21:09.739: INFO: Resourcequota "e2e-rq-status-d8knt" reports status: hard cpu of 1
Jan  4 23:21:09.739: INFO: Resourcequota "e2e-rq-status-d8knt" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-d8knt" /status before checking Spec is unchanged 01/04/23 23:21:09.743
Jan  4 23:21:09.749: INFO: Resourcequota "e2e-rq-status-d8knt" reports status: hard cpu of 2
Jan  4 23:21:09.750: INFO: Resourcequota "e2e-rq-status-d8knt" reports status: hard memory of 2Gi
Jan  4 23:21:09.751: INFO: Found resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan  4 23:25:54.757: INFO: ResourceQuota "e2e-rq-status-d8knt" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 23:25:54.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-435" for this suite. 01/04/23 23:25:54.761
------------------------------
• [SLOW TEST] [285.093 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:21:09.673
    Jan  4 23:21:09.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 23:21:09.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:21:09.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:21:09.693
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-d8knt" 01/04/23 23:21:09.698
    Jan  4 23:21:09.707: INFO: Resource quota "e2e-rq-status-d8knt" reports spec: hard cpu limit of 500m
    Jan  4 23:21:09.707: INFO: Resource quota "e2e-rq-status-d8knt" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-d8knt" /status 01/04/23 23:21:09.707
    STEP: Confirm /status for "e2e-rq-status-d8knt" resourceQuota via watch 01/04/23 23:21:09.72
    Jan  4 23:21:09.722: INFO: observed resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList(nil)
    Jan  4 23:21:09.722: INFO: Found resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan  4 23:21:09.722: INFO: ResourceQuota "e2e-rq-status-d8knt" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/04/23 23:21:09.724
    Jan  4 23:21:09.729: INFO: Resource quota "e2e-rq-status-d8knt" reports spec: hard cpu limit of 1
    Jan  4 23:21:09.729: INFO: Resource quota "e2e-rq-status-d8knt" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-d8knt" /status 01/04/23 23:21:09.729
    STEP: Confirm /status for "e2e-rq-status-d8knt" resourceQuota via watch 01/04/23 23:21:09.735
    Jan  4 23:21:09.736: INFO: observed resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan  4 23:21:09.736: INFO: Found resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan  4 23:21:09.736: INFO: ResourceQuota "e2e-rq-status-d8knt" /status was patched
    STEP: Get "e2e-rq-status-d8knt" /status 01/04/23 23:21:09.736
    Jan  4 23:21:09.739: INFO: Resourcequota "e2e-rq-status-d8knt" reports status: hard cpu of 1
    Jan  4 23:21:09.739: INFO: Resourcequota "e2e-rq-status-d8knt" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-d8knt" /status before checking Spec is unchanged 01/04/23 23:21:09.743
    Jan  4 23:21:09.749: INFO: Resourcequota "e2e-rq-status-d8knt" reports status: hard cpu of 2
    Jan  4 23:21:09.750: INFO: Resourcequota "e2e-rq-status-d8knt" reports status: hard memory of 2Gi
    Jan  4 23:21:09.751: INFO: Found resourceQuota "e2e-rq-status-d8knt" in namespace "resourcequota-435" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan  4 23:25:54.757: INFO: ResourceQuota "e2e-rq-status-d8knt" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:25:54.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-435" for this suite. 01/04/23 23:25:54.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:25:54.769
Jan  4 23:25:54.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 23:25:54.772
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:25:54.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:25:54.814
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-0fffd9e0-d515-408b-a9a8-f79506d2e3ce 01/04/23 23:25:54.817
STEP: Creating a pod to test consume configMaps 01/04/23 23:25:54.822
Jan  4 23:25:54.829: INFO: Waiting up to 5m0s for pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7" in namespace "configmap-8389" to be "Succeeded or Failed"
Jan  4 23:25:54.835: INFO: Pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.130685ms
Jan  4 23:25:56.843: INFO: Pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013146651s
Jan  4 23:25:58.841: INFO: Pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011602797s
STEP: Saw pod success 01/04/23 23:25:58.841
Jan  4 23:25:58.842: INFO: Pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7" satisfied condition "Succeeded or Failed"
Jan  4 23:25:58.844: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 23:25:58.871
Jan  4 23:25:58.898: INFO: Waiting for pod pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7 to disappear
Jan  4 23:25:58.903: INFO: Pod pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:25:58.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8389" for this suite. 01/04/23 23:25:58.911
------------------------------
• [4.150 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:25:54.769
    Jan  4 23:25:54.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 23:25:54.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:25:54.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:25:54.814
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-0fffd9e0-d515-408b-a9a8-f79506d2e3ce 01/04/23 23:25:54.817
    STEP: Creating a pod to test consume configMaps 01/04/23 23:25:54.822
    Jan  4 23:25:54.829: INFO: Waiting up to 5m0s for pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7" in namespace "configmap-8389" to be "Succeeded or Failed"
    Jan  4 23:25:54.835: INFO: Pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.130685ms
    Jan  4 23:25:56.843: INFO: Pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013146651s
    Jan  4 23:25:58.841: INFO: Pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011602797s
    STEP: Saw pod success 01/04/23 23:25:58.841
    Jan  4 23:25:58.842: INFO: Pod "pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7" satisfied condition "Succeeded or Failed"
    Jan  4 23:25:58.844: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 23:25:58.871
    Jan  4 23:25:58.898: INFO: Waiting for pod pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7 to disappear
    Jan  4 23:25:58.903: INFO: Pod pod-configmaps-79f7f41e-4de9-46db-846c-0046853b42c7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:25:58.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8389" for this suite. 01/04/23 23:25:58.911
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:25:58.918
Jan  4 23:25:58.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename ingress 01/04/23 23:25:58.919
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:25:58.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:25:58.938
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/04/23 23:25:58.94
STEP: getting /apis/networking.k8s.io 01/04/23 23:25:58.942
STEP: getting /apis/networking.k8s.iov1 01/04/23 23:25:58.942
STEP: creating 01/04/23 23:25:58.943
STEP: getting 01/04/23 23:26:00.396
STEP: listing 01/04/23 23:26:00.406
STEP: watching 01/04/23 23:26:00.41
Jan  4 23:26:00.411: INFO: starting watch
STEP: cluster-wide listing 01/04/23 23:26:00.413
STEP: cluster-wide watching 01/04/23 23:26:00.418
Jan  4 23:26:00.418: INFO: starting watch
STEP: patching 01/04/23 23:26:00.419
STEP: updating 01/04/23 23:26:00.483
Jan  4 23:26:00.540: INFO: waiting for watch events with expected annotations
Jan  4 23:26:00.540: INFO: saw patched and updated annotations
STEP: patching /status 01/04/23 23:26:00.54
STEP: updating /status 01/04/23 23:26:00.551
STEP: get /status 01/04/23 23:26:00.565
STEP: deleting 01/04/23 23:26:00.57
STEP: deleting a collection 01/04/23 23:26:00.584
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:00.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5021" for this suite. 01/04/23 23:26:00.615
------------------------------
• [1.706 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:25:58.918
    Jan  4 23:25:58.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename ingress 01/04/23 23:25:58.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:25:58.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:25:58.938
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/04/23 23:25:58.94
    STEP: getting /apis/networking.k8s.io 01/04/23 23:25:58.942
    STEP: getting /apis/networking.k8s.iov1 01/04/23 23:25:58.942
    STEP: creating 01/04/23 23:25:58.943
    STEP: getting 01/04/23 23:26:00.396
    STEP: listing 01/04/23 23:26:00.406
    STEP: watching 01/04/23 23:26:00.41
    Jan  4 23:26:00.411: INFO: starting watch
    STEP: cluster-wide listing 01/04/23 23:26:00.413
    STEP: cluster-wide watching 01/04/23 23:26:00.418
    Jan  4 23:26:00.418: INFO: starting watch
    STEP: patching 01/04/23 23:26:00.419
    STEP: updating 01/04/23 23:26:00.483
    Jan  4 23:26:00.540: INFO: waiting for watch events with expected annotations
    Jan  4 23:26:00.540: INFO: saw patched and updated annotations
    STEP: patching /status 01/04/23 23:26:00.54
    STEP: updating /status 01/04/23 23:26:00.551
    STEP: get /status 01/04/23 23:26:00.565
    STEP: deleting 01/04/23 23:26:00.57
    STEP: deleting a collection 01/04/23 23:26:00.584
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:00.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5021" for this suite. 01/04/23 23:26:00.615
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:00.625
Jan  4 23:26:00.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename events 01/04/23 23:26:00.626
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:00.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:00.675
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/04/23 23:26:00.678
STEP: listing all events in all namespaces 01/04/23 23:26:00.684
STEP: patching the test event 01/04/23 23:26:00.688
STEP: fetching the test event 01/04/23 23:26:00.703
STEP: updating the test event 01/04/23 23:26:00.71
STEP: getting the test event 01/04/23 23:26:00.725
STEP: deleting the test event 01/04/23 23:26:00.729
STEP: listing all events in all namespaces 01/04/23 23:26:00.757
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:00.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7562" for this suite. 01/04/23 23:26:00.77
------------------------------
• [0.151 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:00.625
    Jan  4 23:26:00.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename events 01/04/23 23:26:00.626
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:00.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:00.675
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/04/23 23:26:00.678
    STEP: listing all events in all namespaces 01/04/23 23:26:00.684
    STEP: patching the test event 01/04/23 23:26:00.688
    STEP: fetching the test event 01/04/23 23:26:00.703
    STEP: updating the test event 01/04/23 23:26:00.71
    STEP: getting the test event 01/04/23 23:26:00.725
    STEP: deleting the test event 01/04/23 23:26:00.729
    STEP: listing all events in all namespaces 01/04/23 23:26:00.757
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:00.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7562" for this suite. 01/04/23 23:26:00.77
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:00.779
Jan  4 23:26:00.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-lifecycle-hook 01/04/23 23:26:00.78
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:00.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:00.805
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/04/23 23:26:00.814
Jan  4 23:26:00.822: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9628" to be "running and ready"
Jan  4 23:26:00.832: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.698616ms
Jan  4 23:26:00.832: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:26:02.836: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014387058s
Jan  4 23:26:02.836: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  4 23:26:02.836: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/04/23 23:26:02.848
Jan  4 23:26:02.856: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9628" to be "running and ready"
Jan  4 23:26:02.861: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285994ms
Jan  4 23:26:02.861: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:26:04.865: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008826267s
Jan  4 23:26:04.865: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan  4 23:26:04.865: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/04/23 23:26:04.867
Jan  4 23:26:04.873: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  4 23:26:04.876: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  4 23:26:06.876: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  4 23:26:06.880: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  4 23:26:08.877: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  4 23:26:08.880: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/04/23 23:26:08.88
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:08.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9628" for this suite. 01/04/23 23:26:08.897
------------------------------
• [SLOW TEST] [8.128 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:00.779
    Jan  4 23:26:00.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/04/23 23:26:00.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:00.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:00.805
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/04/23 23:26:00.814
    Jan  4 23:26:00.822: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9628" to be "running and ready"
    Jan  4 23:26:00.832: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.698616ms
    Jan  4 23:26:00.832: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:26:02.836: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014387058s
    Jan  4 23:26:02.836: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  4 23:26:02.836: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/04/23 23:26:02.848
    Jan  4 23:26:02.856: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9628" to be "running and ready"
    Jan  4 23:26:02.861: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285994ms
    Jan  4 23:26:02.861: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:26:04.865: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008826267s
    Jan  4 23:26:04.865: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan  4 23:26:04.865: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/04/23 23:26:04.867
    Jan  4 23:26:04.873: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  4 23:26:04.876: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  4 23:26:06.876: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  4 23:26:06.880: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  4 23:26:08.877: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  4 23:26:08.880: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/04/23 23:26:08.88
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:08.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9628" for this suite. 01/04/23 23:26:08.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:08.91
Jan  4 23:26:08.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 23:26:08.911
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:08.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:08.93
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4648" for this suite. 01/04/23 23:26:08.971
------------------------------
• [0.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:08.91
    Jan  4 23:26:08.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 23:26:08.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:08.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:08.93
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4648" for this suite. 01/04/23 23:26:08.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:08.98
Jan  4 23:26:08.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename daemonsets 01/04/23 23:26:08.981
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:08.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:09
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/04/23 23:26:09.05
STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 23:26:09.055
Jan  4 23:26:09.062: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  4 23:26:09.062: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:26:10.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  4 23:26:10.420: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:26:11.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  4 23:26:11.070: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
Jan  4 23:26:12.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  4 23:26:12.071: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets 01/04/23 23:26:12.073
STEP: DeleteCollection of the DaemonSets 01/04/23 23:26:12.076
STEP: Verify that ReplicaSets have been deleted 01/04/23 23:26:12.082
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan  4 23:26:12.099: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62204"},"items":null}

Jan  4 23:26:12.103: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62204"},"items":[{"metadata":{"name":"daemon-set-8v4ng","generateName":"daemon-set-","namespace":"daemonsets-9099","uid":"bd119387-5bcf-41cc-8941-b2c8dc268572","resourceVersion":"62190","creationTimestamp":"2023-01-04T23:26:09Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3b2e04ee545b64f8f7366dbc8c206a809b4e6b4b0e5626fbdeda932f7675e3fe","cni.projectcalico.org/podIP":"10.42.3.76/32","cni.projectcalico.org/podIPs":"10.42.3.76/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95559044-939d-453f-9935-a0d2479dc832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95559044-939d-453f-9935-a0d2479dc832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5ktbs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5ktbs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-13-117.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-13-117.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"}],"hostIP":"172.31.13.117","podIP":"10.42.3.76","podIPs":[{"ip":"10.42.3.76"}],"startTime":"2023-01-04T23:26:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-04T23:26:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a9ea44afa3e25525af01c5d8361db5d968d93f96f80a183b49ac27cf1d3e7853","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-plkdx","generateName":"daemon-set-","namespace":"daemonsets-9099","uid":"b02c7a2c-e9c8-4b71-a2e9-acc67917d8dd","resourceVersion":"62198","creationTimestamp":"2023-01-04T23:26:09Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fec107d16284517f73e3a236a042092238dda9fa4f2e36c770ed5a58cb62b2ff","cni.projectcalico.org/podIP":"10.42.1.105/32","cni.projectcalico.org/podIPs":"10.42.1.105/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95559044-939d-453f-9935-a0d2479dc832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95559044-939d-453f-9935-a0d2479dc832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4b6w7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4b6w7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-3-240.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-3-240.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"}],"hostIP":"172.31.3.240","podIP":"10.42.1.105","podIPs":[{"ip":"10.42.1.105"}],"startTime":"2023-01-04T23:26:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-04T23:26:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://091e8497f6cdf9d095368f4a4bf41bee0e09f5a6a277d3f2312166970e247338","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-s8z4h","generateName":"daemon-set-","namespace":"daemonsets-9099","uid":"10a47326-f5df-40c0-ba96-f8d8064c2205","resourceVersion":"62186","creationTimestamp":"2023-01-04T23:26:09Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"840e1b4c0d9a3b461205e75cae9a7eb836defd3717438ee986f421c275be7828","cni.projectcalico.org/podIP":"10.42.0.102/32","cni.projectcalico.org/podIPs":"10.42.0.102/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95559044-939d-453f-9935-a0d2479dc832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95559044-939d-453f-9935-a0d2479dc832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b5brl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b5brl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-11-54.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-11-54.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"}],"hostIP":"172.31.11.54","podIP":"10.42.0.102","podIPs":[{"ip":"10.42.0.102"}],"startTime":"2023-01-04T23:26:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-04T23:26:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://9f232448b6d16a40a47cccef2a7ec9daf438554f5cf854e0319b897b504b8724","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vtwpq","generateName":"daemon-set-","namespace":"daemonsets-9099","uid":"1d99de2e-9a3a-48ef-b9c1-67fdceb569a3","resourceVersion":"62200","creationTimestamp":"2023-01-04T23:26:09Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a5e7a036921a316f275b2a4df0d12553f989df2eb0629cbda8a5046726e5550f","cni.projectcalico.org/podIP":"10.42.2.121/32","cni.projectcalico.org/podIPs":"10.42.2.121/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95559044-939d-453f-9935-a0d2479dc832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95559044-939d-453f-9935-a0d2479dc832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vbz87","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vbz87","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-9-62.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-9-62.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"}],"hostIP":"172.31.9.62","podIP":"10.42.2.121","podIPs":[{"ip":"10.42.2.121"}],"startTime":"2023-01-04T23:26:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-04T23:26:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e1ff5c9d26d9c208851d26547e347576e9c9d252ce105f69b0aeddcd85ead6e4","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:12.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9099" for this suite. 01/04/23 23:26:12.145
------------------------------
• [3.173 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:08.98
    Jan  4 23:26:08.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename daemonsets 01/04/23 23:26:08.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:08.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:09
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/04/23 23:26:09.05
    STEP: Check that daemon pods launch on every node of the cluster. 01/04/23 23:26:09.055
    Jan  4 23:26:09.062: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  4 23:26:09.062: INFO: Node ip-172-31-11-54.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:26:10.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  4 23:26:10.420: INFO: Node ip-172-31-13-117.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:26:11.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  4 23:26:11.070: INFO: Node ip-172-31-9-62.us-east-2.compute.internal is running 0 daemon pod, expected 1
    Jan  4 23:26:12.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  4 23:26:12.071: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: listing all DeamonSets 01/04/23 23:26:12.073
    STEP: DeleteCollection of the DaemonSets 01/04/23 23:26:12.076
    STEP: Verify that ReplicaSets have been deleted 01/04/23 23:26:12.082
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan  4 23:26:12.099: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62204"},"items":null}

    Jan  4 23:26:12.103: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62204"},"items":[{"metadata":{"name":"daemon-set-8v4ng","generateName":"daemon-set-","namespace":"daemonsets-9099","uid":"bd119387-5bcf-41cc-8941-b2c8dc268572","resourceVersion":"62190","creationTimestamp":"2023-01-04T23:26:09Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3b2e04ee545b64f8f7366dbc8c206a809b4e6b4b0e5626fbdeda932f7675e3fe","cni.projectcalico.org/podIP":"10.42.3.76/32","cni.projectcalico.org/podIPs":"10.42.3.76/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95559044-939d-453f-9935-a0d2479dc832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95559044-939d-453f-9935-a0d2479dc832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5ktbs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5ktbs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-13-117.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-13-117.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"}],"hostIP":"172.31.13.117","podIP":"10.42.3.76","podIPs":[{"ip":"10.42.3.76"}],"startTime":"2023-01-04T23:26:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-04T23:26:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a9ea44afa3e25525af01c5d8361db5d968d93f96f80a183b49ac27cf1d3e7853","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-plkdx","generateName":"daemon-set-","namespace":"daemonsets-9099","uid":"b02c7a2c-e9c8-4b71-a2e9-acc67917d8dd","resourceVersion":"62198","creationTimestamp":"2023-01-04T23:26:09Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fec107d16284517f73e3a236a042092238dda9fa4f2e36c770ed5a58cb62b2ff","cni.projectcalico.org/podIP":"10.42.1.105/32","cni.projectcalico.org/podIPs":"10.42.1.105/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95559044-939d-453f-9935-a0d2479dc832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95559044-939d-453f-9935-a0d2479dc832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4b6w7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4b6w7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-3-240.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-3-240.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"}],"hostIP":"172.31.3.240","podIP":"10.42.1.105","podIPs":[{"ip":"10.42.1.105"}],"startTime":"2023-01-04T23:26:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-04T23:26:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://091e8497f6cdf9d095368f4a4bf41bee0e09f5a6a277d3f2312166970e247338","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-s8z4h","generateName":"daemon-set-","namespace":"daemonsets-9099","uid":"10a47326-f5df-40c0-ba96-f8d8064c2205","resourceVersion":"62186","creationTimestamp":"2023-01-04T23:26:09Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"840e1b4c0d9a3b461205e75cae9a7eb836defd3717438ee986f421c275be7828","cni.projectcalico.org/podIP":"10.42.0.102/32","cni.projectcalico.org/podIPs":"10.42.0.102/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95559044-939d-453f-9935-a0d2479dc832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95559044-939d-453f-9935-a0d2479dc832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b5brl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b5brl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-11-54.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-11-54.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"}],"hostIP":"172.31.11.54","podIP":"10.42.0.102","podIPs":[{"ip":"10.42.0.102"}],"startTime":"2023-01-04T23:26:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-04T23:26:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://9f232448b6d16a40a47cccef2a7ec9daf438554f5cf854e0319b897b504b8724","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vtwpq","generateName":"daemon-set-","namespace":"daemonsets-9099","uid":"1d99de2e-9a3a-48ef-b9c1-67fdceb569a3","resourceVersion":"62200","creationTimestamp":"2023-01-04T23:26:09Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a5e7a036921a316f275b2a4df0d12553f989df2eb0629cbda8a5046726e5550f","cni.projectcalico.org/podIP":"10.42.2.121/32","cni.projectcalico.org/podIPs":"10.42.2.121/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95559044-939d-453f-9935-a0d2479dc832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95559044-939d-453f-9935-a0d2479dc832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-04T23:26:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vbz87","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vbz87","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-9-62.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-9-62.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-04T23:26:09Z"}],"hostIP":"172.31.9.62","podIP":"10.42.2.121","podIPs":[{"ip":"10.42.2.121"}],"startTime":"2023-01-04T23:26:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-04T23:26:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e1ff5c9d26d9c208851d26547e347576e9c9d252ce105f69b0aeddcd85ead6e4","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:12.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9099" for this suite. 01/04/23 23:26:12.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:12.156
Jan  4 23:26:12.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replication-controller 01/04/23 23:26:12.157
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:12.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:12.176
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan  4 23:26:12.178: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/04/23 23:26:13.191
STEP: Checking rc "condition-test" has the desired failure condition set 01/04/23 23:26:13.197
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/04/23 23:26:14.205
Jan  4 23:26:14.214: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/04/23 23:26:14.214
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:14.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-395" for this suite. 01/04/23 23:26:14.235
------------------------------
• [2.089 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:12.156
    Jan  4 23:26:12.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replication-controller 01/04/23 23:26:12.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:12.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:12.176
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan  4 23:26:12.178: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/04/23 23:26:13.191
    STEP: Checking rc "condition-test" has the desired failure condition set 01/04/23 23:26:13.197
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/04/23 23:26:14.205
    Jan  4 23:26:14.214: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/04/23 23:26:14.214
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:14.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-395" for this suite. 01/04/23 23:26:14.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:14.245
Jan  4 23:26:14.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename security-context 01/04/23 23:26:14.246
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:14.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:14.268
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/04/23 23:26:14.27
Jan  4 23:26:14.277: INFO: Waiting up to 5m0s for pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed" in namespace "security-context-3988" to be "Succeeded or Failed"
Jan  4 23:26:14.281: INFO: Pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361342ms
Jan  4 23:26:16.285: INFO: Pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008486434s
Jan  4 23:26:18.284: INFO: Pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007289066s
STEP: Saw pod success 01/04/23 23:26:18.284
Jan  4 23:26:18.285: INFO: Pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed" satisfied condition "Succeeded or Failed"
Jan  4 23:26:18.291: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed container test-container: <nil>
STEP: delete the pod 01/04/23 23:26:18.302
Jan  4 23:26:18.311: INFO: Waiting for pod security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed to disappear
Jan  4 23:26:18.315: INFO: Pod security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:18.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-3988" for this suite. 01/04/23 23:26:18.32
------------------------------
• [4.084 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:14.245
    Jan  4 23:26:14.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename security-context 01/04/23 23:26:14.246
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:14.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:14.268
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/04/23 23:26:14.27
    Jan  4 23:26:14.277: INFO: Waiting up to 5m0s for pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed" in namespace "security-context-3988" to be "Succeeded or Failed"
    Jan  4 23:26:14.281: INFO: Pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361342ms
    Jan  4 23:26:16.285: INFO: Pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008486434s
    Jan  4 23:26:18.284: INFO: Pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007289066s
    STEP: Saw pod success 01/04/23 23:26:18.284
    Jan  4 23:26:18.285: INFO: Pod "security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed" satisfied condition "Succeeded or Failed"
    Jan  4 23:26:18.291: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed container test-container: <nil>
    STEP: delete the pod 01/04/23 23:26:18.302
    Jan  4 23:26:18.311: INFO: Waiting for pod security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed to disappear
    Jan  4 23:26:18.315: INFO: Pod security-context-1ca7bf0e-257d-421c-ad14-47d3f647dbed no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:18.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-3988" for this suite. 01/04/23 23:26:18.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:18.332
Jan  4 23:26:18.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:26:18.334
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:18.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:18.367
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/04/23 23:26:18.37
Jan  4 23:26:18.380: INFO: Waiting up to 5m0s for pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2" in namespace "downward-api-1595" to be "Succeeded or Failed"
Jan  4 23:26:18.384: INFO: Pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.970378ms
Jan  4 23:26:20.388: INFO: Pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007761902s
Jan  4 23:26:22.387: INFO: Pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00758665s
STEP: Saw pod success 01/04/23 23:26:22.387
Jan  4 23:26:22.388: INFO: Pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2" satisfied condition "Succeeded or Failed"
Jan  4 23:26:22.390: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2 container dapi-container: <nil>
STEP: delete the pod 01/04/23 23:26:22.396
Jan  4 23:26:22.411: INFO: Waiting for pod downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2 to disappear
Jan  4 23:26:22.414: INFO: Pod downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:22.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1595" for this suite. 01/04/23 23:26:22.418
------------------------------
• [4.092 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:18.332
    Jan  4 23:26:18.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:26:18.334
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:18.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:18.367
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/04/23 23:26:18.37
    Jan  4 23:26:18.380: INFO: Waiting up to 5m0s for pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2" in namespace "downward-api-1595" to be "Succeeded or Failed"
    Jan  4 23:26:18.384: INFO: Pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.970378ms
    Jan  4 23:26:20.388: INFO: Pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007761902s
    Jan  4 23:26:22.387: INFO: Pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00758665s
    STEP: Saw pod success 01/04/23 23:26:22.387
    Jan  4 23:26:22.388: INFO: Pod "downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2" satisfied condition "Succeeded or Failed"
    Jan  4 23:26:22.390: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2 container dapi-container: <nil>
    STEP: delete the pod 01/04/23 23:26:22.396
    Jan  4 23:26:22.411: INFO: Waiting for pod downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2 to disappear
    Jan  4 23:26:22.414: INFO: Pod downward-api-0bd4f531-45af-41b0-b91a-f02a9daa63a2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:22.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1595" for this suite. 01/04/23 23:26:22.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:22.424
Jan  4 23:26:22.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename events 01/04/23 23:26:22.425
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:22.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:22.442
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/04/23 23:26:22.445
STEP: get a list of Events with a label in the current namespace 01/04/23 23:26:22.458
STEP: delete a list of events 01/04/23 23:26:22.461
Jan  4 23:26:22.461: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/04/23 23:26:22.477
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:22.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-832" for this suite. 01/04/23 23:26:22.484
------------------------------
• [0.065 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:22.424
    Jan  4 23:26:22.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename events 01/04/23 23:26:22.425
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:22.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:22.442
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/04/23 23:26:22.445
    STEP: get a list of Events with a label in the current namespace 01/04/23 23:26:22.458
    STEP: delete a list of events 01/04/23 23:26:22.461
    Jan  4 23:26:22.461: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/04/23 23:26:22.477
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:22.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-832" for this suite. 01/04/23 23:26:22.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:22.49
Jan  4 23:26:22.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 23:26:22.491
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:22.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:22.517
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/04/23 23:26:22.519
STEP: Ensuring ResourceQuota status is calculated 01/04/23 23:26:22.523
STEP: Creating a ResourceQuota with not terminating scope 01/04/23 23:26:24.526
STEP: Ensuring ResourceQuota status is calculated 01/04/23 23:26:24.53
STEP: Creating a long running pod 01/04/23 23:26:26.534
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/04/23 23:26:26.546
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/04/23 23:26:28.55
STEP: Deleting the pod 01/04/23 23:26:30.555
STEP: Ensuring resource quota status released the pod usage 01/04/23 23:26:30.564
STEP: Creating a terminating pod 01/04/23 23:26:32.569
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/04/23 23:26:32.578
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/04/23 23:26:34.585
STEP: Deleting the pod 01/04/23 23:26:36.589
STEP: Ensuring resource quota status released the pod usage 01/04/23 23:26:36.602
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 23:26:38.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8334" for this suite. 01/04/23 23:26:38.611
------------------------------
• [SLOW TEST] [16.128 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:22.49
    Jan  4 23:26:22.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 23:26:22.491
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:22.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:22.517
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/04/23 23:26:22.519
    STEP: Ensuring ResourceQuota status is calculated 01/04/23 23:26:22.523
    STEP: Creating a ResourceQuota with not terminating scope 01/04/23 23:26:24.526
    STEP: Ensuring ResourceQuota status is calculated 01/04/23 23:26:24.53
    STEP: Creating a long running pod 01/04/23 23:26:26.534
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/04/23 23:26:26.546
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/04/23 23:26:28.55
    STEP: Deleting the pod 01/04/23 23:26:30.555
    STEP: Ensuring resource quota status released the pod usage 01/04/23 23:26:30.564
    STEP: Creating a terminating pod 01/04/23 23:26:32.569
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/04/23 23:26:32.578
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/04/23 23:26:34.585
    STEP: Deleting the pod 01/04/23 23:26:36.589
    STEP: Ensuring resource quota status released the pod usage 01/04/23 23:26:36.602
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:26:38.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8334" for this suite. 01/04/23 23:26:38.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:26:38.623
Jan  4 23:26:38.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir-wrapper 01/04/23 23:26:38.625
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:38.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:38.649
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/04/23 23:26:38.652
STEP: Creating RC which spawns configmap-volume pods 01/04/23 23:26:38.912
Jan  4 23:26:38.998: INFO: Pod name wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037: Found 3 pods out of 5
Jan  4 23:26:44.016: INFO: Pod name wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/04/23 23:26:44.016
Jan  4 23:26:44.017: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:26:44.025: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.67146ms
Jan  4 23:26:46.034: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017123158s
Jan  4 23:26:48.031: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013718039s
Jan  4 23:26:50.030: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013070857s
Jan  4 23:26:52.031: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014562036s
Jan  4 23:26:54.030: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Running", Reason="", readiness=true. Elapsed: 10.013130367s
Jan  4 23:26:54.030: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf" satisfied condition "running"
Jan  4 23:26:54.030: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-d2dvb" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:26:54.033: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-d2dvb": Phase="Running", Reason="", readiness=true. Elapsed: 3.025257ms
Jan  4 23:26:54.033: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-d2dvb" satisfied condition "running"
Jan  4 23:26:54.033: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-kmk8v" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:26:54.036: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-kmk8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.876162ms
Jan  4 23:26:54.036: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-kmk8v" satisfied condition "running"
Jan  4 23:26:54.037: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-nc6j9" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:26:54.039: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-nc6j9": Phase="Running", Reason="", readiness=true. Elapsed: 2.716649ms
Jan  4 23:26:54.039: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-nc6j9" satisfied condition "running"
Jan  4 23:26:54.039: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-p9cpg" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:26:54.042: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-p9cpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.835486ms
Jan  4 23:26:54.042: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-p9cpg" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037 in namespace emptydir-wrapper-5277, will wait for the garbage collector to delete the pods 01/04/23 23:26:54.042
Jan  4 23:26:54.104: INFO: Deleting ReplicationController wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037 took: 7.085921ms
Jan  4 23:26:54.205: INFO: Terminating ReplicationController wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037 pods took: 101.214426ms
STEP: Creating RC which spawns configmap-volume pods 01/04/23 23:26:57.91
Jan  4 23:26:57.928: INFO: Pod name wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6: Found 0 pods out of 5
Jan  4 23:27:02.940: INFO: Pod name wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/04/23 23:27:02.94
Jan  4 23:27:02.940: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:02.944: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024956ms
Jan  4 23:27:04.950: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009791198s
Jan  4 23:27:06.949: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008890895s
Jan  4 23:27:08.948: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00845834s
Jan  4 23:27:10.952: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011940177s
Jan  4 23:27:12.953: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.012896655s
Jan  4 23:27:12.953: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5" satisfied condition "running"
Jan  4 23:27:12.953: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-dxdhp" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:12.956: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-dxdhp": Phase="Running", Reason="", readiness=true. Elapsed: 3.116518ms
Jan  4 23:27:12.956: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-dxdhp" satisfied condition "running"
Jan  4 23:27:12.956: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-kgm5r" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:12.959: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-kgm5r": Phase="Running", Reason="", readiness=true. Elapsed: 2.624253ms
Jan  4 23:27:12.959: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-kgm5r" satisfied condition "running"
Jan  4 23:27:12.959: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-qpvqs" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:12.965: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-qpvqs": Phase="Running", Reason="", readiness=true. Elapsed: 5.88803ms
Jan  4 23:27:12.965: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-qpvqs" satisfied condition "running"
Jan  4 23:27:12.965: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-vhdcb" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:12.970: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-vhdcb": Phase="Running", Reason="", readiness=true. Elapsed: 4.848151ms
Jan  4 23:27:12.970: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-vhdcb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6 in namespace emptydir-wrapper-5277, will wait for the garbage collector to delete the pods 01/04/23 23:27:12.97
Jan  4 23:27:13.035: INFO: Deleting ReplicationController wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6 took: 6.502599ms
Jan  4 23:27:13.136: INFO: Terminating ReplicationController wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6 pods took: 101.117112ms
STEP: Creating RC which spawns configmap-volume pods 01/04/23 23:27:17.641
Jan  4 23:27:17.665: INFO: Pod name wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129: Found 0 pods out of 5
Jan  4 23:27:22.673: INFO: Pod name wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/04/23 23:27:22.673
Jan  4 23:27:22.673: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:22.676: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.480572ms
Jan  4 23:27:24.681: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008249526s
Jan  4 23:27:26.683: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010241132s
Jan  4 23:27:28.682: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009303521s
Jan  4 23:27:30.680: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007553889s
Jan  4 23:27:32.680: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Running", Reason="", readiness=true. Elapsed: 10.007281675s
Jan  4 23:27:32.680: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8" satisfied condition "running"
Jan  4 23:27:32.680: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-b96fs" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:32.683: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-b96fs": Phase="Running", Reason="", readiness=true. Elapsed: 2.629931ms
Jan  4 23:27:32.683: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-b96fs" satisfied condition "running"
Jan  4 23:27:32.683: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-pqfcb" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:32.686: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-pqfcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.864994ms
Jan  4 23:27:32.686: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-pqfcb" satisfied condition "running"
Jan  4 23:27:32.686: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-rwt48" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:32.689: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-rwt48": Phase="Running", Reason="", readiness=true. Elapsed: 3.118354ms
Jan  4 23:27:32.689: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-rwt48" satisfied condition "running"
Jan  4 23:27:32.689: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-x8cd6" in namespace "emptydir-wrapper-5277" to be "running"
Jan  4 23:27:32.692: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-x8cd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.665861ms
Jan  4 23:27:32.692: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-x8cd6" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129 in namespace emptydir-wrapper-5277, will wait for the garbage collector to delete the pods 01/04/23 23:27:32.692
Jan  4 23:27:32.751: INFO: Deleting ReplicationController wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129 took: 6.252795ms
Jan  4 23:27:32.852: INFO: Terminating ReplicationController wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129 pods took: 101.101439ms
STEP: Cleaning up the configMaps 01/04/23 23:27:36.753
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:27:37.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5277" for this suite. 01/04/23 23:27:37.159
------------------------------
• [SLOW TEST] [58.540 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:26:38.623
    Jan  4 23:26:38.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir-wrapper 01/04/23 23:26:38.625
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:26:38.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:26:38.649
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/04/23 23:26:38.652
    STEP: Creating RC which spawns configmap-volume pods 01/04/23 23:26:38.912
    Jan  4 23:26:38.998: INFO: Pod name wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037: Found 3 pods out of 5
    Jan  4 23:26:44.016: INFO: Pod name wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/04/23 23:26:44.016
    Jan  4 23:26:44.017: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:26:44.025: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.67146ms
    Jan  4 23:26:46.034: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017123158s
    Jan  4 23:26:48.031: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013718039s
    Jan  4 23:26:50.030: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013070857s
    Jan  4 23:26:52.031: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014562036s
    Jan  4 23:26:54.030: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf": Phase="Running", Reason="", readiness=true. Elapsed: 10.013130367s
    Jan  4 23:26:54.030: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-564zf" satisfied condition "running"
    Jan  4 23:26:54.030: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-d2dvb" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:26:54.033: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-d2dvb": Phase="Running", Reason="", readiness=true. Elapsed: 3.025257ms
    Jan  4 23:26:54.033: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-d2dvb" satisfied condition "running"
    Jan  4 23:26:54.033: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-kmk8v" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:26:54.036: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-kmk8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.876162ms
    Jan  4 23:26:54.036: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-kmk8v" satisfied condition "running"
    Jan  4 23:26:54.037: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-nc6j9" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:26:54.039: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-nc6j9": Phase="Running", Reason="", readiness=true. Elapsed: 2.716649ms
    Jan  4 23:26:54.039: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-nc6j9" satisfied condition "running"
    Jan  4 23:26:54.039: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-p9cpg" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:26:54.042: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-p9cpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.835486ms
    Jan  4 23:26:54.042: INFO: Pod "wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037-p9cpg" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037 in namespace emptydir-wrapper-5277, will wait for the garbage collector to delete the pods 01/04/23 23:26:54.042
    Jan  4 23:26:54.104: INFO: Deleting ReplicationController wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037 took: 7.085921ms
    Jan  4 23:26:54.205: INFO: Terminating ReplicationController wrapped-volume-race-78a0cd13-3ada-4fed-a923-335e3e141037 pods took: 101.214426ms
    STEP: Creating RC which spawns configmap-volume pods 01/04/23 23:26:57.91
    Jan  4 23:26:57.928: INFO: Pod name wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6: Found 0 pods out of 5
    Jan  4 23:27:02.940: INFO: Pod name wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/04/23 23:27:02.94
    Jan  4 23:27:02.940: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:02.944: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024956ms
    Jan  4 23:27:04.950: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009791198s
    Jan  4 23:27:06.949: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008890895s
    Jan  4 23:27:08.948: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00845834s
    Jan  4 23:27:10.952: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011940177s
    Jan  4 23:27:12.953: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.012896655s
    Jan  4 23:27:12.953: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-4wvg5" satisfied condition "running"
    Jan  4 23:27:12.953: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-dxdhp" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:12.956: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-dxdhp": Phase="Running", Reason="", readiness=true. Elapsed: 3.116518ms
    Jan  4 23:27:12.956: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-dxdhp" satisfied condition "running"
    Jan  4 23:27:12.956: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-kgm5r" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:12.959: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-kgm5r": Phase="Running", Reason="", readiness=true. Elapsed: 2.624253ms
    Jan  4 23:27:12.959: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-kgm5r" satisfied condition "running"
    Jan  4 23:27:12.959: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-qpvqs" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:12.965: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-qpvqs": Phase="Running", Reason="", readiness=true. Elapsed: 5.88803ms
    Jan  4 23:27:12.965: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-qpvqs" satisfied condition "running"
    Jan  4 23:27:12.965: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-vhdcb" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:12.970: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-vhdcb": Phase="Running", Reason="", readiness=true. Elapsed: 4.848151ms
    Jan  4 23:27:12.970: INFO: Pod "wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6-vhdcb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6 in namespace emptydir-wrapper-5277, will wait for the garbage collector to delete the pods 01/04/23 23:27:12.97
    Jan  4 23:27:13.035: INFO: Deleting ReplicationController wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6 took: 6.502599ms
    Jan  4 23:27:13.136: INFO: Terminating ReplicationController wrapped-volume-race-d257552d-205e-4774-8e53-d8f1eb79a4b6 pods took: 101.117112ms
    STEP: Creating RC which spawns configmap-volume pods 01/04/23 23:27:17.641
    Jan  4 23:27:17.665: INFO: Pod name wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129: Found 0 pods out of 5
    Jan  4 23:27:22.673: INFO: Pod name wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/04/23 23:27:22.673
    Jan  4 23:27:22.673: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:22.676: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.480572ms
    Jan  4 23:27:24.681: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008249526s
    Jan  4 23:27:26.683: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010241132s
    Jan  4 23:27:28.682: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009303521s
    Jan  4 23:27:30.680: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007553889s
    Jan  4 23:27:32.680: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8": Phase="Running", Reason="", readiness=true. Elapsed: 10.007281675s
    Jan  4 23:27:32.680: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-947n8" satisfied condition "running"
    Jan  4 23:27:32.680: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-b96fs" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:32.683: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-b96fs": Phase="Running", Reason="", readiness=true. Elapsed: 2.629931ms
    Jan  4 23:27:32.683: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-b96fs" satisfied condition "running"
    Jan  4 23:27:32.683: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-pqfcb" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:32.686: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-pqfcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.864994ms
    Jan  4 23:27:32.686: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-pqfcb" satisfied condition "running"
    Jan  4 23:27:32.686: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-rwt48" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:32.689: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-rwt48": Phase="Running", Reason="", readiness=true. Elapsed: 3.118354ms
    Jan  4 23:27:32.689: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-rwt48" satisfied condition "running"
    Jan  4 23:27:32.689: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-x8cd6" in namespace "emptydir-wrapper-5277" to be "running"
    Jan  4 23:27:32.692: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-x8cd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.665861ms
    Jan  4 23:27:32.692: INFO: Pod "wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129-x8cd6" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129 in namespace emptydir-wrapper-5277, will wait for the garbage collector to delete the pods 01/04/23 23:27:32.692
    Jan  4 23:27:32.751: INFO: Deleting ReplicationController wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129 took: 6.252795ms
    Jan  4 23:27:32.852: INFO: Terminating ReplicationController wrapped-volume-race-93d20f99-89e2-4492-9bd4-48b9b3785129 pods took: 101.101439ms
    STEP: Cleaning up the configMaps 01/04/23 23:27:36.753
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:27:37.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5277" for this suite. 01/04/23 23:27:37.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:27:37.164
Jan  4 23:27:37.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:27:37.166
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:27:37.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:27:37.338
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/04/23 23:27:37.34
Jan  4 23:27:37.347: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8" in namespace "emptydir-2050" to be "running"
Jan  4 23:27:37.350: INFO: Pod "pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68024ms
Jan  4 23:27:39.354: INFO: Pod "pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8": Phase="Running", Reason="", readiness=false. Elapsed: 2.006223052s
Jan  4 23:27:39.354: INFO: Pod "pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/04/23 23:27:39.354
Jan  4 23:27:39.354: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2050 PodName:pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:27:39.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:27:39.354: INFO: ExecWithOptions: Clientset creation
Jan  4 23:27:39.355: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/emptydir-2050/pods/pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan  4 23:27:39.428: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:27:39.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2050" for this suite. 01/04/23 23:27:39.433
------------------------------
• [2.277 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:27:37.164
    Jan  4 23:27:37.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:27:37.166
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:27:37.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:27:37.338
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/04/23 23:27:37.34
    Jan  4 23:27:37.347: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8" in namespace "emptydir-2050" to be "running"
    Jan  4 23:27:37.350: INFO: Pod "pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68024ms
    Jan  4 23:27:39.354: INFO: Pod "pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8": Phase="Running", Reason="", readiness=false. Elapsed: 2.006223052s
    Jan  4 23:27:39.354: INFO: Pod "pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/04/23 23:27:39.354
    Jan  4 23:27:39.354: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2050 PodName:pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:27:39.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:27:39.354: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:27:39.355: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/emptydir-2050/pods/pod-sharedvolume-cf55eca5-ed7a-4542-bb3c-f29ff339ebe8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan  4 23:27:39.428: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:27:39.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2050" for this suite. 01/04/23 23:27:39.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:27:39.444
Jan  4 23:27:39.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename subpath 01/04/23 23:27:39.445
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:27:39.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:27:39.464
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/04/23 23:27:39.466
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-vqnz 01/04/23 23:27:39.476
STEP: Creating a pod to test atomic-volume-subpath 01/04/23 23:27:39.477
Jan  4 23:27:39.486: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vqnz" in namespace "subpath-5861" to be "Succeeded or Failed"
Jan  4 23:27:39.492: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.521023ms
Jan  4 23:27:41.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.009995137s
Jan  4 23:27:43.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 4.010400252s
Jan  4 23:27:45.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 6.009756491s
Jan  4 23:27:47.495: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 8.00943517s
Jan  4 23:27:49.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 10.009801374s
Jan  4 23:27:51.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 12.010397079s
Jan  4 23:27:53.498: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 14.012219849s
Jan  4 23:27:55.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 16.009640004s
Jan  4 23:27:57.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 18.009935628s
Jan  4 23:27:59.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 20.010171474s
Jan  4 23:28:01.495: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=false. Elapsed: 22.0091483s
Jan  4 23:28:03.497: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011346101s
STEP: Saw pod success 01/04/23 23:28:03.497
Jan  4 23:28:03.498: INFO: Pod "pod-subpath-test-configmap-vqnz" satisfied condition "Succeeded or Failed"
Jan  4 23:28:03.503: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-configmap-vqnz container test-container-subpath-configmap-vqnz: <nil>
STEP: delete the pod 01/04/23 23:28:03.52
Jan  4 23:28:03.536: INFO: Waiting for pod pod-subpath-test-configmap-vqnz to disappear
Jan  4 23:28:03.539: INFO: Pod pod-subpath-test-configmap-vqnz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vqnz 01/04/23 23:28:03.539
Jan  4 23:28:03.539: INFO: Deleting pod "pod-subpath-test-configmap-vqnz" in namespace "subpath-5861"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:03.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5861" for this suite. 01/04/23 23:28:03.548
------------------------------
• [SLOW TEST] [24.113 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:27:39.444
    Jan  4 23:27:39.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename subpath 01/04/23 23:27:39.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:27:39.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:27:39.464
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/04/23 23:27:39.466
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-vqnz 01/04/23 23:27:39.476
    STEP: Creating a pod to test atomic-volume-subpath 01/04/23 23:27:39.477
    Jan  4 23:27:39.486: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vqnz" in namespace "subpath-5861" to be "Succeeded or Failed"
    Jan  4 23:27:39.492: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.521023ms
    Jan  4 23:27:41.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.009995137s
    Jan  4 23:27:43.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 4.010400252s
    Jan  4 23:27:45.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 6.009756491s
    Jan  4 23:27:47.495: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 8.00943517s
    Jan  4 23:27:49.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 10.009801374s
    Jan  4 23:27:51.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 12.010397079s
    Jan  4 23:27:53.498: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 14.012219849s
    Jan  4 23:27:55.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 16.009640004s
    Jan  4 23:27:57.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 18.009935628s
    Jan  4 23:27:59.496: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=true. Elapsed: 20.010171474s
    Jan  4 23:28:01.495: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Running", Reason="", readiness=false. Elapsed: 22.0091483s
    Jan  4 23:28:03.497: INFO: Pod "pod-subpath-test-configmap-vqnz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011346101s
    STEP: Saw pod success 01/04/23 23:28:03.497
    Jan  4 23:28:03.498: INFO: Pod "pod-subpath-test-configmap-vqnz" satisfied condition "Succeeded or Failed"
    Jan  4 23:28:03.503: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-subpath-test-configmap-vqnz container test-container-subpath-configmap-vqnz: <nil>
    STEP: delete the pod 01/04/23 23:28:03.52
    Jan  4 23:28:03.536: INFO: Waiting for pod pod-subpath-test-configmap-vqnz to disappear
    Jan  4 23:28:03.539: INFO: Pod pod-subpath-test-configmap-vqnz no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-vqnz 01/04/23 23:28:03.539
    Jan  4 23:28:03.539: INFO: Deleting pod "pod-subpath-test-configmap-vqnz" in namespace "subpath-5861"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:03.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5861" for this suite. 01/04/23 23:28:03.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:03.559
Jan  4 23:28:03.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename lease-test 01/04/23 23:28:03.56
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:03.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:03.595
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:03.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-6654" for this suite. 01/04/23 23:28:03.684
------------------------------
• [0.132 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:03.559
    Jan  4 23:28:03.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename lease-test 01/04/23 23:28:03.56
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:03.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:03.595
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:03.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-6654" for this suite. 01/04/23 23:28:03.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:03.694
Jan  4 23:28:03.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename dns 01/04/23 23:28:03.696
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:03.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:03.721
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/04/23 23:28:03.725
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9429 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9429;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9429 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9429;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9429.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9429.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9429.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9429.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9429.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9429.svc;check="$$(dig +notcp +noall +answer +search 173.51.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.51.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.51.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.51.173_tcp@PTR;sleep 1; done
 01/04/23 23:28:03.774
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9429 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9429;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9429 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9429;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9429.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9429.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9429.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9429.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9429.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9429.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9429.svc;check="$$(dig +notcp +noall +answer +search 173.51.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.51.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.51.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.51.173_tcp@PTR;sleep 1; done
 01/04/23 23:28:03.774
STEP: creating a pod to probe DNS 01/04/23 23:28:03.774
STEP: submitting the pod to kubernetes 01/04/23 23:28:03.775
Jan  4 23:28:03.797: INFO: Waiting up to 15m0s for pod "dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a" in namespace "dns-9429" to be "running"
Jan  4 23:28:03.815: INFO: Pod "dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.806655ms
Jan  4 23:28:05.819: INFO: Pod "dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.021619356s
Jan  4 23:28:05.819: INFO: Pod "dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a" satisfied condition "running"
STEP: retrieving the pod 01/04/23 23:28:05.819
STEP: looking for the results for each expected name from probers 01/04/23 23:28:05.823
Jan  4 23:28:05.831: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.834: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.837: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.841: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.844: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.848: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.852: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.856: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.873: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.877: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.880: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.883: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.885: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.888: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.892: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.895: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:05.909: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

Jan  4 23:28:10.914: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.917: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.922: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.926: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.929: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.937: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.940: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.943: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.961: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.964: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.968: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.971: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.974: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.985: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:10.993: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:11.005: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:11.039: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

Jan  4 23:28:15.914: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.917: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.920: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.922: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.925: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.927: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.930: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.933: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.946: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.949: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.951: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.954: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.956: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.959: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.961: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.964: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:15.976: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

Jan  4 23:28:20.917: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.921: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.924: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.927: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.930: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.933: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.936: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.939: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.964: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.970: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.974: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.978: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.983: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.987: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.990: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:20.993: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:21.010: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

Jan  4 23:28:25.916: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.920: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.925: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.928: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.936: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.940: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.943: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.946: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.964: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.967: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.971: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.975: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.979: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.982: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.986: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:25.995: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:26.030: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

Jan  4 23:28:30.922: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.932: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.936: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.939: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.942: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.945: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.948: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.951: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.966: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.969: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.972: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.974: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.978: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.981: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.984: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.987: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
Jan  4 23:28:30.998: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

Jan  4 23:28:35.986: INFO: DNS probes using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a succeeded

STEP: deleting the pod 01/04/23 23:28:35.986
STEP: deleting the test service 01/04/23 23:28:36.021
STEP: deleting the test headless service 01/04/23 23:28:36.341
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:36.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9429" for this suite. 01/04/23 23:28:36.386
------------------------------
• [SLOW TEST] [32.703 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:03.694
    Jan  4 23:28:03.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename dns 01/04/23 23:28:03.696
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:03.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:03.721
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/04/23 23:28:03.725
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9429 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9429;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9429 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9429;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9429.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9429.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9429.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9429.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9429.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9429.svc;check="$$(dig +notcp +noall +answer +search 173.51.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.51.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.51.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.51.173_tcp@PTR;sleep 1; done
     01/04/23 23:28:03.774
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9429 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9429;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9429 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9429;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9429.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9429.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9429.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9429.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9429.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9429.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9429.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9429.svc;check="$$(dig +notcp +noall +answer +search 173.51.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.51.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.51.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.51.173_tcp@PTR;sleep 1; done
     01/04/23 23:28:03.774
    STEP: creating a pod to probe DNS 01/04/23 23:28:03.774
    STEP: submitting the pod to kubernetes 01/04/23 23:28:03.775
    Jan  4 23:28:03.797: INFO: Waiting up to 15m0s for pod "dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a" in namespace "dns-9429" to be "running"
    Jan  4 23:28:03.815: INFO: Pod "dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.806655ms
    Jan  4 23:28:05.819: INFO: Pod "dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.021619356s
    Jan  4 23:28:05.819: INFO: Pod "dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 23:28:05.819
    STEP: looking for the results for each expected name from probers 01/04/23 23:28:05.823
    Jan  4 23:28:05.831: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.834: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.837: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.841: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.844: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.848: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.852: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.856: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.873: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.877: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.880: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.883: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.885: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.888: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.892: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.895: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:05.909: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

    Jan  4 23:28:10.914: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.917: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.922: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.926: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.929: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.937: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.940: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.943: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.961: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.964: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.968: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.971: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.974: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.985: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:10.993: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:11.005: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:11.039: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

    Jan  4 23:28:15.914: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.917: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.920: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.922: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.925: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.927: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.930: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.933: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.946: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.949: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.951: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.954: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.956: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.959: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.961: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.964: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:15.976: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

    Jan  4 23:28:20.917: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.921: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.924: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.927: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.930: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.933: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.936: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.939: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.964: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.970: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.974: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.978: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.983: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.987: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.990: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:20.993: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:21.010: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

    Jan  4 23:28:25.916: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.920: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.925: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.928: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.936: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.940: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.943: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.946: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.964: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.967: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.971: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.975: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.979: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.982: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.986: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:25.995: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:26.030: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

    Jan  4 23:28:30.922: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.932: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.936: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.939: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.942: INFO: Unable to read wheezy_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.945: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.948: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.951: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.966: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.969: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.972: INFO: Unable to read jessie_udp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.974: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429 from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.978: INFO: Unable to read jessie_udp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.981: INFO: Unable to read jessie_tcp@dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.984: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.987: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc from pod dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a: the server could not find the requested resource (get pods dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a)
    Jan  4 23:28:30.998: INFO: Lookups using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9429 wheezy_tcp@dns-test-service.dns-9429 wheezy_udp@dns-test-service.dns-9429.svc wheezy_tcp@dns-test-service.dns-9429.svc wheezy_udp@_http._tcp.dns-test-service.dns-9429.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9429.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9429 jessie_tcp@dns-test-service.dns-9429 jessie_udp@dns-test-service.dns-9429.svc jessie_tcp@dns-test-service.dns-9429.svc jessie_udp@_http._tcp.dns-test-service.dns-9429.svc jessie_tcp@_http._tcp.dns-test-service.dns-9429.svc]

    Jan  4 23:28:35.986: INFO: DNS probes using dns-9429/dns-test-46f0bda7-02a6-44ae-8f57-ac0136b7bb0a succeeded

    STEP: deleting the pod 01/04/23 23:28:35.986
    STEP: deleting the test service 01/04/23 23:28:36.021
    STEP: deleting the test headless service 01/04/23 23:28:36.341
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:36.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9429" for this suite. 01/04/23 23:28:36.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:36.426
Jan  4 23:28:36.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename disruption 01/04/23 23:28:36.433
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:36.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:36.469
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/04/23 23:28:36.492
STEP: Waiting for all pods to be running 01/04/23 23:28:36.526
Jan  4 23:28:36.530: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:38.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8330" for this suite. 01/04/23 23:28:38.542
------------------------------
• [2.122 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:36.426
    Jan  4 23:28:36.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename disruption 01/04/23 23:28:36.433
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:36.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:36.469
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/04/23 23:28:36.492
    STEP: Waiting for all pods to be running 01/04/23 23:28:36.526
    Jan  4 23:28:36.530: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:38.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8330" for this suite. 01/04/23 23:28:38.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:38.55
Jan  4 23:28:38.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:28:38.55
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:38.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:38.568
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/04/23 23:28:38.57
Jan  4 23:28:38.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: mark a version not serverd 01/04/23 23:28:43.102
STEP: check the unserved version gets removed 01/04/23 23:28:43.122
STEP: check the other version is not changed 01/04/23 23:28:44.809
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:48.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9770" for this suite. 01/04/23 23:28:48.616
------------------------------
• [SLOW TEST] [10.072 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:38.55
    Jan  4 23:28:38.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:28:38.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:38.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:38.568
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/04/23 23:28:38.57
    Jan  4 23:28:38.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: mark a version not serverd 01/04/23 23:28:43.102
    STEP: check the unserved version gets removed 01/04/23 23:28:43.122
    STEP: check the other version is not changed 01/04/23 23:28:44.809
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:48.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9770" for this suite. 01/04/23 23:28:48.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:48.624
Jan  4 23:28:48.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename conformance-tests 01/04/23 23:28:48.625
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:48.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:48.648
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/04/23 23:28:48.65
Jan  4 23:28:48.651: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:48.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-8219" for this suite. 01/04/23 23:28:48.665
------------------------------
• [0.048 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:48.624
    Jan  4 23:28:48.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename conformance-tests 01/04/23 23:28:48.625
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:48.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:48.648
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/04/23 23:28:48.65
    Jan  4 23:28:48.651: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:48.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-8219" for this suite. 01/04/23 23:28:48.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:48.673
Jan  4 23:28:48.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename proxy 01/04/23 23:28:48.674
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:48.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:48.691
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan  4 23:28:48.693: INFO: Creating pod...
Jan  4 23:28:48.701: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8441" to be "running"
Jan  4 23:28:48.706: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.005012ms
Jan  4 23:28:50.710: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009678512s
Jan  4 23:28:50.710: INFO: Pod "agnhost" satisfied condition "running"
Jan  4 23:28:50.711: INFO: Creating service...
Jan  4 23:28:50.725: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=DELETE
Jan  4 23:28:50.741: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  4 23:28:50.741: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=OPTIONS
Jan  4 23:28:50.749: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  4 23:28:50.749: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=PATCH
Jan  4 23:28:50.756: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  4 23:28:50.756: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=POST
Jan  4 23:28:50.760: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  4 23:28:50.760: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=PUT
Jan  4 23:28:50.764: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  4 23:28:50.764: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=DELETE
Jan  4 23:28:50.770: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  4 23:28:50.771: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan  4 23:28:50.777: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  4 23:28:50.777: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=PATCH
Jan  4 23:28:50.782: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  4 23:28:50.782: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=POST
Jan  4 23:28:50.787: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  4 23:28:50.787: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=PUT
Jan  4 23:28:50.794: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  4 23:28:50.794: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=GET
Jan  4 23:28:50.797: INFO: http.Client request:GET StatusCode:301
Jan  4 23:28:50.797: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=GET
Jan  4 23:28:50.801: INFO: http.Client request:GET StatusCode:301
Jan  4 23:28:50.801: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=HEAD
Jan  4 23:28:50.803: INFO: http.Client request:HEAD StatusCode:301
Jan  4 23:28:50.803: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=HEAD
Jan  4 23:28:50.811: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:50.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8441" for this suite. 01/04/23 23:28:50.821
------------------------------
• [2.155 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:48.673
    Jan  4 23:28:48.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename proxy 01/04/23 23:28:48.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:48.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:48.691
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan  4 23:28:48.693: INFO: Creating pod...
    Jan  4 23:28:48.701: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8441" to be "running"
    Jan  4 23:28:48.706: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.005012ms
    Jan  4 23:28:50.710: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009678512s
    Jan  4 23:28:50.710: INFO: Pod "agnhost" satisfied condition "running"
    Jan  4 23:28:50.711: INFO: Creating service...
    Jan  4 23:28:50.725: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=DELETE
    Jan  4 23:28:50.741: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  4 23:28:50.741: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=OPTIONS
    Jan  4 23:28:50.749: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  4 23:28:50.749: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=PATCH
    Jan  4 23:28:50.756: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  4 23:28:50.756: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=POST
    Jan  4 23:28:50.760: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  4 23:28:50.760: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=PUT
    Jan  4 23:28:50.764: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  4 23:28:50.764: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan  4 23:28:50.770: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  4 23:28:50.771: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan  4 23:28:50.777: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  4 23:28:50.777: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan  4 23:28:50.782: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  4 23:28:50.782: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=POST
    Jan  4 23:28:50.787: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  4 23:28:50.787: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=PUT
    Jan  4 23:28:50.794: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  4 23:28:50.794: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=GET
    Jan  4 23:28:50.797: INFO: http.Client request:GET StatusCode:301
    Jan  4 23:28:50.797: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=GET
    Jan  4 23:28:50.801: INFO: http.Client request:GET StatusCode:301
    Jan  4 23:28:50.801: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/pods/agnhost/proxy?method=HEAD
    Jan  4 23:28:50.803: INFO: http.Client request:HEAD StatusCode:301
    Jan  4 23:28:50.803: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8441/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan  4 23:28:50.811: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:50.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8441" for this suite. 01/04/23 23:28:50.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:50.831
Jan  4 23:28:50.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:28:50.831
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:50.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:50.86
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-1575fcf1-700d-4c19-9b0c-22a7302d276c 01/04/23 23:28:50.862
STEP: Creating secret with name secret-projected-all-test-volume-a782db50-6186-449d-b808-d82082f8b265 01/04/23 23:28:50.867
STEP: Creating a pod to test Check all projections for projected volume plugin 01/04/23 23:28:50.872
Jan  4 23:28:50.880: INFO: Waiting up to 5m0s for pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f" in namespace "projected-6875" to be "Succeeded or Failed"
Jan  4 23:28:50.884: INFO: Pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.440206ms
Jan  4 23:28:52.889: INFO: Pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008898811s
Jan  4 23:28:54.888: INFO: Pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007694475s
STEP: Saw pod success 01/04/23 23:28:54.888
Jan  4 23:28:54.888: INFO: Pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f" satisfied condition "Succeeded or Failed"
Jan  4 23:28:54.890: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f container projected-all-volume-test: <nil>
STEP: delete the pod 01/04/23 23:28:54.896
Jan  4 23:28:54.904: INFO: Waiting for pod projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f to disappear
Jan  4 23:28:54.906: INFO: Pod projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6875" for this suite. 01/04/23 23:28:54.91
------------------------------
• [4.084 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:50.831
    Jan  4 23:28:50.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:28:50.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:50.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:50.86
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-1575fcf1-700d-4c19-9b0c-22a7302d276c 01/04/23 23:28:50.862
    STEP: Creating secret with name secret-projected-all-test-volume-a782db50-6186-449d-b808-d82082f8b265 01/04/23 23:28:50.867
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/04/23 23:28:50.872
    Jan  4 23:28:50.880: INFO: Waiting up to 5m0s for pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f" in namespace "projected-6875" to be "Succeeded or Failed"
    Jan  4 23:28:50.884: INFO: Pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.440206ms
    Jan  4 23:28:52.889: INFO: Pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008898811s
    Jan  4 23:28:54.888: INFO: Pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007694475s
    STEP: Saw pod success 01/04/23 23:28:54.888
    Jan  4 23:28:54.888: INFO: Pod "projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f" satisfied condition "Succeeded or Failed"
    Jan  4 23:28:54.890: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f container projected-all-volume-test: <nil>
    STEP: delete the pod 01/04/23 23:28:54.896
    Jan  4 23:28:54.904: INFO: Waiting for pod projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f to disappear
    Jan  4 23:28:54.906: INFO: Pod projected-volume-23b248a7-06c3-42a2-8caa-56bdd348dd7f no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6875" for this suite. 01/04/23 23:28:54.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:54.917
Jan  4 23:28:54.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename gc 01/04/23 23:28:54.918
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:54.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:54.935
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/04/23 23:28:54.938
STEP: Wait for the Deployment to create new ReplicaSet 01/04/23 23:28:54.943
STEP: delete the deployment 01/04/23 23:28:55.46
STEP: wait for all rs to be garbage collected 01/04/23 23:28:55.468
STEP: expected 0 rs, got 1 rs 01/04/23 23:28:55.488
STEP: expected 0 pods, got 2 pods 01/04/23 23:28:55.496
STEP: Gathering metrics 01/04/23 23:28:56.031
Jan  4 23:28:56.068: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
Jan  4 23:28:56.071: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.111839ms
Jan  4 23:28:56.071: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
Jan  4 23:28:56.071: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
Jan  4 23:28:56.141: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:56.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9384" for this suite. 01/04/23 23:28:56.145
------------------------------
• [1.234 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:54.917
    Jan  4 23:28:54.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename gc 01/04/23 23:28:54.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:54.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:54.935
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/04/23 23:28:54.938
    STEP: Wait for the Deployment to create new ReplicaSet 01/04/23 23:28:54.943
    STEP: delete the deployment 01/04/23 23:28:55.46
    STEP: wait for all rs to be garbage collected 01/04/23 23:28:55.468
    STEP: expected 0 rs, got 1 rs 01/04/23 23:28:55.488
    STEP: expected 0 pods, got 2 pods 01/04/23 23:28:55.496
    STEP: Gathering metrics 01/04/23 23:28:56.031
    Jan  4 23:28:56.068: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Jan  4 23:28:56.071: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.111839ms
    Jan  4 23:28:56.071: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
    Jan  4 23:28:56.071: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
    Jan  4 23:28:56.141: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:56.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9384" for this suite. 01/04/23 23:28:56.145
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:56.152
Jan  4 23:28:56.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename tables 01/04/23 23:28:56.154
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:56.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:56.189
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan  4 23:28:56.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-4980" for this suite. 01/04/23 23:28:56.221
------------------------------
• [0.074 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:56.152
    Jan  4 23:28:56.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename tables 01/04/23 23:28:56.154
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:56.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:56.189
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:28:56.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-4980" for this suite. 01/04/23 23:28:56.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:28:56.229
Jan  4 23:28:56.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename containers 01/04/23 23:28:56.23
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:56.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:56.288
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/04/23 23:28:56.291
Jan  4 23:28:56.300: INFO: Waiting up to 5m0s for pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922" in namespace "containers-4456" to be "Succeeded or Failed"
Jan  4 23:28:56.306: INFO: Pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922": Phase="Pending", Reason="", readiness=false. Elapsed: 6.155345ms
Jan  4 23:28:58.313: INFO: Pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013080882s
Jan  4 23:29:00.311: INFO: Pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010871495s
STEP: Saw pod success 01/04/23 23:29:00.311
Jan  4 23:29:00.311: INFO: Pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922" satisfied condition "Succeeded or Failed"
Jan  4 23:29:00.314: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922 container agnhost-container: <nil>
STEP: delete the pod 01/04/23 23:29:00.325
Jan  4 23:29:00.337: INFO: Waiting for pod client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922 to disappear
Jan  4 23:29:00.340: INFO: Pod client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:00.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4456" for this suite. 01/04/23 23:29:00.343
------------------------------
• [4.119 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:28:56.229
    Jan  4 23:28:56.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename containers 01/04/23 23:28:56.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:28:56.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:28:56.288
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/04/23 23:28:56.291
    Jan  4 23:28:56.300: INFO: Waiting up to 5m0s for pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922" in namespace "containers-4456" to be "Succeeded or Failed"
    Jan  4 23:28:56.306: INFO: Pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922": Phase="Pending", Reason="", readiness=false. Elapsed: 6.155345ms
    Jan  4 23:28:58.313: INFO: Pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013080882s
    Jan  4 23:29:00.311: INFO: Pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010871495s
    STEP: Saw pod success 01/04/23 23:29:00.311
    Jan  4 23:29:00.311: INFO: Pod "client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922" satisfied condition "Succeeded or Failed"
    Jan  4 23:29:00.314: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922 container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 23:29:00.325
    Jan  4 23:29:00.337: INFO: Waiting for pod client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922 to disappear
    Jan  4 23:29:00.340: INFO: Pod client-containers-cd026aa2-5153-47e7-b147-b5ecbe2ca922 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:00.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4456" for this suite. 01/04/23 23:29:00.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:00.35
Jan  4 23:29:00.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:29:00.353
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:00.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:00.386
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-0b23c941-e8a9-42a7-807f-a2c6f811a175 01/04/23 23:29:00.39
STEP: Creating a pod to test consume secrets 01/04/23 23:29:00.395
Jan  4 23:29:00.403: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b" in namespace "projected-3070" to be "Succeeded or Failed"
Jan  4 23:29:00.411: INFO: Pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.328173ms
Jan  4 23:29:02.419: INFO: Pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015412365s
Jan  4 23:29:04.415: INFO: Pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011723229s
STEP: Saw pod success 01/04/23 23:29:04.415
Jan  4 23:29:04.416: INFO: Pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b" satisfied condition "Succeeded or Failed"
Jan  4 23:29:04.418: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b container projected-secret-volume-test: <nil>
STEP: delete the pod 01/04/23 23:29:04.427
Jan  4 23:29:04.438: INFO: Waiting for pod pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b to disappear
Jan  4 23:29:04.441: INFO: Pod pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:04.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3070" for this suite. 01/04/23 23:29:04.448
------------------------------
• [4.104 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:00.35
    Jan  4 23:29:00.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:29:00.353
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:00.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:00.386
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-0b23c941-e8a9-42a7-807f-a2c6f811a175 01/04/23 23:29:00.39
    STEP: Creating a pod to test consume secrets 01/04/23 23:29:00.395
    Jan  4 23:29:00.403: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b" in namespace "projected-3070" to be "Succeeded or Failed"
    Jan  4 23:29:00.411: INFO: Pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.328173ms
    Jan  4 23:29:02.419: INFO: Pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015412365s
    Jan  4 23:29:04.415: INFO: Pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011723229s
    STEP: Saw pod success 01/04/23 23:29:04.415
    Jan  4 23:29:04.416: INFO: Pod "pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b" satisfied condition "Succeeded or Failed"
    Jan  4 23:29:04.418: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/04/23 23:29:04.427
    Jan  4 23:29:04.438: INFO: Waiting for pod pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b to disappear
    Jan  4 23:29:04.441: INFO: Pod pod-projected-secrets-e29aa38b-325d-4e83-8816-e6aa6bd1849b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:04.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3070" for this suite. 01/04/23 23:29:04.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:04.5
Jan  4 23:29:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replicaset 01/04/23 23:29:04.503
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:04.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:04.522
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/04/23 23:29:04.525
Jan  4 23:29:04.534: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  4 23:29:09.540: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/04/23 23:29:09.54
STEP: getting scale subresource 01/04/23 23:29:09.541
STEP: updating a scale subresource 01/04/23 23:29:09.544
STEP: verifying the replicaset Spec.Replicas was modified 01/04/23 23:29:09.549
STEP: Patch a scale subresource 01/04/23 23:29:09.552
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:09.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5176" for this suite. 01/04/23 23:29:09.582
------------------------------
• [SLOW TEST] [5.100 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:04.5
    Jan  4 23:29:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replicaset 01/04/23 23:29:04.503
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:04.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:04.522
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/04/23 23:29:04.525
    Jan  4 23:29:04.534: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  4 23:29:09.540: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/04/23 23:29:09.54
    STEP: getting scale subresource 01/04/23 23:29:09.541
    STEP: updating a scale subresource 01/04/23 23:29:09.544
    STEP: verifying the replicaset Spec.Replicas was modified 01/04/23 23:29:09.549
    STEP: Patch a scale subresource 01/04/23 23:29:09.552
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:09.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5176" for this suite. 01/04/23 23:29:09.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:09.603
Jan  4 23:29:09.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:29:09.605
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:09.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:09.637
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/04/23 23:29:09.639
Jan  4 23:29:09.648: INFO: Waiting up to 5m0s for pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29" in namespace "emptydir-3593" to be "Succeeded or Failed"
Jan  4 23:29:09.652: INFO: Pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070999ms
Jan  4 23:29:11.658: INFO: Pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009914168s
Jan  4 23:29:13.656: INFO: Pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008128558s
STEP: Saw pod success 01/04/23 23:29:13.656
Jan  4 23:29:13.656: INFO: Pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29" satisfied condition "Succeeded or Failed"
Jan  4 23:29:13.668: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29 container test-container: <nil>
STEP: delete the pod 01/04/23 23:29:13.677
Jan  4 23:29:13.707: INFO: Waiting for pod pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29 to disappear
Jan  4 23:29:13.710: INFO: Pod pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:13.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3593" for this suite. 01/04/23 23:29:13.717
------------------------------
• [4.120 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:09.603
    Jan  4 23:29:09.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:29:09.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:09.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:09.637
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/04/23 23:29:09.639
    Jan  4 23:29:09.648: INFO: Waiting up to 5m0s for pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29" in namespace "emptydir-3593" to be "Succeeded or Failed"
    Jan  4 23:29:09.652: INFO: Pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070999ms
    Jan  4 23:29:11.658: INFO: Pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009914168s
    Jan  4 23:29:13.656: INFO: Pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008128558s
    STEP: Saw pod success 01/04/23 23:29:13.656
    Jan  4 23:29:13.656: INFO: Pod "pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29" satisfied condition "Succeeded or Failed"
    Jan  4 23:29:13.668: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29 container test-container: <nil>
    STEP: delete the pod 01/04/23 23:29:13.677
    Jan  4 23:29:13.707: INFO: Waiting for pod pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29 to disappear
    Jan  4 23:29:13.710: INFO: Pod pod-b5b05cb9-eca3-4d6b-a72c-aca53e0e0b29 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:13.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3593" for this suite. 01/04/23 23:29:13.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:13.724
Jan  4 23:29:13.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename statefulset 01/04/23 23:29:13.725
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:13.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:13.75
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8318 01/04/23 23:29:13.752
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan  4 23:29:13.781: INFO: Found 0 stateful pods, waiting for 1
Jan  4 23:29:23.785: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/04/23 23:29:23.79
W0104 23:29:23.799395      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  4 23:29:23.810: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:29:23.810: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Jan  4 23:29:33.814: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:29:33.814: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/04/23 23:29:33.82
STEP: Delete all of the StatefulSets 01/04/23 23:29:33.823
STEP: Verify that StatefulSets have been deleted 01/04/23 23:29:33.83
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  4 23:29:33.833: INFO: Deleting all statefulset in ns statefulset-8318
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:33.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8318" for this suite. 01/04/23 23:29:33.853
------------------------------
• [SLOW TEST] [20.160 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:13.724
    Jan  4 23:29:13.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename statefulset 01/04/23 23:29:13.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:13.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:13.75
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8318 01/04/23 23:29:13.752
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan  4 23:29:13.781: INFO: Found 0 stateful pods, waiting for 1
    Jan  4 23:29:23.785: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/04/23 23:29:23.79
    W0104 23:29:23.799395      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  4 23:29:23.810: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:29:23.810: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Jan  4 23:29:33.814: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:29:33.814: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/04/23 23:29:33.82
    STEP: Delete all of the StatefulSets 01/04/23 23:29:33.823
    STEP: Verify that StatefulSets have been deleted 01/04/23 23:29:33.83
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  4 23:29:33.833: INFO: Deleting all statefulset in ns statefulset-8318
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:33.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8318" for this suite. 01/04/23 23:29:33.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:33.885
Jan  4 23:29:33.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 23:29:33.887
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:33.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:33.979
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 23:29:34.035
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:29:34.92
STEP: Deploying the webhook pod 01/04/23 23:29:34.931
STEP: Wait for the deployment to be ready 01/04/23 23:29:34.953
Jan  4 23:29:34.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 29, 34, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 29, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/04/23 23:29:37.071
STEP: Verifying the service has paired with the endpoint 01/04/23 23:29:37.085
Jan  4 23:29:38.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/04/23 23:29:38.09
STEP: create a pod 01/04/23 23:29:38.106
Jan  4 23:29:38.113: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3328" to be "running"
Jan  4 23:29:38.116: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.623086ms
Jan  4 23:29:40.120: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006592058s
Jan  4 23:29:40.120: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/04/23 23:29:40.12
Jan  4 23:29:40.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=webhook-3328 attach --namespace=webhook-3328 to-be-attached-pod -i -c=container1'
Jan  4 23:29:40.254: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:40.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3328" for this suite. 01/04/23 23:29:40.329
STEP: Destroying namespace "webhook-3328-markers" for this suite. 01/04/23 23:29:40.345
------------------------------
• [SLOW TEST] [6.468 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:33.885
    Jan  4 23:29:33.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 23:29:33.887
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:33.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:33.979
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 23:29:34.035
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:29:34.92
    STEP: Deploying the webhook pod 01/04/23 23:29:34.931
    STEP: Wait for the deployment to be ready 01/04/23 23:29:34.953
    Jan  4 23:29:34.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 29, 34, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 4, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 29, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/04/23 23:29:37.071
    STEP: Verifying the service has paired with the endpoint 01/04/23 23:29:37.085
    Jan  4 23:29:38.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/04/23 23:29:38.09
    STEP: create a pod 01/04/23 23:29:38.106
    Jan  4 23:29:38.113: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3328" to be "running"
    Jan  4 23:29:38.116: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.623086ms
    Jan  4 23:29:40.120: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006592058s
    Jan  4 23:29:40.120: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/04/23 23:29:40.12
    Jan  4 23:29:40.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=webhook-3328 attach --namespace=webhook-3328 to-be-attached-pod -i -c=container1'
    Jan  4 23:29:40.254: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:40.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3328" for this suite. 01/04/23 23:29:40.329
    STEP: Destroying namespace "webhook-3328-markers" for this suite. 01/04/23 23:29:40.345
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:40.354
Jan  4 23:29:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename job 01/04/23 23:29:40.36
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:40.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:40.41
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/04/23 23:29:40.42
STEP: Patching the Job 01/04/23 23:29:40.434
STEP: Watching for Job to be patched 01/04/23 23:29:40.456
Jan  4 23:29:40.461: INFO: Event ADDED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  4 23:29:40.461: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  4 23:29:40.461: INFO: Event MODIFIED found for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/04/23 23:29:40.461
STEP: Watching for Job to be updated 01/04/23 23:29:40.472
Jan  4 23:29:40.476: INFO: Event MODIFIED found for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  4 23:29:40.476: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/04/23 23:29:40.476
Jan  4 23:29:40.481: INFO: Job: e2e-z8jp9 as labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched]
STEP: Waiting for job to complete 01/04/23 23:29:40.481
STEP: Delete a job collection with a labelselector 01/04/23 23:29:50.489
STEP: Watching for Job to be deleted 01/04/23 23:29:50.496
Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  4 23:29:50.502: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  4 23:29:50.502: INFO: Event DELETED found for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/04/23 23:29:50.502
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:50.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6781" for this suite. 01/04/23 23:29:50.523
------------------------------
• [SLOW TEST] [10.180 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:40.354
    Jan  4 23:29:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename job 01/04/23 23:29:40.36
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:40.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:40.41
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/04/23 23:29:40.42
    STEP: Patching the Job 01/04/23 23:29:40.434
    STEP: Watching for Job to be patched 01/04/23 23:29:40.456
    Jan  4 23:29:40.461: INFO: Event ADDED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  4 23:29:40.461: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  4 23:29:40.461: INFO: Event MODIFIED found for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/04/23 23:29:40.461
    STEP: Watching for Job to be updated 01/04/23 23:29:40.472
    Jan  4 23:29:40.476: INFO: Event MODIFIED found for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  4 23:29:40.476: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/04/23 23:29:40.476
    Jan  4 23:29:40.481: INFO: Job: e2e-z8jp9 as labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched]
    STEP: Waiting for job to complete 01/04/23 23:29:40.481
    STEP: Delete a job collection with a labelselector 01/04/23 23:29:50.489
    STEP: Watching for Job to be deleted 01/04/23 23:29:50.496
    Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  4 23:29:50.501: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  4 23:29:50.502: INFO: Event MODIFIED observed for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  4 23:29:50.502: INFO: Event DELETED found for Job e2e-z8jp9 in namespace job-6781 with labels: map[e2e-job-label:e2e-z8jp9 e2e-z8jp9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/04/23 23:29:50.502
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:50.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6781" for this suite. 01/04/23 23:29:50.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:50.535
Jan  4 23:29:50.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:29:50.536
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:50.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:50.576
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/04/23 23:29:50.578
Jan  4 23:29:50.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c" in namespace "downward-api-1141" to be "Succeeded or Failed"
Jan  4 23:29:50.613: INFO: Pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012571ms
Jan  4 23:29:52.618: INFO: Pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c": Phase="Running", Reason="", readiness=false. Elapsed: 2.018044241s
Jan  4 23:29:54.617: INFO: Pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017558187s
STEP: Saw pod success 01/04/23 23:29:54.617
Jan  4 23:29:54.617: INFO: Pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c" satisfied condition "Succeeded or Failed"
Jan  4 23:29:54.624: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c container client-container: <nil>
STEP: delete the pod 01/04/23 23:29:54.63
Jan  4 23:29:54.639: INFO: Waiting for pod downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c to disappear
Jan  4 23:29:54.641: INFO: Pod downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:54.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1141" for this suite. 01/04/23 23:29:54.644
------------------------------
• [4.116 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:50.535
    Jan  4 23:29:50.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:29:50.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:50.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:50.576
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/04/23 23:29:50.578
    Jan  4 23:29:50.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c" in namespace "downward-api-1141" to be "Succeeded or Failed"
    Jan  4 23:29:50.613: INFO: Pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012571ms
    Jan  4 23:29:52.618: INFO: Pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c": Phase="Running", Reason="", readiness=false. Elapsed: 2.018044241s
    Jan  4 23:29:54.617: INFO: Pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017558187s
    STEP: Saw pod success 01/04/23 23:29:54.617
    Jan  4 23:29:54.617: INFO: Pod "downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c" satisfied condition "Succeeded or Failed"
    Jan  4 23:29:54.624: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c container client-container: <nil>
    STEP: delete the pod 01/04/23 23:29:54.63
    Jan  4 23:29:54.639: INFO: Waiting for pod downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c to disappear
    Jan  4 23:29:54.641: INFO: Pod downwardapi-volume-76ceac84-f46e-4ebd-91ec-b5f92ff66c9c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:54.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1141" for this suite. 01/04/23 23:29:54.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:54.654
Jan  4 23:29:54.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replicaset 01/04/23 23:29:54.655
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:54.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:54.68
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/04/23 23:29:54.682
Jan  4 23:29:54.689: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3531" to be "running and ready"
Jan  4 23:29:54.704: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.596157ms
Jan  4 23:29:54.704: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:29:56.709: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.020018596s
Jan  4 23:29:56.709: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan  4 23:29:56.709: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/04/23 23:29:56.72
STEP: Then the orphan pod is adopted 01/04/23 23:29:56.732
STEP: When the matched label of one of its pods change 01/04/23 23:29:56.744
Jan  4 23:29:56.753: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/04/23 23:29:56.781
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:29:56.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3531" for this suite. 01/04/23 23:29:56.811
------------------------------
• [2.166 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:54.654
    Jan  4 23:29:54.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replicaset 01/04/23 23:29:54.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:54.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:54.68
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/04/23 23:29:54.682
    Jan  4 23:29:54.689: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3531" to be "running and ready"
    Jan  4 23:29:54.704: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.596157ms
    Jan  4 23:29:54.704: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:29:56.709: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.020018596s
    Jan  4 23:29:56.709: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan  4 23:29:56.709: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/04/23 23:29:56.72
    STEP: Then the orphan pod is adopted 01/04/23 23:29:56.732
    STEP: When the matched label of one of its pods change 01/04/23 23:29:56.744
    Jan  4 23:29:56.753: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/04/23 23:29:56.781
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:29:56.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3531" for this suite. 01/04/23 23:29:56.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:29:56.821
Jan  4 23:29:56.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-probe 01/04/23 23:29:56.822
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:56.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:56.845
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975 in namespace container-probe-8797 01/04/23 23:29:56.848
Jan  4 23:29:56.855: INFO: Waiting up to 5m0s for pod "busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975" in namespace "container-probe-8797" to be "not pending"
Jan  4 23:29:56.859: INFO: Pod "busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392707ms
Jan  4 23:29:58.863: INFO: Pod "busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975": Phase="Running", Reason="", readiness=true. Elapsed: 2.008263469s
Jan  4 23:29:58.863: INFO: Pod "busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975" satisfied condition "not pending"
Jan  4 23:29:58.863: INFO: Started pod busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975 in namespace container-probe-8797
STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 23:29:58.863
Jan  4 23:29:58.866: INFO: Initial restart count of pod busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975 is 0
Jan  4 23:30:48.980: INFO: Restart count of pod container-probe-8797/busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975 is now 1 (50.113857642s elapsed)
STEP: deleting the pod 01/04/23 23:30:48.98
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  4 23:30:49.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8797" for this suite. 01/04/23 23:30:49.014
------------------------------
• [SLOW TEST] [52.199 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:29:56.821
    Jan  4 23:29:56.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-probe 01/04/23 23:29:56.822
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:29:56.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:29:56.845
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975 in namespace container-probe-8797 01/04/23 23:29:56.848
    Jan  4 23:29:56.855: INFO: Waiting up to 5m0s for pod "busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975" in namespace "container-probe-8797" to be "not pending"
    Jan  4 23:29:56.859: INFO: Pod "busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392707ms
    Jan  4 23:29:58.863: INFO: Pod "busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975": Phase="Running", Reason="", readiness=true. Elapsed: 2.008263469s
    Jan  4 23:29:58.863: INFO: Pod "busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975" satisfied condition "not pending"
    Jan  4 23:29:58.863: INFO: Started pod busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975 in namespace container-probe-8797
    STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 23:29:58.863
    Jan  4 23:29:58.866: INFO: Initial restart count of pod busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975 is 0
    Jan  4 23:30:48.980: INFO: Restart count of pod container-probe-8797/busybox-38bd286b-73d9-4b83-9000-c5dd51d1d975 is now 1 (50.113857642s elapsed)
    STEP: deleting the pod 01/04/23 23:30:48.98
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:30:49.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8797" for this suite. 01/04/23 23:30:49.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:30:49.022
Jan  4 23:30:49.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:30:49.022
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:30:49.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:30:49.039
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/04/23 23:30:49.042
Jan  4 23:30:49.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:30:51.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:31:00.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9906" for this suite. 01/04/23 23:31:00.471
------------------------------
• [SLOW TEST] [11.457 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:30:49.022
    Jan  4 23:30:49.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:30:49.022
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:30:49.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:30:49.039
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/04/23 23:30:49.042
    Jan  4 23:30:49.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:30:51.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:31:00.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9906" for this suite. 01/04/23 23:31:00.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:31:00.479
Jan  4 23:31:00.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename gc 01/04/23 23:31:00.48
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:00.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:00.508
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan  4 23:31:00.568: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"77db497f-b0ba-42c7-b637-ac84de22b6c7", Controller:(*bool)(0xc0040f69de), BlockOwnerDeletion:(*bool)(0xc0040f69df)}}
Jan  4 23:31:00.593: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7b0529ab-8387-4602-9d2d-858733c7a0cb", Controller:(*bool)(0xc0040f6c6e), BlockOwnerDeletion:(*bool)(0xc0040f6c6f)}}
Jan  4 23:31:00.609: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3356e9c1-dcb6-4780-aaf4-7a9798e50b0d", Controller:(*bool)(0xc00472f736), BlockOwnerDeletion:(*bool)(0xc00472f737)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  4 23:31:05.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6356" for this suite. 01/04/23 23:31:05.633
------------------------------
• [SLOW TEST] [5.162 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:31:00.479
    Jan  4 23:31:00.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename gc 01/04/23 23:31:00.48
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:00.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:00.508
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan  4 23:31:00.568: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"77db497f-b0ba-42c7-b637-ac84de22b6c7", Controller:(*bool)(0xc0040f69de), BlockOwnerDeletion:(*bool)(0xc0040f69df)}}
    Jan  4 23:31:00.593: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7b0529ab-8387-4602-9d2d-858733c7a0cb", Controller:(*bool)(0xc0040f6c6e), BlockOwnerDeletion:(*bool)(0xc0040f6c6f)}}
    Jan  4 23:31:00.609: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3356e9c1-dcb6-4780-aaf4-7a9798e50b0d", Controller:(*bool)(0xc00472f736), BlockOwnerDeletion:(*bool)(0xc00472f737)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:31:05.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6356" for this suite. 01/04/23 23:31:05.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:31:05.641
Jan  4 23:31:05.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:31:05.642
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:05.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:05.662
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/04/23 23:31:05.665
Jan  4 23:31:05.675: INFO: Waiting up to 5m0s for pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc" in namespace "projected-7109" to be "running and ready"
Jan  4 23:31:05.688: INFO: Pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.70199ms
Jan  4 23:31:05.689: INFO: The phase of Pod labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:31:07.695: INFO: Pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc": Phase="Running", Reason="", readiness=true. Elapsed: 2.02037886s
Jan  4 23:31:07.695: INFO: The phase of Pod labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc is Running (Ready = true)
Jan  4 23:31:07.695: INFO: Pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc" satisfied condition "running and ready"
Jan  4 23:31:08.224: INFO: Successfully updated pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  4 23:31:12.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7109" for this suite. 01/04/23 23:31:12.251
------------------------------
• [SLOW TEST] [6.618 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:31:05.641
    Jan  4 23:31:05.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:31:05.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:05.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:05.662
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/04/23 23:31:05.665
    Jan  4 23:31:05.675: INFO: Waiting up to 5m0s for pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc" in namespace "projected-7109" to be "running and ready"
    Jan  4 23:31:05.688: INFO: Pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.70199ms
    Jan  4 23:31:05.689: INFO: The phase of Pod labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:31:07.695: INFO: Pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc": Phase="Running", Reason="", readiness=true. Elapsed: 2.02037886s
    Jan  4 23:31:07.695: INFO: The phase of Pod labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc is Running (Ready = true)
    Jan  4 23:31:07.695: INFO: Pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc" satisfied condition "running and ready"
    Jan  4 23:31:08.224: INFO: Successfully updated pod "labelsupdateff92ceae-4cd2-4a15-8a96-19cda2874ebc"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:31:12.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7109" for this suite. 01/04/23 23:31:12.251
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:31:12.26
Jan  4 23:31:12.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replication-controller 01/04/23 23:31:12.262
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:12.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:12.285
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284 01/04/23 23:31:12.288
Jan  4 23:31:12.302: INFO: Pod name my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284: Found 0 pods out of 1
Jan  4 23:31:17.307: INFO: Pod name my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284: Found 1 pods out of 1
Jan  4 23:31:17.307: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284" are running
Jan  4 23:31:17.307: INFO: Waiting up to 5m0s for pod "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l" in namespace "replication-controller-2434" to be "running"
Jan  4 23:31:17.320: INFO: Pod "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l": Phase="Running", Reason="", readiness=true. Elapsed: 12.916999ms
Jan  4 23:31:17.320: INFO: Pod "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l" satisfied condition "running"
Jan  4 23:31:17.320: INFO: Pod "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:31:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:31:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:31:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:31:12 +0000 UTC Reason: Message:}])
Jan  4 23:31:17.320: INFO: Trying to dial the pod
Jan  4 23:31:22.336: INFO: Controller my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284: Got expected result from replica 1 [my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l]: "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  4 23:31:22.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2434" for this suite. 01/04/23 23:31:22.342
------------------------------
• [SLOW TEST] [10.088 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:31:12.26
    Jan  4 23:31:12.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replication-controller 01/04/23 23:31:12.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:12.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:12.285
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284 01/04/23 23:31:12.288
    Jan  4 23:31:12.302: INFO: Pod name my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284: Found 0 pods out of 1
    Jan  4 23:31:17.307: INFO: Pod name my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284: Found 1 pods out of 1
    Jan  4 23:31:17.307: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284" are running
    Jan  4 23:31:17.307: INFO: Waiting up to 5m0s for pod "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l" in namespace "replication-controller-2434" to be "running"
    Jan  4 23:31:17.320: INFO: Pod "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l": Phase="Running", Reason="", readiness=true. Elapsed: 12.916999ms
    Jan  4 23:31:17.320: INFO: Pod "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l" satisfied condition "running"
    Jan  4 23:31:17.320: INFO: Pod "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:31:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:31:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:31:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-04 23:31:12 +0000 UTC Reason: Message:}])
    Jan  4 23:31:17.320: INFO: Trying to dial the pod
    Jan  4 23:31:22.336: INFO: Controller my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284: Got expected result from replica 1 [my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l]: "my-hostname-basic-689e87e4-f7f6-4983-91b1-1ddbd5157284-n8s4l", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:31:22.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2434" for this suite. 01/04/23 23:31:22.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:31:22.35
Jan  4 23:31:22.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:31:22.352
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:22.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:22.375
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-5w5cz"  01/04/23 23:31:22.377
Jan  4 23:31:22.381: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-5w5cz"  01/04/23 23:31:22.381
Jan  4 23:31:22.388: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  4 23:31:22.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-955" for this suite. 01/04/23 23:31:22.391
------------------------------
• [0.048 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:31:22.35
    Jan  4 23:31:22.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:31:22.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:22.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:22.375
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-5w5cz"  01/04/23 23:31:22.377
    Jan  4 23:31:22.381: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-5w5cz"  01/04/23 23:31:22.381
    Jan  4 23:31:22.388: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:31:22.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-955" for this suite. 01/04/23 23:31:22.391
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:31:22.398
Jan  4 23:31:22.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename statefulset 01/04/23 23:31:22.399
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:22.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:22.43
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5097 01/04/23 23:31:22.433
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/04/23 23:31:22.442
Jan  4 23:31:22.455: INFO: Found 0 stateful pods, waiting for 3
Jan  4 23:31:32.468: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:31:32.468: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:31:32.468: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/04/23 23:31:32.486
Jan  4 23:31:32.511: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/04/23 23:31:32.512
STEP: Not applying an update when the partition is greater than the number of replicas 01/04/23 23:31:42.543
STEP: Performing a canary update 01/04/23 23:31:42.543
Jan  4 23:31:42.562: INFO: Updating stateful set ss2
Jan  4 23:31:42.574: INFO: Waiting for Pod statefulset-5097/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/04/23 23:31:52.583
Jan  4 23:31:52.676: INFO: Found 1 stateful pods, waiting for 3
Jan  4 23:32:02.681: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:32:02.681: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:32:02.681: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/04/23 23:32:02.688
Jan  4 23:32:02.709: INFO: Updating stateful set ss2
Jan  4 23:32:02.726: INFO: Waiting for Pod statefulset-5097/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan  4 23:32:12.752: INFO: Updating stateful set ss2
Jan  4 23:32:12.761: INFO: Waiting for StatefulSet statefulset-5097/ss2 to complete update
Jan  4 23:32:12.761: INFO: Waiting for Pod statefulset-5097/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  4 23:32:22.767: INFO: Deleting all statefulset in ns statefulset-5097
Jan  4 23:32:22.770: INFO: Scaling statefulset ss2 to 0
Jan  4 23:32:32.803: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:32:32.814: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:32:32.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5097" for this suite. 01/04/23 23:32:32.855
------------------------------
• [SLOW TEST] [70.480 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:31:22.398
    Jan  4 23:31:22.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename statefulset 01/04/23 23:31:22.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:31:22.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:31:22.43
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5097 01/04/23 23:31:22.433
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/04/23 23:31:22.442
    Jan  4 23:31:22.455: INFO: Found 0 stateful pods, waiting for 3
    Jan  4 23:31:32.468: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:31:32.468: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:31:32.468: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/04/23 23:31:32.486
    Jan  4 23:31:32.511: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/04/23 23:31:32.512
    STEP: Not applying an update when the partition is greater than the number of replicas 01/04/23 23:31:42.543
    STEP: Performing a canary update 01/04/23 23:31:42.543
    Jan  4 23:31:42.562: INFO: Updating stateful set ss2
    Jan  4 23:31:42.574: INFO: Waiting for Pod statefulset-5097/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/04/23 23:31:52.583
    Jan  4 23:31:52.676: INFO: Found 1 stateful pods, waiting for 3
    Jan  4 23:32:02.681: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:32:02.681: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:32:02.681: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/04/23 23:32:02.688
    Jan  4 23:32:02.709: INFO: Updating stateful set ss2
    Jan  4 23:32:02.726: INFO: Waiting for Pod statefulset-5097/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan  4 23:32:12.752: INFO: Updating stateful set ss2
    Jan  4 23:32:12.761: INFO: Waiting for StatefulSet statefulset-5097/ss2 to complete update
    Jan  4 23:32:12.761: INFO: Waiting for Pod statefulset-5097/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  4 23:32:22.767: INFO: Deleting all statefulset in ns statefulset-5097
    Jan  4 23:32:22.770: INFO: Scaling statefulset ss2 to 0
    Jan  4 23:32:32.803: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:32:32.814: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:32:32.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5097" for this suite. 01/04/23 23:32:32.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:32:32.88
Jan  4 23:32:32.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 23:32:32.881
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:32.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:32.952
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-dfb22244-7f0e-4e7c-aa97-f1d9e6859190 01/04/23 23:32:32.965
STEP: Creating a pod to test consume configMaps 01/04/23 23:32:32.975
Jan  4 23:32:33.012: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85" in namespace "configmap-7252" to be "Succeeded or Failed"
Jan  4 23:32:33.070: INFO: Pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85": Phase="Pending", Reason="", readiness=false. Elapsed: 57.860703ms
Jan  4 23:32:35.075: INFO: Pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85": Phase="Running", Reason="", readiness=false. Elapsed: 2.062639582s
Jan  4 23:32:37.076: INFO: Pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063440803s
STEP: Saw pod success 01/04/23 23:32:37.076
Jan  4 23:32:37.076: INFO: Pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85" satisfied condition "Succeeded or Failed"
Jan  4 23:32:37.079: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85 container configmap-volume-test: <nil>
STEP: delete the pod 01/04/23 23:32:37.089
Jan  4 23:32:37.101: INFO: Waiting for pod pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85 to disappear
Jan  4 23:32:37.103: INFO: Pod pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:32:37.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7252" for this suite. 01/04/23 23:32:37.107
------------------------------
• [4.232 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:32:32.88
    Jan  4 23:32:32.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 23:32:32.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:32.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:32.952
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-dfb22244-7f0e-4e7c-aa97-f1d9e6859190 01/04/23 23:32:32.965
    STEP: Creating a pod to test consume configMaps 01/04/23 23:32:32.975
    Jan  4 23:32:33.012: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85" in namespace "configmap-7252" to be "Succeeded or Failed"
    Jan  4 23:32:33.070: INFO: Pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85": Phase="Pending", Reason="", readiness=false. Elapsed: 57.860703ms
    Jan  4 23:32:35.075: INFO: Pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85": Phase="Running", Reason="", readiness=false. Elapsed: 2.062639582s
    Jan  4 23:32:37.076: INFO: Pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063440803s
    STEP: Saw pod success 01/04/23 23:32:37.076
    Jan  4 23:32:37.076: INFO: Pod "pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85" satisfied condition "Succeeded or Failed"
    Jan  4 23:32:37.079: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85 container configmap-volume-test: <nil>
    STEP: delete the pod 01/04/23 23:32:37.089
    Jan  4 23:32:37.101: INFO: Waiting for pod pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85 to disappear
    Jan  4 23:32:37.103: INFO: Pod pod-configmaps-bc82cfd9-f74d-42dd-88c4-8b03c67ccc85 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:32:37.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7252" for this suite. 01/04/23 23:32:37.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:32:37.113
Jan  4 23:32:37.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename svc-latency 01/04/23 23:32:37.114
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:37.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:37.131
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan  4 23:32:37.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: creating replication controller svc-latency-rc in namespace svc-latency-771 01/04/23 23:32:37.135
I0104 23:32:37.140955      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-771, replica count: 1
I0104 23:32:38.193715      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0104 23:32:39.194823      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 23:32:39.311: INFO: Created: latency-svc-wb6s4
Jan  4 23:32:39.317: INFO: Got endpoints: latency-svc-wb6s4 [22.620104ms]
Jan  4 23:32:39.343: INFO: Created: latency-svc-sqdrh
Jan  4 23:32:39.352: INFO: Got endpoints: latency-svc-sqdrh [33.980821ms]
Jan  4 23:32:39.358: INFO: Created: latency-svc-gx5m7
Jan  4 23:32:39.365: INFO: Got endpoints: latency-svc-gx5m7 [46.34331ms]
Jan  4 23:32:39.376: INFO: Created: latency-svc-8rk27
Jan  4 23:32:39.384: INFO: Got endpoints: latency-svc-8rk27 [65.924163ms]
Jan  4 23:32:39.389: INFO: Created: latency-svc-79rk4
Jan  4 23:32:39.392: INFO: Got endpoints: latency-svc-79rk4 [72.920598ms]
Jan  4 23:32:39.402: INFO: Created: latency-svc-gc2cb
Jan  4 23:32:39.414: INFO: Created: latency-svc-plrgl
Jan  4 23:32:39.416: INFO: Got endpoints: latency-svc-gc2cb [97.006312ms]
Jan  4 23:32:39.424: INFO: Created: latency-svc-ggsgp
Jan  4 23:32:39.427: INFO: Got endpoints: latency-svc-plrgl [107.608128ms]
Jan  4 23:32:39.434: INFO: Created: latency-svc-4jf8d
Jan  4 23:32:39.435: INFO: Got endpoints: latency-svc-ggsgp [116.08263ms]
Jan  4 23:32:39.444: INFO: Got endpoints: latency-svc-4jf8d [124.579174ms]
Jan  4 23:32:39.450: INFO: Created: latency-svc-tngj2
Jan  4 23:32:39.455: INFO: Got endpoints: latency-svc-tngj2 [135.530341ms]
Jan  4 23:32:39.464: INFO: Created: latency-svc-nmlfc
Jan  4 23:32:39.470: INFO: Got endpoints: latency-svc-nmlfc [150.069913ms]
Jan  4 23:32:39.536: INFO: Created: latency-svc-59gtj
Jan  4 23:32:39.551: INFO: Created: latency-svc-6pjx4
Jan  4 23:32:39.551: INFO: Created: latency-svc-mt8bm
Jan  4 23:32:39.557: INFO: Created: latency-svc-kv7tz
Jan  4 23:32:39.560: INFO: Created: latency-svc-ggxjc
Jan  4 23:32:39.564: INFO: Created: latency-svc-244g7
Jan  4 23:32:39.552: INFO: Created: latency-svc-2rkf2
Jan  4 23:32:39.564: INFO: Created: latency-svc-crvlx
Jan  4 23:32:39.565: INFO: Created: latency-svc-rkwhj
Jan  4 23:32:39.565: INFO: Created: latency-svc-wjr9l
Jan  4 23:32:39.566: INFO: Got endpoints: latency-svc-59gtj [245.408674ms]
Jan  4 23:32:39.566: INFO: Created: latency-svc-z5zt2
Jan  4 23:32:39.569: INFO: Created: latency-svc-kcqct
Jan  4 23:32:39.569: INFO: Created: latency-svc-gb57f
Jan  4 23:32:39.552: INFO: Created: latency-svc-kn6bk
Jan  4 23:32:39.566: INFO: Created: latency-svc-87sms
Jan  4 23:32:39.571: INFO: Got endpoints: latency-svc-wjr9l [179.732529ms]
Jan  4 23:32:39.573: INFO: Got endpoints: latency-svc-z5zt2 [156.635971ms]
Jan  4 23:32:39.573: INFO: Got endpoints: latency-svc-ggxjc [253.147437ms]
Jan  4 23:32:39.573: INFO: Got endpoints: latency-svc-gb57f [146.254104ms]
Jan  4 23:32:39.576: INFO: Got endpoints: latency-svc-kv7tz [131.428876ms]
Jan  4 23:32:39.586: INFO: Got endpoints: latency-svc-crvlx [150.887924ms]
Jan  4 23:32:39.589: INFO: Got endpoints: latency-svc-rkwhj [269.523351ms]
Jan  4 23:32:39.590: INFO: Got endpoints: latency-svc-kcqct [269.113289ms]
Jan  4 23:32:39.590: INFO: Got endpoints: latency-svc-2rkf2 [205.477969ms]
Jan  4 23:32:39.591: INFO: Got endpoints: latency-svc-244g7 [135.465504ms]
Jan  4 23:32:39.591: INFO: Got endpoints: latency-svc-87sms [270.523462ms]
Jan  4 23:32:39.598: INFO: Got endpoints: latency-svc-mt8bm [246.299933ms]
Jan  4 23:32:39.599: INFO: Got endpoints: latency-svc-6pjx4 [129.464703ms]
Jan  4 23:32:39.599: INFO: Got endpoints: latency-svc-kn6bk [234.466618ms]
Jan  4 23:32:39.604: INFO: Created: latency-svc-8vmfw
Jan  4 23:32:39.610: INFO: Got endpoints: latency-svc-8vmfw [43.961559ms]
Jan  4 23:32:39.614: INFO: Created: latency-svc-9sn2r
Jan  4 23:32:39.620: INFO: Got endpoints: latency-svc-9sn2r [48.847918ms]
Jan  4 23:32:39.624: INFO: Created: latency-svc-xp4f9
Jan  4 23:32:39.627: INFO: Got endpoints: latency-svc-xp4f9 [53.946849ms]
Jan  4 23:32:39.633: INFO: Created: latency-svc-frjps
Jan  4 23:32:39.637: INFO: Got endpoints: latency-svc-frjps [63.887679ms]
Jan  4 23:32:39.642: INFO: Created: latency-svc-s99j8
Jan  4 23:32:39.647: INFO: Got endpoints: latency-svc-s99j8 [74.340774ms]
Jan  4 23:32:39.653: INFO: Created: latency-svc-jwggt
Jan  4 23:32:39.658: INFO: Got endpoints: latency-svc-jwggt [82.870245ms]
Jan  4 23:32:39.667: INFO: Created: latency-svc-l5d98
Jan  4 23:32:39.668: INFO: Got endpoints: latency-svc-l5d98 [81.362119ms]
Jan  4 23:32:39.674: INFO: Created: latency-svc-99fjg
Jan  4 23:32:39.684: INFO: Got endpoints: latency-svc-99fjg [94.164086ms]
Jan  4 23:32:39.697: INFO: Created: latency-svc-p25bd
Jan  4 23:32:39.697: INFO: Got endpoints: latency-svc-p25bd [106.966607ms]
Jan  4 23:32:39.709: INFO: Created: latency-svc-727bw
Jan  4 23:32:39.715: INFO: Got endpoints: latency-svc-727bw [125.687332ms]
Jan  4 23:32:39.717: INFO: Created: latency-svc-fsvql
Jan  4 23:32:39.729: INFO: Created: latency-svc-2lbm9
Jan  4 23:32:39.731: INFO: Got endpoints: latency-svc-fsvql [140.037169ms]
Jan  4 23:32:39.736: INFO: Got endpoints: latency-svc-2lbm9 [144.935401ms]
Jan  4 23:32:39.740: INFO: Created: latency-svc-wd85d
Jan  4 23:32:39.749: INFO: Created: latency-svc-62jdp
Jan  4 23:32:39.754: INFO: Created: latency-svc-7tscv
Jan  4 23:32:39.761: INFO: Created: latency-svc-vqp6k
Jan  4 23:32:39.769: INFO: Got endpoints: latency-svc-wd85d [170.059985ms]
Jan  4 23:32:39.775: INFO: Created: latency-svc-xp6mz
Jan  4 23:32:39.779: INFO: Created: latency-svc-vxj7g
Jan  4 23:32:39.786: INFO: Created: latency-svc-m2xxt
Jan  4 23:32:39.796: INFO: Created: latency-svc-bnjjd
Jan  4 23:32:39.804: INFO: Created: latency-svc-djpzn
Jan  4 23:32:39.808: INFO: Created: latency-svc-84msv
Jan  4 23:32:39.816: INFO: Got endpoints: latency-svc-62jdp [216.414442ms]
Jan  4 23:32:39.823: INFO: Created: latency-svc-d92rz
Jan  4 23:32:39.829: INFO: Created: latency-svc-wpmdd
Jan  4 23:32:39.836: INFO: Created: latency-svc-mgknc
Jan  4 23:32:39.843: INFO: Created: latency-svc-vnvcf
Jan  4 23:32:39.853: INFO: Created: latency-svc-fgjqt
Jan  4 23:32:39.861: INFO: Created: latency-svc-xgr2w
Jan  4 23:32:39.867: INFO: Got endpoints: latency-svc-7tscv [267.711032ms]
Jan  4 23:32:39.872: INFO: Created: latency-svc-47bg9
Jan  4 23:32:39.879: INFO: Created: latency-svc-qs57w
Jan  4 23:32:39.914: INFO: Got endpoints: latency-svc-vqp6k [304.37418ms]
Jan  4 23:32:39.928: INFO: Created: latency-svc-smk4l
Jan  4 23:32:39.965: INFO: Got endpoints: latency-svc-xp6mz [344.804903ms]
Jan  4 23:32:39.976: INFO: Created: latency-svc-9xdqd
Jan  4 23:32:40.016: INFO: Got endpoints: latency-svc-vxj7g [389.098974ms]
Jan  4 23:32:40.032: INFO: Created: latency-svc-cmfkx
Jan  4 23:32:40.065: INFO: Got endpoints: latency-svc-m2xxt [428.547247ms]
Jan  4 23:32:40.076: INFO: Created: latency-svc-5zvbs
Jan  4 23:32:40.117: INFO: Got endpoints: latency-svc-bnjjd [470.052538ms]
Jan  4 23:32:40.131: INFO: Created: latency-svc-lz4bt
Jan  4 23:32:40.165: INFO: Got endpoints: latency-svc-djpzn [506.954727ms]
Jan  4 23:32:40.175: INFO: Created: latency-svc-p6grl
Jan  4 23:32:40.216: INFO: Got endpoints: latency-svc-84msv [548.176269ms]
Jan  4 23:32:40.227: INFO: Created: latency-svc-mk55t
Jan  4 23:32:40.265: INFO: Got endpoints: latency-svc-d92rz [581.694189ms]
Jan  4 23:32:40.276: INFO: Created: latency-svc-2vllj
Jan  4 23:32:40.314: INFO: Got endpoints: latency-svc-wpmdd [617.117996ms]
Jan  4 23:32:40.329: INFO: Created: latency-svc-xnx2w
Jan  4 23:32:40.365: INFO: Got endpoints: latency-svc-mgknc [649.163596ms]
Jan  4 23:32:40.386: INFO: Created: latency-svc-td2m5
Jan  4 23:32:40.414: INFO: Got endpoints: latency-svc-vnvcf [682.813837ms]
Jan  4 23:32:40.426: INFO: Created: latency-svc-x2489
Jan  4 23:32:40.467: INFO: Got endpoints: latency-svc-fgjqt [730.714027ms]
Jan  4 23:32:40.479: INFO: Created: latency-svc-v7tqb
Jan  4 23:32:40.516: INFO: Got endpoints: latency-svc-xgr2w [747.521339ms]
Jan  4 23:32:40.526: INFO: Created: latency-svc-ljzrx
Jan  4 23:32:40.566: INFO: Got endpoints: latency-svc-47bg9 [750.573451ms]
Jan  4 23:32:40.577: INFO: Created: latency-svc-2wrxp
Jan  4 23:32:40.617: INFO: Got endpoints: latency-svc-qs57w [749.646228ms]
Jan  4 23:32:40.626: INFO: Created: latency-svc-x4jtl
Jan  4 23:32:40.665: INFO: Got endpoints: latency-svc-smk4l [750.76983ms]
Jan  4 23:32:40.675: INFO: Created: latency-svc-b4db8
Jan  4 23:32:40.715: INFO: Got endpoints: latency-svc-9xdqd [749.58812ms]
Jan  4 23:32:40.726: INFO: Created: latency-svc-pjqf8
Jan  4 23:32:40.765: INFO: Got endpoints: latency-svc-cmfkx [749.557393ms]
Jan  4 23:32:40.776: INFO: Created: latency-svc-4z4zk
Jan  4 23:32:40.814: INFO: Got endpoints: latency-svc-5zvbs [748.069854ms]
Jan  4 23:32:40.825: INFO: Created: latency-svc-bps9d
Jan  4 23:32:40.864: INFO: Got endpoints: latency-svc-lz4bt [746.714691ms]
Jan  4 23:32:40.873: INFO: Created: latency-svc-bcqw4
Jan  4 23:32:40.915: INFO: Got endpoints: latency-svc-p6grl [749.922343ms]
Jan  4 23:32:40.926: INFO: Created: latency-svc-q62hk
Jan  4 23:32:40.965: INFO: Got endpoints: latency-svc-mk55t [749.131482ms]
Jan  4 23:32:40.976: INFO: Created: latency-svc-jr7c9
Jan  4 23:32:41.017: INFO: Got endpoints: latency-svc-2vllj [751.317223ms]
Jan  4 23:32:41.032: INFO: Created: latency-svc-drrvf
Jan  4 23:32:41.067: INFO: Got endpoints: latency-svc-xnx2w [752.394293ms]
Jan  4 23:32:41.076: INFO: Created: latency-svc-twcqm
Jan  4 23:32:41.114: INFO: Got endpoints: latency-svc-td2m5 [744.759135ms]
Jan  4 23:32:41.124: INFO: Created: latency-svc-69tjk
Jan  4 23:32:41.164: INFO: Got endpoints: latency-svc-x2489 [749.801771ms]
Jan  4 23:32:41.177: INFO: Created: latency-svc-n2qcg
Jan  4 23:32:41.215: INFO: Got endpoints: latency-svc-v7tqb [747.822689ms]
Jan  4 23:32:41.225: INFO: Created: latency-svc-gmz5r
Jan  4 23:32:41.264: INFO: Got endpoints: latency-svc-ljzrx [747.522258ms]
Jan  4 23:32:41.274: INFO: Created: latency-svc-8tqv9
Jan  4 23:32:41.316: INFO: Got endpoints: latency-svc-2wrxp [749.205916ms]
Jan  4 23:32:41.331: INFO: Created: latency-svc-ldmb9
Jan  4 23:32:41.365: INFO: Got endpoints: latency-svc-x4jtl [748.123542ms]
Jan  4 23:32:41.376: INFO: Created: latency-svc-hllcp
Jan  4 23:32:41.415: INFO: Got endpoints: latency-svc-b4db8 [749.428308ms]
Jan  4 23:32:41.427: INFO: Created: latency-svc-lhxvs
Jan  4 23:32:41.465: INFO: Got endpoints: latency-svc-pjqf8 [750.318551ms]
Jan  4 23:32:41.476: INFO: Created: latency-svc-fllw5
Jan  4 23:32:41.513: INFO: Got endpoints: latency-svc-4z4zk [747.756417ms]
Jan  4 23:32:41.526: INFO: Created: latency-svc-fwrw5
Jan  4 23:32:41.565: INFO: Got endpoints: latency-svc-bps9d [751.855924ms]
Jan  4 23:32:41.576: INFO: Created: latency-svc-8pf9c
Jan  4 23:32:41.621: INFO: Got endpoints: latency-svc-bcqw4 [756.332466ms]
Jan  4 23:32:41.630: INFO: Created: latency-svc-dn547
Jan  4 23:32:41.665: INFO: Got endpoints: latency-svc-q62hk [749.514289ms]
Jan  4 23:32:41.683: INFO: Created: latency-svc-xkf69
Jan  4 23:32:41.716: INFO: Got endpoints: latency-svc-jr7c9 [751.265099ms]
Jan  4 23:32:41.733: INFO: Created: latency-svc-n9wgx
Jan  4 23:32:41.767: INFO: Got endpoints: latency-svc-drrvf [749.753926ms]
Jan  4 23:32:41.778: INFO: Created: latency-svc-sz89k
Jan  4 23:32:41.820: INFO: Got endpoints: latency-svc-twcqm [753.542382ms]
Jan  4 23:32:41.831: INFO: Created: latency-svc-lqlrn
Jan  4 23:32:41.866: INFO: Got endpoints: latency-svc-69tjk [751.429098ms]
Jan  4 23:32:41.877: INFO: Created: latency-svc-npvsf
Jan  4 23:32:41.915: INFO: Got endpoints: latency-svc-n2qcg [751.235146ms]
Jan  4 23:32:41.925: INFO: Created: latency-svc-fx459
Jan  4 23:32:41.964: INFO: Got endpoints: latency-svc-gmz5r [748.60946ms]
Jan  4 23:32:41.974: INFO: Created: latency-svc-8qhrc
Jan  4 23:32:42.029: INFO: Got endpoints: latency-svc-8tqv9 [765.07415ms]
Jan  4 23:32:42.042: INFO: Created: latency-svc-qtzxt
Jan  4 23:32:42.065: INFO: Got endpoints: latency-svc-ldmb9 [748.845986ms]
Jan  4 23:32:42.081: INFO: Created: latency-svc-8r5nf
Jan  4 23:32:42.115: INFO: Got endpoints: latency-svc-hllcp [749.527539ms]
Jan  4 23:32:42.129: INFO: Created: latency-svc-hsfdw
Jan  4 23:32:42.163: INFO: Got endpoints: latency-svc-lhxvs [748.047315ms]
Jan  4 23:32:42.175: INFO: Created: latency-svc-wgpsj
Jan  4 23:32:42.215: INFO: Got endpoints: latency-svc-fllw5 [749.394802ms]
Jan  4 23:32:42.226: INFO: Created: latency-svc-dmmdn
Jan  4 23:32:42.265: INFO: Got endpoints: latency-svc-fwrw5 [751.159355ms]
Jan  4 23:32:42.275: INFO: Created: latency-svc-pmgps
Jan  4 23:32:42.316: INFO: Got endpoints: latency-svc-8pf9c [750.509135ms]
Jan  4 23:32:42.332: INFO: Created: latency-svc-b7vts
Jan  4 23:32:42.375: INFO: Got endpoints: latency-svc-dn547 [754.21843ms]
Jan  4 23:32:42.389: INFO: Created: latency-svc-jwf2h
Jan  4 23:32:42.419: INFO: Got endpoints: latency-svc-xkf69 [754.346872ms]
Jan  4 23:32:42.429: INFO: Created: latency-svc-676z6
Jan  4 23:32:42.465: INFO: Got endpoints: latency-svc-n9wgx [748.43242ms]
Jan  4 23:32:42.476: INFO: Created: latency-svc-4w97d
Jan  4 23:32:42.516: INFO: Got endpoints: latency-svc-sz89k [749.095253ms]
Jan  4 23:32:42.528: INFO: Created: latency-svc-zzxgl
Jan  4 23:32:42.567: INFO: Got endpoints: latency-svc-lqlrn [746.738198ms]
Jan  4 23:32:42.579: INFO: Created: latency-svc-hnbtt
Jan  4 23:32:42.614: INFO: Got endpoints: latency-svc-npvsf [747.896216ms]
Jan  4 23:32:42.625: INFO: Created: latency-svc-b6zfx
Jan  4 23:32:42.666: INFO: Got endpoints: latency-svc-fx459 [750.758496ms]
Jan  4 23:32:42.677: INFO: Created: latency-svc-z7ws5
Jan  4 23:32:42.716: INFO: Got endpoints: latency-svc-8qhrc [752.380607ms]
Jan  4 23:32:42.725: INFO: Created: latency-svc-tgr6l
Jan  4 23:32:42.773: INFO: Got endpoints: latency-svc-qtzxt [743.980835ms]
Jan  4 23:32:42.790: INFO: Created: latency-svc-gg7zl
Jan  4 23:32:42.816: INFO: Got endpoints: latency-svc-8r5nf [750.638679ms]
Jan  4 23:32:42.833: INFO: Created: latency-svc-gdfbc
Jan  4 23:32:42.866: INFO: Got endpoints: latency-svc-hsfdw [750.224584ms]
Jan  4 23:32:42.881: INFO: Created: latency-svc-bds2v
Jan  4 23:32:42.914: INFO: Got endpoints: latency-svc-wgpsj [751.112552ms]
Jan  4 23:32:42.927: INFO: Created: latency-svc-bs92b
Jan  4 23:32:42.965: INFO: Got endpoints: latency-svc-dmmdn [749.836477ms]
Jan  4 23:32:42.986: INFO: Created: latency-svc-9dl5v
Jan  4 23:32:43.025: INFO: Got endpoints: latency-svc-pmgps [759.917318ms]
Jan  4 23:32:43.045: INFO: Created: latency-svc-qjtgw
Jan  4 23:32:43.065: INFO: Got endpoints: latency-svc-b7vts [748.877904ms]
Jan  4 23:32:43.076: INFO: Created: latency-svc-gzrb7
Jan  4 23:32:43.116: INFO: Got endpoints: latency-svc-jwf2h [741.301174ms]
Jan  4 23:32:43.126: INFO: Created: latency-svc-qfl7z
Jan  4 23:32:43.163: INFO: Got endpoints: latency-svc-676z6 [743.898191ms]
Jan  4 23:32:43.176: INFO: Created: latency-svc-gl7rr
Jan  4 23:32:43.216: INFO: Got endpoints: latency-svc-4w97d [751.278239ms]
Jan  4 23:32:43.226: INFO: Created: latency-svc-9g4g8
Jan  4 23:32:43.266: INFO: Got endpoints: latency-svc-zzxgl [749.709664ms]
Jan  4 23:32:43.278: INFO: Created: latency-svc-t4x22
Jan  4 23:32:43.323: INFO: Got endpoints: latency-svc-hnbtt [755.410479ms]
Jan  4 23:32:43.350: INFO: Created: latency-svc-l9nxw
Jan  4 23:32:43.370: INFO: Got endpoints: latency-svc-b6zfx [755.776104ms]
Jan  4 23:32:43.392: INFO: Created: latency-svc-pqd58
Jan  4 23:32:43.416: INFO: Got endpoints: latency-svc-z7ws5 [750.046927ms]
Jan  4 23:32:43.437: INFO: Created: latency-svc-zzwk9
Jan  4 23:32:43.469: INFO: Got endpoints: latency-svc-tgr6l [752.960303ms]
Jan  4 23:32:43.490: INFO: Created: latency-svc-l7v4g
Jan  4 23:32:43.516: INFO: Got endpoints: latency-svc-gg7zl [742.72109ms]
Jan  4 23:32:43.527: INFO: Created: latency-svc-kmpwq
Jan  4 23:32:43.565: INFO: Got endpoints: latency-svc-gdfbc [748.927091ms]
Jan  4 23:32:43.581: INFO: Created: latency-svc-h5t5w
Jan  4 23:32:43.615: INFO: Got endpoints: latency-svc-bds2v [749.084874ms]
Jan  4 23:32:43.627: INFO: Created: latency-svc-xxnw4
Jan  4 23:32:43.665: INFO: Got endpoints: latency-svc-bs92b [750.473904ms]
Jan  4 23:32:43.675: INFO: Created: latency-svc-djsx9
Jan  4 23:32:43.714: INFO: Got endpoints: latency-svc-9dl5v [748.49295ms]
Jan  4 23:32:43.724: INFO: Created: latency-svc-nm985
Jan  4 23:32:43.765: INFO: Got endpoints: latency-svc-qjtgw [740.05951ms]
Jan  4 23:32:43.777: INFO: Created: latency-svc-z9bqr
Jan  4 23:32:43.815: INFO: Got endpoints: latency-svc-gzrb7 [748.489334ms]
Jan  4 23:32:43.826: INFO: Created: latency-svc-l72c9
Jan  4 23:32:43.865: INFO: Got endpoints: latency-svc-qfl7z [748.580998ms]
Jan  4 23:32:43.879: INFO: Created: latency-svc-67zl6
Jan  4 23:32:43.915: INFO: Got endpoints: latency-svc-gl7rr [751.865251ms]
Jan  4 23:32:43.925: INFO: Created: latency-svc-n7rmk
Jan  4 23:32:43.964: INFO: Got endpoints: latency-svc-9g4g8 [748.134014ms]
Jan  4 23:32:43.976: INFO: Created: latency-svc-rnx8p
Jan  4 23:32:44.014: INFO: Got endpoints: latency-svc-t4x22 [748.523925ms]
Jan  4 23:32:44.029: INFO: Created: latency-svc-pm9nq
Jan  4 23:32:44.067: INFO: Got endpoints: latency-svc-l9nxw [743.603939ms]
Jan  4 23:32:44.077: INFO: Created: latency-svc-s22pq
Jan  4 23:32:44.124: INFO: Got endpoints: latency-svc-pqd58 [753.623333ms]
Jan  4 23:32:44.137: INFO: Created: latency-svc-dpktr
Jan  4 23:32:44.165: INFO: Got endpoints: latency-svc-zzwk9 [748.299735ms]
Jan  4 23:32:44.177: INFO: Created: latency-svc-lf967
Jan  4 23:32:44.217: INFO: Got endpoints: latency-svc-l7v4g [747.855886ms]
Jan  4 23:32:44.227: INFO: Created: latency-svc-7wk6g
Jan  4 23:32:44.264: INFO: Got endpoints: latency-svc-kmpwq [747.697392ms]
Jan  4 23:32:44.280: INFO: Created: latency-svc-547gt
Jan  4 23:32:44.320: INFO: Got endpoints: latency-svc-h5t5w [755.540169ms]
Jan  4 23:32:44.333: INFO: Created: latency-svc-76w7s
Jan  4 23:32:44.371: INFO: Got endpoints: latency-svc-xxnw4 [756.111295ms]
Jan  4 23:32:44.381: INFO: Created: latency-svc-54k9s
Jan  4 23:32:44.414: INFO: Got endpoints: latency-svc-djsx9 [748.109434ms]
Jan  4 23:32:44.427: INFO: Created: latency-svc-bgfvx
Jan  4 23:32:44.465: INFO: Got endpoints: latency-svc-nm985 [750.782629ms]
Jan  4 23:32:44.476: INFO: Created: latency-svc-lkt2r
Jan  4 23:32:44.533: INFO: Got endpoints: latency-svc-z9bqr [767.283407ms]
Jan  4 23:32:44.545: INFO: Created: latency-svc-lwt9b
Jan  4 23:32:44.565: INFO: Got endpoints: latency-svc-l72c9 [749.57184ms]
Jan  4 23:32:44.575: INFO: Created: latency-svc-x8q9x
Jan  4 23:32:44.616: INFO: Got endpoints: latency-svc-67zl6 [750.434097ms]
Jan  4 23:32:44.626: INFO: Created: latency-svc-s8p7l
Jan  4 23:32:44.664: INFO: Got endpoints: latency-svc-n7rmk [748.978073ms]
Jan  4 23:32:44.676: INFO: Created: latency-svc-6wktp
Jan  4 23:32:44.715: INFO: Got endpoints: latency-svc-rnx8p [750.677294ms]
Jan  4 23:32:44.727: INFO: Created: latency-svc-5drrx
Jan  4 23:32:44.764: INFO: Got endpoints: latency-svc-pm9nq [749.135678ms]
Jan  4 23:32:44.773: INFO: Created: latency-svc-mr6sw
Jan  4 23:32:44.814: INFO: Got endpoints: latency-svc-s22pq [746.603647ms]
Jan  4 23:32:44.824: INFO: Created: latency-svc-45wzv
Jan  4 23:32:44.867: INFO: Got endpoints: latency-svc-dpktr [743.040607ms]
Jan  4 23:32:44.885: INFO: Created: latency-svc-p8qgr
Jan  4 23:32:44.915: INFO: Got endpoints: latency-svc-lf967 [750.03325ms]
Jan  4 23:32:44.926: INFO: Created: latency-svc-pr7nx
Jan  4 23:32:44.964: INFO: Got endpoints: latency-svc-7wk6g [746.92065ms]
Jan  4 23:32:44.974: INFO: Created: latency-svc-rkvd4
Jan  4 23:32:45.014: INFO: Got endpoints: latency-svc-547gt [749.562117ms]
Jan  4 23:32:45.024: INFO: Created: latency-svc-4mplk
Jan  4 23:32:45.065: INFO: Got endpoints: latency-svc-76w7s [744.480805ms]
Jan  4 23:32:45.075: INFO: Created: latency-svc-c6tdl
Jan  4 23:32:45.115: INFO: Got endpoints: latency-svc-54k9s [743.771496ms]
Jan  4 23:32:45.125: INFO: Created: latency-svc-9mhfh
Jan  4 23:32:45.163: INFO: Got endpoints: latency-svc-bgfvx [749.487258ms]
Jan  4 23:32:45.175: INFO: Created: latency-svc-6p82h
Jan  4 23:32:45.214: INFO: Got endpoints: latency-svc-lkt2r [748.403774ms]
Jan  4 23:32:45.227: INFO: Created: latency-svc-5pd4h
Jan  4 23:32:45.266: INFO: Got endpoints: latency-svc-lwt9b [732.598988ms]
Jan  4 23:32:45.278: INFO: Created: latency-svc-mxf69
Jan  4 23:32:45.316: INFO: Got endpoints: latency-svc-x8q9x [750.981032ms]
Jan  4 23:32:45.331: INFO: Created: latency-svc-bdgvn
Jan  4 23:32:45.368: INFO: Got endpoints: latency-svc-s8p7l [752.508986ms]
Jan  4 23:32:45.386: INFO: Created: latency-svc-t6xd8
Jan  4 23:32:45.415: INFO: Got endpoints: latency-svc-6wktp [750.674184ms]
Jan  4 23:32:45.429: INFO: Created: latency-svc-kvg8j
Jan  4 23:32:45.465: INFO: Got endpoints: latency-svc-5drrx [749.214544ms]
Jan  4 23:32:45.481: INFO: Created: latency-svc-j66cb
Jan  4 23:32:45.516: INFO: Got endpoints: latency-svc-mr6sw [750.329712ms]
Jan  4 23:32:45.534: INFO: Created: latency-svc-v6n46
Jan  4 23:32:45.565: INFO: Got endpoints: latency-svc-45wzv [751.164362ms]
Jan  4 23:32:45.576: INFO: Created: latency-svc-stcgc
Jan  4 23:32:45.622: INFO: Got endpoints: latency-svc-p8qgr [747.99814ms]
Jan  4 23:32:45.633: INFO: Created: latency-svc-tv44k
Jan  4 23:32:45.665: INFO: Got endpoints: latency-svc-pr7nx [749.879189ms]
Jan  4 23:32:45.677: INFO: Created: latency-svc-glrdr
Jan  4 23:32:45.714: INFO: Got endpoints: latency-svc-rkvd4 [750.191236ms]
Jan  4 23:32:45.727: INFO: Created: latency-svc-9j7sq
Jan  4 23:32:45.766: INFO: Got endpoints: latency-svc-4mplk [752.403544ms]
Jan  4 23:32:45.779: INFO: Created: latency-svc-rj65h
Jan  4 23:32:45.816: INFO: Got endpoints: latency-svc-c6tdl [751.449088ms]
Jan  4 23:32:45.829: INFO: Created: latency-svc-fmlbh
Jan  4 23:32:45.865: INFO: Got endpoints: latency-svc-9mhfh [749.640404ms]
Jan  4 23:32:45.877: INFO: Created: latency-svc-shjcf
Jan  4 23:32:45.917: INFO: Got endpoints: latency-svc-6p82h [753.1712ms]
Jan  4 23:32:45.928: INFO: Created: latency-svc-mm4vk
Jan  4 23:32:45.966: INFO: Got endpoints: latency-svc-5pd4h [751.747768ms]
Jan  4 23:32:45.979: INFO: Created: latency-svc-rv6b5
Jan  4 23:32:46.013: INFO: Got endpoints: latency-svc-mxf69 [747.739129ms]
Jan  4 23:32:46.024: INFO: Created: latency-svc-6n54h
Jan  4 23:32:46.066: INFO: Got endpoints: latency-svc-bdgvn [750.293641ms]
Jan  4 23:32:46.078: INFO: Created: latency-svc-gw25p
Jan  4 23:32:46.115: INFO: Got endpoints: latency-svc-t6xd8 [746.269022ms]
Jan  4 23:32:46.125: INFO: Created: latency-svc-vncp6
Jan  4 23:32:46.164: INFO: Got endpoints: latency-svc-kvg8j [749.021994ms]
Jan  4 23:32:46.180: INFO: Created: latency-svc-5tgqp
Jan  4 23:32:46.215: INFO: Got endpoints: latency-svc-j66cb [750.479828ms]
Jan  4 23:32:46.227: INFO: Created: latency-svc-wx7lp
Jan  4 23:32:46.267: INFO: Got endpoints: latency-svc-v6n46 [750.675166ms]
Jan  4 23:32:46.277: INFO: Created: latency-svc-q7bgp
Jan  4 23:32:46.315: INFO: Got endpoints: latency-svc-stcgc [749.810233ms]
Jan  4 23:32:46.329: INFO: Created: latency-svc-mgndl
Jan  4 23:32:46.368: INFO: Got endpoints: latency-svc-tv44k [745.135885ms]
Jan  4 23:32:46.387: INFO: Created: latency-svc-x2hh4
Jan  4 23:32:46.416: INFO: Got endpoints: latency-svc-glrdr [750.972403ms]
Jan  4 23:32:46.427: INFO: Created: latency-svc-jxxdm
Jan  4 23:32:46.464: INFO: Got endpoints: latency-svc-9j7sq [749.508112ms]
Jan  4 23:32:46.474: INFO: Created: latency-svc-zxktb
Jan  4 23:32:46.515: INFO: Got endpoints: latency-svc-rj65h [748.267578ms]
Jan  4 23:32:46.525: INFO: Created: latency-svc-nsbxc
Jan  4 23:32:46.567: INFO: Got endpoints: latency-svc-fmlbh [750.574902ms]
Jan  4 23:32:46.577: INFO: Created: latency-svc-swb52
Jan  4 23:32:46.615: INFO: Got endpoints: latency-svc-shjcf [749.511657ms]
Jan  4 23:32:46.627: INFO: Created: latency-svc-bhqx8
Jan  4 23:32:46.666: INFO: Got endpoints: latency-svc-mm4vk [749.087621ms]
Jan  4 23:32:46.676: INFO: Created: latency-svc-p2zzk
Jan  4 23:32:46.716: INFO: Got endpoints: latency-svc-rv6b5 [750.026615ms]
Jan  4 23:32:46.725: INFO: Created: latency-svc-8zbgw
Jan  4 23:32:46.766: INFO: Got endpoints: latency-svc-6n54h [752.783657ms]
Jan  4 23:32:46.776: INFO: Created: latency-svc-v9gxt
Jan  4 23:32:46.815: INFO: Got endpoints: latency-svc-gw25p [748.954557ms]
Jan  4 23:32:46.828: INFO: Created: latency-svc-gk8c7
Jan  4 23:32:46.867: INFO: Got endpoints: latency-svc-vncp6 [752.608638ms]
Jan  4 23:32:46.885: INFO: Created: latency-svc-8d84g
Jan  4 23:32:46.914: INFO: Got endpoints: latency-svc-5tgqp [749.308526ms]
Jan  4 23:32:46.924: INFO: Created: latency-svc-99qjm
Jan  4 23:32:46.966: INFO: Got endpoints: latency-svc-wx7lp [750.434433ms]
Jan  4 23:32:46.976: INFO: Created: latency-svc-xnk5p
Jan  4 23:32:47.014: INFO: Got endpoints: latency-svc-q7bgp [747.431145ms]
Jan  4 23:32:47.028: INFO: Created: latency-svc-tmxjn
Jan  4 23:32:47.064: INFO: Got endpoints: latency-svc-mgndl [748.694235ms]
Jan  4 23:32:47.076: INFO: Created: latency-svc-ghflm
Jan  4 23:32:47.115: INFO: Got endpoints: latency-svc-x2hh4 [747.16363ms]
Jan  4 23:32:47.125: INFO: Created: latency-svc-bc2qk
Jan  4 23:32:47.166: INFO: Got endpoints: latency-svc-jxxdm [749.940238ms]
Jan  4 23:32:47.215: INFO: Got endpoints: latency-svc-zxktb [750.682884ms]
Jan  4 23:32:47.264: INFO: Got endpoints: latency-svc-nsbxc [749.50098ms]
Jan  4 23:32:47.314: INFO: Got endpoints: latency-svc-swb52 [746.225585ms]
Jan  4 23:32:47.368: INFO: Got endpoints: latency-svc-bhqx8 [753.521536ms]
Jan  4 23:32:47.419: INFO: Got endpoints: latency-svc-p2zzk [753.187373ms]
Jan  4 23:32:47.465: INFO: Got endpoints: latency-svc-8zbgw [748.881539ms]
Jan  4 23:32:47.518: INFO: Got endpoints: latency-svc-v9gxt [751.686382ms]
Jan  4 23:32:47.564: INFO: Got endpoints: latency-svc-gk8c7 [747.851857ms]
Jan  4 23:32:47.614: INFO: Got endpoints: latency-svc-8d84g [746.865465ms]
Jan  4 23:32:47.665: INFO: Got endpoints: latency-svc-99qjm [750.810874ms]
Jan  4 23:32:47.716: INFO: Got endpoints: latency-svc-xnk5p [749.872231ms]
Jan  4 23:32:47.766: INFO: Got endpoints: latency-svc-tmxjn [751.850048ms]
Jan  4 23:32:47.815: INFO: Got endpoints: latency-svc-ghflm [750.061302ms]
Jan  4 23:32:47.867: INFO: Got endpoints: latency-svc-bc2qk [751.653282ms]
Jan  4 23:32:47.868: INFO: Latencies: [33.980821ms 43.961559ms 46.34331ms 48.847918ms 53.946849ms 63.887679ms 65.924163ms 72.920598ms 74.340774ms 81.362119ms 82.870245ms 94.164086ms 97.006312ms 106.966607ms 107.608128ms 116.08263ms 124.579174ms 125.687332ms 129.464703ms 131.428876ms 135.465504ms 135.530341ms 140.037169ms 144.935401ms 146.254104ms 150.069913ms 150.887924ms 156.635971ms 170.059985ms 179.732529ms 205.477969ms 216.414442ms 234.466618ms 245.408674ms 246.299933ms 253.147437ms 267.711032ms 269.113289ms 269.523351ms 270.523462ms 304.37418ms 344.804903ms 389.098974ms 428.547247ms 470.052538ms 506.954727ms 548.176269ms 581.694189ms 617.117996ms 649.163596ms 682.813837ms 730.714027ms 732.598988ms 740.05951ms 741.301174ms 742.72109ms 743.040607ms 743.603939ms 743.771496ms 743.898191ms 743.980835ms 744.480805ms 744.759135ms 745.135885ms 746.225585ms 746.269022ms 746.603647ms 746.714691ms 746.738198ms 746.865465ms 746.92065ms 747.16363ms 747.431145ms 747.521339ms 747.522258ms 747.697392ms 747.739129ms 747.756417ms 747.822689ms 747.851857ms 747.855886ms 747.896216ms 747.99814ms 748.047315ms 748.069854ms 748.109434ms 748.123542ms 748.134014ms 748.267578ms 748.299735ms 748.403774ms 748.43242ms 748.489334ms 748.49295ms 748.523925ms 748.580998ms 748.60946ms 748.694235ms 748.845986ms 748.877904ms 748.881539ms 748.927091ms 748.954557ms 748.978073ms 749.021994ms 749.084874ms 749.087621ms 749.095253ms 749.131482ms 749.135678ms 749.205916ms 749.214544ms 749.308526ms 749.394802ms 749.428308ms 749.487258ms 749.50098ms 749.508112ms 749.511657ms 749.514289ms 749.527539ms 749.557393ms 749.562117ms 749.57184ms 749.58812ms 749.640404ms 749.646228ms 749.709664ms 749.753926ms 749.801771ms 749.810233ms 749.836477ms 749.872231ms 749.879189ms 749.922343ms 749.940238ms 750.026615ms 750.03325ms 750.046927ms 750.061302ms 750.191236ms 750.224584ms 750.293641ms 750.318551ms 750.329712ms 750.434097ms 750.434433ms 750.473904ms 750.479828ms 750.509135ms 750.573451ms 750.574902ms 750.638679ms 750.674184ms 750.675166ms 750.677294ms 750.682884ms 750.758496ms 750.76983ms 750.782629ms 750.810874ms 750.972403ms 750.981032ms 751.112552ms 751.159355ms 751.164362ms 751.235146ms 751.265099ms 751.278239ms 751.317223ms 751.429098ms 751.449088ms 751.653282ms 751.686382ms 751.747768ms 751.850048ms 751.855924ms 751.865251ms 752.380607ms 752.394293ms 752.403544ms 752.508986ms 752.608638ms 752.783657ms 752.960303ms 753.1712ms 753.187373ms 753.521536ms 753.542382ms 753.623333ms 754.21843ms 754.346872ms 755.410479ms 755.540169ms 755.776104ms 756.111295ms 756.332466ms 759.917318ms 765.07415ms 767.283407ms]
Jan  4 23:32:47.868: INFO: 50 %ile: 748.881539ms
Jan  4 23:32:47.868: INFO: 90 %ile: 752.403544ms
Jan  4 23:32:47.868: INFO: 99 %ile: 765.07415ms
Jan  4 23:32:47.868: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan  4 23:32:47.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-771" for this suite. 01/04/23 23:32:47.874
------------------------------
• [SLOW TEST] [10.767 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:32:37.113
    Jan  4 23:32:37.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename svc-latency 01/04/23 23:32:37.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:37.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:37.131
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan  4 23:32:37.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-771 01/04/23 23:32:37.135
    I0104 23:32:37.140955      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-771, replica count: 1
    I0104 23:32:38.193715      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0104 23:32:39.194823      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 23:32:39.311: INFO: Created: latency-svc-wb6s4
    Jan  4 23:32:39.317: INFO: Got endpoints: latency-svc-wb6s4 [22.620104ms]
    Jan  4 23:32:39.343: INFO: Created: latency-svc-sqdrh
    Jan  4 23:32:39.352: INFO: Got endpoints: latency-svc-sqdrh [33.980821ms]
    Jan  4 23:32:39.358: INFO: Created: latency-svc-gx5m7
    Jan  4 23:32:39.365: INFO: Got endpoints: latency-svc-gx5m7 [46.34331ms]
    Jan  4 23:32:39.376: INFO: Created: latency-svc-8rk27
    Jan  4 23:32:39.384: INFO: Got endpoints: latency-svc-8rk27 [65.924163ms]
    Jan  4 23:32:39.389: INFO: Created: latency-svc-79rk4
    Jan  4 23:32:39.392: INFO: Got endpoints: latency-svc-79rk4 [72.920598ms]
    Jan  4 23:32:39.402: INFO: Created: latency-svc-gc2cb
    Jan  4 23:32:39.414: INFO: Created: latency-svc-plrgl
    Jan  4 23:32:39.416: INFO: Got endpoints: latency-svc-gc2cb [97.006312ms]
    Jan  4 23:32:39.424: INFO: Created: latency-svc-ggsgp
    Jan  4 23:32:39.427: INFO: Got endpoints: latency-svc-plrgl [107.608128ms]
    Jan  4 23:32:39.434: INFO: Created: latency-svc-4jf8d
    Jan  4 23:32:39.435: INFO: Got endpoints: latency-svc-ggsgp [116.08263ms]
    Jan  4 23:32:39.444: INFO: Got endpoints: latency-svc-4jf8d [124.579174ms]
    Jan  4 23:32:39.450: INFO: Created: latency-svc-tngj2
    Jan  4 23:32:39.455: INFO: Got endpoints: latency-svc-tngj2 [135.530341ms]
    Jan  4 23:32:39.464: INFO: Created: latency-svc-nmlfc
    Jan  4 23:32:39.470: INFO: Got endpoints: latency-svc-nmlfc [150.069913ms]
    Jan  4 23:32:39.536: INFO: Created: latency-svc-59gtj
    Jan  4 23:32:39.551: INFO: Created: latency-svc-6pjx4
    Jan  4 23:32:39.551: INFO: Created: latency-svc-mt8bm
    Jan  4 23:32:39.557: INFO: Created: latency-svc-kv7tz
    Jan  4 23:32:39.560: INFO: Created: latency-svc-ggxjc
    Jan  4 23:32:39.564: INFO: Created: latency-svc-244g7
    Jan  4 23:32:39.552: INFO: Created: latency-svc-2rkf2
    Jan  4 23:32:39.564: INFO: Created: latency-svc-crvlx
    Jan  4 23:32:39.565: INFO: Created: latency-svc-rkwhj
    Jan  4 23:32:39.565: INFO: Created: latency-svc-wjr9l
    Jan  4 23:32:39.566: INFO: Got endpoints: latency-svc-59gtj [245.408674ms]
    Jan  4 23:32:39.566: INFO: Created: latency-svc-z5zt2
    Jan  4 23:32:39.569: INFO: Created: latency-svc-kcqct
    Jan  4 23:32:39.569: INFO: Created: latency-svc-gb57f
    Jan  4 23:32:39.552: INFO: Created: latency-svc-kn6bk
    Jan  4 23:32:39.566: INFO: Created: latency-svc-87sms
    Jan  4 23:32:39.571: INFO: Got endpoints: latency-svc-wjr9l [179.732529ms]
    Jan  4 23:32:39.573: INFO: Got endpoints: latency-svc-z5zt2 [156.635971ms]
    Jan  4 23:32:39.573: INFO: Got endpoints: latency-svc-ggxjc [253.147437ms]
    Jan  4 23:32:39.573: INFO: Got endpoints: latency-svc-gb57f [146.254104ms]
    Jan  4 23:32:39.576: INFO: Got endpoints: latency-svc-kv7tz [131.428876ms]
    Jan  4 23:32:39.586: INFO: Got endpoints: latency-svc-crvlx [150.887924ms]
    Jan  4 23:32:39.589: INFO: Got endpoints: latency-svc-rkwhj [269.523351ms]
    Jan  4 23:32:39.590: INFO: Got endpoints: latency-svc-kcqct [269.113289ms]
    Jan  4 23:32:39.590: INFO: Got endpoints: latency-svc-2rkf2 [205.477969ms]
    Jan  4 23:32:39.591: INFO: Got endpoints: latency-svc-244g7 [135.465504ms]
    Jan  4 23:32:39.591: INFO: Got endpoints: latency-svc-87sms [270.523462ms]
    Jan  4 23:32:39.598: INFO: Got endpoints: latency-svc-mt8bm [246.299933ms]
    Jan  4 23:32:39.599: INFO: Got endpoints: latency-svc-6pjx4 [129.464703ms]
    Jan  4 23:32:39.599: INFO: Got endpoints: latency-svc-kn6bk [234.466618ms]
    Jan  4 23:32:39.604: INFO: Created: latency-svc-8vmfw
    Jan  4 23:32:39.610: INFO: Got endpoints: latency-svc-8vmfw [43.961559ms]
    Jan  4 23:32:39.614: INFO: Created: latency-svc-9sn2r
    Jan  4 23:32:39.620: INFO: Got endpoints: latency-svc-9sn2r [48.847918ms]
    Jan  4 23:32:39.624: INFO: Created: latency-svc-xp4f9
    Jan  4 23:32:39.627: INFO: Got endpoints: latency-svc-xp4f9 [53.946849ms]
    Jan  4 23:32:39.633: INFO: Created: latency-svc-frjps
    Jan  4 23:32:39.637: INFO: Got endpoints: latency-svc-frjps [63.887679ms]
    Jan  4 23:32:39.642: INFO: Created: latency-svc-s99j8
    Jan  4 23:32:39.647: INFO: Got endpoints: latency-svc-s99j8 [74.340774ms]
    Jan  4 23:32:39.653: INFO: Created: latency-svc-jwggt
    Jan  4 23:32:39.658: INFO: Got endpoints: latency-svc-jwggt [82.870245ms]
    Jan  4 23:32:39.667: INFO: Created: latency-svc-l5d98
    Jan  4 23:32:39.668: INFO: Got endpoints: latency-svc-l5d98 [81.362119ms]
    Jan  4 23:32:39.674: INFO: Created: latency-svc-99fjg
    Jan  4 23:32:39.684: INFO: Got endpoints: latency-svc-99fjg [94.164086ms]
    Jan  4 23:32:39.697: INFO: Created: latency-svc-p25bd
    Jan  4 23:32:39.697: INFO: Got endpoints: latency-svc-p25bd [106.966607ms]
    Jan  4 23:32:39.709: INFO: Created: latency-svc-727bw
    Jan  4 23:32:39.715: INFO: Got endpoints: latency-svc-727bw [125.687332ms]
    Jan  4 23:32:39.717: INFO: Created: latency-svc-fsvql
    Jan  4 23:32:39.729: INFO: Created: latency-svc-2lbm9
    Jan  4 23:32:39.731: INFO: Got endpoints: latency-svc-fsvql [140.037169ms]
    Jan  4 23:32:39.736: INFO: Got endpoints: latency-svc-2lbm9 [144.935401ms]
    Jan  4 23:32:39.740: INFO: Created: latency-svc-wd85d
    Jan  4 23:32:39.749: INFO: Created: latency-svc-62jdp
    Jan  4 23:32:39.754: INFO: Created: latency-svc-7tscv
    Jan  4 23:32:39.761: INFO: Created: latency-svc-vqp6k
    Jan  4 23:32:39.769: INFO: Got endpoints: latency-svc-wd85d [170.059985ms]
    Jan  4 23:32:39.775: INFO: Created: latency-svc-xp6mz
    Jan  4 23:32:39.779: INFO: Created: latency-svc-vxj7g
    Jan  4 23:32:39.786: INFO: Created: latency-svc-m2xxt
    Jan  4 23:32:39.796: INFO: Created: latency-svc-bnjjd
    Jan  4 23:32:39.804: INFO: Created: latency-svc-djpzn
    Jan  4 23:32:39.808: INFO: Created: latency-svc-84msv
    Jan  4 23:32:39.816: INFO: Got endpoints: latency-svc-62jdp [216.414442ms]
    Jan  4 23:32:39.823: INFO: Created: latency-svc-d92rz
    Jan  4 23:32:39.829: INFO: Created: latency-svc-wpmdd
    Jan  4 23:32:39.836: INFO: Created: latency-svc-mgknc
    Jan  4 23:32:39.843: INFO: Created: latency-svc-vnvcf
    Jan  4 23:32:39.853: INFO: Created: latency-svc-fgjqt
    Jan  4 23:32:39.861: INFO: Created: latency-svc-xgr2w
    Jan  4 23:32:39.867: INFO: Got endpoints: latency-svc-7tscv [267.711032ms]
    Jan  4 23:32:39.872: INFO: Created: latency-svc-47bg9
    Jan  4 23:32:39.879: INFO: Created: latency-svc-qs57w
    Jan  4 23:32:39.914: INFO: Got endpoints: latency-svc-vqp6k [304.37418ms]
    Jan  4 23:32:39.928: INFO: Created: latency-svc-smk4l
    Jan  4 23:32:39.965: INFO: Got endpoints: latency-svc-xp6mz [344.804903ms]
    Jan  4 23:32:39.976: INFO: Created: latency-svc-9xdqd
    Jan  4 23:32:40.016: INFO: Got endpoints: latency-svc-vxj7g [389.098974ms]
    Jan  4 23:32:40.032: INFO: Created: latency-svc-cmfkx
    Jan  4 23:32:40.065: INFO: Got endpoints: latency-svc-m2xxt [428.547247ms]
    Jan  4 23:32:40.076: INFO: Created: latency-svc-5zvbs
    Jan  4 23:32:40.117: INFO: Got endpoints: latency-svc-bnjjd [470.052538ms]
    Jan  4 23:32:40.131: INFO: Created: latency-svc-lz4bt
    Jan  4 23:32:40.165: INFO: Got endpoints: latency-svc-djpzn [506.954727ms]
    Jan  4 23:32:40.175: INFO: Created: latency-svc-p6grl
    Jan  4 23:32:40.216: INFO: Got endpoints: latency-svc-84msv [548.176269ms]
    Jan  4 23:32:40.227: INFO: Created: latency-svc-mk55t
    Jan  4 23:32:40.265: INFO: Got endpoints: latency-svc-d92rz [581.694189ms]
    Jan  4 23:32:40.276: INFO: Created: latency-svc-2vllj
    Jan  4 23:32:40.314: INFO: Got endpoints: latency-svc-wpmdd [617.117996ms]
    Jan  4 23:32:40.329: INFO: Created: latency-svc-xnx2w
    Jan  4 23:32:40.365: INFO: Got endpoints: latency-svc-mgknc [649.163596ms]
    Jan  4 23:32:40.386: INFO: Created: latency-svc-td2m5
    Jan  4 23:32:40.414: INFO: Got endpoints: latency-svc-vnvcf [682.813837ms]
    Jan  4 23:32:40.426: INFO: Created: latency-svc-x2489
    Jan  4 23:32:40.467: INFO: Got endpoints: latency-svc-fgjqt [730.714027ms]
    Jan  4 23:32:40.479: INFO: Created: latency-svc-v7tqb
    Jan  4 23:32:40.516: INFO: Got endpoints: latency-svc-xgr2w [747.521339ms]
    Jan  4 23:32:40.526: INFO: Created: latency-svc-ljzrx
    Jan  4 23:32:40.566: INFO: Got endpoints: latency-svc-47bg9 [750.573451ms]
    Jan  4 23:32:40.577: INFO: Created: latency-svc-2wrxp
    Jan  4 23:32:40.617: INFO: Got endpoints: latency-svc-qs57w [749.646228ms]
    Jan  4 23:32:40.626: INFO: Created: latency-svc-x4jtl
    Jan  4 23:32:40.665: INFO: Got endpoints: latency-svc-smk4l [750.76983ms]
    Jan  4 23:32:40.675: INFO: Created: latency-svc-b4db8
    Jan  4 23:32:40.715: INFO: Got endpoints: latency-svc-9xdqd [749.58812ms]
    Jan  4 23:32:40.726: INFO: Created: latency-svc-pjqf8
    Jan  4 23:32:40.765: INFO: Got endpoints: latency-svc-cmfkx [749.557393ms]
    Jan  4 23:32:40.776: INFO: Created: latency-svc-4z4zk
    Jan  4 23:32:40.814: INFO: Got endpoints: latency-svc-5zvbs [748.069854ms]
    Jan  4 23:32:40.825: INFO: Created: latency-svc-bps9d
    Jan  4 23:32:40.864: INFO: Got endpoints: latency-svc-lz4bt [746.714691ms]
    Jan  4 23:32:40.873: INFO: Created: latency-svc-bcqw4
    Jan  4 23:32:40.915: INFO: Got endpoints: latency-svc-p6grl [749.922343ms]
    Jan  4 23:32:40.926: INFO: Created: latency-svc-q62hk
    Jan  4 23:32:40.965: INFO: Got endpoints: latency-svc-mk55t [749.131482ms]
    Jan  4 23:32:40.976: INFO: Created: latency-svc-jr7c9
    Jan  4 23:32:41.017: INFO: Got endpoints: latency-svc-2vllj [751.317223ms]
    Jan  4 23:32:41.032: INFO: Created: latency-svc-drrvf
    Jan  4 23:32:41.067: INFO: Got endpoints: latency-svc-xnx2w [752.394293ms]
    Jan  4 23:32:41.076: INFO: Created: latency-svc-twcqm
    Jan  4 23:32:41.114: INFO: Got endpoints: latency-svc-td2m5 [744.759135ms]
    Jan  4 23:32:41.124: INFO: Created: latency-svc-69tjk
    Jan  4 23:32:41.164: INFO: Got endpoints: latency-svc-x2489 [749.801771ms]
    Jan  4 23:32:41.177: INFO: Created: latency-svc-n2qcg
    Jan  4 23:32:41.215: INFO: Got endpoints: latency-svc-v7tqb [747.822689ms]
    Jan  4 23:32:41.225: INFO: Created: latency-svc-gmz5r
    Jan  4 23:32:41.264: INFO: Got endpoints: latency-svc-ljzrx [747.522258ms]
    Jan  4 23:32:41.274: INFO: Created: latency-svc-8tqv9
    Jan  4 23:32:41.316: INFO: Got endpoints: latency-svc-2wrxp [749.205916ms]
    Jan  4 23:32:41.331: INFO: Created: latency-svc-ldmb9
    Jan  4 23:32:41.365: INFO: Got endpoints: latency-svc-x4jtl [748.123542ms]
    Jan  4 23:32:41.376: INFO: Created: latency-svc-hllcp
    Jan  4 23:32:41.415: INFO: Got endpoints: latency-svc-b4db8 [749.428308ms]
    Jan  4 23:32:41.427: INFO: Created: latency-svc-lhxvs
    Jan  4 23:32:41.465: INFO: Got endpoints: latency-svc-pjqf8 [750.318551ms]
    Jan  4 23:32:41.476: INFO: Created: latency-svc-fllw5
    Jan  4 23:32:41.513: INFO: Got endpoints: latency-svc-4z4zk [747.756417ms]
    Jan  4 23:32:41.526: INFO: Created: latency-svc-fwrw5
    Jan  4 23:32:41.565: INFO: Got endpoints: latency-svc-bps9d [751.855924ms]
    Jan  4 23:32:41.576: INFO: Created: latency-svc-8pf9c
    Jan  4 23:32:41.621: INFO: Got endpoints: latency-svc-bcqw4 [756.332466ms]
    Jan  4 23:32:41.630: INFO: Created: latency-svc-dn547
    Jan  4 23:32:41.665: INFO: Got endpoints: latency-svc-q62hk [749.514289ms]
    Jan  4 23:32:41.683: INFO: Created: latency-svc-xkf69
    Jan  4 23:32:41.716: INFO: Got endpoints: latency-svc-jr7c9 [751.265099ms]
    Jan  4 23:32:41.733: INFO: Created: latency-svc-n9wgx
    Jan  4 23:32:41.767: INFO: Got endpoints: latency-svc-drrvf [749.753926ms]
    Jan  4 23:32:41.778: INFO: Created: latency-svc-sz89k
    Jan  4 23:32:41.820: INFO: Got endpoints: latency-svc-twcqm [753.542382ms]
    Jan  4 23:32:41.831: INFO: Created: latency-svc-lqlrn
    Jan  4 23:32:41.866: INFO: Got endpoints: latency-svc-69tjk [751.429098ms]
    Jan  4 23:32:41.877: INFO: Created: latency-svc-npvsf
    Jan  4 23:32:41.915: INFO: Got endpoints: latency-svc-n2qcg [751.235146ms]
    Jan  4 23:32:41.925: INFO: Created: latency-svc-fx459
    Jan  4 23:32:41.964: INFO: Got endpoints: latency-svc-gmz5r [748.60946ms]
    Jan  4 23:32:41.974: INFO: Created: latency-svc-8qhrc
    Jan  4 23:32:42.029: INFO: Got endpoints: latency-svc-8tqv9 [765.07415ms]
    Jan  4 23:32:42.042: INFO: Created: latency-svc-qtzxt
    Jan  4 23:32:42.065: INFO: Got endpoints: latency-svc-ldmb9 [748.845986ms]
    Jan  4 23:32:42.081: INFO: Created: latency-svc-8r5nf
    Jan  4 23:32:42.115: INFO: Got endpoints: latency-svc-hllcp [749.527539ms]
    Jan  4 23:32:42.129: INFO: Created: latency-svc-hsfdw
    Jan  4 23:32:42.163: INFO: Got endpoints: latency-svc-lhxvs [748.047315ms]
    Jan  4 23:32:42.175: INFO: Created: latency-svc-wgpsj
    Jan  4 23:32:42.215: INFO: Got endpoints: latency-svc-fllw5 [749.394802ms]
    Jan  4 23:32:42.226: INFO: Created: latency-svc-dmmdn
    Jan  4 23:32:42.265: INFO: Got endpoints: latency-svc-fwrw5 [751.159355ms]
    Jan  4 23:32:42.275: INFO: Created: latency-svc-pmgps
    Jan  4 23:32:42.316: INFO: Got endpoints: latency-svc-8pf9c [750.509135ms]
    Jan  4 23:32:42.332: INFO: Created: latency-svc-b7vts
    Jan  4 23:32:42.375: INFO: Got endpoints: latency-svc-dn547 [754.21843ms]
    Jan  4 23:32:42.389: INFO: Created: latency-svc-jwf2h
    Jan  4 23:32:42.419: INFO: Got endpoints: latency-svc-xkf69 [754.346872ms]
    Jan  4 23:32:42.429: INFO: Created: latency-svc-676z6
    Jan  4 23:32:42.465: INFO: Got endpoints: latency-svc-n9wgx [748.43242ms]
    Jan  4 23:32:42.476: INFO: Created: latency-svc-4w97d
    Jan  4 23:32:42.516: INFO: Got endpoints: latency-svc-sz89k [749.095253ms]
    Jan  4 23:32:42.528: INFO: Created: latency-svc-zzxgl
    Jan  4 23:32:42.567: INFO: Got endpoints: latency-svc-lqlrn [746.738198ms]
    Jan  4 23:32:42.579: INFO: Created: latency-svc-hnbtt
    Jan  4 23:32:42.614: INFO: Got endpoints: latency-svc-npvsf [747.896216ms]
    Jan  4 23:32:42.625: INFO: Created: latency-svc-b6zfx
    Jan  4 23:32:42.666: INFO: Got endpoints: latency-svc-fx459 [750.758496ms]
    Jan  4 23:32:42.677: INFO: Created: latency-svc-z7ws5
    Jan  4 23:32:42.716: INFO: Got endpoints: latency-svc-8qhrc [752.380607ms]
    Jan  4 23:32:42.725: INFO: Created: latency-svc-tgr6l
    Jan  4 23:32:42.773: INFO: Got endpoints: latency-svc-qtzxt [743.980835ms]
    Jan  4 23:32:42.790: INFO: Created: latency-svc-gg7zl
    Jan  4 23:32:42.816: INFO: Got endpoints: latency-svc-8r5nf [750.638679ms]
    Jan  4 23:32:42.833: INFO: Created: latency-svc-gdfbc
    Jan  4 23:32:42.866: INFO: Got endpoints: latency-svc-hsfdw [750.224584ms]
    Jan  4 23:32:42.881: INFO: Created: latency-svc-bds2v
    Jan  4 23:32:42.914: INFO: Got endpoints: latency-svc-wgpsj [751.112552ms]
    Jan  4 23:32:42.927: INFO: Created: latency-svc-bs92b
    Jan  4 23:32:42.965: INFO: Got endpoints: latency-svc-dmmdn [749.836477ms]
    Jan  4 23:32:42.986: INFO: Created: latency-svc-9dl5v
    Jan  4 23:32:43.025: INFO: Got endpoints: latency-svc-pmgps [759.917318ms]
    Jan  4 23:32:43.045: INFO: Created: latency-svc-qjtgw
    Jan  4 23:32:43.065: INFO: Got endpoints: latency-svc-b7vts [748.877904ms]
    Jan  4 23:32:43.076: INFO: Created: latency-svc-gzrb7
    Jan  4 23:32:43.116: INFO: Got endpoints: latency-svc-jwf2h [741.301174ms]
    Jan  4 23:32:43.126: INFO: Created: latency-svc-qfl7z
    Jan  4 23:32:43.163: INFO: Got endpoints: latency-svc-676z6 [743.898191ms]
    Jan  4 23:32:43.176: INFO: Created: latency-svc-gl7rr
    Jan  4 23:32:43.216: INFO: Got endpoints: latency-svc-4w97d [751.278239ms]
    Jan  4 23:32:43.226: INFO: Created: latency-svc-9g4g8
    Jan  4 23:32:43.266: INFO: Got endpoints: latency-svc-zzxgl [749.709664ms]
    Jan  4 23:32:43.278: INFO: Created: latency-svc-t4x22
    Jan  4 23:32:43.323: INFO: Got endpoints: latency-svc-hnbtt [755.410479ms]
    Jan  4 23:32:43.350: INFO: Created: latency-svc-l9nxw
    Jan  4 23:32:43.370: INFO: Got endpoints: latency-svc-b6zfx [755.776104ms]
    Jan  4 23:32:43.392: INFO: Created: latency-svc-pqd58
    Jan  4 23:32:43.416: INFO: Got endpoints: latency-svc-z7ws5 [750.046927ms]
    Jan  4 23:32:43.437: INFO: Created: latency-svc-zzwk9
    Jan  4 23:32:43.469: INFO: Got endpoints: latency-svc-tgr6l [752.960303ms]
    Jan  4 23:32:43.490: INFO: Created: latency-svc-l7v4g
    Jan  4 23:32:43.516: INFO: Got endpoints: latency-svc-gg7zl [742.72109ms]
    Jan  4 23:32:43.527: INFO: Created: latency-svc-kmpwq
    Jan  4 23:32:43.565: INFO: Got endpoints: latency-svc-gdfbc [748.927091ms]
    Jan  4 23:32:43.581: INFO: Created: latency-svc-h5t5w
    Jan  4 23:32:43.615: INFO: Got endpoints: latency-svc-bds2v [749.084874ms]
    Jan  4 23:32:43.627: INFO: Created: latency-svc-xxnw4
    Jan  4 23:32:43.665: INFO: Got endpoints: latency-svc-bs92b [750.473904ms]
    Jan  4 23:32:43.675: INFO: Created: latency-svc-djsx9
    Jan  4 23:32:43.714: INFO: Got endpoints: latency-svc-9dl5v [748.49295ms]
    Jan  4 23:32:43.724: INFO: Created: latency-svc-nm985
    Jan  4 23:32:43.765: INFO: Got endpoints: latency-svc-qjtgw [740.05951ms]
    Jan  4 23:32:43.777: INFO: Created: latency-svc-z9bqr
    Jan  4 23:32:43.815: INFO: Got endpoints: latency-svc-gzrb7 [748.489334ms]
    Jan  4 23:32:43.826: INFO: Created: latency-svc-l72c9
    Jan  4 23:32:43.865: INFO: Got endpoints: latency-svc-qfl7z [748.580998ms]
    Jan  4 23:32:43.879: INFO: Created: latency-svc-67zl6
    Jan  4 23:32:43.915: INFO: Got endpoints: latency-svc-gl7rr [751.865251ms]
    Jan  4 23:32:43.925: INFO: Created: latency-svc-n7rmk
    Jan  4 23:32:43.964: INFO: Got endpoints: latency-svc-9g4g8 [748.134014ms]
    Jan  4 23:32:43.976: INFO: Created: latency-svc-rnx8p
    Jan  4 23:32:44.014: INFO: Got endpoints: latency-svc-t4x22 [748.523925ms]
    Jan  4 23:32:44.029: INFO: Created: latency-svc-pm9nq
    Jan  4 23:32:44.067: INFO: Got endpoints: latency-svc-l9nxw [743.603939ms]
    Jan  4 23:32:44.077: INFO: Created: latency-svc-s22pq
    Jan  4 23:32:44.124: INFO: Got endpoints: latency-svc-pqd58 [753.623333ms]
    Jan  4 23:32:44.137: INFO: Created: latency-svc-dpktr
    Jan  4 23:32:44.165: INFO: Got endpoints: latency-svc-zzwk9 [748.299735ms]
    Jan  4 23:32:44.177: INFO: Created: latency-svc-lf967
    Jan  4 23:32:44.217: INFO: Got endpoints: latency-svc-l7v4g [747.855886ms]
    Jan  4 23:32:44.227: INFO: Created: latency-svc-7wk6g
    Jan  4 23:32:44.264: INFO: Got endpoints: latency-svc-kmpwq [747.697392ms]
    Jan  4 23:32:44.280: INFO: Created: latency-svc-547gt
    Jan  4 23:32:44.320: INFO: Got endpoints: latency-svc-h5t5w [755.540169ms]
    Jan  4 23:32:44.333: INFO: Created: latency-svc-76w7s
    Jan  4 23:32:44.371: INFO: Got endpoints: latency-svc-xxnw4 [756.111295ms]
    Jan  4 23:32:44.381: INFO: Created: latency-svc-54k9s
    Jan  4 23:32:44.414: INFO: Got endpoints: latency-svc-djsx9 [748.109434ms]
    Jan  4 23:32:44.427: INFO: Created: latency-svc-bgfvx
    Jan  4 23:32:44.465: INFO: Got endpoints: latency-svc-nm985 [750.782629ms]
    Jan  4 23:32:44.476: INFO: Created: latency-svc-lkt2r
    Jan  4 23:32:44.533: INFO: Got endpoints: latency-svc-z9bqr [767.283407ms]
    Jan  4 23:32:44.545: INFO: Created: latency-svc-lwt9b
    Jan  4 23:32:44.565: INFO: Got endpoints: latency-svc-l72c9 [749.57184ms]
    Jan  4 23:32:44.575: INFO: Created: latency-svc-x8q9x
    Jan  4 23:32:44.616: INFO: Got endpoints: latency-svc-67zl6 [750.434097ms]
    Jan  4 23:32:44.626: INFO: Created: latency-svc-s8p7l
    Jan  4 23:32:44.664: INFO: Got endpoints: latency-svc-n7rmk [748.978073ms]
    Jan  4 23:32:44.676: INFO: Created: latency-svc-6wktp
    Jan  4 23:32:44.715: INFO: Got endpoints: latency-svc-rnx8p [750.677294ms]
    Jan  4 23:32:44.727: INFO: Created: latency-svc-5drrx
    Jan  4 23:32:44.764: INFO: Got endpoints: latency-svc-pm9nq [749.135678ms]
    Jan  4 23:32:44.773: INFO: Created: latency-svc-mr6sw
    Jan  4 23:32:44.814: INFO: Got endpoints: latency-svc-s22pq [746.603647ms]
    Jan  4 23:32:44.824: INFO: Created: latency-svc-45wzv
    Jan  4 23:32:44.867: INFO: Got endpoints: latency-svc-dpktr [743.040607ms]
    Jan  4 23:32:44.885: INFO: Created: latency-svc-p8qgr
    Jan  4 23:32:44.915: INFO: Got endpoints: latency-svc-lf967 [750.03325ms]
    Jan  4 23:32:44.926: INFO: Created: latency-svc-pr7nx
    Jan  4 23:32:44.964: INFO: Got endpoints: latency-svc-7wk6g [746.92065ms]
    Jan  4 23:32:44.974: INFO: Created: latency-svc-rkvd4
    Jan  4 23:32:45.014: INFO: Got endpoints: latency-svc-547gt [749.562117ms]
    Jan  4 23:32:45.024: INFO: Created: latency-svc-4mplk
    Jan  4 23:32:45.065: INFO: Got endpoints: latency-svc-76w7s [744.480805ms]
    Jan  4 23:32:45.075: INFO: Created: latency-svc-c6tdl
    Jan  4 23:32:45.115: INFO: Got endpoints: latency-svc-54k9s [743.771496ms]
    Jan  4 23:32:45.125: INFO: Created: latency-svc-9mhfh
    Jan  4 23:32:45.163: INFO: Got endpoints: latency-svc-bgfvx [749.487258ms]
    Jan  4 23:32:45.175: INFO: Created: latency-svc-6p82h
    Jan  4 23:32:45.214: INFO: Got endpoints: latency-svc-lkt2r [748.403774ms]
    Jan  4 23:32:45.227: INFO: Created: latency-svc-5pd4h
    Jan  4 23:32:45.266: INFO: Got endpoints: latency-svc-lwt9b [732.598988ms]
    Jan  4 23:32:45.278: INFO: Created: latency-svc-mxf69
    Jan  4 23:32:45.316: INFO: Got endpoints: latency-svc-x8q9x [750.981032ms]
    Jan  4 23:32:45.331: INFO: Created: latency-svc-bdgvn
    Jan  4 23:32:45.368: INFO: Got endpoints: latency-svc-s8p7l [752.508986ms]
    Jan  4 23:32:45.386: INFO: Created: latency-svc-t6xd8
    Jan  4 23:32:45.415: INFO: Got endpoints: latency-svc-6wktp [750.674184ms]
    Jan  4 23:32:45.429: INFO: Created: latency-svc-kvg8j
    Jan  4 23:32:45.465: INFO: Got endpoints: latency-svc-5drrx [749.214544ms]
    Jan  4 23:32:45.481: INFO: Created: latency-svc-j66cb
    Jan  4 23:32:45.516: INFO: Got endpoints: latency-svc-mr6sw [750.329712ms]
    Jan  4 23:32:45.534: INFO: Created: latency-svc-v6n46
    Jan  4 23:32:45.565: INFO: Got endpoints: latency-svc-45wzv [751.164362ms]
    Jan  4 23:32:45.576: INFO: Created: latency-svc-stcgc
    Jan  4 23:32:45.622: INFO: Got endpoints: latency-svc-p8qgr [747.99814ms]
    Jan  4 23:32:45.633: INFO: Created: latency-svc-tv44k
    Jan  4 23:32:45.665: INFO: Got endpoints: latency-svc-pr7nx [749.879189ms]
    Jan  4 23:32:45.677: INFO: Created: latency-svc-glrdr
    Jan  4 23:32:45.714: INFO: Got endpoints: latency-svc-rkvd4 [750.191236ms]
    Jan  4 23:32:45.727: INFO: Created: latency-svc-9j7sq
    Jan  4 23:32:45.766: INFO: Got endpoints: latency-svc-4mplk [752.403544ms]
    Jan  4 23:32:45.779: INFO: Created: latency-svc-rj65h
    Jan  4 23:32:45.816: INFO: Got endpoints: latency-svc-c6tdl [751.449088ms]
    Jan  4 23:32:45.829: INFO: Created: latency-svc-fmlbh
    Jan  4 23:32:45.865: INFO: Got endpoints: latency-svc-9mhfh [749.640404ms]
    Jan  4 23:32:45.877: INFO: Created: latency-svc-shjcf
    Jan  4 23:32:45.917: INFO: Got endpoints: latency-svc-6p82h [753.1712ms]
    Jan  4 23:32:45.928: INFO: Created: latency-svc-mm4vk
    Jan  4 23:32:45.966: INFO: Got endpoints: latency-svc-5pd4h [751.747768ms]
    Jan  4 23:32:45.979: INFO: Created: latency-svc-rv6b5
    Jan  4 23:32:46.013: INFO: Got endpoints: latency-svc-mxf69 [747.739129ms]
    Jan  4 23:32:46.024: INFO: Created: latency-svc-6n54h
    Jan  4 23:32:46.066: INFO: Got endpoints: latency-svc-bdgvn [750.293641ms]
    Jan  4 23:32:46.078: INFO: Created: latency-svc-gw25p
    Jan  4 23:32:46.115: INFO: Got endpoints: latency-svc-t6xd8 [746.269022ms]
    Jan  4 23:32:46.125: INFO: Created: latency-svc-vncp6
    Jan  4 23:32:46.164: INFO: Got endpoints: latency-svc-kvg8j [749.021994ms]
    Jan  4 23:32:46.180: INFO: Created: latency-svc-5tgqp
    Jan  4 23:32:46.215: INFO: Got endpoints: latency-svc-j66cb [750.479828ms]
    Jan  4 23:32:46.227: INFO: Created: latency-svc-wx7lp
    Jan  4 23:32:46.267: INFO: Got endpoints: latency-svc-v6n46 [750.675166ms]
    Jan  4 23:32:46.277: INFO: Created: latency-svc-q7bgp
    Jan  4 23:32:46.315: INFO: Got endpoints: latency-svc-stcgc [749.810233ms]
    Jan  4 23:32:46.329: INFO: Created: latency-svc-mgndl
    Jan  4 23:32:46.368: INFO: Got endpoints: latency-svc-tv44k [745.135885ms]
    Jan  4 23:32:46.387: INFO: Created: latency-svc-x2hh4
    Jan  4 23:32:46.416: INFO: Got endpoints: latency-svc-glrdr [750.972403ms]
    Jan  4 23:32:46.427: INFO: Created: latency-svc-jxxdm
    Jan  4 23:32:46.464: INFO: Got endpoints: latency-svc-9j7sq [749.508112ms]
    Jan  4 23:32:46.474: INFO: Created: latency-svc-zxktb
    Jan  4 23:32:46.515: INFO: Got endpoints: latency-svc-rj65h [748.267578ms]
    Jan  4 23:32:46.525: INFO: Created: latency-svc-nsbxc
    Jan  4 23:32:46.567: INFO: Got endpoints: latency-svc-fmlbh [750.574902ms]
    Jan  4 23:32:46.577: INFO: Created: latency-svc-swb52
    Jan  4 23:32:46.615: INFO: Got endpoints: latency-svc-shjcf [749.511657ms]
    Jan  4 23:32:46.627: INFO: Created: latency-svc-bhqx8
    Jan  4 23:32:46.666: INFO: Got endpoints: latency-svc-mm4vk [749.087621ms]
    Jan  4 23:32:46.676: INFO: Created: latency-svc-p2zzk
    Jan  4 23:32:46.716: INFO: Got endpoints: latency-svc-rv6b5 [750.026615ms]
    Jan  4 23:32:46.725: INFO: Created: latency-svc-8zbgw
    Jan  4 23:32:46.766: INFO: Got endpoints: latency-svc-6n54h [752.783657ms]
    Jan  4 23:32:46.776: INFO: Created: latency-svc-v9gxt
    Jan  4 23:32:46.815: INFO: Got endpoints: latency-svc-gw25p [748.954557ms]
    Jan  4 23:32:46.828: INFO: Created: latency-svc-gk8c7
    Jan  4 23:32:46.867: INFO: Got endpoints: latency-svc-vncp6 [752.608638ms]
    Jan  4 23:32:46.885: INFO: Created: latency-svc-8d84g
    Jan  4 23:32:46.914: INFO: Got endpoints: latency-svc-5tgqp [749.308526ms]
    Jan  4 23:32:46.924: INFO: Created: latency-svc-99qjm
    Jan  4 23:32:46.966: INFO: Got endpoints: latency-svc-wx7lp [750.434433ms]
    Jan  4 23:32:46.976: INFO: Created: latency-svc-xnk5p
    Jan  4 23:32:47.014: INFO: Got endpoints: latency-svc-q7bgp [747.431145ms]
    Jan  4 23:32:47.028: INFO: Created: latency-svc-tmxjn
    Jan  4 23:32:47.064: INFO: Got endpoints: latency-svc-mgndl [748.694235ms]
    Jan  4 23:32:47.076: INFO: Created: latency-svc-ghflm
    Jan  4 23:32:47.115: INFO: Got endpoints: latency-svc-x2hh4 [747.16363ms]
    Jan  4 23:32:47.125: INFO: Created: latency-svc-bc2qk
    Jan  4 23:32:47.166: INFO: Got endpoints: latency-svc-jxxdm [749.940238ms]
    Jan  4 23:32:47.215: INFO: Got endpoints: latency-svc-zxktb [750.682884ms]
    Jan  4 23:32:47.264: INFO: Got endpoints: latency-svc-nsbxc [749.50098ms]
    Jan  4 23:32:47.314: INFO: Got endpoints: latency-svc-swb52 [746.225585ms]
    Jan  4 23:32:47.368: INFO: Got endpoints: latency-svc-bhqx8 [753.521536ms]
    Jan  4 23:32:47.419: INFO: Got endpoints: latency-svc-p2zzk [753.187373ms]
    Jan  4 23:32:47.465: INFO: Got endpoints: latency-svc-8zbgw [748.881539ms]
    Jan  4 23:32:47.518: INFO: Got endpoints: latency-svc-v9gxt [751.686382ms]
    Jan  4 23:32:47.564: INFO: Got endpoints: latency-svc-gk8c7 [747.851857ms]
    Jan  4 23:32:47.614: INFO: Got endpoints: latency-svc-8d84g [746.865465ms]
    Jan  4 23:32:47.665: INFO: Got endpoints: latency-svc-99qjm [750.810874ms]
    Jan  4 23:32:47.716: INFO: Got endpoints: latency-svc-xnk5p [749.872231ms]
    Jan  4 23:32:47.766: INFO: Got endpoints: latency-svc-tmxjn [751.850048ms]
    Jan  4 23:32:47.815: INFO: Got endpoints: latency-svc-ghflm [750.061302ms]
    Jan  4 23:32:47.867: INFO: Got endpoints: latency-svc-bc2qk [751.653282ms]
    Jan  4 23:32:47.868: INFO: Latencies: [33.980821ms 43.961559ms 46.34331ms 48.847918ms 53.946849ms 63.887679ms 65.924163ms 72.920598ms 74.340774ms 81.362119ms 82.870245ms 94.164086ms 97.006312ms 106.966607ms 107.608128ms 116.08263ms 124.579174ms 125.687332ms 129.464703ms 131.428876ms 135.465504ms 135.530341ms 140.037169ms 144.935401ms 146.254104ms 150.069913ms 150.887924ms 156.635971ms 170.059985ms 179.732529ms 205.477969ms 216.414442ms 234.466618ms 245.408674ms 246.299933ms 253.147437ms 267.711032ms 269.113289ms 269.523351ms 270.523462ms 304.37418ms 344.804903ms 389.098974ms 428.547247ms 470.052538ms 506.954727ms 548.176269ms 581.694189ms 617.117996ms 649.163596ms 682.813837ms 730.714027ms 732.598988ms 740.05951ms 741.301174ms 742.72109ms 743.040607ms 743.603939ms 743.771496ms 743.898191ms 743.980835ms 744.480805ms 744.759135ms 745.135885ms 746.225585ms 746.269022ms 746.603647ms 746.714691ms 746.738198ms 746.865465ms 746.92065ms 747.16363ms 747.431145ms 747.521339ms 747.522258ms 747.697392ms 747.739129ms 747.756417ms 747.822689ms 747.851857ms 747.855886ms 747.896216ms 747.99814ms 748.047315ms 748.069854ms 748.109434ms 748.123542ms 748.134014ms 748.267578ms 748.299735ms 748.403774ms 748.43242ms 748.489334ms 748.49295ms 748.523925ms 748.580998ms 748.60946ms 748.694235ms 748.845986ms 748.877904ms 748.881539ms 748.927091ms 748.954557ms 748.978073ms 749.021994ms 749.084874ms 749.087621ms 749.095253ms 749.131482ms 749.135678ms 749.205916ms 749.214544ms 749.308526ms 749.394802ms 749.428308ms 749.487258ms 749.50098ms 749.508112ms 749.511657ms 749.514289ms 749.527539ms 749.557393ms 749.562117ms 749.57184ms 749.58812ms 749.640404ms 749.646228ms 749.709664ms 749.753926ms 749.801771ms 749.810233ms 749.836477ms 749.872231ms 749.879189ms 749.922343ms 749.940238ms 750.026615ms 750.03325ms 750.046927ms 750.061302ms 750.191236ms 750.224584ms 750.293641ms 750.318551ms 750.329712ms 750.434097ms 750.434433ms 750.473904ms 750.479828ms 750.509135ms 750.573451ms 750.574902ms 750.638679ms 750.674184ms 750.675166ms 750.677294ms 750.682884ms 750.758496ms 750.76983ms 750.782629ms 750.810874ms 750.972403ms 750.981032ms 751.112552ms 751.159355ms 751.164362ms 751.235146ms 751.265099ms 751.278239ms 751.317223ms 751.429098ms 751.449088ms 751.653282ms 751.686382ms 751.747768ms 751.850048ms 751.855924ms 751.865251ms 752.380607ms 752.394293ms 752.403544ms 752.508986ms 752.608638ms 752.783657ms 752.960303ms 753.1712ms 753.187373ms 753.521536ms 753.542382ms 753.623333ms 754.21843ms 754.346872ms 755.410479ms 755.540169ms 755.776104ms 756.111295ms 756.332466ms 759.917318ms 765.07415ms 767.283407ms]
    Jan  4 23:32:47.868: INFO: 50 %ile: 748.881539ms
    Jan  4 23:32:47.868: INFO: 90 %ile: 752.403544ms
    Jan  4 23:32:47.868: INFO: 99 %ile: 765.07415ms
    Jan  4 23:32:47.868: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:32:47.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-771" for this suite. 01/04/23 23:32:47.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:32:47.882
Jan  4 23:32:47.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubelet-test 01/04/23 23:32:47.883
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:47.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:47.9
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:32:51.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4013" for this suite. 01/04/23 23:32:51.97
------------------------------
• [4.097 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:32:47.882
    Jan  4 23:32:47.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubelet-test 01/04/23 23:32:47.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:47.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:47.9
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:32:51.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4013" for this suite. 01/04/23 23:32:51.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:32:51.979
Jan  4 23:32:51.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replicaset 01/04/23 23:32:51.981
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:52.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:52.006
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/04/23 23:32:52.013
STEP: Verify that the required pods have come up. 01/04/23 23:32:52.019
Jan  4 23:32:52.022: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  4 23:32:57.028: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/04/23 23:32:57.028
STEP: Getting /status 01/04/23 23:32:57.028
Jan  4 23:32:57.036: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/04/23 23:32:57.036
Jan  4 23:32:57.053: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/04/23 23:32:57.053
Jan  4 23:32:57.055: INFO: Observed &ReplicaSet event: ADDED
Jan  4 23:32:57.055: INFO: Observed &ReplicaSet event: MODIFIED
Jan  4 23:32:57.055: INFO: Observed &ReplicaSet event: MODIFIED
Jan  4 23:32:57.056: INFO: Observed &ReplicaSet event: MODIFIED
Jan  4 23:32:57.056: INFO: Found replicaset test-rs in namespace replicaset-8780 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  4 23:32:57.056: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/04/23 23:32:57.056
Jan  4 23:32:57.056: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  4 23:32:57.063: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/04/23 23:32:57.063
Jan  4 23:32:57.064: INFO: Observed &ReplicaSet event: ADDED
Jan  4 23:32:57.064: INFO: Observed &ReplicaSet event: MODIFIED
Jan  4 23:32:57.065: INFO: Observed &ReplicaSet event: MODIFIED
Jan  4 23:32:57.065: INFO: Observed &ReplicaSet event: MODIFIED
Jan  4 23:32:57.065: INFO: Observed replicaset test-rs in namespace replicaset-8780 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  4 23:32:57.065: INFO: Observed &ReplicaSet event: MODIFIED
Jan  4 23:32:57.065: INFO: Found replicaset test-rs in namespace replicaset-8780 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan  4 23:32:57.065: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:32:57.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8780" for this suite. 01/04/23 23:32:57.071
------------------------------
• [SLOW TEST] [5.102 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:32:51.979
    Jan  4 23:32:51.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replicaset 01/04/23 23:32:51.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:52.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:52.006
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/04/23 23:32:52.013
    STEP: Verify that the required pods have come up. 01/04/23 23:32:52.019
    Jan  4 23:32:52.022: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  4 23:32:57.028: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/04/23 23:32:57.028
    STEP: Getting /status 01/04/23 23:32:57.028
    Jan  4 23:32:57.036: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/04/23 23:32:57.036
    Jan  4 23:32:57.053: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/04/23 23:32:57.053
    Jan  4 23:32:57.055: INFO: Observed &ReplicaSet event: ADDED
    Jan  4 23:32:57.055: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  4 23:32:57.055: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  4 23:32:57.056: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  4 23:32:57.056: INFO: Found replicaset test-rs in namespace replicaset-8780 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  4 23:32:57.056: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/04/23 23:32:57.056
    Jan  4 23:32:57.056: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  4 23:32:57.063: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/04/23 23:32:57.063
    Jan  4 23:32:57.064: INFO: Observed &ReplicaSet event: ADDED
    Jan  4 23:32:57.064: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  4 23:32:57.065: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  4 23:32:57.065: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  4 23:32:57.065: INFO: Observed replicaset test-rs in namespace replicaset-8780 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  4 23:32:57.065: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  4 23:32:57.065: INFO: Found replicaset test-rs in namespace replicaset-8780 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan  4 23:32:57.065: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:32:57.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8780" for this suite. 01/04/23 23:32:57.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:32:57.086
Jan  4 23:32:57.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename configmap 01/04/23 23:32:57.087
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:57.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:57.114
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-9874ac90-9a8f-485c-931a-4a6ab8a4efcc 01/04/23 23:32:57.12
STEP: Creating the pod 01/04/23 23:32:57.124
Jan  4 23:32:57.135: INFO: Waiting up to 5m0s for pod "pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066" in namespace "configmap-727" to be "running and ready"
Jan  4 23:32:57.139: INFO: Pod "pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066": Phase="Pending", Reason="", readiness=false. Elapsed: 4.152458ms
Jan  4 23:32:57.139: INFO: The phase of Pod pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:32:59.149: INFO: Pod "pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066": Phase="Running", Reason="", readiness=true. Elapsed: 2.014647272s
Jan  4 23:32:59.150: INFO: The phase of Pod pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066 is Running (Ready = true)
Jan  4 23:32:59.150: INFO: Pod "pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-9874ac90-9a8f-485c-931a-4a6ab8a4efcc 01/04/23 23:32:59.17
STEP: waiting to observe update in volume 01/04/23 23:32:59.178
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:33:03.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-727" for this suite. 01/04/23 23:33:03.22
------------------------------
• [SLOW TEST] [6.143 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:32:57.086
    Jan  4 23:32:57.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename configmap 01/04/23 23:32:57.087
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:32:57.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:32:57.114
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-9874ac90-9a8f-485c-931a-4a6ab8a4efcc 01/04/23 23:32:57.12
    STEP: Creating the pod 01/04/23 23:32:57.124
    Jan  4 23:32:57.135: INFO: Waiting up to 5m0s for pod "pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066" in namespace "configmap-727" to be "running and ready"
    Jan  4 23:32:57.139: INFO: Pod "pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066": Phase="Pending", Reason="", readiness=false. Elapsed: 4.152458ms
    Jan  4 23:32:57.139: INFO: The phase of Pod pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:32:59.149: INFO: Pod "pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066": Phase="Running", Reason="", readiness=true. Elapsed: 2.014647272s
    Jan  4 23:32:59.150: INFO: The phase of Pod pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066 is Running (Ready = true)
    Jan  4 23:32:59.150: INFO: Pod "pod-configmaps-5375c51f-9258-4f30-a876-a1a01afa2066" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-9874ac90-9a8f-485c-931a-4a6ab8a4efcc 01/04/23 23:32:59.17
    STEP: waiting to observe update in volume 01/04/23 23:32:59.178
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:33:03.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-727" for this suite. 01/04/23 23:33:03.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:33:03.23
Jan  4 23:33:03.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 23:33:03.231
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:33:03.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:33:03.263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 23:33:03.288
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:33:03.652
STEP: Deploying the webhook pod 01/04/23 23:33:03.66
STEP: Wait for the deployment to be ready 01/04/23 23:33:03.672
Jan  4 23:33:03.684: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 23:33:05.693
STEP: Verifying the service has paired with the endpoint 01/04/23 23:33:05.703
Jan  4 23:33:06.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan  4 23:33:06.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/04/23 23:33:07.221
STEP: Creating a custom resource that should be denied by the webhook 01/04/23 23:33:07.256
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/04/23 23:33:09.285
STEP: Updating the custom resource with disallowed data should be denied 01/04/23 23:33:09.291
STEP: Deleting the custom resource should be denied 01/04/23 23:33:09.299
STEP: Remove the offending key and value from the custom resource data 01/04/23 23:33:09.305
STEP: Deleting the updated custom resource should be successful 01/04/23 23:33:09.315
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:33:09.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8826" for this suite. 01/04/23 23:33:09.904
STEP: Destroying namespace "webhook-8826-markers" for this suite. 01/04/23 23:33:09.924
------------------------------
• [SLOW TEST] [6.704 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:33:03.23
    Jan  4 23:33:03.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 23:33:03.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:33:03.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:33:03.263
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 23:33:03.288
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:33:03.652
    STEP: Deploying the webhook pod 01/04/23 23:33:03.66
    STEP: Wait for the deployment to be ready 01/04/23 23:33:03.672
    Jan  4 23:33:03.684: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 23:33:05.693
    STEP: Verifying the service has paired with the endpoint 01/04/23 23:33:05.703
    Jan  4 23:33:06.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan  4 23:33:06.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/04/23 23:33:07.221
    STEP: Creating a custom resource that should be denied by the webhook 01/04/23 23:33:07.256
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/04/23 23:33:09.285
    STEP: Updating the custom resource with disallowed data should be denied 01/04/23 23:33:09.291
    STEP: Deleting the custom resource should be denied 01/04/23 23:33:09.299
    STEP: Remove the offending key and value from the custom resource data 01/04/23 23:33:09.305
    STEP: Deleting the updated custom resource should be successful 01/04/23 23:33:09.315
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:33:09.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8826" for this suite. 01/04/23 23:33:09.904
    STEP: Destroying namespace "webhook-8826-markers" for this suite. 01/04/23 23:33:09.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:33:09.952
Jan  4 23:33:09.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename certificates 01/04/23 23:33:09.955
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:33:09.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:33:09.977
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/04/23 23:33:10.43
STEP: getting /apis/certificates.k8s.io 01/04/23 23:33:10.436
STEP: getting /apis/certificates.k8s.io/v1 01/04/23 23:33:10.437
STEP: creating 01/04/23 23:33:10.441
STEP: getting 01/04/23 23:33:10.536
STEP: listing 01/04/23 23:33:10.546
STEP: watching 01/04/23 23:33:10.55
Jan  4 23:33:10.550: INFO: starting watch
STEP: patching 01/04/23 23:33:10.551
STEP: updating 01/04/23 23:33:10.557
Jan  4 23:33:10.562: INFO: waiting for watch events with expected annotations
Jan  4 23:33:10.562: INFO: saw patched and updated annotations
STEP: getting /approval 01/04/23 23:33:10.562
STEP: patching /approval 01/04/23 23:33:10.564
STEP: updating /approval 01/04/23 23:33:10.572
STEP: getting /status 01/04/23 23:33:10.578
STEP: patching /status 01/04/23 23:33:10.581
STEP: updating /status 01/04/23 23:33:10.587
STEP: deleting 01/04/23 23:33:10.594
STEP: deleting a collection 01/04/23 23:33:10.618
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:33:10.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-2440" for this suite. 01/04/23 23:33:10.634
------------------------------
• [0.688 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:33:09.952
    Jan  4 23:33:09.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename certificates 01/04/23 23:33:09.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:33:09.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:33:09.977
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/04/23 23:33:10.43
    STEP: getting /apis/certificates.k8s.io 01/04/23 23:33:10.436
    STEP: getting /apis/certificates.k8s.io/v1 01/04/23 23:33:10.437
    STEP: creating 01/04/23 23:33:10.441
    STEP: getting 01/04/23 23:33:10.536
    STEP: listing 01/04/23 23:33:10.546
    STEP: watching 01/04/23 23:33:10.55
    Jan  4 23:33:10.550: INFO: starting watch
    STEP: patching 01/04/23 23:33:10.551
    STEP: updating 01/04/23 23:33:10.557
    Jan  4 23:33:10.562: INFO: waiting for watch events with expected annotations
    Jan  4 23:33:10.562: INFO: saw patched and updated annotations
    STEP: getting /approval 01/04/23 23:33:10.562
    STEP: patching /approval 01/04/23 23:33:10.564
    STEP: updating /approval 01/04/23 23:33:10.572
    STEP: getting /status 01/04/23 23:33:10.578
    STEP: patching /status 01/04/23 23:33:10.581
    STEP: updating /status 01/04/23 23:33:10.587
    STEP: deleting 01/04/23 23:33:10.594
    STEP: deleting a collection 01/04/23 23:33:10.618
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:33:10.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-2440" for this suite. 01/04/23 23:33:10.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:33:10.641
Jan  4 23:33:10.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 23:33:10.642
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:33:10.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:33:10.658
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 23:33:10.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4881" for this suite. 01/04/23 23:33:10.665
------------------------------
• [0.030 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:33:10.641
    Jan  4 23:33:10.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 23:33:10.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:33:10.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:33:10.658
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:33:10.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4881" for this suite. 01/04/23 23:33:10.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:33:10.672
Jan  4 23:33:10.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename cronjob 01/04/23 23:33:10.674
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:33:10.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:33:10.692
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/04/23 23:33:10.694
STEP: Ensuring more than one job is running at a time 01/04/23 23:33:10.701
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/04/23 23:35:00.706
STEP: Removing cronjob 01/04/23 23:35:00.71
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  4 23:35:00.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6100" for this suite. 01/04/23 23:35:00.739
------------------------------
• [SLOW TEST] [110.081 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:33:10.672
    Jan  4 23:33:10.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename cronjob 01/04/23 23:33:10.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:33:10.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:33:10.692
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/04/23 23:33:10.694
    STEP: Ensuring more than one job is running at a time 01/04/23 23:33:10.701
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/04/23 23:35:00.706
    STEP: Removing cronjob 01/04/23 23:35:00.71
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:35:00.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6100" for this suite. 01/04/23 23:35:00.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:35:00.757
Jan  4 23:35:00.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename endpointslice 01/04/23 23:35:00.758
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:35:00.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:35:00.795
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  4 23:35:02.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2452" for this suite. 01/04/23 23:35:03.017
------------------------------
• [2.288 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:35:00.757
    Jan  4 23:35:00.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename endpointslice 01/04/23 23:35:00.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:35:00.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:35:00.795
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:35:02.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2452" for this suite. 01/04/23 23:35:03.017
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:35:03.06
Jan  4 23:35:03.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:35:03.063
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:35:03.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:35:03.115
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan  4 23:35:03.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/04/23 23:35:06.253
Jan  4 23:35:06.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 create -f -'
Jan  4 23:35:07.019: INFO: stderr: ""
Jan  4 23:35:07.019: INFO: stdout: "e2e-test-crd-publish-openapi-6407-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  4 23:35:07.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 delete e2e-test-crd-publish-openapi-6407-crds test-foo'
Jan  4 23:35:07.131: INFO: stderr: ""
Jan  4 23:35:07.131: INFO: stdout: "e2e-test-crd-publish-openapi-6407-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan  4 23:35:07.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 apply -f -'
Jan  4 23:35:07.859: INFO: stderr: ""
Jan  4 23:35:07.859: INFO: stdout: "e2e-test-crd-publish-openapi-6407-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  4 23:35:07.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 delete e2e-test-crd-publish-openapi-6407-crds test-foo'
Jan  4 23:35:07.946: INFO: stderr: ""
Jan  4 23:35:07.946: INFO: stdout: "e2e-test-crd-publish-openapi-6407-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/04/23 23:35:07.946
Jan  4 23:35:07.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 create -f -'
Jan  4 23:35:08.154: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/04/23 23:35:08.155
Jan  4 23:35:08.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 create -f -'
Jan  4 23:35:08.462: INFO: rc: 1
Jan  4 23:35:08.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 apply -f -'
Jan  4 23:35:09.085: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/04/23 23:35:09.085
Jan  4 23:35:09.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 create -f -'
Jan  4 23:35:09.290: INFO: rc: 1
Jan  4 23:35:09.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 apply -f -'
Jan  4 23:35:09.523: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/04/23 23:35:09.523
Jan  4 23:35:09.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds'
Jan  4 23:35:09.723: INFO: stderr: ""
Jan  4 23:35:09.723: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6407-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/04/23 23:35:09.723
Jan  4 23:35:09.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds.metadata'
Jan  4 23:35:09.978: INFO: stderr: ""
Jan  4 23:35:09.978: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6407-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan  4 23:35:09.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds.spec'
Jan  4 23:35:10.239: INFO: stderr: ""
Jan  4 23:35:10.239: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6407-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan  4 23:35:10.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds.spec.bars'
Jan  4 23:35:10.414: INFO: stderr: ""
Jan  4 23:35:10.414: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6407-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/04/23 23:35:10.414
Jan  4 23:35:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds.spec.bars2'
Jan  4 23:35:10.599: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:35:12.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7447" for this suite. 01/04/23 23:35:12.954
------------------------------
• [SLOW TEST] [9.913 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:35:03.06
    Jan  4 23:35:03.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:35:03.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:35:03.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:35:03.115
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan  4 23:35:03.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/04/23 23:35:06.253
    Jan  4 23:35:06.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 create -f -'
    Jan  4 23:35:07.019: INFO: stderr: ""
    Jan  4 23:35:07.019: INFO: stdout: "e2e-test-crd-publish-openapi-6407-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  4 23:35:07.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 delete e2e-test-crd-publish-openapi-6407-crds test-foo'
    Jan  4 23:35:07.131: INFO: stderr: ""
    Jan  4 23:35:07.131: INFO: stdout: "e2e-test-crd-publish-openapi-6407-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan  4 23:35:07.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 apply -f -'
    Jan  4 23:35:07.859: INFO: stderr: ""
    Jan  4 23:35:07.859: INFO: stdout: "e2e-test-crd-publish-openapi-6407-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  4 23:35:07.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 delete e2e-test-crd-publish-openapi-6407-crds test-foo'
    Jan  4 23:35:07.946: INFO: stderr: ""
    Jan  4 23:35:07.946: INFO: stdout: "e2e-test-crd-publish-openapi-6407-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/04/23 23:35:07.946
    Jan  4 23:35:07.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 create -f -'
    Jan  4 23:35:08.154: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/04/23 23:35:08.155
    Jan  4 23:35:08.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 create -f -'
    Jan  4 23:35:08.462: INFO: rc: 1
    Jan  4 23:35:08.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 apply -f -'
    Jan  4 23:35:09.085: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/04/23 23:35:09.085
    Jan  4 23:35:09.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 create -f -'
    Jan  4 23:35:09.290: INFO: rc: 1
    Jan  4 23:35:09.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 --namespace=crd-publish-openapi-7447 apply -f -'
    Jan  4 23:35:09.523: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/04/23 23:35:09.523
    Jan  4 23:35:09.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds'
    Jan  4 23:35:09.723: INFO: stderr: ""
    Jan  4 23:35:09.723: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6407-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/04/23 23:35:09.723
    Jan  4 23:35:09.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds.metadata'
    Jan  4 23:35:09.978: INFO: stderr: ""
    Jan  4 23:35:09.978: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6407-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan  4 23:35:09.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds.spec'
    Jan  4 23:35:10.239: INFO: stderr: ""
    Jan  4 23:35:10.239: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6407-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan  4 23:35:10.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds.spec.bars'
    Jan  4 23:35:10.414: INFO: stderr: ""
    Jan  4 23:35:10.414: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6407-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/04/23 23:35:10.414
    Jan  4 23:35:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-7447 explain e2e-test-crd-publish-openapi-6407-crds.spec.bars2'
    Jan  4 23:35:10.599: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:35:12.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7447" for this suite. 01/04/23 23:35:12.954
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:35:12.968
Jan  4 23:35:12.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename statefulset 01/04/23 23:35:12.969
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:35:12.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:35:13.005
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7207 01/04/23 23:35:13.013
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/04/23 23:35:13.024
STEP: Creating stateful set ss in namespace statefulset-7207 01/04/23 23:35:13.029
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7207 01/04/23 23:35:13.035
Jan  4 23:35:13.039: INFO: Found 0 stateful pods, waiting for 1
Jan  4 23:35:23.043: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/04/23 23:35:23.043
Jan  4 23:35:23.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 23:35:23.188: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 23:35:23.188: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 23:35:23.188: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 23:35:23.191: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  4 23:35:33.195: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  4 23:35:33.195: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:35:33.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999696s
Jan  4 23:35:34.214: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99716988s
Jan  4 23:35:35.219: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991829552s
Jan  4 23:35:36.223: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987621937s
Jan  4 23:35:37.227: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982902782s
Jan  4 23:35:38.231: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979407157s
Jan  4 23:35:39.235: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975957054s
Jan  4 23:35:40.241: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972046972s
Jan  4 23:35:41.245: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965317954s
Jan  4 23:35:42.249: INFO: Verifying statefulset ss doesn't scale past 1 for another 961.238141ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7207 01/04/23 23:35:43.249
Jan  4 23:35:43.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 23:35:43.412: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  4 23:35:43.412: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 23:35:43.412: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  4 23:35:43.420: INFO: Found 1 stateful pods, waiting for 3
Jan  4 23:35:53.426: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:35:53.426: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  4 23:35:53.426: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/04/23 23:35:53.426
STEP: Scale down will halt with unhealthy stateful pod 01/04/23 23:35:53.426
Jan  4 23:35:53.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 23:35:53.582: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 23:35:53.582: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 23:35:53.582: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 23:35:53.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 23:35:53.765: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 23:35:53.765: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 23:35:53.765: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 23:35:53.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  4 23:35:53.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  4 23:35:53.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  4 23:35:53.910: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  4 23:35:53.910: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:35:53.913: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan  4 23:36:03.920: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  4 23:36:03.920: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  4 23:36:03.920: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  4 23:36:03.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999686s
Jan  4 23:36:04.939: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993210063s
Jan  4 23:36:05.943: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989520519s
Jan  4 23:36:06.947: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985745535s
Jan  4 23:36:07.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982145547s
Jan  4 23:36:08.955: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977622972s
Jan  4 23:36:09.958: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973869271s
Jan  4 23:36:10.962: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969736591s
Jan  4 23:36:11.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966136121s
Jan  4 23:36:12.971: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.574101ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7207 01/04/23 23:36:13.971
Jan  4 23:36:13.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 23:36:14.165: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  4 23:36:14.165: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 23:36:14.165: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  4 23:36:14.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 23:36:14.334: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  4 23:36:14.334: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 23:36:14.334: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  4 23:36:14.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  4 23:36:14.484: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  4 23:36:14.484: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  4 23:36:14.484: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  4 23:36:14.484: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/04/23 23:36:24.501
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  4 23:36:24.501: INFO: Deleting all statefulset in ns statefulset-7207
Jan  4 23:36:24.505: INFO: Scaling statefulset ss to 0
Jan  4 23:36:24.513: INFO: Waiting for statefulset status.replicas updated to 0
Jan  4 23:36:24.515: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:36:24.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7207" for this suite. 01/04/23 23:36:24.528
------------------------------
• [SLOW TEST] [71.573 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:35:12.968
    Jan  4 23:35:12.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename statefulset 01/04/23 23:35:12.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:35:12.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:35:13.005
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7207 01/04/23 23:35:13.013
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/04/23 23:35:13.024
    STEP: Creating stateful set ss in namespace statefulset-7207 01/04/23 23:35:13.029
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7207 01/04/23 23:35:13.035
    Jan  4 23:35:13.039: INFO: Found 0 stateful pods, waiting for 1
    Jan  4 23:35:23.043: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/04/23 23:35:23.043
    Jan  4 23:35:23.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 23:35:23.188: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 23:35:23.188: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 23:35:23.188: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 23:35:23.191: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  4 23:35:33.195: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  4 23:35:33.195: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:35:33.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999696s
    Jan  4 23:35:34.214: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99716988s
    Jan  4 23:35:35.219: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991829552s
    Jan  4 23:35:36.223: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987621937s
    Jan  4 23:35:37.227: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982902782s
    Jan  4 23:35:38.231: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979407157s
    Jan  4 23:35:39.235: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975957054s
    Jan  4 23:35:40.241: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972046972s
    Jan  4 23:35:41.245: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965317954s
    Jan  4 23:35:42.249: INFO: Verifying statefulset ss doesn't scale past 1 for another 961.238141ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7207 01/04/23 23:35:43.249
    Jan  4 23:35:43.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 23:35:43.412: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  4 23:35:43.412: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 23:35:43.412: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  4 23:35:43.420: INFO: Found 1 stateful pods, waiting for 3
    Jan  4 23:35:53.426: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:35:53.426: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  4 23:35:53.426: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/04/23 23:35:53.426
    STEP: Scale down will halt with unhealthy stateful pod 01/04/23 23:35:53.426
    Jan  4 23:35:53.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 23:35:53.582: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 23:35:53.582: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 23:35:53.582: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 23:35:53.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 23:35:53.765: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 23:35:53.765: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 23:35:53.765: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 23:35:53.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  4 23:35:53.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  4 23:35:53.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  4 23:35:53.910: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  4 23:35:53.910: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:35:53.913: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan  4 23:36:03.920: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  4 23:36:03.920: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  4 23:36:03.920: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  4 23:36:03.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999686s
    Jan  4 23:36:04.939: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993210063s
    Jan  4 23:36:05.943: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989520519s
    Jan  4 23:36:06.947: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985745535s
    Jan  4 23:36:07.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982145547s
    Jan  4 23:36:08.955: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977622972s
    Jan  4 23:36:09.958: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973869271s
    Jan  4 23:36:10.962: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969736591s
    Jan  4 23:36:11.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966136121s
    Jan  4 23:36:12.971: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.574101ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7207 01/04/23 23:36:13.971
    Jan  4 23:36:13.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 23:36:14.165: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  4 23:36:14.165: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 23:36:14.165: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  4 23:36:14.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 23:36:14.334: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  4 23:36:14.334: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 23:36:14.334: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  4 23:36:14.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=statefulset-7207 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  4 23:36:14.484: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  4 23:36:14.484: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  4 23:36:14.484: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  4 23:36:14.484: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/04/23 23:36:24.501
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  4 23:36:24.501: INFO: Deleting all statefulset in ns statefulset-7207
    Jan  4 23:36:24.505: INFO: Scaling statefulset ss to 0
    Jan  4 23:36:24.513: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  4 23:36:24.515: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:36:24.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7207" for this suite. 01/04/23 23:36:24.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:36:24.543
Jan  4 23:36:24.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:36:24.544
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:24.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:24.565
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/04/23 23:36:24.567
Jan  4 23:36:24.575: INFO: Waiting up to 5m0s for pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902" in namespace "downward-api-7637" to be "Succeeded or Failed"
Jan  4 23:36:24.579: INFO: Pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61645ms
Jan  4 23:36:26.584: INFO: Pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008919912s
Jan  4 23:36:28.584: INFO: Pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008812671s
STEP: Saw pod success 01/04/23 23:36:28.584
Jan  4 23:36:28.584: INFO: Pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902" satisfied condition "Succeeded or Failed"
Jan  4 23:36:28.587: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902 container dapi-container: <nil>
STEP: delete the pod 01/04/23 23:36:28.601
Jan  4 23:36:28.611: INFO: Waiting for pod downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902 to disappear
Jan  4 23:36:28.614: INFO: Pod downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  4 23:36:28.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7637" for this suite. 01/04/23 23:36:28.618
------------------------------
• [4.082 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:36:24.543
    Jan  4 23:36:24.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:36:24.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:24.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:24.565
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/04/23 23:36:24.567
    Jan  4 23:36:24.575: INFO: Waiting up to 5m0s for pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902" in namespace "downward-api-7637" to be "Succeeded or Failed"
    Jan  4 23:36:24.579: INFO: Pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61645ms
    Jan  4 23:36:26.584: INFO: Pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008919912s
    Jan  4 23:36:28.584: INFO: Pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008812671s
    STEP: Saw pod success 01/04/23 23:36:28.584
    Jan  4 23:36:28.584: INFO: Pod "downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902" satisfied condition "Succeeded or Failed"
    Jan  4 23:36:28.587: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902 container dapi-container: <nil>
    STEP: delete the pod 01/04/23 23:36:28.601
    Jan  4 23:36:28.611: INFO: Waiting for pod downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902 to disappear
    Jan  4 23:36:28.614: INFO: Pod downward-api-b4829fa2-efcb-48ed-9283-c610c1f9b902 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:36:28.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7637" for this suite. 01/04/23 23:36:28.618
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:36:28.626
Jan  4 23:36:28.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:36:28.627
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:28.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:28.646
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/04/23 23:36:28.649
Jan  4 23:36:28.656: INFO: Waiting up to 5m0s for pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb" in namespace "downward-api-5724" to be "Succeeded or Failed"
Jan  4 23:36:28.658: INFO: Pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414684ms
Jan  4 23:36:30.661: INFO: Pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005638857s
Jan  4 23:36:32.666: INFO: Pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009770901s
STEP: Saw pod success 01/04/23 23:36:32.666
Jan  4 23:36:32.666: INFO: Pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb" satisfied condition "Succeeded or Failed"
Jan  4 23:36:32.668: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb container client-container: <nil>
STEP: delete the pod 01/04/23 23:36:32.681
Jan  4 23:36:32.698: INFO: Waiting for pod downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb to disappear
Jan  4 23:36:32.701: INFO: Pod downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 23:36:32.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5724" for this suite. 01/04/23 23:36:32.705
------------------------------
• [4.085 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:36:28.626
    Jan  4 23:36:28.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:36:28.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:28.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:28.646
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/04/23 23:36:28.649
    Jan  4 23:36:28.656: INFO: Waiting up to 5m0s for pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb" in namespace "downward-api-5724" to be "Succeeded or Failed"
    Jan  4 23:36:28.658: INFO: Pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414684ms
    Jan  4 23:36:30.661: INFO: Pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005638857s
    Jan  4 23:36:32.666: INFO: Pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009770901s
    STEP: Saw pod success 01/04/23 23:36:32.666
    Jan  4 23:36:32.666: INFO: Pod "downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb" satisfied condition "Succeeded or Failed"
    Jan  4 23:36:32.668: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb container client-container: <nil>
    STEP: delete the pod 01/04/23 23:36:32.681
    Jan  4 23:36:32.698: INFO: Waiting for pod downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb to disappear
    Jan  4 23:36:32.701: INFO: Pod downwardapi-volume-250ede21-c1d0-41de-ae15-aae29bae22cb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:36:32.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5724" for this suite. 01/04/23 23:36:32.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:36:32.713
Jan  4 23:36:32.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename namespaces 01/04/23 23:36:32.714
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:32.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:32.732
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/04/23 23:36:32.735
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:32.751
STEP: Creating a service in the namespace 01/04/23 23:36:32.755
STEP: Deleting the namespace 01/04/23 23:36:32.766
STEP: Waiting for the namespace to be removed. 01/04/23 23:36:32.787
STEP: Recreating the namespace 01/04/23 23:36:38.791
STEP: Verifying there is no service in the namespace 01/04/23 23:36:38.806
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:36:38.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1330" for this suite. 01/04/23 23:36:38.819
STEP: Destroying namespace "nsdeletetest-2879" for this suite. 01/04/23 23:36:38.824
Jan  4 23:36:38.828: INFO: Namespace nsdeletetest-2879 was already deleted
STEP: Destroying namespace "nsdeletetest-8397" for this suite. 01/04/23 23:36:38.828
------------------------------
• [SLOW TEST] [6.121 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:36:32.713
    Jan  4 23:36:32.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename namespaces 01/04/23 23:36:32.714
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:32.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:32.732
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/04/23 23:36:32.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:32.751
    STEP: Creating a service in the namespace 01/04/23 23:36:32.755
    STEP: Deleting the namespace 01/04/23 23:36:32.766
    STEP: Waiting for the namespace to be removed. 01/04/23 23:36:32.787
    STEP: Recreating the namespace 01/04/23 23:36:38.791
    STEP: Verifying there is no service in the namespace 01/04/23 23:36:38.806
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:36:38.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1330" for this suite. 01/04/23 23:36:38.819
    STEP: Destroying namespace "nsdeletetest-2879" for this suite. 01/04/23 23:36:38.824
    Jan  4 23:36:38.828: INFO: Namespace nsdeletetest-2879 was already deleted
    STEP: Destroying namespace "nsdeletetest-8397" for this suite. 01/04/23 23:36:38.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:36:38.836
Jan  4 23:36:38.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename var-expansion 01/04/23 23:36:38.837
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:38.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:38.866
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/04/23 23:36:38.869
Jan  4 23:36:38.876: INFO: Waiting up to 5m0s for pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6" in namespace "var-expansion-7013" to be "Succeeded or Failed"
Jan  4 23:36:38.884: INFO: Pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.608441ms
Jan  4 23:36:40.888: INFO: Pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011866946s
Jan  4 23:36:42.888: INFO: Pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011840395s
STEP: Saw pod success 01/04/23 23:36:42.888
Jan  4 23:36:42.888: INFO: Pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6" satisfied condition "Succeeded or Failed"
Jan  4 23:36:42.891: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6 container dapi-container: <nil>
STEP: delete the pod 01/04/23 23:36:42.898
Jan  4 23:36:42.909: INFO: Waiting for pod var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6 to disappear
Jan  4 23:36:42.912: INFO: Pod var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  4 23:36:42.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7013" for this suite. 01/04/23 23:36:42.915
------------------------------
• [4.087 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:36:38.836
    Jan  4 23:36:38.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename var-expansion 01/04/23 23:36:38.837
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:38.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:38.866
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/04/23 23:36:38.869
    Jan  4 23:36:38.876: INFO: Waiting up to 5m0s for pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6" in namespace "var-expansion-7013" to be "Succeeded or Failed"
    Jan  4 23:36:38.884: INFO: Pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.608441ms
    Jan  4 23:36:40.888: INFO: Pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011866946s
    Jan  4 23:36:42.888: INFO: Pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011840395s
    STEP: Saw pod success 01/04/23 23:36:42.888
    Jan  4 23:36:42.888: INFO: Pod "var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6" satisfied condition "Succeeded or Failed"
    Jan  4 23:36:42.891: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6 container dapi-container: <nil>
    STEP: delete the pod 01/04/23 23:36:42.898
    Jan  4 23:36:42.909: INFO: Waiting for pod var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6 to disappear
    Jan  4 23:36:42.912: INFO: Pod var-expansion-cc6ba68c-fd1c-463a-957e-175eec6f1cb6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:36:42.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7013" for this suite. 01/04/23 23:36:42.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:36:42.924
Jan  4 23:36:42.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 23:36:42.924
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:42.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:42.957
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan  4 23:36:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:36:44.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5136" for this suite. 01/04/23 23:36:44.011
------------------------------
• [1.115 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:36:42.924
    Jan  4 23:36:42.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename custom-resource-definition 01/04/23 23:36:42.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:42.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:42.957
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan  4 23:36:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:36:44.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5136" for this suite. 01/04/23 23:36:44.011
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:36:44.039
Jan  4 23:36:44.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename dns 01/04/23 23:36:44.04
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:44.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:44.087
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-430.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-430.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/04/23 23:36:44.089
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-430.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-430.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/04/23 23:36:44.089
STEP: creating a pod to probe /etc/hosts 01/04/23 23:36:44.089
STEP: submitting the pod to kubernetes 01/04/23 23:36:44.089
Jan  4 23:36:44.114: INFO: Waiting up to 15m0s for pod "dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32" in namespace "dns-430" to be "running"
Jan  4 23:36:44.124: INFO: Pod "dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32": Phase="Pending", Reason="", readiness=false. Elapsed: 9.809443ms
Jan  4 23:36:46.129: INFO: Pod "dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32": Phase="Running", Reason="", readiness=true. Elapsed: 2.015084634s
Jan  4 23:36:46.129: INFO: Pod "dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32" satisfied condition "running"
STEP: retrieving the pod 01/04/23 23:36:46.129
STEP: looking for the results for each expected name from probers 01/04/23 23:36:46.133
Jan  4 23:36:46.148: INFO: DNS probes using dns-430/dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32 succeeded

STEP: deleting the pod 01/04/23 23:36:46.148
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  4 23:36:46.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-430" for this suite. 01/04/23 23:36:46.174
------------------------------
• [2.142 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:36:44.039
    Jan  4 23:36:44.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename dns 01/04/23 23:36:44.04
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:44.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:44.087
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-430.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-430.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/04/23 23:36:44.089
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-430.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-430.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/04/23 23:36:44.089
    STEP: creating a pod to probe /etc/hosts 01/04/23 23:36:44.089
    STEP: submitting the pod to kubernetes 01/04/23 23:36:44.089
    Jan  4 23:36:44.114: INFO: Waiting up to 15m0s for pod "dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32" in namespace "dns-430" to be "running"
    Jan  4 23:36:44.124: INFO: Pod "dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32": Phase="Pending", Reason="", readiness=false. Elapsed: 9.809443ms
    Jan  4 23:36:46.129: INFO: Pod "dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32": Phase="Running", Reason="", readiness=true. Elapsed: 2.015084634s
    Jan  4 23:36:46.129: INFO: Pod "dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 23:36:46.129
    STEP: looking for the results for each expected name from probers 01/04/23 23:36:46.133
    Jan  4 23:36:46.148: INFO: DNS probes using dns-430/dns-test-8b47d36d-88c8-46b8-aba7-afa231c63a32 succeeded

    STEP: deleting the pod 01/04/23 23:36:46.148
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:36:46.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-430" for this suite. 01/04/23 23:36:46.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:36:46.184
Jan  4 23:36:46.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename pods 01/04/23 23:36:46.188
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:46.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:46.258
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/04/23 23:36:46.261
Jan  4 23:36:46.269: INFO: created test-pod-1
Jan  4 23:36:46.281: INFO: created test-pod-2
Jan  4 23:36:46.291: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/04/23 23:36:46.291
Jan  4 23:36:46.292: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2165' to be running and ready
Jan  4 23:36:46.312: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  4 23:36:46.312: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  4 23:36:46.312: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  4 23:36:46.312: INFO: 0 / 3 pods in namespace 'pods-2165' are running and ready (0 seconds elapsed)
Jan  4 23:36:46.312: INFO: expected 0 pod replicas in namespace 'pods-2165', 0 are Running and Ready.
Jan  4 23:36:46.312: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
Jan  4 23:36:46.312: INFO: test-pod-1  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
Jan  4 23:36:46.312: INFO: test-pod-2  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
Jan  4 23:36:46.312: INFO: test-pod-3  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
Jan  4 23:36:46.312: INFO: 
Jan  4 23:36:48.328: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  4 23:36:48.328: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  4 23:36:48.328: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  4 23:36:48.328: INFO: 0 / 3 pods in namespace 'pods-2165' are running and ready (2 seconds elapsed)
Jan  4 23:36:48.328: INFO: expected 0 pod replicas in namespace 'pods-2165', 0 are Running and Ready.
Jan  4 23:36:48.328: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
Jan  4 23:36:48.328: INFO: test-pod-1  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
Jan  4 23:36:48.328: INFO: test-pod-2  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
Jan  4 23:36:48.328: INFO: test-pod-3  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
Jan  4 23:36:48.328: INFO: 
Jan  4 23:36:50.321: INFO: 3 / 3 pods in namespace 'pods-2165' are running and ready (4 seconds elapsed)
Jan  4 23:36:50.321: INFO: expected 0 pod replicas in namespace 'pods-2165', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/04/23 23:36:50.342
Jan  4 23:36:50.346: INFO: Pod quantity 3 is different from expected quantity 0
Jan  4 23:36:51.352: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  4 23:36:52.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2165" for this suite. 01/04/23 23:36:52.354
------------------------------
• [SLOW TEST] [6.176 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:36:46.184
    Jan  4 23:36:46.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename pods 01/04/23 23:36:46.188
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:46.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:46.258
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/04/23 23:36:46.261
    Jan  4 23:36:46.269: INFO: created test-pod-1
    Jan  4 23:36:46.281: INFO: created test-pod-2
    Jan  4 23:36:46.291: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/04/23 23:36:46.291
    Jan  4 23:36:46.292: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2165' to be running and ready
    Jan  4 23:36:46.312: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  4 23:36:46.312: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  4 23:36:46.312: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  4 23:36:46.312: INFO: 0 / 3 pods in namespace 'pods-2165' are running and ready (0 seconds elapsed)
    Jan  4 23:36:46.312: INFO: expected 0 pod replicas in namespace 'pods-2165', 0 are Running and Ready.
    Jan  4 23:36:46.312: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
    Jan  4 23:36:46.312: INFO: test-pod-1  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
    Jan  4 23:36:46.312: INFO: test-pod-2  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
    Jan  4 23:36:46.312: INFO: test-pod-3  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
    Jan  4 23:36:46.312: INFO: 
    Jan  4 23:36:48.328: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  4 23:36:48.328: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  4 23:36:48.328: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  4 23:36:48.328: INFO: 0 / 3 pods in namespace 'pods-2165' are running and ready (2 seconds elapsed)
    Jan  4 23:36:48.328: INFO: expected 0 pod replicas in namespace 'pods-2165', 0 are Running and Ready.
    Jan  4 23:36:48.328: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
    Jan  4 23:36:48.328: INFO: test-pod-1  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
    Jan  4 23:36:48.328: INFO: test-pod-2  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
    Jan  4 23:36:48.328: INFO: test-pod-3  ip-172-31-13-117.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-04 23:36:46 +0000 UTC  }]
    Jan  4 23:36:48.328: INFO: 
    Jan  4 23:36:50.321: INFO: 3 / 3 pods in namespace 'pods-2165' are running and ready (4 seconds elapsed)
    Jan  4 23:36:50.321: INFO: expected 0 pod replicas in namespace 'pods-2165', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/04/23 23:36:50.342
    Jan  4 23:36:50.346: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  4 23:36:51.352: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:36:52.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2165" for this suite. 01/04/23 23:36:52.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:36:52.361
Jan  4 23:36:52.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 23:36:52.362
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:52.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:52.379
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/04/23 23:36:52.381
STEP: Creating a ResourceQuota 01/04/23 23:36:57.386
STEP: Ensuring resource quota status is calculated 01/04/23 23:36:57.394
STEP: Creating a ReplicationController 01/04/23 23:36:59.399
STEP: Ensuring resource quota status captures replication controller creation 01/04/23 23:36:59.41
STEP: Deleting a ReplicationController 01/04/23 23:37:01.416
STEP: Ensuring resource quota status released usage 01/04/23 23:37:01.427
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:03.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2837" for this suite. 01/04/23 23:37:03.443
------------------------------
• [SLOW TEST] [11.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:36:52.361
    Jan  4 23:36:52.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 23:36:52.362
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:36:52.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:36:52.379
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/04/23 23:36:52.381
    STEP: Creating a ResourceQuota 01/04/23 23:36:57.386
    STEP: Ensuring resource quota status is calculated 01/04/23 23:36:57.394
    STEP: Creating a ReplicationController 01/04/23 23:36:59.399
    STEP: Ensuring resource quota status captures replication controller creation 01/04/23 23:36:59.41
    STEP: Deleting a ReplicationController 01/04/23 23:37:01.416
    STEP: Ensuring resource quota status released usage 01/04/23 23:37:01.427
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:03.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2837" for this suite. 01/04/23 23:37:03.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:03.457
Jan  4 23:37:03.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-lifecycle-hook 01/04/23 23:37:03.458
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:03.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:03.492
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/04/23 23:37:03.499
Jan  4 23:37:03.514: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1813" to be "running and ready"
Jan  4 23:37:03.525: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.67501ms
Jan  4 23:37:03.525: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:37:05.529: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014360841s
Jan  4 23:37:05.529: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  4 23:37:05.529: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/04/23 23:37:05.531
Jan  4 23:37:05.536: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1813" to be "running and ready"
Jan  4 23:37:05.539: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055482ms
Jan  4 23:37:05.539: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:37:07.543: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007171962s
Jan  4 23:37:07.543: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan  4 23:37:07.543: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/04/23 23:37:07.546
Jan  4 23:37:07.554: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  4 23:37:07.558: INFO: Pod pod-with-prestop-http-hook still exists
Jan  4 23:37:09.559: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  4 23:37:09.564: INFO: Pod pod-with-prestop-http-hook still exists
Jan  4 23:37:11.559: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  4 23:37:11.563: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/04/23 23:37:11.563
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:11.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1813" for this suite. 01/04/23 23:37:11.583
------------------------------
• [SLOW TEST] [8.134 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:03.457
    Jan  4 23:37:03.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/04/23 23:37:03.458
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:03.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:03.492
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/04/23 23:37:03.499
    Jan  4 23:37:03.514: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1813" to be "running and ready"
    Jan  4 23:37:03.525: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.67501ms
    Jan  4 23:37:03.525: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:37:05.529: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014360841s
    Jan  4 23:37:05.529: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  4 23:37:05.529: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/04/23 23:37:05.531
    Jan  4 23:37:05.536: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1813" to be "running and ready"
    Jan  4 23:37:05.539: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055482ms
    Jan  4 23:37:05.539: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:37:07.543: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007171962s
    Jan  4 23:37:07.543: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan  4 23:37:07.543: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/04/23 23:37:07.546
    Jan  4 23:37:07.554: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  4 23:37:07.558: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  4 23:37:09.559: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  4 23:37:09.564: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  4 23:37:11.559: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  4 23:37:11.563: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/04/23 23:37:11.563
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:11.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1813" for this suite. 01/04/23 23:37:11.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:11.592
Jan  4 23:37:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename proxy 01/04/23 23:37:11.594
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:11.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:11.61
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan  4 23:37:11.613: INFO: Creating pod...
Jan  4 23:37:11.623: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8403" to be "running"
Jan  4 23:37:11.629: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.831787ms
Jan  4 23:37:13.633: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009500644s
Jan  4 23:37:13.633: INFO: Pod "agnhost" satisfied condition "running"
Jan  4 23:37:13.633: INFO: Creating service...
Jan  4 23:37:13.649: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/DELETE
Jan  4 23:37:13.664: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  4 23:37:13.664: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/GET
Jan  4 23:37:13.678: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  4 23:37:13.679: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/HEAD
Jan  4 23:37:13.684: INFO: http.Client request:HEAD | StatusCode:200
Jan  4 23:37:13.684: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/OPTIONS
Jan  4 23:37:13.693: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  4 23:37:13.693: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/PATCH
Jan  4 23:37:13.713: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  4 23:37:13.713: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/POST
Jan  4 23:37:13.721: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  4 23:37:13.721: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/PUT
Jan  4 23:37:13.725: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  4 23:37:13.726: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/DELETE
Jan  4 23:37:13.734: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  4 23:37:13.734: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/GET
Jan  4 23:37:13.753: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  4 23:37:13.753: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/HEAD
Jan  4 23:37:13.758: INFO: http.Client request:HEAD | StatusCode:200
Jan  4 23:37:13.758: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/OPTIONS
Jan  4 23:37:13.763: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  4 23:37:13.763: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/PATCH
Jan  4 23:37:13.766: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  4 23:37:13.766: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/POST
Jan  4 23:37:13.771: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  4 23:37:13.771: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/PUT
Jan  4 23:37:13.774: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:13.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8403" for this suite. 01/04/23 23:37:13.783
------------------------------
• [2.207 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:11.592
    Jan  4 23:37:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename proxy 01/04/23 23:37:11.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:11.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:11.61
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan  4 23:37:11.613: INFO: Creating pod...
    Jan  4 23:37:11.623: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8403" to be "running"
    Jan  4 23:37:11.629: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.831787ms
    Jan  4 23:37:13.633: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009500644s
    Jan  4 23:37:13.633: INFO: Pod "agnhost" satisfied condition "running"
    Jan  4 23:37:13.633: INFO: Creating service...
    Jan  4 23:37:13.649: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/DELETE
    Jan  4 23:37:13.664: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  4 23:37:13.664: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/GET
    Jan  4 23:37:13.678: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  4 23:37:13.679: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/HEAD
    Jan  4 23:37:13.684: INFO: http.Client request:HEAD | StatusCode:200
    Jan  4 23:37:13.684: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan  4 23:37:13.693: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  4 23:37:13.693: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/PATCH
    Jan  4 23:37:13.713: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  4 23:37:13.713: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/POST
    Jan  4 23:37:13.721: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  4 23:37:13.721: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/pods/agnhost/proxy/some/path/with/PUT
    Jan  4 23:37:13.725: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  4 23:37:13.726: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/DELETE
    Jan  4 23:37:13.734: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  4 23:37:13.734: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/GET
    Jan  4 23:37:13.753: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  4 23:37:13.753: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/HEAD
    Jan  4 23:37:13.758: INFO: http.Client request:HEAD | StatusCode:200
    Jan  4 23:37:13.758: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/OPTIONS
    Jan  4 23:37:13.763: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  4 23:37:13.763: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/PATCH
    Jan  4 23:37:13.766: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  4 23:37:13.766: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/POST
    Jan  4 23:37:13.771: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  4 23:37:13.771: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-8403/services/test-service/proxy/some/path/with/PUT
    Jan  4 23:37:13.774: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:13.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8403" for this suite. 01/04/23 23:37:13.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:13.801
Jan  4 23:37:13.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:37:13.802
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:13.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:13.828
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-bef1f643-8a40-4018-a62c-5125e14e9dd5 01/04/23 23:37:13.83
STEP: Creating a pod to test consume configMaps 01/04/23 23:37:13.835
Jan  4 23:37:13.841: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab" in namespace "projected-2992" to be "Succeeded or Failed"
Jan  4 23:37:13.857: INFO: Pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab": Phase="Pending", Reason="", readiness=false. Elapsed: 15.772191ms
Jan  4 23:37:15.860: INFO: Pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019267808s
Jan  4 23:37:17.861: INFO: Pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019708801s
STEP: Saw pod success 01/04/23 23:37:17.861
Jan  4 23:37:17.861: INFO: Pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab" satisfied condition "Succeeded or Failed"
Jan  4 23:37:17.870: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab container agnhost-container: <nil>
STEP: delete the pod 01/04/23 23:37:17.876
Jan  4 23:37:17.886: INFO: Waiting for pod pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab to disappear
Jan  4 23:37:17.888: INFO: Pod pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:17.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2992" for this suite. 01/04/23 23:37:17.893
------------------------------
• [4.098 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:13.801
    Jan  4 23:37:13.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:37:13.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:13.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:13.828
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-bef1f643-8a40-4018-a62c-5125e14e9dd5 01/04/23 23:37:13.83
    STEP: Creating a pod to test consume configMaps 01/04/23 23:37:13.835
    Jan  4 23:37:13.841: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab" in namespace "projected-2992" to be "Succeeded or Failed"
    Jan  4 23:37:13.857: INFO: Pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab": Phase="Pending", Reason="", readiness=false. Elapsed: 15.772191ms
    Jan  4 23:37:15.860: INFO: Pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019267808s
    Jan  4 23:37:17.861: INFO: Pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019708801s
    STEP: Saw pod success 01/04/23 23:37:17.861
    Jan  4 23:37:17.861: INFO: Pod "pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab" satisfied condition "Succeeded or Failed"
    Jan  4 23:37:17.870: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 23:37:17.876
    Jan  4 23:37:17.886: INFO: Waiting for pod pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab to disappear
    Jan  4 23:37:17.888: INFO: Pod pod-projected-configmaps-26e8777f-72a5-4c14-b106-db166fca4dab no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:17.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2992" for this suite. 01/04/23 23:37:17.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:17.9
Jan  4 23:37:17.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubelet-test 01/04/23 23:37:17.902
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:17.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:17.917
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/04/23 23:37:17.926
Jan  4 23:37:17.926: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b" in namespace "kubelet-test-7716" to be "completed"
Jan  4 23:37:17.929: INFO: Pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20656ms
Jan  4 23:37:19.934: INFO: Pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007664569s
Jan  4 23:37:21.933: INFO: Pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007524236s
Jan  4 23:37:21.934: INFO: Pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:21.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7716" for this suite. 01/04/23 23:37:21.944
------------------------------
• [4.049 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:17.9
    Jan  4 23:37:17.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubelet-test 01/04/23 23:37:17.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:17.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:17.917
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/04/23 23:37:17.926
    Jan  4 23:37:17.926: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b" in namespace "kubelet-test-7716" to be "completed"
    Jan  4 23:37:17.929: INFO: Pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20656ms
    Jan  4 23:37:19.934: INFO: Pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007664569s
    Jan  4 23:37:21.933: INFO: Pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007524236s
    Jan  4 23:37:21.934: INFO: Pod "agnhost-host-aliases2e6f7549-940d-40f9-b2c0-2efc4fc17a7b" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:21.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7716" for this suite. 01/04/23 23:37:21.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:21.95
Jan  4 23:37:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-runtime 01/04/23 23:37:21.952
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:21.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:21.971
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/04/23 23:37:21.974
STEP: wait for the container to reach Succeeded 01/04/23 23:37:21.98
STEP: get the container status 01/04/23 23:37:26.01
STEP: the container should be terminated 01/04/23 23:37:26.016
STEP: the termination message should be set 01/04/23 23:37:26.016
Jan  4 23:37:26.017: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/04/23 23:37:26.017
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:26.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6289" for this suite. 01/04/23 23:37:26.043
------------------------------
• [4.100 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:21.95
    Jan  4 23:37:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-runtime 01/04/23 23:37:21.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:21.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:21.971
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/04/23 23:37:21.974
    STEP: wait for the container to reach Succeeded 01/04/23 23:37:21.98
    STEP: get the container status 01/04/23 23:37:26.01
    STEP: the container should be terminated 01/04/23 23:37:26.016
    STEP: the termination message should be set 01/04/23 23:37:26.016
    Jan  4 23:37:26.017: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/04/23 23:37:26.017
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:26.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6289" for this suite. 01/04/23 23:37:26.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:26.052
Jan  4 23:37:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:37:26.054
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:26.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:26.081
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/04/23 23:37:26.086
Jan  4 23:37:26.099: INFO: Waiting up to 5m0s for pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd" in namespace "emptydir-5245" to be "Succeeded or Failed"
Jan  4 23:37:26.105: INFO: Pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942798ms
Jan  4 23:37:28.109: INFO: Pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008943631s
Jan  4 23:37:30.109: INFO: Pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009722448s
STEP: Saw pod success 01/04/23 23:37:30.109
Jan  4 23:37:30.110: INFO: Pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd" satisfied condition "Succeeded or Failed"
Jan  4 23:37:30.114: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-f4fb3a65-562a-4123-b505-ca2668c521fd container test-container: <nil>
STEP: delete the pod 01/04/23 23:37:30.123
Jan  4 23:37:30.137: INFO: Waiting for pod pod-f4fb3a65-562a-4123-b505-ca2668c521fd to disappear
Jan  4 23:37:30.140: INFO: Pod pod-f4fb3a65-562a-4123-b505-ca2668c521fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:30.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5245" for this suite. 01/04/23 23:37:30.149
------------------------------
• [4.104 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:26.052
    Jan  4 23:37:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:37:26.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:26.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:26.081
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/04/23 23:37:26.086
    Jan  4 23:37:26.099: INFO: Waiting up to 5m0s for pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd" in namespace "emptydir-5245" to be "Succeeded or Failed"
    Jan  4 23:37:26.105: INFO: Pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942798ms
    Jan  4 23:37:28.109: INFO: Pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008943631s
    Jan  4 23:37:30.109: INFO: Pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009722448s
    STEP: Saw pod success 01/04/23 23:37:30.109
    Jan  4 23:37:30.110: INFO: Pod "pod-f4fb3a65-562a-4123-b505-ca2668c521fd" satisfied condition "Succeeded or Failed"
    Jan  4 23:37:30.114: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-f4fb3a65-562a-4123-b505-ca2668c521fd container test-container: <nil>
    STEP: delete the pod 01/04/23 23:37:30.123
    Jan  4 23:37:30.137: INFO: Waiting for pod pod-f4fb3a65-562a-4123-b505-ca2668c521fd to disappear
    Jan  4 23:37:30.140: INFO: Pod pod-f4fb3a65-562a-4123-b505-ca2668c521fd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:30.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5245" for this suite. 01/04/23 23:37:30.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:30.157
Jan  4 23:37:30.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 23:37:30.158
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:30.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:30.185
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/04/23 23:37:30.188
STEP: Getting a ResourceQuota 01/04/23 23:37:30.196
STEP: Updating a ResourceQuota 01/04/23 23:37:30.202
STEP: Verifying a ResourceQuota was modified 01/04/23 23:37:30.208
STEP: Deleting a ResourceQuota 01/04/23 23:37:30.215
STEP: Verifying the deleted ResourceQuota 01/04/23 23:37:30.225
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:30.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-727" for this suite. 01/04/23 23:37:30.236
------------------------------
• [0.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:30.157
    Jan  4 23:37:30.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 23:37:30.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:30.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:30.185
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/04/23 23:37:30.188
    STEP: Getting a ResourceQuota 01/04/23 23:37:30.196
    STEP: Updating a ResourceQuota 01/04/23 23:37:30.202
    STEP: Verifying a ResourceQuota was modified 01/04/23 23:37:30.208
    STEP: Deleting a ResourceQuota 01/04/23 23:37:30.215
    STEP: Verifying the deleted ResourceQuota 01/04/23 23:37:30.225
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:30.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-727" for this suite. 01/04/23 23:37:30.236
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:30.243
Jan  4 23:37:30.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:37:30.244
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:30.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:30.271
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/04/23 23:37:30.274
Jan  4 23:37:30.287: INFO: Waiting up to 5m0s for pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0" in namespace "downward-api-2349" to be "Succeeded or Failed"
Jan  4 23:37:30.294: INFO: Pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.747464ms
Jan  4 23:37:32.298: INFO: Pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010737293s
Jan  4 23:37:34.306: INFO: Pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018718187s
STEP: Saw pod success 01/04/23 23:37:34.306
Jan  4 23:37:34.306: INFO: Pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0" satisfied condition "Succeeded or Failed"
Jan  4 23:37:34.310: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0 container dapi-container: <nil>
STEP: delete the pod 01/04/23 23:37:34.327
Jan  4 23:37:34.342: INFO: Waiting for pod downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0 to disappear
Jan  4 23:37:34.345: INFO: Pod downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  4 23:37:34.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2349" for this suite. 01/04/23 23:37:34.365
------------------------------
• [4.128 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:30.243
    Jan  4 23:37:30.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:37:30.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:30.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:30.271
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/04/23 23:37:30.274
    Jan  4 23:37:30.287: INFO: Waiting up to 5m0s for pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0" in namespace "downward-api-2349" to be "Succeeded or Failed"
    Jan  4 23:37:30.294: INFO: Pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.747464ms
    Jan  4 23:37:32.298: INFO: Pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010737293s
    Jan  4 23:37:34.306: INFO: Pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018718187s
    STEP: Saw pod success 01/04/23 23:37:34.306
    Jan  4 23:37:34.306: INFO: Pod "downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0" satisfied condition "Succeeded or Failed"
    Jan  4 23:37:34.310: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0 container dapi-container: <nil>
    STEP: delete the pod 01/04/23 23:37:34.327
    Jan  4 23:37:34.342: INFO: Waiting for pod downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0 to disappear
    Jan  4 23:37:34.345: INFO: Pod downward-api-4cc6e579-a5a5-4d92-8ee0-7381122987d0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:37:34.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2349" for this suite. 01/04/23 23:37:34.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:37:34.372
Jan  4 23:37:34.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename container-probe 01/04/23 23:37:34.373
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:34.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:34.393
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc in namespace container-probe-1922 01/04/23 23:37:34.396
Jan  4 23:37:34.403: INFO: Waiting up to 5m0s for pod "test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc" in namespace "container-probe-1922" to be "not pending"
Jan  4 23:37:34.409: INFO: Pod "test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.149935ms
Jan  4 23:37:36.413: INFO: Pod "test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009868201s
Jan  4 23:37:36.413: INFO: Pod "test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc" satisfied condition "not pending"
Jan  4 23:37:36.413: INFO: Started pod test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc in namespace container-probe-1922
STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 23:37:36.413
Jan  4 23:37:36.416: INFO: Initial restart count of pod test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc is 0
STEP: deleting the pod 01/04/23 23:41:38.329
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  4 23:41:38.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1922" for this suite. 01/04/23 23:41:38.365
------------------------------
• [SLOW TEST] [244.005 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:37:34.372
    Jan  4 23:37:34.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename container-probe 01/04/23 23:37:34.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:37:34.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:37:34.393
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc in namespace container-probe-1922 01/04/23 23:37:34.396
    Jan  4 23:37:34.403: INFO: Waiting up to 5m0s for pod "test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc" in namespace "container-probe-1922" to be "not pending"
    Jan  4 23:37:34.409: INFO: Pod "test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.149935ms
    Jan  4 23:37:36.413: INFO: Pod "test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009868201s
    Jan  4 23:37:36.413: INFO: Pod "test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc" satisfied condition "not pending"
    Jan  4 23:37:36.413: INFO: Started pod test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc in namespace container-probe-1922
    STEP: checking the pod's current state and verifying that restartCount is present 01/04/23 23:37:36.413
    Jan  4 23:37:36.416: INFO: Initial restart count of pod test-webserver-42c0d7a3-1abd-4d78-a4ef-71eca46f54bc is 0
    STEP: deleting the pod 01/04/23 23:41:38.329
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:41:38.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1922" for this suite. 01/04/23 23:41:38.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:41:38.38
Jan  4 23:41:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename replicaset 01/04/23 23:41:38.382
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:38.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:38.415
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan  4 23:41:38.430: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  4 23:41:43.438: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/04/23 23:41:43.438
STEP: Scaling up "test-rs" replicaset  01/04/23 23:41:43.438
Jan  4 23:41:43.456: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/04/23 23:41:43.456
W0104 23:41:43.476397      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  4 23:41:43.477: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 1, AvailableReplicas 1
Jan  4 23:41:43.511: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 1, AvailableReplicas 1
Jan  4 23:41:43.557: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 1, AvailableReplicas 1
Jan  4 23:41:43.573: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 1, AvailableReplicas 1
Jan  4 23:41:44.353: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 2, AvailableReplicas 2
Jan  4 23:41:44.953: INFO: observed Replicaset test-rs in namespace replicaset-735 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:41:44.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-735" for this suite. 01/04/23 23:41:44.959
------------------------------
• [SLOW TEST] [6.585 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:41:38.38
    Jan  4 23:41:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename replicaset 01/04/23 23:41:38.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:38.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:38.415
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan  4 23:41:38.430: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  4 23:41:43.438: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/04/23 23:41:43.438
    STEP: Scaling up "test-rs" replicaset  01/04/23 23:41:43.438
    Jan  4 23:41:43.456: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/04/23 23:41:43.456
    W0104 23:41:43.476397      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  4 23:41:43.477: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 1, AvailableReplicas 1
    Jan  4 23:41:43.511: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 1, AvailableReplicas 1
    Jan  4 23:41:43.557: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 1, AvailableReplicas 1
    Jan  4 23:41:43.573: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 1, AvailableReplicas 1
    Jan  4 23:41:44.353: INFO: observed ReplicaSet test-rs in namespace replicaset-735 with ReadyReplicas 2, AvailableReplicas 2
    Jan  4 23:41:44.953: INFO: observed Replicaset test-rs in namespace replicaset-735 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:41:44.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-735" for this suite. 01/04/23 23:41:44.959
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:41:44.966
Jan  4 23:41:44.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename endpointslice 01/04/23 23:41:44.966
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:44.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:44.987
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan  4 23:41:44.997: INFO: Endpoints addresses: [18.217.16.178 3.142.134.49 3.145.156.117] , ports: [6443]
Jan  4 23:41:44.997: INFO: EndpointSlices addresses: [18.217.16.178 3.142.134.49 3.145.156.117] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  4 23:41:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3169" for this suite. 01/04/23 23:41:45.008
------------------------------
• [0.049 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:41:44.966
    Jan  4 23:41:44.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename endpointslice 01/04/23 23:41:44.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:44.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:44.987
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan  4 23:41:44.997: INFO: Endpoints addresses: [18.217.16.178 3.142.134.49 3.145.156.117] , ports: [6443]
    Jan  4 23:41:44.997: INFO: EndpointSlices addresses: [18.217.16.178 3.142.134.49 3.145.156.117] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:41:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3169" for this suite. 01/04/23 23:41:45.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:41:45.024
Jan  4 23:41:45.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename namespaces 01/04/23 23:41:45.028
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:45.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:45.052
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-2935" 01/04/23 23:41:45.059
Jan  4 23:41:45.067: INFO: Namespace "namespaces-2935" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"fcc81411-4ecc-4b5e-a2b9-1b71a19b6b05", "kubernetes.io/metadata.name":"namespaces-2935", "namespaces-2935":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:41:45.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2935" for this suite. 01/04/23 23:41:45.077
------------------------------
• [0.065 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:41:45.024
    Jan  4 23:41:45.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename namespaces 01/04/23 23:41:45.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:45.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:45.052
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-2935" 01/04/23 23:41:45.059
    Jan  4 23:41:45.067: INFO: Namespace "namespaces-2935" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"fcc81411-4ecc-4b5e-a2b9-1b71a19b6b05", "kubernetes.io/metadata.name":"namespaces-2935", "namespaces-2935":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:41:45.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2935" for this suite. 01/04/23 23:41:45.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:41:45.092
Jan  4 23:41:45.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/04/23 23:41:45.094
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:45.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:45.114
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/04/23 23:41:45.117
STEP: Creating hostNetwork=false pod 01/04/23 23:41:45.117
Jan  4 23:41:45.125: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8321" to be "running and ready"
Jan  4 23:41:45.133: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.061479ms
Jan  4 23:41:45.133: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:41:47.137: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011401072s
Jan  4 23:41:47.138: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan  4 23:41:47.138: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/04/23 23:41:47.141
Jan  4 23:41:47.147: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8321" to be "running and ready"
Jan  4 23:41:47.150: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.516402ms
Jan  4 23:41:47.150: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:41:49.156: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008511556s
Jan  4 23:41:49.156: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan  4 23:41:49.156: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/04/23 23:41:49.159
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/04/23 23:41:49.159
Jan  4 23:41:49.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.160: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.160: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  4 23:41:49.254: INFO: Exec stderr: ""
Jan  4 23:41:49.255: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.255: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.255: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  4 23:41:49.329: INFO: Exec stderr: ""
Jan  4 23:41:49.329: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.330: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.330: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  4 23:41:49.417: INFO: Exec stderr: ""
Jan  4 23:41:49.417: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.418: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.418: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  4 23:41:49.524: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/04/23 23:41:49.525
Jan  4 23:41:49.525: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.529: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.529: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  4 23:41:49.661: INFO: Exec stderr: ""
Jan  4 23:41:49.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.664: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.664: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  4 23:41:49.742: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/04/23 23:41:49.742
Jan  4 23:41:49.742: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.742: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.742: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  4 23:41:49.812: INFO: Exec stderr: ""
Jan  4 23:41:49.812: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.813: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.813: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  4 23:41:49.927: INFO: Exec stderr: ""
Jan  4 23:41:49.928: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:49.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:49.928: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:49.929: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  4 23:41:50.049: INFO: Exec stderr: ""
Jan  4 23:41:50.049: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:41:50.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:41:50.049: INFO: ExecWithOptions: Clientset creation
Jan  4 23:41:50.049: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  4 23:41:50.146: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan  4 23:41:50.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8321" for this suite. 01/04/23 23:41:50.154
------------------------------
• [SLOW TEST] [5.071 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:41:45.092
    Jan  4 23:41:45.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/04/23 23:41:45.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:45.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:45.114
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/04/23 23:41:45.117
    STEP: Creating hostNetwork=false pod 01/04/23 23:41:45.117
    Jan  4 23:41:45.125: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8321" to be "running and ready"
    Jan  4 23:41:45.133: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.061479ms
    Jan  4 23:41:45.133: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:41:47.137: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011401072s
    Jan  4 23:41:47.138: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan  4 23:41:47.138: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/04/23 23:41:47.141
    Jan  4 23:41:47.147: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8321" to be "running and ready"
    Jan  4 23:41:47.150: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.516402ms
    Jan  4 23:41:47.150: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:41:49.156: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008511556s
    Jan  4 23:41:49.156: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan  4 23:41:49.156: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/04/23 23:41:49.159
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/04/23 23:41:49.159
    Jan  4 23:41:49.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.160: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.160: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  4 23:41:49.254: INFO: Exec stderr: ""
    Jan  4 23:41:49.255: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.255: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.255: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  4 23:41:49.329: INFO: Exec stderr: ""
    Jan  4 23:41:49.329: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.330: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.330: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  4 23:41:49.417: INFO: Exec stderr: ""
    Jan  4 23:41:49.417: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.418: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.418: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  4 23:41:49.524: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/04/23 23:41:49.525
    Jan  4 23:41:49.525: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.529: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.529: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  4 23:41:49.661: INFO: Exec stderr: ""
    Jan  4 23:41:49.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.664: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.664: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  4 23:41:49.742: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/04/23 23:41:49.742
    Jan  4 23:41:49.742: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.742: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.742: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  4 23:41:49.812: INFO: Exec stderr: ""
    Jan  4 23:41:49.812: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.813: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.813: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  4 23:41:49.927: INFO: Exec stderr: ""
    Jan  4 23:41:49.928: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:49.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:49.928: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:49.929: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  4 23:41:50.049: INFO: Exec stderr: ""
    Jan  4 23:41:50.049: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8321 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:41:50.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:41:50.049: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:41:50.049: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8321/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  4 23:41:50.146: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:41:50.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8321" for this suite. 01/04/23 23:41:50.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:41:50.165
Jan  4 23:41:50.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-preemption 01/04/23 23:41:50.167
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:50.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:50.232
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan  4 23:41:50.264: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  4 23:42:50.311: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 01/04/23 23:42:50.318
Jan  4 23:42:50.352: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  4 23:42:50.357: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  4 23:42:50.391: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  4 23:42:50.404: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  4 23:42:50.433: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  4 23:42:50.442: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan  4 23:42:50.475: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan  4 23:42:50.488: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/04/23 23:42:50.488
Jan  4 23:42:50.488: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8541" to be "running"
Jan  4 23:42:50.499: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.622565ms
Jan  4 23:42:52.505: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01661243s
Jan  4 23:42:54.503: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014384528s
Jan  4 23:42:56.506: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017692499s
Jan  4 23:42:58.503: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014653672s
Jan  4 23:43:00.504: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015310058s
Jan  4 23:43:02.505: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.016931699s
Jan  4 23:43:02.505: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  4 23:43:02.505: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
Jan  4 23:43:02.508: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.914948ms
Jan  4 23:43:02.508: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 23:43:02.508: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
Jan  4 23:43:02.512: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.995832ms
Jan  4 23:43:02.513: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 23:43:02.513: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
Jan  4 23:43:02.516: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.227318ms
Jan  4 23:43:02.516: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 23:43:02.516: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
Jan  4 23:43:02.518: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.295284ms
Jan  4 23:43:02.518: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 23:43:02.518: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
Jan  4 23:43:02.520: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.092927ms
Jan  4 23:43:02.520: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 23:43:02.520: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
Jan  4 23:43:02.523: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.571029ms
Jan  4 23:43:02.523: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  4 23:43:02.523: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
Jan  4 23:43:02.525: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.396691ms
Jan  4 23:43:02.525: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/04/23 23:43:02.525
Jan  4 23:43:02.535: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan  4 23:43:02.539: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867027ms
Jan  4 23:43:04.543: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007688914s
Jan  4 23:43:06.544: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008666146s
Jan  4 23:43:08.545: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009584015s
Jan  4 23:43:08.545: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:43:08.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8541" for this suite. 01/04/23 23:43:08.713
------------------------------
• [SLOW TEST] [78.567 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:41:50.165
    Jan  4 23:41:50.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-preemption 01/04/23 23:41:50.167
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:41:50.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:41:50.232
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan  4 23:41:50.264: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  4 23:42:50.311: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 01/04/23 23:42:50.318
    Jan  4 23:42:50.352: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  4 23:42:50.357: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  4 23:42:50.391: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  4 23:42:50.404: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  4 23:42:50.433: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  4 23:42:50.442: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jan  4 23:42:50.475: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jan  4 23:42:50.488: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/04/23 23:42:50.488
    Jan  4 23:42:50.488: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8541" to be "running"
    Jan  4 23:42:50.499: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.622565ms
    Jan  4 23:42:52.505: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01661243s
    Jan  4 23:42:54.503: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014384528s
    Jan  4 23:42:56.506: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017692499s
    Jan  4 23:42:58.503: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014653672s
    Jan  4 23:43:00.504: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015310058s
    Jan  4 23:43:02.505: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.016931699s
    Jan  4 23:43:02.505: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  4 23:43:02.505: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
    Jan  4 23:43:02.508: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.914948ms
    Jan  4 23:43:02.508: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 23:43:02.508: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
    Jan  4 23:43:02.512: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.995832ms
    Jan  4 23:43:02.513: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 23:43:02.513: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
    Jan  4 23:43:02.516: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.227318ms
    Jan  4 23:43:02.516: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 23:43:02.516: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
    Jan  4 23:43:02.518: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.295284ms
    Jan  4 23:43:02.518: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 23:43:02.518: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
    Jan  4 23:43:02.520: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.092927ms
    Jan  4 23:43:02.520: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 23:43:02.520: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
    Jan  4 23:43:02.523: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.571029ms
    Jan  4 23:43:02.523: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  4 23:43:02.523: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-8541" to be "running"
    Jan  4 23:43:02.525: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.396691ms
    Jan  4 23:43:02.525: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/04/23 23:43:02.525
    Jan  4 23:43:02.535: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan  4 23:43:02.539: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867027ms
    Jan  4 23:43:04.543: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007688914s
    Jan  4 23:43:06.544: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008666146s
    Jan  4 23:43:08.545: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009584015s
    Jan  4 23:43:08.545: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:43:08.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8541" for this suite. 01/04/23 23:43:08.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:43:08.734
Jan  4 23:43:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:43:08.738
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:43:08.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:43:08.769
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/04/23 23:43:08.772
Jan  4 23:43:08.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: rename a version 01/04/23 23:43:13.094
STEP: check the new version name is served 01/04/23 23:43:13.111
STEP: check the old version name is removed 01/04/23 23:43:15.608
STEP: check the other version is not changed 01/04/23 23:43:16.327
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:43:19.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2295" for this suite. 01/04/23 23:43:19.984
------------------------------
• [SLOW TEST] [11.256 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:43:08.734
    Jan  4 23:43:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:43:08.738
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:43:08.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:43:08.769
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/04/23 23:43:08.772
    Jan  4 23:43:08.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: rename a version 01/04/23 23:43:13.094
    STEP: check the new version name is served 01/04/23 23:43:13.111
    STEP: check the old version name is removed 01/04/23 23:43:15.608
    STEP: check the other version is not changed 01/04/23 23:43:16.327
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:43:19.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2295" for this suite. 01/04/23 23:43:19.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:43:19.993
Jan  4 23:43:19.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename namespaces 01/04/23 23:43:19.995
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:43:20.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:43:20.028
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/04/23 23:43:20.032
Jan  4 23:43:20.036: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/04/23 23:43:20.036
Jan  4 23:43:20.042: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/04/23 23:43:20.042
Jan  4 23:43:20.063: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:43:20.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6252" for this suite. 01/04/23 23:43:20.067
------------------------------
• [0.081 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:43:19.993
    Jan  4 23:43:19.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename namespaces 01/04/23 23:43:19.995
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:43:20.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:43:20.028
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/04/23 23:43:20.032
    Jan  4 23:43:20.036: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/04/23 23:43:20.036
    Jan  4 23:43:20.042: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/04/23 23:43:20.042
    Jan  4 23:43:20.063: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:43:20.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6252" for this suite. 01/04/23 23:43:20.067
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:43:20.076
Jan  4 23:43:20.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-watch 01/04/23 23:43:20.077
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:43:20.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:43:20.095
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan  4 23:43:20.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Creating first CR  01/04/23 23:43:22.639
Jan  4 23:43:22.654: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:22Z]] name:name1 resourceVersion:71503 uid:8c9b6982-acad-408d-b049-80f27d2d1637] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/04/23 23:43:32.654
Jan  4 23:43:32.660: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:32Z]] name:name2 resourceVersion:71549 uid:cb0729b4-059e-4644-95f0-469fa7b766f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/04/23 23:43:42.662
Jan  4 23:43:42.669: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:42Z]] name:name1 resourceVersion:71586 uid:8c9b6982-acad-408d-b049-80f27d2d1637] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/04/23 23:43:52.67
Jan  4 23:43:52.677: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:52Z]] name:name2 resourceVersion:71622 uid:cb0729b4-059e-4644-95f0-469fa7b766f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/04/23 23:44:02.678
Jan  4 23:44:02.685: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:42Z]] name:name1 resourceVersion:71659 uid:8c9b6982-acad-408d-b049-80f27d2d1637] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/04/23 23:44:12.685
Jan  4 23:44:12.692: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:52Z]] name:name2 resourceVersion:71697 uid:cb0729b4-059e-4644-95f0-469fa7b766f6] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:44:23.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-3548" for this suite. 01/04/23 23:44:23.218
------------------------------
• [SLOW TEST] [63.147 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:43:20.076
    Jan  4 23:43:20.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-watch 01/04/23 23:43:20.077
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:43:20.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:43:20.095
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan  4 23:43:20.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Creating first CR  01/04/23 23:43:22.639
    Jan  4 23:43:22.654: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:22Z]] name:name1 resourceVersion:71503 uid:8c9b6982-acad-408d-b049-80f27d2d1637] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/04/23 23:43:32.654
    Jan  4 23:43:32.660: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:32Z]] name:name2 resourceVersion:71549 uid:cb0729b4-059e-4644-95f0-469fa7b766f6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/04/23 23:43:42.662
    Jan  4 23:43:42.669: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:42Z]] name:name1 resourceVersion:71586 uid:8c9b6982-acad-408d-b049-80f27d2d1637] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/04/23 23:43:52.67
    Jan  4 23:43:52.677: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:52Z]] name:name2 resourceVersion:71622 uid:cb0729b4-059e-4644-95f0-469fa7b766f6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/04/23 23:44:02.678
    Jan  4 23:44:02.685: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:42Z]] name:name1 resourceVersion:71659 uid:8c9b6982-acad-408d-b049-80f27d2d1637] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/04/23 23:44:12.685
    Jan  4 23:44:12.692: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-04T23:43:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-04T23:43:52Z]] name:name2 resourceVersion:71697 uid:cb0729b4-059e-4644-95f0-469fa7b766f6] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:44:23.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-3548" for this suite. 01/04/23 23:44:23.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:44:23.224
Jan  4 23:44:23.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 23:44:23.225
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:44:23.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:44:23.269
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 23:44:23.294
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:44:23.634
STEP: Deploying the webhook pod 01/04/23 23:44:23.669
STEP: Wait for the deployment to be ready 01/04/23 23:44:23.715
Jan  4 23:44:23.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 23:44:25.767
STEP: Verifying the service has paired with the endpoint 01/04/23 23:44:25.779
Jan  4 23:44:26.779: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/04/23 23:44:26.783
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/04/23 23:44:26.803
STEP: Creating a dummy validating-webhook-configuration object 01/04/23 23:44:26.82
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/04/23 23:44:26.827
STEP: Creating a dummy mutating-webhook-configuration object 01/04/23 23:44:26.835
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/04/23 23:44:26.842
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:44:26.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9410" for this suite. 01/04/23 23:44:26.938
STEP: Destroying namespace "webhook-9410-markers" for this suite. 01/04/23 23:44:26.947
------------------------------
• [3.750 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:44:23.224
    Jan  4 23:44:23.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 23:44:23.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:44:23.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:44:23.269
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 23:44:23.294
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:44:23.634
    STEP: Deploying the webhook pod 01/04/23 23:44:23.669
    STEP: Wait for the deployment to be ready 01/04/23 23:44:23.715
    Jan  4 23:44:23.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 23:44:25.767
    STEP: Verifying the service has paired with the endpoint 01/04/23 23:44:25.779
    Jan  4 23:44:26.779: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/04/23 23:44:26.783
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/04/23 23:44:26.803
    STEP: Creating a dummy validating-webhook-configuration object 01/04/23 23:44:26.82
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/04/23 23:44:26.827
    STEP: Creating a dummy mutating-webhook-configuration object 01/04/23 23:44:26.835
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/04/23 23:44:26.842
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:44:26.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9410" for this suite. 01/04/23 23:44:26.938
    STEP: Destroying namespace "webhook-9410-markers" for this suite. 01/04/23 23:44:26.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:44:26.978
Jan  4 23:44:26.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename downward-api 01/04/23 23:44:26.979
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:44:27.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:44:27.008
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/04/23 23:44:27.011
Jan  4 23:44:27.020: INFO: Waiting up to 5m0s for pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f" in namespace "downward-api-1349" to be "running and ready"
Jan  4 23:44:27.037: INFO: Pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.114822ms
Jan  4 23:44:27.038: INFO: The phase of Pod labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:44:29.042: INFO: Pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f": Phase="Running", Reason="", readiness=true. Elapsed: 2.021466344s
Jan  4 23:44:29.042: INFO: The phase of Pod labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f is Running (Ready = true)
Jan  4 23:44:29.042: INFO: Pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f" satisfied condition "running and ready"
Jan  4 23:44:29.570: INFO: Successfully updated pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  4 23:44:33.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1349" for this suite. 01/04/23 23:44:33.599
------------------------------
• [SLOW TEST] [6.628 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:44:26.978
    Jan  4 23:44:26.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename downward-api 01/04/23 23:44:26.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:44:27.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:44:27.008
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/04/23 23:44:27.011
    Jan  4 23:44:27.020: INFO: Waiting up to 5m0s for pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f" in namespace "downward-api-1349" to be "running and ready"
    Jan  4 23:44:27.037: INFO: Pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.114822ms
    Jan  4 23:44:27.038: INFO: The phase of Pod labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:44:29.042: INFO: Pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f": Phase="Running", Reason="", readiness=true. Elapsed: 2.021466344s
    Jan  4 23:44:29.042: INFO: The phase of Pod labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f is Running (Ready = true)
    Jan  4 23:44:29.042: INFO: Pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f" satisfied condition "running and ready"
    Jan  4 23:44:29.570: INFO: Successfully updated pod "labelsupdateb8e2ed2e-3385-48a9-b9ed-11c3e10be23f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:44:33.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1349" for this suite. 01/04/23 23:44:33.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:44:33.606
Jan  4 23:44:33.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 23:44:33.606
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:44:33.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:44:33.625
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/04/23 23:44:50.631
STEP: Creating a ResourceQuota 01/04/23 23:44:55.635
STEP: Ensuring resource quota status is calculated 01/04/23 23:44:55.641
STEP: Creating a ConfigMap 01/04/23 23:44:57.645
STEP: Ensuring resource quota status captures configMap creation 01/04/23 23:44:57.659
STEP: Deleting a ConfigMap 01/04/23 23:44:59.663
STEP: Ensuring resource quota status released usage 01/04/23 23:44:59.668
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 23:45:01.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-749" for this suite. 01/04/23 23:45:01.679
------------------------------
• [SLOW TEST] [28.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:44:33.606
    Jan  4 23:44:33.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 23:44:33.606
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:44:33.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:44:33.625
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/04/23 23:44:50.631
    STEP: Creating a ResourceQuota 01/04/23 23:44:55.635
    STEP: Ensuring resource quota status is calculated 01/04/23 23:44:55.641
    STEP: Creating a ConfigMap 01/04/23 23:44:57.645
    STEP: Ensuring resource quota status captures configMap creation 01/04/23 23:44:57.659
    STEP: Deleting a ConfigMap 01/04/23 23:44:59.663
    STEP: Ensuring resource quota status released usage 01/04/23 23:44:59.668
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:45:01.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-749" for this suite. 01/04/23 23:45:01.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:45:01.689
Jan  4 23:45:01.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 23:45:01.69
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:01.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:01.726
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-9696 01/04/23 23:45:01.73
STEP: creating service affinity-clusterip in namespace services-9696 01/04/23 23:45:01.73
STEP: creating replication controller affinity-clusterip in namespace services-9696 01/04/23 23:45:01.75
I0104 23:45:01.763913      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9696, replica count: 3
I0104 23:45:04.816080      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  4 23:45:04.822: INFO: Creating new exec pod
Jan  4 23:45:04.829: INFO: Waiting up to 5m0s for pod "execpod-affinityxw7bz" in namespace "services-9696" to be "running"
Jan  4 23:45:04.835: INFO: Pod "execpod-affinityxw7bz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485172ms
Jan  4 23:45:06.840: INFO: Pod "execpod-affinityxw7bz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011552053s
Jan  4 23:45:06.840: INFO: Pod "execpod-affinityxw7bz" satisfied condition "running"
Jan  4 23:45:07.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9696 exec execpod-affinityxw7bz -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan  4 23:45:08.052: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan  4 23:45:08.052: INFO: stdout: ""
Jan  4 23:45:08.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9696 exec execpod-affinityxw7bz -- /bin/sh -x -c nc -v -z -w 2 10.43.154.31 80'
Jan  4 23:45:08.244: INFO: stderr: "+ nc -v -z -w 2 10.43.154.31 80\nConnection to 10.43.154.31 80 port [tcp/http] succeeded!\n"
Jan  4 23:45:08.244: INFO: stdout: ""
Jan  4 23:45:08.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9696 exec execpod-affinityxw7bz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.154.31:80/ ; done'
Jan  4 23:45:08.498: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n"
Jan  4 23:45:08.498: INFO: stdout: "\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz"
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
Jan  4 23:45:08.498: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9696, will wait for the garbage collector to delete the pods 01/04/23 23:45:08.516
Jan  4 23:45:08.581: INFO: Deleting ReplicationController affinity-clusterip took: 5.834679ms
Jan  4 23:45:08.681: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.262699ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 23:45:10.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9696" for this suite. 01/04/23 23:45:10.924
------------------------------
• [SLOW TEST] [9.246 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:45:01.689
    Jan  4 23:45:01.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 23:45:01.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:01.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:01.726
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-9696 01/04/23 23:45:01.73
    STEP: creating service affinity-clusterip in namespace services-9696 01/04/23 23:45:01.73
    STEP: creating replication controller affinity-clusterip in namespace services-9696 01/04/23 23:45:01.75
    I0104 23:45:01.763913      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9696, replica count: 3
    I0104 23:45:04.816080      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  4 23:45:04.822: INFO: Creating new exec pod
    Jan  4 23:45:04.829: INFO: Waiting up to 5m0s for pod "execpod-affinityxw7bz" in namespace "services-9696" to be "running"
    Jan  4 23:45:04.835: INFO: Pod "execpod-affinityxw7bz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485172ms
    Jan  4 23:45:06.840: INFO: Pod "execpod-affinityxw7bz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011552053s
    Jan  4 23:45:06.840: INFO: Pod "execpod-affinityxw7bz" satisfied condition "running"
    Jan  4 23:45:07.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9696 exec execpod-affinityxw7bz -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan  4 23:45:08.052: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan  4 23:45:08.052: INFO: stdout: ""
    Jan  4 23:45:08.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9696 exec execpod-affinityxw7bz -- /bin/sh -x -c nc -v -z -w 2 10.43.154.31 80'
    Jan  4 23:45:08.244: INFO: stderr: "+ nc -v -z -w 2 10.43.154.31 80\nConnection to 10.43.154.31 80 port [tcp/http] succeeded!\n"
    Jan  4 23:45:08.244: INFO: stdout: ""
    Jan  4 23:45:08.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=services-9696 exec execpod-affinityxw7bz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.154.31:80/ ; done'
    Jan  4 23:45:08.498: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.154.31:80/\n"
    Jan  4 23:45:08.498: INFO: stdout: "\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz\naffinity-clusterip-s8xrz"
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Received response from host: affinity-clusterip-s8xrz
    Jan  4 23:45:08.498: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-9696, will wait for the garbage collector to delete the pods 01/04/23 23:45:08.516
    Jan  4 23:45:08.581: INFO: Deleting ReplicationController affinity-clusterip took: 5.834679ms
    Jan  4 23:45:08.681: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.262699ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:45:10.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9696" for this suite. 01/04/23 23:45:10.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:45:10.937
Jan  4 23:45:10.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:45:10.938
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:10.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:10.959
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan  4 23:45:10.979: INFO: created pod
Jan  4 23:45:10.979: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4018" to be "Succeeded or Failed"
Jan  4 23:45:10.982: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.106877ms
Jan  4 23:45:12.985: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006848786s
Jan  4 23:45:14.986: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007674931s
STEP: Saw pod success 01/04/23 23:45:14.987
Jan  4 23:45:14.987: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan  4 23:45:44.989: INFO: polling logs
Jan  4 23:45:44.998: INFO: Pod logs: 
I0104 23:45:11.768542       1 log.go:198] OK: Got token
I0104 23:45:11.768772       1 log.go:198] validating with in-cluster discovery
I0104 23:45:11.769311       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0104 23:45:11.769345       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4018:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672876511, NotBefore:1672875911, IssuedAt:1672875911, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4018", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"56e4ae91-a798-46c3-a7c5-2a1ddf187e00"}}}
I0104 23:45:11.797453       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0104 23:45:11.804340       1 log.go:198] OK: Validated signature on JWT
I0104 23:45:11.804464       1 log.go:198] OK: Got valid claims from token!
I0104 23:45:11.804493       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4018:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672876511, NotBefore:1672875911, IssuedAt:1672875911, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4018", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"56e4ae91-a798-46c3-a7c5-2a1ddf187e00"}}}

Jan  4 23:45:44.998: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  4 23:45:45.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4018" for this suite. 01/04/23 23:45:45.017
------------------------------
• [SLOW TEST] [34.086 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:45:10.937
    Jan  4 23:45:10.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename svcaccounts 01/04/23 23:45:10.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:10.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:10.959
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan  4 23:45:10.979: INFO: created pod
    Jan  4 23:45:10.979: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4018" to be "Succeeded or Failed"
    Jan  4 23:45:10.982: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.106877ms
    Jan  4 23:45:12.985: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006848786s
    Jan  4 23:45:14.986: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007674931s
    STEP: Saw pod success 01/04/23 23:45:14.987
    Jan  4 23:45:14.987: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan  4 23:45:44.989: INFO: polling logs
    Jan  4 23:45:44.998: INFO: Pod logs: 
    I0104 23:45:11.768542       1 log.go:198] OK: Got token
    I0104 23:45:11.768772       1 log.go:198] validating with in-cluster discovery
    I0104 23:45:11.769311       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0104 23:45:11.769345       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4018:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672876511, NotBefore:1672875911, IssuedAt:1672875911, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4018", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"56e4ae91-a798-46c3-a7c5-2a1ddf187e00"}}}
    I0104 23:45:11.797453       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0104 23:45:11.804340       1 log.go:198] OK: Validated signature on JWT
    I0104 23:45:11.804464       1 log.go:198] OK: Got valid claims from token!
    I0104 23:45:11.804493       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4018:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672876511, NotBefore:1672875911, IssuedAt:1672875911, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4018", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"56e4ae91-a798-46c3-a7c5-2a1ddf187e00"}}}

    Jan  4 23:45:44.998: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:45:45.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4018" for this suite. 01/04/23 23:45:45.017
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:45:45.023
Jan  4 23:45:45.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename csiinlinevolumes 01/04/23 23:45:45.025
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:45.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:45.045
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/04/23 23:45:45.048
STEP: getting 01/04/23 23:45:45.068
STEP: listing 01/04/23 23:45:45.075
STEP: deleting 01/04/23 23:45:45.079
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:45:45.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-816" for this suite. 01/04/23 23:45:45.103
------------------------------
• [0.088 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:45:45.023
    Jan  4 23:45:45.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename csiinlinevolumes 01/04/23 23:45:45.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:45.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:45.045
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/04/23 23:45:45.048
    STEP: getting 01/04/23 23:45:45.068
    STEP: listing 01/04/23 23:45:45.075
    STEP: deleting 01/04/23 23:45:45.079
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:45:45.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-816" for this suite. 01/04/23 23:45:45.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:45:45.111
Jan  4 23:45:45.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename hostport 01/04/23 23:45:45.113
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:45.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:45.133
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/04/23 23:45:45.139
Jan  4 23:45:45.152: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3337" to be "running and ready"
Jan  4 23:45:45.155: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985473ms
Jan  4 23:45:45.155: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:45:47.159: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006661343s
Jan  4 23:45:47.159: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  4 23:45:47.159: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.9.62 on the node which pod1 resides and expect scheduled 01/04/23 23:45:47.159
Jan  4 23:45:47.168: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3337" to be "running and ready"
Jan  4 23:45:47.171: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981125ms
Jan  4 23:45:47.171: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:45:49.176: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007613977s
Jan  4 23:45:49.176: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  4 23:45:49.176: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.9.62 but use UDP protocol on the node which pod2 resides 01/04/23 23:45:49.176
Jan  4 23:45:49.188: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3337" to be "running and ready"
Jan  4 23:45:49.193: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146643ms
Jan  4 23:45:49.193: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:45:51.196: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00879918s
Jan  4 23:45:51.196: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan  4 23:45:51.196: INFO: Pod "pod3" satisfied condition "running and ready"
Jan  4 23:45:51.202: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3337" to be "running and ready"
Jan  4 23:45:51.204: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389242ms
Jan  4 23:45:51.204: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:45:53.209: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.006796494s
Jan  4 23:45:53.209: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan  4 23:45:53.209: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/04/23 23:45:53.211
Jan  4 23:45:53.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.9.62 http://127.0.0.1:54323/hostname] Namespace:hostport-3337 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:45:53.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:45:53.212: INFO: ExecWithOptions: Clientset creation
Jan  4 23:45:53.212: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-3337/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.9.62+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.9.62, port: 54323 01/04/23 23:45:53.283
Jan  4 23:45:53.283: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.9.62:54323/hostname] Namespace:hostport-3337 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:45:53.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:45:53.284: INFO: ExecWithOptions: Clientset creation
Jan  4 23:45:53.284: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-3337/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.9.62%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.9.62, port: 54323 UDP 01/04/23 23:45:53.379
Jan  4 23:45:53.379: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.9.62 54323] Namespace:hostport-3337 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  4 23:45:53.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
Jan  4 23:45:53.380: INFO: ExecWithOptions: Clientset creation
Jan  4 23:45:53.380: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-3337/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.9.62+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan  4 23:45:58.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-3337" for this suite. 01/04/23 23:45:58.459
------------------------------
• [SLOW TEST] [13.354 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:45:45.111
    Jan  4 23:45:45.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename hostport 01/04/23 23:45:45.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:45.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:45.133
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/04/23 23:45:45.139
    Jan  4 23:45:45.152: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3337" to be "running and ready"
    Jan  4 23:45:45.155: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985473ms
    Jan  4 23:45:45.155: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:45:47.159: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006661343s
    Jan  4 23:45:47.159: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  4 23:45:47.159: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.9.62 on the node which pod1 resides and expect scheduled 01/04/23 23:45:47.159
    Jan  4 23:45:47.168: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3337" to be "running and ready"
    Jan  4 23:45:47.171: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981125ms
    Jan  4 23:45:47.171: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:45:49.176: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007613977s
    Jan  4 23:45:49.176: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  4 23:45:49.176: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.9.62 but use UDP protocol on the node which pod2 resides 01/04/23 23:45:49.176
    Jan  4 23:45:49.188: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3337" to be "running and ready"
    Jan  4 23:45:49.193: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146643ms
    Jan  4 23:45:49.193: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:45:51.196: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00879918s
    Jan  4 23:45:51.196: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan  4 23:45:51.196: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan  4 23:45:51.202: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3337" to be "running and ready"
    Jan  4 23:45:51.204: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389242ms
    Jan  4 23:45:51.204: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:45:53.209: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.006796494s
    Jan  4 23:45:53.209: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan  4 23:45:53.209: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/04/23 23:45:53.211
    Jan  4 23:45:53.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.9.62 http://127.0.0.1:54323/hostname] Namespace:hostport-3337 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:45:53.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:45:53.212: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:45:53.212: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-3337/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.9.62+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.9.62, port: 54323 01/04/23 23:45:53.283
    Jan  4 23:45:53.283: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.9.62:54323/hostname] Namespace:hostport-3337 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:45:53.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:45:53.284: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:45:53.284: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-3337/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.9.62%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.9.62, port: 54323 UDP 01/04/23 23:45:53.379
    Jan  4 23:45:53.379: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.9.62 54323] Namespace:hostport-3337 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  4 23:45:53.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    Jan  4 23:45:53.380: INFO: ExecWithOptions: Clientset creation
    Jan  4 23:45:53.380: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-3337/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.9.62+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:45:58.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-3337" for this suite. 01/04/23 23:45:58.459
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:45:58.467
Jan  4 23:45:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename resourcequota 01/04/23 23:45:58.468
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:58.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:58.49
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/04/23 23:45:58.493
STEP: Counting existing ResourceQuota 01/04/23 23:46:03.501
STEP: Creating a ResourceQuota 01/04/23 23:46:08.504
STEP: Ensuring resource quota status is calculated 01/04/23 23:46:08.509
STEP: Creating a Secret 01/04/23 23:46:10.514
STEP: Ensuring resource quota status captures secret creation 01/04/23 23:46:10.56
STEP: Deleting a secret 01/04/23 23:46:12.565
STEP: Ensuring resource quota status released usage 01/04/23 23:46:12.571
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  4 23:46:14.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-780" for this suite. 01/04/23 23:46:14.579
------------------------------
• [SLOW TEST] [16.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:45:58.467
    Jan  4 23:45:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename resourcequota 01/04/23 23:45:58.468
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:45:58.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:45:58.49
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/04/23 23:45:58.493
    STEP: Counting existing ResourceQuota 01/04/23 23:46:03.501
    STEP: Creating a ResourceQuota 01/04/23 23:46:08.504
    STEP: Ensuring resource quota status is calculated 01/04/23 23:46:08.509
    STEP: Creating a Secret 01/04/23 23:46:10.514
    STEP: Ensuring resource quota status captures secret creation 01/04/23 23:46:10.56
    STEP: Deleting a secret 01/04/23 23:46:12.565
    STEP: Ensuring resource quota status released usage 01/04/23 23:46:12.571
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:46:14.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-780" for this suite. 01/04/23 23:46:14.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:46:14.588
Jan  4 23:46:14.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-preemption 01/04/23 23:46:14.589
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:46:14.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:46:14.613
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan  4 23:46:14.630: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  4 23:47:14.675: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:47:14.678
Jan  4 23:47:14.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename sched-preemption-path 01/04/23 23:47:14.678
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:47:14.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:47:14.695
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 01/04/23 23:47:14.697
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/04/23 23:47:14.698
Jan  4 23:47:14.706: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9036" to be "running"
Jan  4 23:47:14.709: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255993ms
Jan  4 23:47:16.713: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007266281s
Jan  4 23:47:16.713: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/04/23 23:47:16.715
Jan  4 23:47:16.729: INFO: found a healthy node: ip-172-31-13-117.us-east-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Jan  4 23:47:30.830: INFO: pods created so far: [1 1 1]
Jan  4 23:47:30.830: INFO: length of pods created so far: 3
Jan  4 23:47:36.840: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan  4 23:47:43.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:47:43.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9036" for this suite. 01/04/23 23:47:43.945
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-536" for this suite. 01/04/23 23:47:43.954
------------------------------
• [SLOW TEST] [89.372 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:46:14.588
    Jan  4 23:46:14.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-preemption 01/04/23 23:46:14.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:46:14.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:46:14.613
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan  4 23:46:14.630: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  4 23:47:14.675: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:47:14.678
    Jan  4 23:47:14.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename sched-preemption-path 01/04/23 23:47:14.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:47:14.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:47:14.695
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 01/04/23 23:47:14.697
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/04/23 23:47:14.698
    Jan  4 23:47:14.706: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9036" to be "running"
    Jan  4 23:47:14.709: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255993ms
    Jan  4 23:47:16.713: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007266281s
    Jan  4 23:47:16.713: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/04/23 23:47:16.715
    Jan  4 23:47:16.729: INFO: found a healthy node: ip-172-31-13-117.us-east-2.compute.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Jan  4 23:47:30.830: INFO: pods created so far: [1 1 1]
    Jan  4 23:47:30.830: INFO: length of pods created so far: 3
    Jan  4 23:47:36.840: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:47:43.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:47:43.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9036" for this suite. 01/04/23 23:47:43.945
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-536" for this suite. 01/04/23 23:47:43.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:47:43.965
Jan  4 23:47:43.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename services 01/04/23 23:47:43.966
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:47:43.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:47:43.992
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/04/23 23:47:43.994
Jan  4 23:47:43.994: INFO: Creating e2e-svc-a-ndhx4
Jan  4 23:47:44.006: INFO: Creating e2e-svc-b-j2d7w
Jan  4 23:47:44.020: INFO: Creating e2e-svc-c-4799s
STEP: deleting service collection 01/04/23 23:47:44.039
Jan  4 23:47:44.088: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  4 23:47:44.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9363" for this suite. 01/04/23 23:47:44.098
------------------------------
• [0.142 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:47:43.965
    Jan  4 23:47:43.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename services 01/04/23 23:47:43.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:47:43.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:47:43.992
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/04/23 23:47:43.994
    Jan  4 23:47:43.994: INFO: Creating e2e-svc-a-ndhx4
    Jan  4 23:47:44.006: INFO: Creating e2e-svc-b-j2d7w
    Jan  4 23:47:44.020: INFO: Creating e2e-svc-c-4799s
    STEP: deleting service collection 01/04/23 23:47:44.039
    Jan  4 23:47:44.088: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:47:44.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9363" for this suite. 01/04/23 23:47:44.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:47:44.111
Jan  4 23:47:44.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubelet-test 01/04/23 23:47:44.112
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:47:44.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:47:44.14
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:47:44.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8632" for this suite. 01/04/23 23:47:44.178
------------------------------
• [0.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:47:44.111
    Jan  4 23:47:44.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubelet-test 01/04/23 23:47:44.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:47:44.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:47:44.14
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:47:44.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8632" for this suite. 01/04/23 23:47:44.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:47:44.188
Jan  4 23:47:44.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename deployment 01/04/23 23:47:44.189
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:47:44.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:47:44.222
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan  4 23:47:44.244: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/04/23 23:47:44.244
Jan  4 23:47:44.245: INFO: Waiting up to 5m0s for pod "test-rollover-controller-cmlhg" in namespace "deployment-8657" to be "running"
Jan  4 23:47:44.251: INFO: Pod "test-rollover-controller-cmlhg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.391084ms
Jan  4 23:47:46.257: INFO: Pod "test-rollover-controller-cmlhg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011761539s
Jan  4 23:47:46.257: INFO: Pod "test-rollover-controller-cmlhg" satisfied condition "running"
Jan  4 23:47:46.257: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan  4 23:47:48.262: INFO: Creating deployment "test-rollover-deployment"
Jan  4 23:47:48.272: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan  4 23:47:50.279: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan  4 23:47:50.287: INFO: Ensure that both replica sets have 1 created replica
Jan  4 23:47:50.292: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan  4 23:47:50.300: INFO: Updating deployment test-rollover-deployment
Jan  4 23:47:50.300: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan  4 23:47:52.311: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan  4 23:47:52.316: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan  4 23:47:52.321: INFO: all replica sets need to contain the pod-template-hash label
Jan  4 23:47:52.321: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:47:54.336: INFO: all replica sets need to contain the pod-template-hash label
Jan  4 23:47:54.336: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:47:56.328: INFO: all replica sets need to contain the pod-template-hash label
Jan  4 23:47:56.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:47:58.328: INFO: all replica sets need to contain the pod-template-hash label
Jan  4 23:47:58.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:48:00.330: INFO: all replica sets need to contain the pod-template-hash label
Jan  4 23:48:00.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  4 23:48:02.330: INFO: 
Jan  4 23:48:02.330: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  4 23:48:02.338: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8657  59d617b0-f97b-4c48-8e5a-4c3b67889caa 73218 2 2023-01-04 23:47:48 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f610c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-04 23:47:48 +0000 UTC,LastTransitionTime:2023-01-04 23:47:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-04 23:48:01 +0000 UTC,LastTransitionTime:2023-01-04 23:47:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  4 23:48:02.341: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8657  eed4226d-bcbc-4300-9aa0-ced5f254d9c6 73208 2 2023-01-04 23:47:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 59d617b0-f97b-4c48-8e5a-4c3b67889caa 0xc003f61597 0xc003f61598}] [] [{kube-controller-manager Update apps/v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59d617b0-f97b-4c48-8e5a-4c3b67889caa\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f61648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  4 23:48:02.341: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan  4 23:48:02.341: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8657  5cec8f43-e84e-4b9e-9275-bf800c3a0ad6 73217 2 2023-01-04 23:47:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 59d617b0-f97b-4c48-8e5a-4c3b67889caa 0xc003f61467 0xc003f61468}] [] [{e2e.test Update apps/v1 2023-01-04 23:47:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59d617b0-f97b-4c48-8e5a-4c3b67889caa\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f61528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  4 23:48:02.341: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8657  54a70197-24b0-4d1e-a7a9-9b3b42961774 73142 2 2023-01-04 23:47:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 59d617b0-f97b-4c48-8e5a-4c3b67889caa 0xc003f616b7 0xc003f616b8}] [] [{kube-controller-manager Update apps/v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59d617b0-f97b-4c48-8e5a-4c3b67889caa\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f61768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  4 23:48:02.346: INFO: Pod "test-rollover-deployment-6c6df9974f-8vl87" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-8vl87 test-rollover-deployment-6c6df9974f- deployment-8657  e7b22c61-52d0-4905-b8ba-7d45cb85d75f 73168 0 2023-01-04 23:47:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:89c66716b2119701d15d190ad4d165387f0a0cc0ed6a7985e80ca94eb5d81312 cni.projectcalico.org/podIP:10.42.3.154/32 cni.projectcalico.org/podIPs:10.42.3.154/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f eed4226d-bcbc-4300-9aa0-ced5f254d9c6 0xc003f61cd7 0xc003f61cd8}] [] [{calico Update v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eed4226d-bcbc-4300-9aa0-ced5f254d9c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:47:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dsh92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dsh92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:47:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:47:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:47:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.154,StartTime:2023-01-04 23:47:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:47:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://25117026e89235f58c31b71836741da89ccda5783e22f39c81f5af8644cfa0be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:02.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8657" for this suite. 01/04/23 23:48:02.351
------------------------------
• [SLOW TEST] [18.173 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:47:44.188
    Jan  4 23:47:44.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename deployment 01/04/23 23:47:44.189
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:47:44.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:47:44.222
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan  4 23:47:44.244: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/04/23 23:47:44.244
    Jan  4 23:47:44.245: INFO: Waiting up to 5m0s for pod "test-rollover-controller-cmlhg" in namespace "deployment-8657" to be "running"
    Jan  4 23:47:44.251: INFO: Pod "test-rollover-controller-cmlhg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.391084ms
    Jan  4 23:47:46.257: INFO: Pod "test-rollover-controller-cmlhg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011761539s
    Jan  4 23:47:46.257: INFO: Pod "test-rollover-controller-cmlhg" satisfied condition "running"
    Jan  4 23:47:46.257: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan  4 23:47:48.262: INFO: Creating deployment "test-rollover-deployment"
    Jan  4 23:47:48.272: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan  4 23:47:50.279: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan  4 23:47:50.287: INFO: Ensure that both replica sets have 1 created replica
    Jan  4 23:47:50.292: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan  4 23:47:50.300: INFO: Updating deployment test-rollover-deployment
    Jan  4 23:47:50.300: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan  4 23:47:52.311: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan  4 23:47:52.316: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan  4 23:47:52.321: INFO: all replica sets need to contain the pod-template-hash label
    Jan  4 23:47:52.321: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:47:54.336: INFO: all replica sets need to contain the pod-template-hash label
    Jan  4 23:47:54.336: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:47:56.328: INFO: all replica sets need to contain the pod-template-hash label
    Jan  4 23:47:56.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:47:58.328: INFO: all replica sets need to contain the pod-template-hash label
    Jan  4 23:47:58.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:48:00.330: INFO: all replica sets need to contain the pod-template-hash label
    Jan  4 23:48:00.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 4, 23, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 4, 23, 47, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  4 23:48:02.330: INFO: 
    Jan  4 23:48:02.330: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  4 23:48:02.338: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8657  59d617b0-f97b-4c48-8e5a-4c3b67889caa 73218 2 2023-01-04 23:47:48 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f610c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-04 23:47:48 +0000 UTC,LastTransitionTime:2023-01-04 23:47:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-04 23:48:01 +0000 UTC,LastTransitionTime:2023-01-04 23:47:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  4 23:48:02.341: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8657  eed4226d-bcbc-4300-9aa0-ced5f254d9c6 73208 2 2023-01-04 23:47:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 59d617b0-f97b-4c48-8e5a-4c3b67889caa 0xc003f61597 0xc003f61598}] [] [{kube-controller-manager Update apps/v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59d617b0-f97b-4c48-8e5a-4c3b67889caa\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f61648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 23:48:02.341: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan  4 23:48:02.341: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8657  5cec8f43-e84e-4b9e-9275-bf800c3a0ad6 73217 2 2023-01-04 23:47:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 59d617b0-f97b-4c48-8e5a-4c3b67889caa 0xc003f61467 0xc003f61468}] [] [{e2e.test Update apps/v1 2023-01-04 23:47:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59d617b0-f97b-4c48-8e5a-4c3b67889caa\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f61528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 23:48:02.341: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8657  54a70197-24b0-4d1e-a7a9-9b3b42961774 73142 2 2023-01-04 23:47:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 59d617b0-f97b-4c48-8e5a-4c3b67889caa 0xc003f616b7 0xc003f616b8}] [] [{kube-controller-manager Update apps/v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59d617b0-f97b-4c48-8e5a-4c3b67889caa\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f61768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 23:48:02.346: INFO: Pod "test-rollover-deployment-6c6df9974f-8vl87" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-8vl87 test-rollover-deployment-6c6df9974f- deployment-8657  e7b22c61-52d0-4905-b8ba-7d45cb85d75f 73168 0 2023-01-04 23:47:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:89c66716b2119701d15d190ad4d165387f0a0cc0ed6a7985e80ca94eb5d81312 cni.projectcalico.org/podIP:10.42.3.154/32 cni.projectcalico.org/podIPs:10.42.3.154/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f eed4226d-bcbc-4300-9aa0-ced5f254d9c6 0xc003f61cd7 0xc003f61cd8}] [] [{calico Update v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eed4226d-bcbc-4300-9aa0-ced5f254d9c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:47:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dsh92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dsh92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:47:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:47:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:47:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.154,StartTime:2023-01-04 23:47:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:47:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://25117026e89235f58c31b71836741da89ccda5783e22f39c81f5af8644cfa0be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:02.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8657" for this suite. 01/04/23 23:48:02.351
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:02.362
Jan  4 23:48:02.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:48:02.363
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:02.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:02.395
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan  4 23:48:02.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/04/23 23:48:04.68
Jan  4 23:48:04.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 create -f -'
Jan  4 23:48:05.450: INFO: stderr: ""
Jan  4 23:48:05.450: INFO: stdout: "e2e-test-crd-publish-openapi-3393-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  4 23:48:05.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 delete e2e-test-crd-publish-openapi-3393-crds test-cr'
Jan  4 23:48:05.571: INFO: stderr: ""
Jan  4 23:48:05.571: INFO: stdout: "e2e-test-crd-publish-openapi-3393-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan  4 23:48:05.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 apply -f -'
Jan  4 23:48:05.816: INFO: stderr: ""
Jan  4 23:48:05.816: INFO: stdout: "e2e-test-crd-publish-openapi-3393-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  4 23:48:05.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 delete e2e-test-crd-publish-openapi-3393-crds test-cr'
Jan  4 23:48:05.890: INFO: stderr: ""
Jan  4 23:48:05.890: INFO: stdout: "e2e-test-crd-publish-openapi-3393-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/04/23 23:48:05.89
Jan  4 23:48:05.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 explain e2e-test-crd-publish-openapi-3393-crds'
Jan  4 23:48:06.086: INFO: stderr: ""
Jan  4 23:48:06.086: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3393-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:08.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5047" for this suite. 01/04/23 23:48:08.786
------------------------------
• [SLOW TEST] [6.440 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:02.362
    Jan  4 23:48:02.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:48:02.363
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:02.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:02.395
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan  4 23:48:02.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/04/23 23:48:04.68
    Jan  4 23:48:04.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 create -f -'
    Jan  4 23:48:05.450: INFO: stderr: ""
    Jan  4 23:48:05.450: INFO: stdout: "e2e-test-crd-publish-openapi-3393-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  4 23:48:05.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 delete e2e-test-crd-publish-openapi-3393-crds test-cr'
    Jan  4 23:48:05.571: INFO: stderr: ""
    Jan  4 23:48:05.571: INFO: stdout: "e2e-test-crd-publish-openapi-3393-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan  4 23:48:05.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 apply -f -'
    Jan  4 23:48:05.816: INFO: stderr: ""
    Jan  4 23:48:05.816: INFO: stdout: "e2e-test-crd-publish-openapi-3393-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  4 23:48:05.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 delete e2e-test-crd-publish-openapi-3393-crds test-cr'
    Jan  4 23:48:05.890: INFO: stderr: ""
    Jan  4 23:48:05.890: INFO: stdout: "e2e-test-crd-publish-openapi-3393-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/04/23 23:48:05.89
    Jan  4 23:48:05.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-5047 explain e2e-test-crd-publish-openapi-3393-crds'
    Jan  4 23:48:06.086: INFO: stderr: ""
    Jan  4 23:48:06.086: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3393-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:08.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5047" for this suite. 01/04/23 23:48:08.786
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:08.805
Jan  4 23:48:08.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename limitrange 01/04/23 23:48:08.806
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:08.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:08.832
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/04/23 23:48:08.834
STEP: Setting up watch 01/04/23 23:48:08.834
STEP: Submitting a LimitRange 01/04/23 23:48:08.937
STEP: Verifying LimitRange creation was observed 01/04/23 23:48:08.942
STEP: Fetching the LimitRange to ensure it has proper values 01/04/23 23:48:08.942
Jan  4 23:48:08.945: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  4 23:48:08.945: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/04/23 23:48:08.945
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/04/23 23:48:08.95
Jan  4 23:48:08.958: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  4 23:48:08.958: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/04/23 23:48:08.958
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/04/23 23:48:08.964
Jan  4 23:48:08.969: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan  4 23:48:08.969: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/04/23 23:48:08.969
STEP: Failing to create a Pod with more than max resources 01/04/23 23:48:08.973
STEP: Updating a LimitRange 01/04/23 23:48:08.974
STEP: Verifying LimitRange updating is effective 01/04/23 23:48:08.98
STEP: Creating a Pod with less than former min resources 01/04/23 23:48:10.987
STEP: Failing to create a Pod with more than max resources 01/04/23 23:48:10.992
STEP: Deleting a LimitRange 01/04/23 23:48:10.995
STEP: Verifying the LimitRange was deleted 01/04/23 23:48:11.01
Jan  4 23:48:16.014: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/04/23 23:48:16.014
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:16.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7435" for this suite. 01/04/23 23:48:16.027
------------------------------
• [SLOW TEST] [7.233 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:08.805
    Jan  4 23:48:08.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename limitrange 01/04/23 23:48:08.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:08.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:08.832
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/04/23 23:48:08.834
    STEP: Setting up watch 01/04/23 23:48:08.834
    STEP: Submitting a LimitRange 01/04/23 23:48:08.937
    STEP: Verifying LimitRange creation was observed 01/04/23 23:48:08.942
    STEP: Fetching the LimitRange to ensure it has proper values 01/04/23 23:48:08.942
    Jan  4 23:48:08.945: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  4 23:48:08.945: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/04/23 23:48:08.945
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/04/23 23:48:08.95
    Jan  4 23:48:08.958: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  4 23:48:08.958: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/04/23 23:48:08.958
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/04/23 23:48:08.964
    Jan  4 23:48:08.969: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan  4 23:48:08.969: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/04/23 23:48:08.969
    STEP: Failing to create a Pod with more than max resources 01/04/23 23:48:08.973
    STEP: Updating a LimitRange 01/04/23 23:48:08.974
    STEP: Verifying LimitRange updating is effective 01/04/23 23:48:08.98
    STEP: Creating a Pod with less than former min resources 01/04/23 23:48:10.987
    STEP: Failing to create a Pod with more than max resources 01/04/23 23:48:10.992
    STEP: Deleting a LimitRange 01/04/23 23:48:10.995
    STEP: Verifying the LimitRange was deleted 01/04/23 23:48:11.01
    Jan  4 23:48:16.014: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/04/23 23:48:16.014
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:16.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7435" for this suite. 01/04/23 23:48:16.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:16.04
Jan  4 23:48:16.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 23:48:16.041
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:16.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:16.064
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/04/23 23:48:16.067
Jan  4 23:48:16.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 create -f -'
Jan  4 23:48:17.005: INFO: stderr: ""
Jan  4 23:48:17.005: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/04/23 23:48:17.005
Jan  4 23:48:17.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  4 23:48:17.200: INFO: stderr: ""
Jan  4 23:48:17.200: INFO: stdout: "update-demo-nautilus-qj4f5 update-demo-nautilus-vr8kr "
Jan  4 23:48:17.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-qj4f5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 23:48:17.364: INFO: stderr: ""
Jan  4 23:48:17.364: INFO: stdout: ""
Jan  4 23:48:17.364: INFO: update-demo-nautilus-qj4f5 is created but not running
Jan  4 23:48:22.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  4 23:48:22.443: INFO: stderr: ""
Jan  4 23:48:22.443: INFO: stdout: "update-demo-nautilus-qj4f5 update-demo-nautilus-vr8kr "
Jan  4 23:48:22.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-qj4f5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 23:48:22.515: INFO: stderr: ""
Jan  4 23:48:22.515: INFO: stdout: "true"
Jan  4 23:48:22.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-qj4f5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  4 23:48:22.588: INFO: stderr: ""
Jan  4 23:48:22.588: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  4 23:48:22.588: INFO: validating pod update-demo-nautilus-qj4f5
Jan  4 23:48:22.594: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  4 23:48:22.594: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  4 23:48:22.594: INFO: update-demo-nautilus-qj4f5 is verified up and running
Jan  4 23:48:22.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 23:48:22.719: INFO: stderr: ""
Jan  4 23:48:22.719: INFO: stdout: "true"
Jan  4 23:48:22.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  4 23:48:22.828: INFO: stderr: ""
Jan  4 23:48:22.828: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  4 23:48:22.828: INFO: validating pod update-demo-nautilus-vr8kr
Jan  4 23:48:22.839: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  4 23:48:22.839: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  4 23:48:22.839: INFO: update-demo-nautilus-vr8kr is verified up and running
STEP: scaling down the replication controller 01/04/23 23:48:22.839
Jan  4 23:48:22.840: INFO: scanned /root for discovery docs: <nil>
Jan  4 23:48:22.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan  4 23:48:23.953: INFO: stderr: ""
Jan  4 23:48:23.953: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/04/23 23:48:23.953
Jan  4 23:48:23.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  4 23:48:24.021: INFO: stderr: ""
Jan  4 23:48:24.021: INFO: stdout: "update-demo-nautilus-vr8kr "
Jan  4 23:48:24.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 23:48:24.086: INFO: stderr: ""
Jan  4 23:48:24.086: INFO: stdout: "true"
Jan  4 23:48:24.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  4 23:48:24.154: INFO: stderr: ""
Jan  4 23:48:24.154: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  4 23:48:24.154: INFO: validating pod update-demo-nautilus-vr8kr
Jan  4 23:48:24.165: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  4 23:48:24.165: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  4 23:48:24.165: INFO: update-demo-nautilus-vr8kr is verified up and running
STEP: scaling up the replication controller 01/04/23 23:48:24.165
Jan  4 23:48:24.166: INFO: scanned /root for discovery docs: <nil>
Jan  4 23:48:24.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan  4 23:48:25.248: INFO: stderr: ""
Jan  4 23:48:25.248: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/04/23 23:48:25.248
Jan  4 23:48:25.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  4 23:48:25.315: INFO: stderr: ""
Jan  4 23:48:25.315: INFO: stdout: "update-demo-nautilus-vr8kr update-demo-nautilus-xf6w6 "
Jan  4 23:48:25.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 23:48:25.391: INFO: stderr: ""
Jan  4 23:48:25.391: INFO: stdout: "true"
Jan  4 23:48:25.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  4 23:48:25.525: INFO: stderr: ""
Jan  4 23:48:25.525: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  4 23:48:25.525: INFO: validating pod update-demo-nautilus-vr8kr
Jan  4 23:48:25.529: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  4 23:48:25.529: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  4 23:48:25.529: INFO: update-demo-nautilus-vr8kr is verified up and running
Jan  4 23:48:25.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-xf6w6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  4 23:48:25.597: INFO: stderr: ""
Jan  4 23:48:25.597: INFO: stdout: "true"
Jan  4 23:48:25.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-xf6w6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  4 23:48:25.674: INFO: stderr: ""
Jan  4 23:48:25.674: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  4 23:48:25.674: INFO: validating pod update-demo-nautilus-xf6w6
Jan  4 23:48:25.681: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  4 23:48:25.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  4 23:48:25.681: INFO: update-demo-nautilus-xf6w6 is verified up and running
STEP: using delete to clean up resources 01/04/23 23:48:25.681
Jan  4 23:48:25.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 delete --grace-period=0 --force -f -'
Jan  4 23:48:25.758: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  4 23:48:25.758: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  4 23:48:25.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get rc,svc -l name=update-demo --no-headers'
Jan  4 23:48:25.851: INFO: stderr: "No resources found in kubectl-9870 namespace.\n"
Jan  4 23:48:25.851: INFO: stdout: ""
Jan  4 23:48:25.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  4 23:48:26.044: INFO: stderr: ""
Jan  4 23:48:26.045: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:26.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9870" for this suite. 01/04/23 23:48:26.054
------------------------------
• [SLOW TEST] [10.021 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:16.04
    Jan  4 23:48:16.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 23:48:16.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:16.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:16.064
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/04/23 23:48:16.067
    Jan  4 23:48:16.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 create -f -'
    Jan  4 23:48:17.005: INFO: stderr: ""
    Jan  4 23:48:17.005: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/04/23 23:48:17.005
    Jan  4 23:48:17.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  4 23:48:17.200: INFO: stderr: ""
    Jan  4 23:48:17.200: INFO: stdout: "update-demo-nautilus-qj4f5 update-demo-nautilus-vr8kr "
    Jan  4 23:48:17.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-qj4f5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 23:48:17.364: INFO: stderr: ""
    Jan  4 23:48:17.364: INFO: stdout: ""
    Jan  4 23:48:17.364: INFO: update-demo-nautilus-qj4f5 is created but not running
    Jan  4 23:48:22.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  4 23:48:22.443: INFO: stderr: ""
    Jan  4 23:48:22.443: INFO: stdout: "update-demo-nautilus-qj4f5 update-demo-nautilus-vr8kr "
    Jan  4 23:48:22.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-qj4f5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 23:48:22.515: INFO: stderr: ""
    Jan  4 23:48:22.515: INFO: stdout: "true"
    Jan  4 23:48:22.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-qj4f5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  4 23:48:22.588: INFO: stderr: ""
    Jan  4 23:48:22.588: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  4 23:48:22.588: INFO: validating pod update-demo-nautilus-qj4f5
    Jan  4 23:48:22.594: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  4 23:48:22.594: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  4 23:48:22.594: INFO: update-demo-nautilus-qj4f5 is verified up and running
    Jan  4 23:48:22.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 23:48:22.719: INFO: stderr: ""
    Jan  4 23:48:22.719: INFO: stdout: "true"
    Jan  4 23:48:22.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  4 23:48:22.828: INFO: stderr: ""
    Jan  4 23:48:22.828: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  4 23:48:22.828: INFO: validating pod update-demo-nautilus-vr8kr
    Jan  4 23:48:22.839: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  4 23:48:22.839: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  4 23:48:22.839: INFO: update-demo-nautilus-vr8kr is verified up and running
    STEP: scaling down the replication controller 01/04/23 23:48:22.839
    Jan  4 23:48:22.840: INFO: scanned /root for discovery docs: <nil>
    Jan  4 23:48:22.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan  4 23:48:23.953: INFO: stderr: ""
    Jan  4 23:48:23.953: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/04/23 23:48:23.953
    Jan  4 23:48:23.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  4 23:48:24.021: INFO: stderr: ""
    Jan  4 23:48:24.021: INFO: stdout: "update-demo-nautilus-vr8kr "
    Jan  4 23:48:24.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 23:48:24.086: INFO: stderr: ""
    Jan  4 23:48:24.086: INFO: stdout: "true"
    Jan  4 23:48:24.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  4 23:48:24.154: INFO: stderr: ""
    Jan  4 23:48:24.154: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  4 23:48:24.154: INFO: validating pod update-demo-nautilus-vr8kr
    Jan  4 23:48:24.165: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  4 23:48:24.165: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  4 23:48:24.165: INFO: update-demo-nautilus-vr8kr is verified up and running
    STEP: scaling up the replication controller 01/04/23 23:48:24.165
    Jan  4 23:48:24.166: INFO: scanned /root for discovery docs: <nil>
    Jan  4 23:48:24.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan  4 23:48:25.248: INFO: stderr: ""
    Jan  4 23:48:25.248: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/04/23 23:48:25.248
    Jan  4 23:48:25.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  4 23:48:25.315: INFO: stderr: ""
    Jan  4 23:48:25.315: INFO: stdout: "update-demo-nautilus-vr8kr update-demo-nautilus-xf6w6 "
    Jan  4 23:48:25.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 23:48:25.391: INFO: stderr: ""
    Jan  4 23:48:25.391: INFO: stdout: "true"
    Jan  4 23:48:25.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-vr8kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  4 23:48:25.525: INFO: stderr: ""
    Jan  4 23:48:25.525: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  4 23:48:25.525: INFO: validating pod update-demo-nautilus-vr8kr
    Jan  4 23:48:25.529: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  4 23:48:25.529: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  4 23:48:25.529: INFO: update-demo-nautilus-vr8kr is verified up and running
    Jan  4 23:48:25.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-xf6w6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  4 23:48:25.597: INFO: stderr: ""
    Jan  4 23:48:25.597: INFO: stdout: "true"
    Jan  4 23:48:25.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods update-demo-nautilus-xf6w6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  4 23:48:25.674: INFO: stderr: ""
    Jan  4 23:48:25.674: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  4 23:48:25.674: INFO: validating pod update-demo-nautilus-xf6w6
    Jan  4 23:48:25.681: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  4 23:48:25.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  4 23:48:25.681: INFO: update-demo-nautilus-xf6w6 is verified up and running
    STEP: using delete to clean up resources 01/04/23 23:48:25.681
    Jan  4 23:48:25.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 delete --grace-period=0 --force -f -'
    Jan  4 23:48:25.758: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  4 23:48:25.758: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  4 23:48:25.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get rc,svc -l name=update-demo --no-headers'
    Jan  4 23:48:25.851: INFO: stderr: "No resources found in kubectl-9870 namespace.\n"
    Jan  4 23:48:25.851: INFO: stdout: ""
    Jan  4 23:48:25.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-9870 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  4 23:48:26.044: INFO: stderr: ""
    Jan  4 23:48:26.045: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:26.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9870" for this suite. 01/04/23 23:48:26.054
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:26.061
Jan  4 23:48:26.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubelet-test 01/04/23 23:48:26.062
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:26.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:26.085
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan  4 23:48:26.099: INFO: Waiting up to 5m0s for pod "busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1" in namespace "kubelet-test-1255" to be "running and ready"
Jan  4 23:48:26.102: INFO: Pod "busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.590912ms
Jan  4 23:48:26.102: INFO: The phase of Pod busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1 is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:48:28.105: INFO: Pod "busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006147826s
Jan  4 23:48:28.105: INFO: The phase of Pod busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1 is Running (Ready = true)
Jan  4 23:48:28.105: INFO: Pod "busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:28.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1255" for this suite. 01/04/23 23:48:28.122
------------------------------
• [2.066 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:26.061
    Jan  4 23:48:26.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubelet-test 01/04/23 23:48:26.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:26.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:26.085
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan  4 23:48:26.099: INFO: Waiting up to 5m0s for pod "busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1" in namespace "kubelet-test-1255" to be "running and ready"
    Jan  4 23:48:26.102: INFO: Pod "busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.590912ms
    Jan  4 23:48:26.102: INFO: The phase of Pod busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:48:28.105: INFO: Pod "busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006147826s
    Jan  4 23:48:28.105: INFO: The phase of Pod busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1 is Running (Ready = true)
    Jan  4 23:48:28.105: INFO: Pod "busybox-readonly-fse8df00ed-4965-4683-9803-9e9f8f4f09a1" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:28.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1255" for this suite. 01/04/23 23:48:28.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:28.128
Jan  4 23:48:28.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename dns 01/04/23 23:48:28.129
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:28.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:28.147
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/04/23 23:48:28.15
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/04/23 23:48:28.15
STEP: creating a pod to probe DNS 01/04/23 23:48:28.15
STEP: submitting the pod to kubernetes 01/04/23 23:48:28.15
Jan  4 23:48:28.160: INFO: Waiting up to 15m0s for pod "dns-test-c71368cd-80d5-423f-a971-c915c742cb68" in namespace "dns-8690" to be "running"
Jan  4 23:48:28.166: INFO: Pod "dns-test-c71368cd-80d5-423f-a971-c915c742cb68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.593488ms
Jan  4 23:48:30.170: INFO: Pod "dns-test-c71368cd-80d5-423f-a971-c915c742cb68": Phase="Running", Reason="", readiness=true. Elapsed: 2.009221281s
Jan  4 23:48:30.170: INFO: Pod "dns-test-c71368cd-80d5-423f-a971-c915c742cb68" satisfied condition "running"
STEP: retrieving the pod 01/04/23 23:48:30.17
STEP: looking for the results for each expected name from probers 01/04/23 23:48:30.174
Jan  4 23:48:30.190: INFO: DNS probes using dns-8690/dns-test-c71368cd-80d5-423f-a971-c915c742cb68 succeeded

STEP: deleting the pod 01/04/23 23:48:30.19
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:30.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8690" for this suite. 01/04/23 23:48:30.226
------------------------------
• [2.107 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:28.128
    Jan  4 23:48:28.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename dns 01/04/23 23:48:28.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:28.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:28.147
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/04/23 23:48:28.15
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/04/23 23:48:28.15
    STEP: creating a pod to probe DNS 01/04/23 23:48:28.15
    STEP: submitting the pod to kubernetes 01/04/23 23:48:28.15
    Jan  4 23:48:28.160: INFO: Waiting up to 15m0s for pod "dns-test-c71368cd-80d5-423f-a971-c915c742cb68" in namespace "dns-8690" to be "running"
    Jan  4 23:48:28.166: INFO: Pod "dns-test-c71368cd-80d5-423f-a971-c915c742cb68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.593488ms
    Jan  4 23:48:30.170: INFO: Pod "dns-test-c71368cd-80d5-423f-a971-c915c742cb68": Phase="Running", Reason="", readiness=true. Elapsed: 2.009221281s
    Jan  4 23:48:30.170: INFO: Pod "dns-test-c71368cd-80d5-423f-a971-c915c742cb68" satisfied condition "running"
    STEP: retrieving the pod 01/04/23 23:48:30.17
    STEP: looking for the results for each expected name from probers 01/04/23 23:48:30.174
    Jan  4 23:48:30.190: INFO: DNS probes using dns-8690/dns-test-c71368cd-80d5-423f-a971-c915c742cb68 succeeded

    STEP: deleting the pod 01/04/23 23:48:30.19
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:30.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8690" for this suite. 01/04/23 23:48:30.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:30.237
Jan  4 23:48:30.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename webhook 01/04/23 23:48:30.238
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:30.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:30.265
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/04/23 23:48:30.28
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:48:31.119
STEP: Deploying the webhook pod 01/04/23 23:48:31.139
STEP: Wait for the deployment to be ready 01/04/23 23:48:31.157
Jan  4 23:48:31.172: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/04/23 23:48:33.186
STEP: Verifying the service has paired with the endpoint 01/04/23 23:48:33.202
Jan  4 23:48:34.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/04/23 23:48:34.206
STEP: create a configmap that should be updated by the webhook 01/04/23 23:48:34.229
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:34.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3680" for this suite. 01/04/23 23:48:34.298
STEP: Destroying namespace "webhook-3680-markers" for this suite. 01/04/23 23:48:34.323
------------------------------
• [4.097 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:30.237
    Jan  4 23:48:30.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename webhook 01/04/23 23:48:30.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:30.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:30.265
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/04/23 23:48:30.28
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/04/23 23:48:31.119
    STEP: Deploying the webhook pod 01/04/23 23:48:31.139
    STEP: Wait for the deployment to be ready 01/04/23 23:48:31.157
    Jan  4 23:48:31.172: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/04/23 23:48:33.186
    STEP: Verifying the service has paired with the endpoint 01/04/23 23:48:33.202
    Jan  4 23:48:34.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/04/23 23:48:34.206
    STEP: create a configmap that should be updated by the webhook 01/04/23 23:48:34.229
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:34.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3680" for this suite. 01/04/23 23:48:34.298
    STEP: Destroying namespace "webhook-3680-markers" for this suite. 01/04/23 23:48:34.323
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:34.334
Jan  4 23:48:34.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename containers 01/04/23 23:48:34.335
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:34.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:34.365
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan  4 23:48:34.375: INFO: Waiting up to 5m0s for pod "client-containers-c44a7159-c27f-47a2-9fb8-5ed6cb432c7d" in namespace "containers-7837" to be "running"
Jan  4 23:48:34.378: INFO: Pod "client-containers-c44a7159-c27f-47a2-9fb8-5ed6cb432c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.759274ms
Jan  4 23:48:36.382: INFO: Pod "client-containers-c44a7159-c27f-47a2-9fb8-5ed6cb432c7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007540936s
Jan  4 23:48:36.382: INFO: Pod "client-containers-c44a7159-c27f-47a2-9fb8-5ed6cb432c7d" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:36.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7837" for this suite. 01/04/23 23:48:36.403
------------------------------
• [2.078 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:34.334
    Jan  4 23:48:34.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename containers 01/04/23 23:48:34.335
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:34.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:34.365
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan  4 23:48:34.375: INFO: Waiting up to 5m0s for pod "client-containers-c44a7159-c27f-47a2-9fb8-5ed6cb432c7d" in namespace "containers-7837" to be "running"
    Jan  4 23:48:34.378: INFO: Pod "client-containers-c44a7159-c27f-47a2-9fb8-5ed6cb432c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.759274ms
    Jan  4 23:48:36.382: INFO: Pod "client-containers-c44a7159-c27f-47a2-9fb8-5ed6cb432c7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007540936s
    Jan  4 23:48:36.382: INFO: Pod "client-containers-c44a7159-c27f-47a2-9fb8-5ed6cb432c7d" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:36.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7837" for this suite. 01/04/23 23:48:36.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:36.415
Jan  4 23:48:36.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename job 01/04/23 23:48:36.416
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:36.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:36.433
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/04/23 23:48:36.436
STEP: Ensure pods equal to parallelism count is attached to the job 01/04/23 23:48:36.443
STEP: patching /status 01/04/23 23:48:38.449
STEP: updating /status 01/04/23 23:48:38.459
STEP: get /status 01/04/23 23:48:38.5
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:38.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7205" for this suite. 01/04/23 23:48:38.517
------------------------------
• [2.116 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:36.415
    Jan  4 23:48:36.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename job 01/04/23 23:48:36.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:36.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:36.433
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/04/23 23:48:36.436
    STEP: Ensure pods equal to parallelism count is attached to the job 01/04/23 23:48:36.443
    STEP: patching /status 01/04/23 23:48:38.449
    STEP: updating /status 01/04/23 23:48:38.459
    STEP: get /status 01/04/23 23:48:38.5
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:38.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7205" for this suite. 01/04/23 23:48:38.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:38.532
Jan  4 23:48:38.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename gc 01/04/23 23:48:38.533
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:38.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:38.566
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/04/23 23:48:38.584
STEP: Wait for the Deployment to create new ReplicaSet 01/04/23 23:48:38.595
STEP: delete the deployment 01/04/23 23:48:39.112
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/04/23 23:48:39.145
STEP: Gathering metrics 01/04/23 23:48:39.672
Jan  4 23:48:39.698: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
Jan  4 23:48:39.701: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.324511ms
Jan  4 23:48:39.701: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
Jan  4 23:48:39.701: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
Jan  4 23:48:39.735: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:39.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9007" for this suite. 01/04/23 23:48:39.74
------------------------------
• [1.214 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:38.532
    Jan  4 23:48:38.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename gc 01/04/23 23:48:38.533
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:38.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:38.566
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/04/23 23:48:38.584
    STEP: Wait for the Deployment to create new ReplicaSet 01/04/23 23:48:38.595
    STEP: delete the deployment 01/04/23 23:48:39.112
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/04/23 23:48:39.145
    STEP: Gathering metrics 01/04/23 23:48:39.672
    Jan  4 23:48:39.698: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Jan  4 23:48:39.701: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.324511ms
    Jan  4 23:48:39.701: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
    Jan  4 23:48:39.701: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
    Jan  4 23:48:39.735: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:39.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9007" for this suite. 01/04/23 23:48:39.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:39.749
Jan  4 23:48:39.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:48:39.75
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:39.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:39.766
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan  4 23:48:39.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/04/23 23:48:42.249
Jan  4 23:48:42.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 --namespace=crd-publish-openapi-1171 create -f -'
Jan  4 23:48:42.808: INFO: stderr: ""
Jan  4 23:48:42.808: INFO: stdout: "e2e-test-crd-publish-openapi-1348-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  4 23:48:42.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 --namespace=crd-publish-openapi-1171 delete e2e-test-crd-publish-openapi-1348-crds test-cr'
Jan  4 23:48:42.886: INFO: stderr: ""
Jan  4 23:48:42.886: INFO: stdout: "e2e-test-crd-publish-openapi-1348-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan  4 23:48:42.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 --namespace=crd-publish-openapi-1171 apply -f -'
Jan  4 23:48:43.187: INFO: stderr: ""
Jan  4 23:48:43.187: INFO: stdout: "e2e-test-crd-publish-openapi-1348-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  4 23:48:43.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 --namespace=crd-publish-openapi-1171 delete e2e-test-crd-publish-openapi-1348-crds test-cr'
Jan  4 23:48:43.279: INFO: stderr: ""
Jan  4 23:48:43.279: INFO: stdout: "e2e-test-crd-publish-openapi-1348-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/04/23 23:48:43.279
Jan  4 23:48:43.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 explain e2e-test-crd-publish-openapi-1348-crds'
Jan  4 23:48:43.975: INFO: stderr: ""
Jan  4 23:48:43.975: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1348-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:46.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1171" for this suite. 01/04/23 23:48:46.032
------------------------------
• [SLOW TEST] [6.289 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:39.749
    Jan  4 23:48:39.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename crd-publish-openapi 01/04/23 23:48:39.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:39.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:39.766
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan  4 23:48:39.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/04/23 23:48:42.249
    Jan  4 23:48:42.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 --namespace=crd-publish-openapi-1171 create -f -'
    Jan  4 23:48:42.808: INFO: stderr: ""
    Jan  4 23:48:42.808: INFO: stdout: "e2e-test-crd-publish-openapi-1348-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  4 23:48:42.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 --namespace=crd-publish-openapi-1171 delete e2e-test-crd-publish-openapi-1348-crds test-cr'
    Jan  4 23:48:42.886: INFO: stderr: ""
    Jan  4 23:48:42.886: INFO: stdout: "e2e-test-crd-publish-openapi-1348-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan  4 23:48:42.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 --namespace=crd-publish-openapi-1171 apply -f -'
    Jan  4 23:48:43.187: INFO: stderr: ""
    Jan  4 23:48:43.187: INFO: stdout: "e2e-test-crd-publish-openapi-1348-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  4 23:48:43.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 --namespace=crd-publish-openapi-1171 delete e2e-test-crd-publish-openapi-1348-crds test-cr'
    Jan  4 23:48:43.279: INFO: stderr: ""
    Jan  4 23:48:43.279: INFO: stdout: "e2e-test-crd-publish-openapi-1348-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/04/23 23:48:43.279
    Jan  4 23:48:43.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=crd-publish-openapi-1171 explain e2e-test-crd-publish-openapi-1348-crds'
    Jan  4 23:48:43.975: INFO: stderr: ""
    Jan  4 23:48:43.975: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1348-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:46.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1171" for this suite. 01/04/23 23:48:46.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:46.041
Jan  4 23:48:46.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename deployment 01/04/23 23:48:46.042
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:46.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:46.06
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan  4 23:48:46.063: INFO: Creating deployment "webserver-deployment"
Jan  4 23:48:46.068: INFO: Waiting for observed generation 1
Jan  4 23:48:48.074: INFO: Waiting for all required pods to come up
Jan  4 23:48:48.078: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/04/23 23:48:48.078
Jan  4 23:48:48.078: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4bw62" in namespace "deployment-861" to be "running"
Jan  4 23:48:48.078: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-njnf6" in namespace "deployment-861" to be "running"
Jan  4 23:48:48.078: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9bwb4" in namespace "deployment-861" to be "running"
Jan  4 23:48:48.078: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nttsb" in namespace "deployment-861" to be "running"
Jan  4 23:48:48.081: INFO: Pod "webserver-deployment-7f5969cbc7-4bw62": Phase="Pending", Reason="", readiness=false. Elapsed: 3.44196ms
Jan  4 23:48:48.082: INFO: Pod "webserver-deployment-7f5969cbc7-nttsb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95769ms
Jan  4 23:48:48.082: INFO: Pod "webserver-deployment-7f5969cbc7-9bwb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146608ms
Jan  4 23:48:48.082: INFO: Pod "webserver-deployment-7f5969cbc7-njnf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33621ms
Jan  4 23:48:50.086: INFO: Pod "webserver-deployment-7f5969cbc7-njnf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352983s
Jan  4 23:48:50.086: INFO: Pod "webserver-deployment-7f5969cbc7-njnf6" satisfied condition "running"
Jan  4 23:48:50.086: INFO: Pod "webserver-deployment-7f5969cbc7-4bw62": Phase="Running", Reason="", readiness=true. Elapsed: 2.008686185s
Jan  4 23:48:50.086: INFO: Pod "webserver-deployment-7f5969cbc7-4bw62" satisfied condition "running"
Jan  4 23:48:50.087: INFO: Pod "webserver-deployment-7f5969cbc7-9bwb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009116678s
Jan  4 23:48:50.087: INFO: Pod "webserver-deployment-7f5969cbc7-9bwb4" satisfied condition "running"
Jan  4 23:48:50.087: INFO: Pod "webserver-deployment-7f5969cbc7-nttsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009050477s
Jan  4 23:48:50.087: INFO: Pod "webserver-deployment-7f5969cbc7-nttsb" satisfied condition "running"
Jan  4 23:48:50.087: INFO: Waiting for deployment "webserver-deployment" to complete
Jan  4 23:48:50.092: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan  4 23:48:50.100: INFO: Updating deployment webserver-deployment
Jan  4 23:48:50.100: INFO: Waiting for observed generation 2
Jan  4 23:48:52.110: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan  4 23:48:52.113: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan  4 23:48:52.115: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  4 23:48:52.123: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan  4 23:48:52.123: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan  4 23:48:52.125: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  4 23:48:52.131: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan  4 23:48:52.131: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan  4 23:48:52.137: INFO: Updating deployment webserver-deployment
Jan  4 23:48:52.137: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan  4 23:48:52.146: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan  4 23:48:52.156: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  4 23:48:52.182: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-861  5205ac0a-6b76-464a-8c23-2b3fcd66a0c5 74083 3 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040f7788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-04 23:48:50 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-04 23:48:52 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan  4 23:48:52.194: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-861  fb578d93-f2f9-4ad6-a1a9-02464b34d083 74079 3 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5205ac0a-6b76-464a-8c23-2b3fcd66a0c5 0xc0040f7c47 0xc0040f7c48}] [] [{kube-controller-manager Update apps/v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5205ac0a-6b76-464a-8c23-2b3fcd66a0c5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040f7ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  4 23:48:52.194: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan  4 23:48:52.204: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-861  1b847150-7b07-4299-a073-4b7c7823af39 74078 3 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5205ac0a-6b76-464a-8c23-2b3fcd66a0c5 0xc0040f7b57 0xc0040f7b58}] [] [{kube-controller-manager Update apps/v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5205ac0a-6b76-464a-8c23-2b3fcd66a0c5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040f7be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan  4 23:48:52.255: INFO: Pod "webserver-deployment-7f5969cbc7-2kdlq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2kdlq webserver-deployment-7f5969cbc7- deployment-861  649bcb40-0356-4c01-9875-501d3a255822 73952 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:673782f4cf41fcee98c5b433f0cd2486bbe18262049ca50029e6177af88115f4 cni.projectcalico.org/podIP:10.42.0.112/32 cni.projectcalico.org/podIPs:10.42.0.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc00466f887 0xc00466f888}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4kdnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4kdnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-54.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.54,PodIP:10.42.0.112,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://86ce7acf463848ec1eed8eb95ffb2a1be1dc17b4435d9cccd67ecf2afe58f6c2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.256: INFO: Pod "webserver-deployment-7f5969cbc7-4bw62" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4bw62 webserver-deployment-7f5969cbc7- deployment-861  aeb56183-aa00-4979-9e41-c50f6205bccb 73966 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c55c1e090771c0eeffd1e1cfc0d22de477e9849eaa6945a512d730b16ef42f5b cni.projectcalico.org/podIP:10.42.2.143/32 cni.projectcalico.org/podIPs:10.42.2.143/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc00466fc40 0xc00466fc41}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5788d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5788d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.62,PodIP:10.42.2.143,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c77da74e09f8f4870f48e4837720247d0ea384b52468f82f6eb80c4b528144ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.256: INFO: Pod "webserver-deployment-7f5969cbc7-bnh59" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bnh59 webserver-deployment-7f5969cbc7- deployment-861  673ed802-76a9-462b-b62d-9d1d6769919f 73944 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:59517dae3568f7daf71a37f1d20abc915e68be8a097c7a4cfd1686f49c9441ec cni.projectcalico.org/podIP:10.42.1.118/32 cni.projectcalico.org/podIPs:10.42.1.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc00466ffe0 0xc00466ffe1}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h94pl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h94pl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.240,PodIP:10.42.1.118,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7ca1a9123674433e8c77498e07c9dd8b8b0ebfa00345f3a37062d6650125c473,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.257: INFO: Pod "webserver-deployment-7f5969cbc7-fxkd7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fxkd7 webserver-deployment-7f5969cbc7- deployment-861  5de39f17-ecd8-4771-a8d5-fa2ecb10a491 74099 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc1f0 0xc004edc1f1}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grsjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grsjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.257: INFO: Pod "webserver-deployment-7f5969cbc7-gkx89" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gkx89 webserver-deployment-7f5969cbc7- deployment-861  7b1ad4d2-7389-455c-be2f-c5399e553b6a 73923 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e922d9d1d5773a8c3e0332f4064303fa7c40a5bdf1e2b25c6a905ccf52f60356 cni.projectcalico.org/podIP:10.42.3.163/32 cni.projectcalico.org/podIPs:10.42.3.163/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc347 0xc004edc348}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrpgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrpgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.163,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://21f87380c9aa4d50fc4471bf770be9c8e8f9b99c19171faf9b9fea55bc97b78d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.257: INFO: Pod "webserver-deployment-7f5969cbc7-jzjf4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jzjf4 webserver-deployment-7f5969cbc7- deployment-861  92f7e8f4-e1f3-41d4-a8ff-d08ffeb50196 74097 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc560 0xc004edc561}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmvdm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmvdm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.257: INFO: Pod "webserver-deployment-7f5969cbc7-kpn5h" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kpn5h webserver-deployment-7f5969cbc7- deployment-861  c8974a01-3986-4d85-a7ed-0c0de2763655 73948 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:725229acdb944e6c553c6a2eaba2bc7e72fd954a6562af21b7db8c3bbd5dbef7 cni.projectcalico.org/podIP:10.42.1.117/32 cni.projectcalico.org/podIPs:10.42.1.117/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc6b7 0xc004edc6b8}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwdsk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwdsk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.240,PodIP:10.42.1.117,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2bde441ce07af4d7a3ef7dc2837215963151b769bba232ddb32b98a0f44ee727,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.258: INFO: Pod "webserver-deployment-7f5969cbc7-kwh68" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kwh68 webserver-deployment-7f5969cbc7- deployment-861  c6a1735b-aef4-4db2-ad48-58fb5366bb93 73937 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:19d54266ead5559204505ac21daf583f437a74bb8b39eeb7043d935628d2af3d cni.projectcalico.org/podIP:10.42.2.142/32 cni.projectcalico.org/podIPs:10.42.2.142/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc8c0 0xc004edc8c1}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2gv8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2gv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.62,PodIP:10.42.2.142,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://30e83698f90c1d01f4f3f1f4f3a08a740421f5ac98d6fef7b3a331fb8dba73d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.258: INFO: Pod "webserver-deployment-7f5969cbc7-m8r4s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m8r4s webserver-deployment-7f5969cbc7- deployment-861  de6efeaf-908e-4b8f-a2c9-31e3e491499f 74094 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edcab0 0xc004edcab1}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4jzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4jzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.258: INFO: Pod "webserver-deployment-7f5969cbc7-ncx47" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ncx47 webserver-deployment-7f5969cbc7- deployment-861  ed18d414-851c-47b8-a9cc-ae39499fa278 74096 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edcc00 0xc004edcc01}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p5xkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p5xkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.258: INFO: Pod "webserver-deployment-7f5969cbc7-njnf6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-njnf6 webserver-deployment-7f5969cbc7- deployment-861  d2f13d45-d2da-4d7b-8f5f-427817f6c790 73960 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4540afb07e31babb9c9020fa858206c2abdccc80060a849152d722691e709510 cni.projectcalico.org/podIP:10.42.3.164/32 cni.projectcalico.org/podIPs:10.42.3.164/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edcd57 0xc004edcd58}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9mdpz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9mdpz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.164,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c03db3e8025336534d7f6761bc3c2bde13398453c3515621e7341e1f1bb64ca8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.259: INFO: Pod "webserver-deployment-7f5969cbc7-pvj74" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pvj74 webserver-deployment-7f5969cbc7- deployment-861  20d15537-5f9b-4c41-a46f-eea34af91cfb 74100 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edcf50 0xc004edcf51}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8s4bn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8s4bn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:,StartTime:2023-01-04 23:48:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-rts2g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rts2g webserver-deployment-7f5969cbc7- deployment-861  911ea90b-db3d-4f15-8021-6537dc8977a8 74098 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edd117 0xc004edd118}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5cxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5cxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-vkp5s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vkp5s webserver-deployment-7f5969cbc7- deployment-861  58fe9115-993c-4fc1-93c1-1a4bd15d9cd0 74095 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edd257 0xc004edd258}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cgmdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cgmdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-wgvl7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wgvl7 webserver-deployment-7f5969cbc7- deployment-861  59661edd-d57f-4cb1-bcc7-4c1360a42658 73946 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:87492ce69340b9bb8ecb8bf534ec84c2b366958326536a63d57e744570aaee2a cni.projectcalico.org/podIP:10.42.0.111/32 cni.projectcalico.org/podIPs:10.42.0.111/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edd3d0 0xc004edd3d1}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-92d7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-92d7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-54.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.54,PodIP:10.42.0.111,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://19836154681ccbfcc124091d823d76f61b61a4cd1a4cc7c5585c0229b48290a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-d9f79cb5-257t5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-257t5 webserver-deployment-d9f79cb5- deployment-861  1d8a9c86-af7d-48b1-b647-eea1e9526187 74075 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1fc2d2377ab97c269d10f7341b5989f887ff104872cb46b94629e2656f6657ed cni.projectcalico.org/podIP:10.42.0.113/32 cni.projectcalico.org/podIPs:10.42.0.113/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004edd5af 0xc004edd5e0}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nz47p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nz47p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-54.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.54,PodIP:10.42.0.113,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-d9f79cb5-5wxm9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5wxm9 webserver-deployment-d9f79cb5- deployment-861  eca21d0a-4c18-4d72-8dd0-3403042e6d89 74105 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004edd8ff 0xc004edd910}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm9jp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm9jp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-d9f79cb5-726xr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-726xr webserver-deployment-d9f79cb5- deployment-861  07dbcce2-d175-433b-83c5-13125a716751 74072 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9b73344ada5320d2fb8704abf0049ae1d163f9880e78b7a5b20fffbbfb211dd3 cni.projectcalico.org/podIP:10.42.1.119/32 cni.projectcalico.org/podIPs:10.42.1.119/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004edda77 0xc004edda78}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqp9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqp9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.240,PodIP:10.42.1.119,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.263: INFO: Pod "webserver-deployment-d9f79cb5-9hfcf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9hfcf webserver-deployment-d9f79cb5- deployment-861  9061ed25-9620-4c72-9094-5bc19c44ddc7 74063 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b32c7a9d653845a9d115c3a45f6947430ae4578d264ba626aab2b15970704347 cni.projectcalico.org/podIP:10.42.2.145/32 cni.projectcalico.org/podIPs:10.42.2.145/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004eddc7f 0xc004eddcb0}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78nq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78nq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.62,PodIP:10.42.2.145,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.264: INFO: Pod "webserver-deployment-d9f79cb5-9w6mf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9w6mf webserver-deployment-d9f79cb5- deployment-861  5d36fd5c-2a3a-44de-8ae2-065e49f3caea 74109 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004eddebf 0xc004edded0}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnwtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnwtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.264: INFO: Pod "webserver-deployment-d9f79cb5-b6hn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b6hn9 webserver-deployment-d9f79cb5- deployment-861  a96137ae-d5fc-4019-86a7-a916d17cb0a3 74106 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c017 0xc00395c018}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdbj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdbj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.264: INFO: Pod "webserver-deployment-d9f79cb5-dvs9l" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dvs9l webserver-deployment-d9f79cb5- deployment-861  2c0190dc-6560-42c1-8300-7a952b13d18d 74107 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c167 0xc00395c168}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbfqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbfqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.265: INFO: Pod "webserver-deployment-d9f79cb5-hc49z" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hc49z webserver-deployment-d9f79cb5- deployment-861  790fe679-0e56-41b0-9f41-849b1cda9227 74088 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c2b7 0xc00395c2b8}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2dfsn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2dfsn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.265: INFO: Pod "webserver-deployment-d9f79cb5-j8q9m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j8q9m webserver-deployment-d9f79cb5- deployment-861  89cca38d-8fd9-46b5-9847-184158c23b44 74104 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c40f 0xc00395c420}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zv89w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zv89w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-54.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.265: INFO: Pod "webserver-deployment-d9f79cb5-trjnq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-trjnq webserver-deployment-d9f79cb5- deployment-861  c5eff518-759f-4d23-b1f8-c229f26097c1 74059 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2dd64b0e516bfa42aa2229c164041adf982e5a68d9cecc2e76c78163ca0cb7b4 cni.projectcalico.org/podIP:10.42.3.166/32 cni.projectcalico.org/podIPs:10.42.3.166/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c56f 0xc00395c5a0}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gsk7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gsk7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.166,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.166,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.265: INFO: Pod "webserver-deployment-d9f79cb5-vj2zs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vj2zs webserver-deployment-d9f79cb5- deployment-861  5805b6b4-0179-42fa-a501-998d6e63734c 74103 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c7bf 0xc00395c7d0}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnrrx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnrrx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  4 23:48:52.266: INFO: Pod "webserver-deployment-d9f79cb5-vpb8w" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vpb8w webserver-deployment-d9f79cb5- deployment-861  4c667450-5865-4ed4-a88b-57a9c997d3a0 74056 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b4904d930fbcaafbb47dda529ed8107eecd97ad5b61b4bc0e7ce02aedb33d8bf cni.projectcalico.org/podIP:10.42.3.167/32 cni.projectcalico.org/podIPs:10.42.3.167/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c94f 0xc00395c980}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fln5b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fln5b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.167,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-861" for this suite. 01/04/23 23:48:52.301
------------------------------
• [SLOW TEST] [6.317 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:46.041
    Jan  4 23:48:46.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename deployment 01/04/23 23:48:46.042
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:46.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:46.06
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan  4 23:48:46.063: INFO: Creating deployment "webserver-deployment"
    Jan  4 23:48:46.068: INFO: Waiting for observed generation 1
    Jan  4 23:48:48.074: INFO: Waiting for all required pods to come up
    Jan  4 23:48:48.078: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/04/23 23:48:48.078
    Jan  4 23:48:48.078: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4bw62" in namespace "deployment-861" to be "running"
    Jan  4 23:48:48.078: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-njnf6" in namespace "deployment-861" to be "running"
    Jan  4 23:48:48.078: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9bwb4" in namespace "deployment-861" to be "running"
    Jan  4 23:48:48.078: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nttsb" in namespace "deployment-861" to be "running"
    Jan  4 23:48:48.081: INFO: Pod "webserver-deployment-7f5969cbc7-4bw62": Phase="Pending", Reason="", readiness=false. Elapsed: 3.44196ms
    Jan  4 23:48:48.082: INFO: Pod "webserver-deployment-7f5969cbc7-nttsb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95769ms
    Jan  4 23:48:48.082: INFO: Pod "webserver-deployment-7f5969cbc7-9bwb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146608ms
    Jan  4 23:48:48.082: INFO: Pod "webserver-deployment-7f5969cbc7-njnf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33621ms
    Jan  4 23:48:50.086: INFO: Pod "webserver-deployment-7f5969cbc7-njnf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352983s
    Jan  4 23:48:50.086: INFO: Pod "webserver-deployment-7f5969cbc7-njnf6" satisfied condition "running"
    Jan  4 23:48:50.086: INFO: Pod "webserver-deployment-7f5969cbc7-4bw62": Phase="Running", Reason="", readiness=true. Elapsed: 2.008686185s
    Jan  4 23:48:50.086: INFO: Pod "webserver-deployment-7f5969cbc7-4bw62" satisfied condition "running"
    Jan  4 23:48:50.087: INFO: Pod "webserver-deployment-7f5969cbc7-9bwb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009116678s
    Jan  4 23:48:50.087: INFO: Pod "webserver-deployment-7f5969cbc7-9bwb4" satisfied condition "running"
    Jan  4 23:48:50.087: INFO: Pod "webserver-deployment-7f5969cbc7-nttsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009050477s
    Jan  4 23:48:50.087: INFO: Pod "webserver-deployment-7f5969cbc7-nttsb" satisfied condition "running"
    Jan  4 23:48:50.087: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan  4 23:48:50.092: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan  4 23:48:50.100: INFO: Updating deployment webserver-deployment
    Jan  4 23:48:50.100: INFO: Waiting for observed generation 2
    Jan  4 23:48:52.110: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan  4 23:48:52.113: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan  4 23:48:52.115: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  4 23:48:52.123: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan  4 23:48:52.123: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan  4 23:48:52.125: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  4 23:48:52.131: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan  4 23:48:52.131: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan  4 23:48:52.137: INFO: Updating deployment webserver-deployment
    Jan  4 23:48:52.137: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan  4 23:48:52.146: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan  4 23:48:52.156: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  4 23:48:52.182: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-861  5205ac0a-6b76-464a-8c23-2b3fcd66a0c5 74083 3 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040f7788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-04 23:48:50 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-04 23:48:52 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan  4 23:48:52.194: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-861  fb578d93-f2f9-4ad6-a1a9-02464b34d083 74079 3 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5205ac0a-6b76-464a-8c23-2b3fcd66a0c5 0xc0040f7c47 0xc0040f7c48}] [] [{kube-controller-manager Update apps/v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5205ac0a-6b76-464a-8c23-2b3fcd66a0c5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040f7ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 23:48:52.194: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan  4 23:48:52.204: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-861  1b847150-7b07-4299-a073-4b7c7823af39 74078 3 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5205ac0a-6b76-464a-8c23-2b3fcd66a0c5 0xc0040f7b57 0xc0040f7b58}] [] [{kube-controller-manager Update apps/v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5205ac0a-6b76-464a-8c23-2b3fcd66a0c5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040f7be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan  4 23:48:52.255: INFO: Pod "webserver-deployment-7f5969cbc7-2kdlq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2kdlq webserver-deployment-7f5969cbc7- deployment-861  649bcb40-0356-4c01-9875-501d3a255822 73952 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:673782f4cf41fcee98c5b433f0cd2486bbe18262049ca50029e6177af88115f4 cni.projectcalico.org/podIP:10.42.0.112/32 cni.projectcalico.org/podIPs:10.42.0.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc00466f887 0xc00466f888}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4kdnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4kdnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-54.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.54,PodIP:10.42.0.112,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://86ce7acf463848ec1eed8eb95ffb2a1be1dc17b4435d9cccd67ecf2afe58f6c2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.256: INFO: Pod "webserver-deployment-7f5969cbc7-4bw62" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4bw62 webserver-deployment-7f5969cbc7- deployment-861  aeb56183-aa00-4979-9e41-c50f6205bccb 73966 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c55c1e090771c0eeffd1e1cfc0d22de477e9849eaa6945a512d730b16ef42f5b cni.projectcalico.org/podIP:10.42.2.143/32 cni.projectcalico.org/podIPs:10.42.2.143/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc00466fc40 0xc00466fc41}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5788d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5788d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.62,PodIP:10.42.2.143,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c77da74e09f8f4870f48e4837720247d0ea384b52468f82f6eb80c4b528144ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.256: INFO: Pod "webserver-deployment-7f5969cbc7-bnh59" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bnh59 webserver-deployment-7f5969cbc7- deployment-861  673ed802-76a9-462b-b62d-9d1d6769919f 73944 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:59517dae3568f7daf71a37f1d20abc915e68be8a097c7a4cfd1686f49c9441ec cni.projectcalico.org/podIP:10.42.1.118/32 cni.projectcalico.org/podIPs:10.42.1.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc00466ffe0 0xc00466ffe1}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h94pl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h94pl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.240,PodIP:10.42.1.118,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7ca1a9123674433e8c77498e07c9dd8b8b0ebfa00345f3a37062d6650125c473,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.257: INFO: Pod "webserver-deployment-7f5969cbc7-fxkd7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fxkd7 webserver-deployment-7f5969cbc7- deployment-861  5de39f17-ecd8-4771-a8d5-fa2ecb10a491 74099 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc1f0 0xc004edc1f1}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grsjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grsjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.257: INFO: Pod "webserver-deployment-7f5969cbc7-gkx89" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gkx89 webserver-deployment-7f5969cbc7- deployment-861  7b1ad4d2-7389-455c-be2f-c5399e553b6a 73923 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e922d9d1d5773a8c3e0332f4064303fa7c40a5bdf1e2b25c6a905ccf52f60356 cni.projectcalico.org/podIP:10.42.3.163/32 cni.projectcalico.org/podIPs:10.42.3.163/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc347 0xc004edc348}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrpgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrpgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.163,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://21f87380c9aa4d50fc4471bf770be9c8e8f9b99c19171faf9b9fea55bc97b78d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.257: INFO: Pod "webserver-deployment-7f5969cbc7-jzjf4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jzjf4 webserver-deployment-7f5969cbc7- deployment-861  92f7e8f4-e1f3-41d4-a8ff-d08ffeb50196 74097 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc560 0xc004edc561}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmvdm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmvdm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.257: INFO: Pod "webserver-deployment-7f5969cbc7-kpn5h" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kpn5h webserver-deployment-7f5969cbc7- deployment-861  c8974a01-3986-4d85-a7ed-0c0de2763655 73948 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:725229acdb944e6c553c6a2eaba2bc7e72fd954a6562af21b7db8c3bbd5dbef7 cni.projectcalico.org/podIP:10.42.1.117/32 cni.projectcalico.org/podIPs:10.42.1.117/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc6b7 0xc004edc6b8}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwdsk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwdsk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.240,PodIP:10.42.1.117,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2bde441ce07af4d7a3ef7dc2837215963151b769bba232ddb32b98a0f44ee727,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.258: INFO: Pod "webserver-deployment-7f5969cbc7-kwh68" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kwh68 webserver-deployment-7f5969cbc7- deployment-861  c6a1735b-aef4-4db2-ad48-58fb5366bb93 73937 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:19d54266ead5559204505ac21daf583f437a74bb8b39eeb7043d935628d2af3d cni.projectcalico.org/podIP:10.42.2.142/32 cni.projectcalico.org/podIPs:10.42.2.142/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edc8c0 0xc004edc8c1}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2gv8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2gv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.62,PodIP:10.42.2.142,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://30e83698f90c1d01f4f3f1f4f3a08a740421f5ac98d6fef7b3a331fb8dba73d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.258: INFO: Pod "webserver-deployment-7f5969cbc7-m8r4s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m8r4s webserver-deployment-7f5969cbc7- deployment-861  de6efeaf-908e-4b8f-a2c9-31e3e491499f 74094 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edcab0 0xc004edcab1}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4jzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4jzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.258: INFO: Pod "webserver-deployment-7f5969cbc7-ncx47" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ncx47 webserver-deployment-7f5969cbc7- deployment-861  ed18d414-851c-47b8-a9cc-ae39499fa278 74096 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edcc00 0xc004edcc01}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p5xkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p5xkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.258: INFO: Pod "webserver-deployment-7f5969cbc7-njnf6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-njnf6 webserver-deployment-7f5969cbc7- deployment-861  d2f13d45-d2da-4d7b-8f5f-427817f6c790 73960 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4540afb07e31babb9c9020fa858206c2abdccc80060a849152d722691e709510 cni.projectcalico.org/podIP:10.42.3.164/32 cni.projectcalico.org/podIPs:10.42.3.164/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edcd57 0xc004edcd58}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9mdpz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9mdpz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.164,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c03db3e8025336534d7f6761bc3c2bde13398453c3515621e7341e1f1bb64ca8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.259: INFO: Pod "webserver-deployment-7f5969cbc7-pvj74" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pvj74 webserver-deployment-7f5969cbc7- deployment-861  20d15537-5f9b-4c41-a46f-eea34af91cfb 74100 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edcf50 0xc004edcf51}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8s4bn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8s4bn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:,StartTime:2023-01-04 23:48:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-rts2g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rts2g webserver-deployment-7f5969cbc7- deployment-861  911ea90b-db3d-4f15-8021-6537dc8977a8 74098 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edd117 0xc004edd118}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5cxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5cxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-vkp5s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vkp5s webserver-deployment-7f5969cbc7- deployment-861  58fe9115-993c-4fc1-93c1-1a4bd15d9cd0 74095 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edd257 0xc004edd258}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cgmdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cgmdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-wgvl7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wgvl7 webserver-deployment-7f5969cbc7- deployment-861  59661edd-d57f-4cb1-bcc7-4c1360a42658 73946 0 2023-01-04 23:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:87492ce69340b9bb8ecb8bf534ec84c2b366958326536a63d57e744570aaee2a cni.projectcalico.org/podIP:10.42.0.111/32 cni.projectcalico.org/podIPs:10.42.0.111/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b847150-7b07-4299-a073-4b7c7823af39 0xc004edd3d0 0xc004edd3d1}] [] [{calico Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b847150-7b07-4299-a073-4b7c7823af39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-92d7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-92d7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-54.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.54,PodIP:10.42.0.111,StartTime:2023-01-04 23:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-04 23:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://19836154681ccbfcc124091d823d76f61b61a4cd1a4cc7c5585c0229b48290a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-d9f79cb5-257t5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-257t5 webserver-deployment-d9f79cb5- deployment-861  1d8a9c86-af7d-48b1-b647-eea1e9526187 74075 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1fc2d2377ab97c269d10f7341b5989f887ff104872cb46b94629e2656f6657ed cni.projectcalico.org/podIP:10.42.0.113/32 cni.projectcalico.org/podIPs:10.42.0.113/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004edd5af 0xc004edd5e0}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nz47p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nz47p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-54.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.54,PodIP:10.42.0.113,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-d9f79cb5-5wxm9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5wxm9 webserver-deployment-d9f79cb5- deployment-861  eca21d0a-4c18-4d72-8dd0-3403042e6d89 74105 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004edd8ff 0xc004edd910}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm9jp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm9jp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.262: INFO: Pod "webserver-deployment-d9f79cb5-726xr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-726xr webserver-deployment-d9f79cb5- deployment-861  07dbcce2-d175-433b-83c5-13125a716751 74072 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9b73344ada5320d2fb8704abf0049ae1d163f9880e78b7a5b20fffbbfb211dd3 cni.projectcalico.org/podIP:10.42.1.119/32 cni.projectcalico.org/podIPs:10.42.1.119/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004edda77 0xc004edda78}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqp9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqp9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.240,PodIP:10.42.1.119,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.263: INFO: Pod "webserver-deployment-d9f79cb5-9hfcf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9hfcf webserver-deployment-d9f79cb5- deployment-861  9061ed25-9620-4c72-9094-5bc19c44ddc7 74063 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b32c7a9d653845a9d115c3a45f6947430ae4578d264ba626aab2b15970704347 cni.projectcalico.org/podIP:10.42.2.145/32 cni.projectcalico.org/podIPs:10.42.2.145/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004eddc7f 0xc004eddcb0}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78nq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78nq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.62,PodIP:10.42.2.145,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.264: INFO: Pod "webserver-deployment-d9f79cb5-9w6mf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9w6mf webserver-deployment-d9f79cb5- deployment-861  5d36fd5c-2a3a-44de-8ae2-065e49f3caea 74109 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc004eddebf 0xc004edded0}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnwtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnwtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.264: INFO: Pod "webserver-deployment-d9f79cb5-b6hn9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b6hn9 webserver-deployment-d9f79cb5- deployment-861  a96137ae-d5fc-4019-86a7-a916d17cb0a3 74106 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c017 0xc00395c018}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdbj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdbj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.264: INFO: Pod "webserver-deployment-d9f79cb5-dvs9l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dvs9l webserver-deployment-d9f79cb5- deployment-861  2c0190dc-6560-42c1-8300-7a952b13d18d 74107 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c167 0xc00395c168}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbfqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbfqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.265: INFO: Pod "webserver-deployment-d9f79cb5-hc49z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hc49z webserver-deployment-d9f79cb5- deployment-861  790fe679-0e56-41b0-9f41-849b1cda9227 74088 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c2b7 0xc00395c2b8}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2dfsn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2dfsn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-62.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.265: INFO: Pod "webserver-deployment-d9f79cb5-j8q9m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j8q9m webserver-deployment-d9f79cb5- deployment-861  89cca38d-8fd9-46b5-9847-184158c23b44 74104 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c40f 0xc00395c420}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zv89w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zv89w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-54.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.265: INFO: Pod "webserver-deployment-d9f79cb5-trjnq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-trjnq webserver-deployment-d9f79cb5- deployment-861  c5eff518-759f-4d23-b1f8-c229f26097c1 74059 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2dd64b0e516bfa42aa2229c164041adf982e5a68d9cecc2e76c78163ca0cb7b4 cni.projectcalico.org/podIP:10.42.3.166/32 cni.projectcalico.org/podIPs:10.42.3.166/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c56f 0xc00395c5a0}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gsk7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gsk7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.166,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.166,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.265: INFO: Pod "webserver-deployment-d9f79cb5-vj2zs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vj2zs webserver-deployment-d9f79cb5- deployment-861  5805b6b4-0179-42fa-a501-998d6e63734c 74103 0 2023-01-04 23:48:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c7bf 0xc00395c7d0}] [] [{kube-controller-manager Update v1 2023-01-04 23:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnrrx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnrrx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-240.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  4 23:48:52.266: INFO: Pod "webserver-deployment-d9f79cb5-vpb8w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vpb8w webserver-deployment-d9f79cb5- deployment-861  4c667450-5865-4ed4-a88b-57a9c997d3a0 74056 0 2023-01-04 23:48:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b4904d930fbcaafbb47dda529ed8107eecd97ad5b61b4bc0e7ce02aedb33d8bf cni.projectcalico.org/podIP:10.42.3.167/32 cni.projectcalico.org/podIPs:10.42.3.167/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fb578d93-f2f9-4ad6-a1a9-02464b34d083 0xc00395c94f 0xc00395c980}] [] [{calico Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-04 23:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb578d93-f2f9-4ad6-a1a9-02464b34d083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-04 23:48:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fln5b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fln5b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-117.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-04 23:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.117,PodIP:10.42.3.167,StartTime:2023-01-04 23:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-861" for this suite. 01/04/23 23:48:52.301
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:52.367
Jan  4 23:48:52.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:48:52.37
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:52.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:52.507
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/04/23 23:48:52.511
Jan  4 23:48:52.524: INFO: Waiting up to 5m0s for pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794" in namespace "emptydir-6510" to be "Succeeded or Failed"
Jan  4 23:48:52.539: INFO: Pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794": Phase="Pending", Reason="", readiness=false. Elapsed: 15.218027ms
Jan  4 23:48:54.542: INFO: Pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018430259s
Jan  4 23:48:56.542: INFO: Pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018797932s
STEP: Saw pod success 01/04/23 23:48:56.543
Jan  4 23:48:56.543: INFO: Pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794" satisfied condition "Succeeded or Failed"
Jan  4 23:48:56.546: INFO: Trying to get logs from node ip-172-31-9-62.us-east-2.compute.internal pod pod-166c55e8-1f08-4794-9832-4882b2bf4794 container test-container: <nil>
STEP: delete the pod 01/04/23 23:48:56.555
Jan  4 23:48:56.566: INFO: Waiting for pod pod-166c55e8-1f08-4794-9832-4882b2bf4794 to disappear
Jan  4 23:48:56.568: INFO: Pod pod-166c55e8-1f08-4794-9832-4882b2bf4794 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:48:56.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6510" for this suite. 01/04/23 23:48:56.572
------------------------------
• [4.210 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:52.367
    Jan  4 23:48:52.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:48:52.37
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:52.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:52.507
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/04/23 23:48:52.511
    Jan  4 23:48:52.524: INFO: Waiting up to 5m0s for pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794" in namespace "emptydir-6510" to be "Succeeded or Failed"
    Jan  4 23:48:52.539: INFO: Pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794": Phase="Pending", Reason="", readiness=false. Elapsed: 15.218027ms
    Jan  4 23:48:54.542: INFO: Pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018430259s
    Jan  4 23:48:56.542: INFO: Pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018797932s
    STEP: Saw pod success 01/04/23 23:48:56.543
    Jan  4 23:48:56.543: INFO: Pod "pod-166c55e8-1f08-4794-9832-4882b2bf4794" satisfied condition "Succeeded or Failed"
    Jan  4 23:48:56.546: INFO: Trying to get logs from node ip-172-31-9-62.us-east-2.compute.internal pod pod-166c55e8-1f08-4794-9832-4882b2bf4794 container test-container: <nil>
    STEP: delete the pod 01/04/23 23:48:56.555
    Jan  4 23:48:56.566: INFO: Waiting for pod pod-166c55e8-1f08-4794-9832-4882b2bf4794 to disappear
    Jan  4 23:48:56.568: INFO: Pod pod-166c55e8-1f08-4794-9832-4882b2bf4794 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:48:56.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6510" for this suite. 01/04/23 23:48:56.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:48:56.581
Jan  4 23:48:56.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename gc 01/04/23 23:48:56.582
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:56.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:56.607
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/04/23 23:48:56.61
STEP: delete the rc 01/04/23 23:49:01.657
STEP: wait for all pods to be garbage collected 01/04/23 23:49:01.7
STEP: Gathering metrics 01/04/23 23:49:06.711
Jan  4 23:49:06.768: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
Jan  4 23:49:06.773: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 5.173101ms
Jan  4 23:49:06.774: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
Jan  4 23:49:06.774: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
Jan  4 23:49:06.838: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  4 23:49:06.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4346" for this suite. 01/04/23 23:49:06.855
------------------------------
• [SLOW TEST] [10.290 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:48:56.581
    Jan  4 23:48:56.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename gc 01/04/23 23:48:56.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:48:56.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:48:56.607
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/04/23 23:48:56.61
    STEP: delete the rc 01/04/23 23:49:01.657
    STEP: wait for all pods to be garbage collected 01/04/23 23:49:01.7
    STEP: Gathering metrics 01/04/23 23:49:06.711
    Jan  4 23:49:06.768: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Jan  4 23:49:06.773: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 5.173101ms
    Jan  4 23:49:06.774: INFO: The phase of Pod kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal is Running (Ready = true)
    Jan  4 23:49:06.774: INFO: Pod "kube-controller-manager-ip-172-31-9-62.us-east-2.compute.internal" satisfied condition "running and ready"
    Jan  4 23:49:06.838: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:49:06.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4346" for this suite. 01/04/23 23:49:06.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:49:06.877
Jan  4 23:49:06.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename ingressclass 01/04/23 23:49:06.878
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:06.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:06.947
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/04/23 23:49:06.95
STEP: getting /apis/networking.k8s.io 01/04/23 23:49:06.957
STEP: getting /apis/networking.k8s.iov1 01/04/23 23:49:06.959
STEP: creating 01/04/23 23:49:06.964
STEP: getting 01/04/23 23:49:06.981
STEP: listing 01/04/23 23:49:06.984
STEP: watching 01/04/23 23:49:06.987
Jan  4 23:49:06.987: INFO: starting watch
STEP: patching 01/04/23 23:49:06.988
STEP: updating 01/04/23 23:49:06.993
Jan  4 23:49:06.998: INFO: waiting for watch events with expected annotations
Jan  4 23:49:06.998: INFO: saw patched and updated annotations
STEP: deleting 01/04/23 23:49:06.998
STEP: deleting a collection 01/04/23 23:49:07.017
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan  4 23:49:07.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-8178" for this suite. 01/04/23 23:49:07.046
------------------------------
• [0.175 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:49:06.877
    Jan  4 23:49:06.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename ingressclass 01/04/23 23:49:06.878
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:06.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:06.947
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/04/23 23:49:06.95
    STEP: getting /apis/networking.k8s.io 01/04/23 23:49:06.957
    STEP: getting /apis/networking.k8s.iov1 01/04/23 23:49:06.959
    STEP: creating 01/04/23 23:49:06.964
    STEP: getting 01/04/23 23:49:06.981
    STEP: listing 01/04/23 23:49:06.984
    STEP: watching 01/04/23 23:49:06.987
    Jan  4 23:49:06.987: INFO: starting watch
    STEP: patching 01/04/23 23:49:06.988
    STEP: updating 01/04/23 23:49:06.993
    Jan  4 23:49:06.998: INFO: waiting for watch events with expected annotations
    Jan  4 23:49:06.998: INFO: saw patched and updated annotations
    STEP: deleting 01/04/23 23:49:06.998
    STEP: deleting a collection 01/04/23 23:49:07.017
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:49:07.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-8178" for this suite. 01/04/23 23:49:07.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:49:07.053
Jan  4 23:49:07.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:49:07.054
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:07.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:07.074
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-9f2aebc0-4e43-4c34-b6d3-d63019bd3c06 01/04/23 23:49:07.08
STEP: Creating configMap with name cm-test-opt-upd-d7bf07d1-d828-414a-8237-6ec930d3a2e4 01/04/23 23:49:07.084
STEP: Creating the pod 01/04/23 23:49:07.09
Jan  4 23:49:07.119: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d" in namespace "projected-191" to be "running and ready"
Jan  4 23:49:07.129: INFO: Pod "pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.084913ms
Jan  4 23:49:07.129: INFO: The phase of Pod pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d is Pending, waiting for it to be Running (with Ready = true)
Jan  4 23:49:09.138: INFO: Pod "pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d": Phase="Running", Reason="", readiness=true. Elapsed: 2.019340895s
Jan  4 23:49:09.138: INFO: The phase of Pod pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d is Running (Ready = true)
Jan  4 23:49:09.138: INFO: Pod "pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9f2aebc0-4e43-4c34-b6d3-d63019bd3c06 01/04/23 23:49:09.182
STEP: Updating configmap cm-test-opt-upd-d7bf07d1-d828-414a-8237-6ec930d3a2e4 01/04/23 23:49:09.188
STEP: Creating configMap with name cm-test-opt-create-3a1c4bf9-f788-490a-a932-dc3578582524 01/04/23 23:49:09.193
STEP: waiting to observe update in volume 01/04/23 23:49:09.198
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:49:11.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-191" for this suite. 01/04/23 23:49:11.234
------------------------------
• [4.189 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:49:07.053
    Jan  4 23:49:07.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:49:07.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:07.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:07.074
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-9f2aebc0-4e43-4c34-b6d3-d63019bd3c06 01/04/23 23:49:07.08
    STEP: Creating configMap with name cm-test-opt-upd-d7bf07d1-d828-414a-8237-6ec930d3a2e4 01/04/23 23:49:07.084
    STEP: Creating the pod 01/04/23 23:49:07.09
    Jan  4 23:49:07.119: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d" in namespace "projected-191" to be "running and ready"
    Jan  4 23:49:07.129: INFO: Pod "pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.084913ms
    Jan  4 23:49:07.129: INFO: The phase of Pod pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d is Pending, waiting for it to be Running (with Ready = true)
    Jan  4 23:49:09.138: INFO: Pod "pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d": Phase="Running", Reason="", readiness=true. Elapsed: 2.019340895s
    Jan  4 23:49:09.138: INFO: The phase of Pod pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d is Running (Ready = true)
    Jan  4 23:49:09.138: INFO: Pod "pod-projected-configmaps-5699421d-3370-4769-bdf2-ba20a860160d" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9f2aebc0-4e43-4c34-b6d3-d63019bd3c06 01/04/23 23:49:09.182
    STEP: Updating configmap cm-test-opt-upd-d7bf07d1-d828-414a-8237-6ec930d3a2e4 01/04/23 23:49:09.188
    STEP: Creating configMap with name cm-test-opt-create-3a1c4bf9-f788-490a-a932-dc3578582524 01/04/23 23:49:09.193
    STEP: waiting to observe update in volume 01/04/23 23:49:09.198
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:49:11.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-191" for this suite. 01/04/23 23:49:11.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:49:11.251
Jan  4 23:49:11.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename projected 01/04/23 23:49:11.251
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:11.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:11.276
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-a94e0374-f124-4d60-9808-dd44a300edb6 01/04/23 23:49:11.278
STEP: Creating a pod to test consume configMaps 01/04/23 23:49:11.283
Jan  4 23:49:11.292: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f" in namespace "projected-1935" to be "Succeeded or Failed"
Jan  4 23:49:11.295: INFO: Pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.68054ms
Jan  4 23:49:13.299: INFO: Pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007529545s
Jan  4 23:49:15.299: INFO: Pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007245929s
STEP: Saw pod success 01/04/23 23:49:15.299
Jan  4 23:49:15.299: INFO: Pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f" satisfied condition "Succeeded or Failed"
Jan  4 23:49:15.302: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f container agnhost-container: <nil>
STEP: delete the pod 01/04/23 23:49:15.308
Jan  4 23:49:15.324: INFO: Waiting for pod pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f to disappear
Jan  4 23:49:15.332: INFO: Pod pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  4 23:49:15.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1935" for this suite. 01/04/23 23:49:15.336
------------------------------
• [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:49:11.251
    Jan  4 23:49:11.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename projected 01/04/23 23:49:11.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:11.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:11.276
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-a94e0374-f124-4d60-9808-dd44a300edb6 01/04/23 23:49:11.278
    STEP: Creating a pod to test consume configMaps 01/04/23 23:49:11.283
    Jan  4 23:49:11.292: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f" in namespace "projected-1935" to be "Succeeded or Failed"
    Jan  4 23:49:11.295: INFO: Pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.68054ms
    Jan  4 23:49:13.299: INFO: Pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007529545s
    Jan  4 23:49:15.299: INFO: Pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007245929s
    STEP: Saw pod success 01/04/23 23:49:15.299
    Jan  4 23:49:15.299: INFO: Pod "pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f" satisfied condition "Succeeded or Failed"
    Jan  4 23:49:15.302: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f container agnhost-container: <nil>
    STEP: delete the pod 01/04/23 23:49:15.308
    Jan  4 23:49:15.324: INFO: Waiting for pod pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f to disappear
    Jan  4 23:49:15.332: INFO: Pod pod-projected-configmaps-0cbe0733-7952-46cf-963f-f9e8255c330f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:49:15.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1935" for this suite. 01/04/23 23:49:15.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:49:15.344
Jan  4 23:49:15.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename emptydir 01/04/23 23:49:15.345
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:15.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:15.364
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/04/23 23:49:15.366
Jan  4 23:49:15.373: INFO: Waiting up to 5m0s for pod "pod-34deb91c-129e-426a-ad7e-432447559b5d" in namespace "emptydir-6369" to be "Succeeded or Failed"
Jan  4 23:49:15.376: INFO: Pod "pod-34deb91c-129e-426a-ad7e-432447559b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.79333ms
Jan  4 23:49:17.379: INFO: Pod "pod-34deb91c-129e-426a-ad7e-432447559b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006621945s
Jan  4 23:49:19.380: INFO: Pod "pod-34deb91c-129e-426a-ad7e-432447559b5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007227901s
STEP: Saw pod success 01/04/23 23:49:19.38
Jan  4 23:49:19.380: INFO: Pod "pod-34deb91c-129e-426a-ad7e-432447559b5d" satisfied condition "Succeeded or Failed"
Jan  4 23:49:19.382: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-34deb91c-129e-426a-ad7e-432447559b5d container test-container: <nil>
STEP: delete the pod 01/04/23 23:49:19.389
Jan  4 23:49:19.403: INFO: Waiting for pod pod-34deb91c-129e-426a-ad7e-432447559b5d to disappear
Jan  4 23:49:19.406: INFO: Pod pod-34deb91c-129e-426a-ad7e-432447559b5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  4 23:49:19.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6369" for this suite. 01/04/23 23:49:19.41
------------------------------
• [4.072 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:49:15.344
    Jan  4 23:49:15.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename emptydir 01/04/23 23:49:15.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:15.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:15.364
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/04/23 23:49:15.366
    Jan  4 23:49:15.373: INFO: Waiting up to 5m0s for pod "pod-34deb91c-129e-426a-ad7e-432447559b5d" in namespace "emptydir-6369" to be "Succeeded or Failed"
    Jan  4 23:49:15.376: INFO: Pod "pod-34deb91c-129e-426a-ad7e-432447559b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.79333ms
    Jan  4 23:49:17.379: INFO: Pod "pod-34deb91c-129e-426a-ad7e-432447559b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006621945s
    Jan  4 23:49:19.380: INFO: Pod "pod-34deb91c-129e-426a-ad7e-432447559b5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007227901s
    STEP: Saw pod success 01/04/23 23:49:19.38
    Jan  4 23:49:19.380: INFO: Pod "pod-34deb91c-129e-426a-ad7e-432447559b5d" satisfied condition "Succeeded or Failed"
    Jan  4 23:49:19.382: INFO: Trying to get logs from node ip-172-31-13-117.us-east-2.compute.internal pod pod-34deb91c-129e-426a-ad7e-432447559b5d container test-container: <nil>
    STEP: delete the pod 01/04/23 23:49:19.389
    Jan  4 23:49:19.403: INFO: Waiting for pod pod-34deb91c-129e-426a-ad7e-432447559b5d to disappear
    Jan  4 23:49:19.406: INFO: Pod pod-34deb91c-129e-426a-ad7e-432447559b5d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:49:19.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6369" for this suite. 01/04/23 23:49:19.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/04/23 23:49:19.419
Jan  4 23:49:19.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
STEP: Building a namespace api object, basename kubectl 01/04/23 23:49:19.42
STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:19.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:19.442
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/04/23 23:49:19.445
Jan  4 23:49:19.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-7836 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  4 23:49:19.533: INFO: stderr: ""
Jan  4 23:49:19.533: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/04/23 23:49:19.533
STEP: verifying the pod e2e-test-httpd-pod was created 01/04/23 23:49:24.589
Jan  4 23:49:24.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-7836 get pod e2e-test-httpd-pod -o json'
Jan  4 23:49:24.771: INFO: stderr: ""
Jan  4 23:49:24.771: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a6880b42de2d56b1fc181eb77dd1b2f2c4ac997ecb4a301d8dbca93d1e2c0508\",\n            \"cni.projectcalico.org/podIP\": \"10.42.3.171/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.42.3.171/32\"\n        },\n        \"creationTimestamp\": \"2023-01-04T23:49:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7836\",\n        \"resourceVersion\": \"74658\",\n        \"uid\": \"f6a291b5-a2c1-4674-8693-7b42f634c746\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-b9tg4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-13-117.us-east-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-b9tg4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-04T23:49:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-04T23:49:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-04T23:49:20Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-04T23:49:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://abc16758ba206ade83ccbe181d766768a7da4fc0f5f1145008f871b3b1bc1781\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-04T23:49:20Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.13.117\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.3.171\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.42.3.171\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-04T23:49:19Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/04/23 23:49:24.771
Jan  4 23:49:24.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-7836 replace -f -'
Jan  4 23:49:25.664: INFO: stderr: ""
Jan  4 23:49:25.664: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/04/23 23:49:25.664
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan  4 23:49:25.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-7836 delete pods e2e-test-httpd-pod'
Jan  4 23:49:27.324: INFO: stderr: ""
Jan  4 23:49:27.324: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  4 23:49:27.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7836" for this suite. 01/04/23 23:49:27.334
------------------------------
• [SLOW TEST] [7.921 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/04/23 23:49:19.419
    Jan  4 23:49:19.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3304244550
    STEP: Building a namespace api object, basename kubectl 01/04/23 23:49:19.42
    STEP: Waiting for a default service account to be provisioned in namespace 01/04/23 23:49:19.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/04/23 23:49:19.442
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/04/23 23:49:19.445
    Jan  4 23:49:19.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-7836 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  4 23:49:19.533: INFO: stderr: ""
    Jan  4 23:49:19.533: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/04/23 23:49:19.533
    STEP: verifying the pod e2e-test-httpd-pod was created 01/04/23 23:49:24.589
    Jan  4 23:49:24.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-7836 get pod e2e-test-httpd-pod -o json'
    Jan  4 23:49:24.771: INFO: stderr: ""
    Jan  4 23:49:24.771: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a6880b42de2d56b1fc181eb77dd1b2f2c4ac997ecb4a301d8dbca93d1e2c0508\",\n            \"cni.projectcalico.org/podIP\": \"10.42.3.171/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.42.3.171/32\"\n        },\n        \"creationTimestamp\": \"2023-01-04T23:49:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7836\",\n        \"resourceVersion\": \"74658\",\n        \"uid\": \"f6a291b5-a2c1-4674-8693-7b42f634c746\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-b9tg4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-13-117.us-east-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-b9tg4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-04T23:49:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-04T23:49:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-04T23:49:20Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-04T23:49:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://abc16758ba206ade83ccbe181d766768a7da4fc0f5f1145008f871b3b1bc1781\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-04T23:49:20Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.13.117\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.3.171\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.42.3.171\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-04T23:49:19Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/04/23 23:49:24.771
    Jan  4 23:49:24.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-7836 replace -f -'
    Jan  4 23:49:25.664: INFO: stderr: ""
    Jan  4 23:49:25.664: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/04/23 23:49:25.664
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan  4 23:49:25.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3304244550 --namespace=kubectl-7836 delete pods e2e-test-httpd-pod'
    Jan  4 23:49:27.324: INFO: stderr: ""
    Jan  4 23:49:27.324: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  4 23:49:27.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7836" for this suite. 01/04/23 23:49:27.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan  4 23:49:27.342: INFO: Running AfterSuite actions on node 1
Jan  4 23:49:27.342: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan  4 23:49:27.342: INFO: Running AfterSuite actions on node 1
    Jan  4 23:49:27.342: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.117 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5876.070 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h37m56.682769692s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

