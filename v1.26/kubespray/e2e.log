I0906 10:01:55.396165      22 e2e.go:126] Starting e2e run "4a83d354-710a-403a-981f-71e69f687fbc" on Ginkgo node 1
Sep  6 10:01:55.408: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1693994515 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  6 10:01:55.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:01:55.506: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0906 10:01:55.507708      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Sep  6 10:01:55.535: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  6 10:01:55.567: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  6 10:01:55.567: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep  6 10:01:55.567: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  6 10:01:55.597: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep  6 10:01:55.597: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  6 10:01:55.597: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Sep  6 10:01:55.597: INFO: e2e test version: v1.26.5
Sep  6 10:01:55.601: INFO: kube-apiserver version: v1.26.5
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  6 10:01:55.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:01:55.607: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.103 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  6 10:01:55.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:01:55.506: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0906 10:01:55.507708      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Sep  6 10:01:55.535: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Sep  6 10:01:55.567: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Sep  6 10:01:55.567: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Sep  6 10:01:55.567: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Sep  6 10:01:55.597: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Sep  6 10:01:55.597: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Sep  6 10:01:55.597: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
    Sep  6 10:01:55.597: INFO: e2e test version: v1.26.5
    Sep  6 10:01:55.601: INFO: kube-apiserver version: v1.26.5
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  6 10:01:55.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:01:55.607: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:01:55.627
Sep  6 10:01:55.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename gc 09/06/23 10:01:55.628
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:05.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:05.335
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 09/06/23 10:02:05.36
STEP: Wait for the Deployment to create new ReplicaSet 09/06/23 10:02:05.368
STEP: delete the deployment 09/06/23 10:02:05.877
STEP: wait for all rs to be garbage collected 09/06/23 10:02:05.887
STEP: expected 0 rs, got 1 rs 09/06/23 10:02:05.895
STEP: expected 0 pods, got 2 pods 09/06/23 10:02:05.899
STEP: Gathering metrics 09/06/23 10:02:06.418
Sep  6 10:02:06.453: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Sep  6 10:02:06.457: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.957451ms
Sep  6 10:02:06.457: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Sep  6 10:02:06.457: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Sep  6 10:02:06.668: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  6 10:02:06.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-878" for this suite. 09/06/23 10:02:07.002
------------------------------
• [SLOW TEST] [11.485 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:01:55.627
    Sep  6 10:01:55.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename gc 09/06/23 10:01:55.628
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:05.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:05.335
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 09/06/23 10:02:05.36
    STEP: Wait for the Deployment to create new ReplicaSet 09/06/23 10:02:05.368
    STEP: delete the deployment 09/06/23 10:02:05.877
    STEP: wait for all rs to be garbage collected 09/06/23 10:02:05.887
    STEP: expected 0 rs, got 1 rs 09/06/23 10:02:05.895
    STEP: expected 0 pods, got 2 pods 09/06/23 10:02:05.899
    STEP: Gathering metrics 09/06/23 10:02:06.418
    Sep  6 10:02:06.453: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Sep  6 10:02:06.457: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.957451ms
    Sep  6 10:02:06.457: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Sep  6 10:02:06.457: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Sep  6 10:02:06.668: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:02:06.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-878" for this suite. 09/06/23 10:02:07.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:02:07.112
Sep  6 10:02:07.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename job 09/06/23 10:02:07.113
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:07.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:07.39
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 09/06/23 10:02:07.583
STEP: Ensure pods equal to parallelism count is attached to the job 09/06/23 10:02:07.728
STEP: patching /status 09/06/23 10:02:15.746
STEP: updating /status 09/06/23 10:02:15.774
STEP: get /status 09/06/23 10:02:15.789
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  6 10:02:15.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7590" for this suite. 09/06/23 10:02:15.8
------------------------------
• [SLOW TEST] [8.694 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:02:07.112
    Sep  6 10:02:07.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename job 09/06/23 10:02:07.113
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:07.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:07.39
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 09/06/23 10:02:07.583
    STEP: Ensure pods equal to parallelism count is attached to the job 09/06/23 10:02:07.728
    STEP: patching /status 09/06/23 10:02:15.746
    STEP: updating /status 09/06/23 10:02:15.774
    STEP: get /status 09/06/23 10:02:15.789
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:02:15.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7590" for this suite. 09/06/23 10:02:15.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:02:15.808
Sep  6 10:02:15.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:02:15.809
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:15.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:15.844
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:02:15.866
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:02:16.207
STEP: Deploying the webhook pod 09/06/23 10:02:16.215
STEP: Wait for the deployment to be ready 09/06/23 10:02:16.232
Sep  6 10:02:16.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 10:02:18.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:21.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:22.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:25.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:26.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:28.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/06/23 10:02:30.288
STEP: Verifying the service has paired with the endpoint 09/06/23 10:02:30.298
Sep  6 10:02:31.299: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/06/23 10:02:31.303
STEP: create a namespace for the webhook 09/06/23 10:02:31.32
STEP: create a configmap should be unconditionally rejected by the webhook 09/06/23 10:02:31.329
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:02:31.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-92" for this suite. 09/06/23 10:02:31.435
STEP: Destroying namespace "webhook-92-markers" for this suite. 09/06/23 10:02:31.455
------------------------------
• [SLOW TEST] [15.669 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:02:15.808
    Sep  6 10:02:15.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:02:15.809
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:15.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:15.844
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:02:15.866
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:02:16.207
    STEP: Deploying the webhook pod 09/06/23 10:02:16.215
    STEP: Wait for the deployment to be ready 09/06/23 10:02:16.232
    Sep  6 10:02:16.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  6 10:02:18.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:21.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:22.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:25.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:26.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:28.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/06/23 10:02:30.288
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:02:30.298
    Sep  6 10:02:31.299: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/06/23 10:02:31.303
    STEP: create a namespace for the webhook 09/06/23 10:02:31.32
    STEP: create a configmap should be unconditionally rejected by the webhook 09/06/23 10:02:31.329
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:02:31.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-92" for this suite. 09/06/23 10:02:31.435
    STEP: Destroying namespace "webhook-92-markers" for this suite. 09/06/23 10:02:31.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:02:31.482
Sep  6 10:02:31.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 10:02:31.484
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:31.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:31.539
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8730 09/06/23 10:02:31.545
STEP: changing the ExternalName service to type=ClusterIP 09/06/23 10:02:31.555
STEP: creating replication controller externalname-service in namespace services-8730 09/06/23 10:02:31.586
I0906 10:02:31.595468      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8730, replica count: 2
I0906 10:02:34.651331      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 10:02:37.651543      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 10:02:40.652294      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 10:02:43.652457      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 10:02:43.652: INFO: Creating new exec pod
Sep  6 10:02:43.665: INFO: Waiting up to 5m0s for pod "execpodjxbz8" in namespace "services-8730" to be "running"
Sep  6 10:02:43.674: INFO: Pod "execpodjxbz8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.139949ms
Sep  6 10:02:45.687: INFO: Pod "execpodjxbz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.021681798s
Sep  6 10:02:45.687: INFO: Pod "execpodjxbz8" satisfied condition "running"
Sep  6 10:02:46.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8730 exec execpodjxbz8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  6 10:02:46.825: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  6 10:02:46.825: INFO: stdout: ""
Sep  6 10:02:46.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8730 exec execpodjxbz8 -- /bin/sh -x -c nc -v -z -w 2 10.233.18.173 80'
Sep  6 10:02:46.956: INFO: stderr: "+ nc -v -z -w 2 10.233.18.173 80\nConnection to 10.233.18.173 80 port [tcp/http] succeeded!\n"
Sep  6 10:02:46.956: INFO: stdout: ""
Sep  6 10:02:46.956: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 10:02:46.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8730" for this suite. 09/06/23 10:02:46.991
------------------------------
• [SLOW TEST] [15.524 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:02:31.482
    Sep  6 10:02:31.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 10:02:31.484
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:31.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:31.539
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8730 09/06/23 10:02:31.545
    STEP: changing the ExternalName service to type=ClusterIP 09/06/23 10:02:31.555
    STEP: creating replication controller externalname-service in namespace services-8730 09/06/23 10:02:31.586
    I0906 10:02:31.595468      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8730, replica count: 2
    I0906 10:02:34.651331      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0906 10:02:37.651543      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0906 10:02:40.652294      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0906 10:02:43.652457      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 10:02:43.652: INFO: Creating new exec pod
    Sep  6 10:02:43.665: INFO: Waiting up to 5m0s for pod "execpodjxbz8" in namespace "services-8730" to be "running"
    Sep  6 10:02:43.674: INFO: Pod "execpodjxbz8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.139949ms
    Sep  6 10:02:45.687: INFO: Pod "execpodjxbz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.021681798s
    Sep  6 10:02:45.687: INFO: Pod "execpodjxbz8" satisfied condition "running"
    Sep  6 10:02:46.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8730 exec execpodjxbz8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  6 10:02:46.825: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  6 10:02:46.825: INFO: stdout: ""
    Sep  6 10:02:46.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8730 exec execpodjxbz8 -- /bin/sh -x -c nc -v -z -w 2 10.233.18.173 80'
    Sep  6 10:02:46.956: INFO: stderr: "+ nc -v -z -w 2 10.233.18.173 80\nConnection to 10.233.18.173 80 port [tcp/http] succeeded!\n"
    Sep  6 10:02:46.956: INFO: stdout: ""
    Sep  6 10:02:46.956: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:02:46.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8730" for this suite. 09/06/23 10:02:46.991
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:02:47.006
Sep  6 10:02:47.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename deployment 09/06/23 10:02:47.007
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:47.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:47.042
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 09/06/23 10:02:47.051
Sep  6 10:02:47.051: INFO: Creating simple deployment test-deployment-nh27h
Sep  6 10:02:47.077: INFO: deployment "test-deployment-nh27h" doesn't have the required revision set
Sep  6 10:02:49.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:51.105: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:54.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:55.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:58.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:02:59.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 09/06/23 10:03:01.106
Sep  6 10:03:01.112: INFO: Deployment test-deployment-nh27h has Conditions: [{Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 09/06/23 10:03:01.112
Sep  6 10:03:01.122: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 3, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 3, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 3, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 09/06/23 10:03:01.122
Sep  6 10:03:01.124: INFO: Observed &Deployment event: ADDED
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-nh27h-54bc444df"}
Sep  6 10:03:01.124: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-nh27h-54bc444df"}
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  6 10:03:01.124: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-nh27h-54bc444df" is progressing.}
Sep  6 10:03:01.124: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}
Sep  6 10:03:01.124: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}
Sep  6 10:03:01.124: INFO: Found Deployment test-deployment-nh27h in namespace deployment-2176 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  6 10:03:01.124: INFO: Deployment test-deployment-nh27h has an updated status
STEP: patching the Statefulset Status 09/06/23 10:03:01.124
Sep  6 10:03:01.125: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  6 10:03:01.132: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 09/06/23 10:03:01.132
Sep  6 10:03:01.134: INFO: Observed &Deployment event: ADDED
Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-nh27h-54bc444df"}
Sep  6 10:03:01.134: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-nh27h-54bc444df"}
Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  6 10:03:01.134: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-nh27h-54bc444df" is progressing.}
Sep  6 10:03:01.135: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}
Sep  6 10:03:01.135: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}
Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  6 10:03:01.135: INFO: Observed &Deployment event: MODIFIED
Sep  6 10:03:01.135: INFO: Found deployment test-deployment-nh27h in namespace deployment-2176 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep  6 10:03:01.135: INFO: Deployment test-deployment-nh27h has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  6 10:03:01.144: INFO: Deployment "test-deployment-nh27h":
&Deployment{ObjectMeta:{test-deployment-nh27h  deployment-2176  50c94cc3-3bed-4258-a9fb-89012b6b0a1c 2845 1 2023-09-06 10:02:47 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-06 10:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:03:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-09-06 10:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045fcb98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 10:03:01.149: INFO: New ReplicaSet "test-deployment-nh27h-54bc444df" of Deployment "test-deployment-nh27h":
&ReplicaSet{ObjectMeta:{test-deployment-nh27h-54bc444df  deployment-2176  749bc93a-28cd-4a79-b403-cc79ea40b982 2836 1 2023-09-06 10:02:47 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-nh27h 50c94cc3-3bed-4258-a9fb-89012b6b0a1c 0xc004a44047 0xc004a44048}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50c94cc3-3bed-4258-a9fb-89012b6b0a1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:03:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a440f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:03:01.154: INFO: Pod "test-deployment-nh27h-54bc444df-mxk5l" is available:
&Pod{ObjectMeta:{test-deployment-nh27h-54bc444df-mxk5l test-deployment-nh27h-54bc444df- deployment-2176  534d9ee2-cd24-40ca-bb10-4075541ae2e3 2835 0 2023-09-06 10:02:47 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:7a66d16da13daba06bf0b0db3a8369af3c22b5d73d43085cf91a029e4d0edc95 cni.projectcalico.org/podIP:10.233.120.67/32 cni.projectcalico.org/podIPs:10.233.120.67/32] [{apps/v1 ReplicaSet test-deployment-nh27h-54bc444df 749bc93a-28cd-4a79-b403-cc79ea40b982 0xc0045fcf57 0xc0045fcf58}] [] [{kube-controller-manager Update v1 2023-09-06 10:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"749bc93a-28cd-4a79-b403-cc79ea40b982\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 10:02:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 10:03:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7d8xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7d8xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:02:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:03:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:03:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.67,StartTime:2023-09-06 10:02:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 10:02:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a99d97fb57f82648b1b192d486701cd764ee53f73bc2ea827c87b45b8cf30976,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  6 10:03:01.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2176" for this suite. 09/06/23 10:03:01.167
------------------------------
• [SLOW TEST] [14.170 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:02:47.006
    Sep  6 10:02:47.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename deployment 09/06/23 10:02:47.007
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:02:47.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:02:47.042
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 09/06/23 10:02:47.051
    Sep  6 10:02:47.051: INFO: Creating simple deployment test-deployment-nh27h
    Sep  6 10:02:47.077: INFO: deployment "test-deployment-nh27h" doesn't have the required revision set
    Sep  6 10:02:49.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:51.105: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:54.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:55.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:58.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:02:59.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 09/06/23 10:03:01.106
    Sep  6 10:03:01.112: INFO: Deployment test-deployment-nh27h has Conditions: [{Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 09/06/23 10:03:01.112
    Sep  6 10:03:01.122: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 3, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 3, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 3, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 2, 47, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-nh27h-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 09/06/23 10:03:01.122
    Sep  6 10:03:01.124: INFO: Observed &Deployment event: ADDED
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-nh27h-54bc444df"}
    Sep  6 10:03:01.124: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-nh27h-54bc444df"}
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  6 10:03:01.124: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-nh27h-54bc444df" is progressing.}
    Sep  6 10:03:01.124: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}
    Sep  6 10:03:01.124: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  6 10:03:01.124: INFO: Observed Deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}
    Sep  6 10:03:01.124: INFO: Found Deployment test-deployment-nh27h in namespace deployment-2176 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  6 10:03:01.124: INFO: Deployment test-deployment-nh27h has an updated status
    STEP: patching the Statefulset Status 09/06/23 10:03:01.124
    Sep  6 10:03:01.125: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  6 10:03:01.132: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 09/06/23 10:03:01.132
    Sep  6 10:03:01.134: INFO: Observed &Deployment event: ADDED
    Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-nh27h-54bc444df"}
    Sep  6 10:03:01.134: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-nh27h-54bc444df"}
    Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  6 10:03:01.134: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  6 10:03:01.134: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:02:47 +0000 UTC 2023-09-06 10:02:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-nh27h-54bc444df" is progressing.}
    Sep  6 10:03:01.135: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}
    Sep  6 10:03:01.135: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:03:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-06 10:03:00 +0000 UTC 2023-09-06 10:02:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-nh27h-54bc444df" has successfully progressed.}
    Sep  6 10:03:01.135: INFO: Observed deployment test-deployment-nh27h in namespace deployment-2176 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  6 10:03:01.135: INFO: Observed &Deployment event: MODIFIED
    Sep  6 10:03:01.135: INFO: Found deployment test-deployment-nh27h in namespace deployment-2176 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Sep  6 10:03:01.135: INFO: Deployment test-deployment-nh27h has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  6 10:03:01.144: INFO: Deployment "test-deployment-nh27h":
    &Deployment{ObjectMeta:{test-deployment-nh27h  deployment-2176  50c94cc3-3bed-4258-a9fb-89012b6b0a1c 2845 1 2023-09-06 10:02:47 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-06 10:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:03:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-09-06 10:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045fcb98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  6 10:03:01.149: INFO: New ReplicaSet "test-deployment-nh27h-54bc444df" of Deployment "test-deployment-nh27h":
    &ReplicaSet{ObjectMeta:{test-deployment-nh27h-54bc444df  deployment-2176  749bc93a-28cd-4a79-b403-cc79ea40b982 2836 1 2023-09-06 10:02:47 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-nh27h 50c94cc3-3bed-4258-a9fb-89012b6b0a1c 0xc004a44047 0xc004a44048}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50c94cc3-3bed-4258-a9fb-89012b6b0a1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:03:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a440f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:03:01.154: INFO: Pod "test-deployment-nh27h-54bc444df-mxk5l" is available:
    &Pod{ObjectMeta:{test-deployment-nh27h-54bc444df-mxk5l test-deployment-nh27h-54bc444df- deployment-2176  534d9ee2-cd24-40ca-bb10-4075541ae2e3 2835 0 2023-09-06 10:02:47 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:7a66d16da13daba06bf0b0db3a8369af3c22b5d73d43085cf91a029e4d0edc95 cni.projectcalico.org/podIP:10.233.120.67/32 cni.projectcalico.org/podIPs:10.233.120.67/32] [{apps/v1 ReplicaSet test-deployment-nh27h-54bc444df 749bc93a-28cd-4a79-b403-cc79ea40b982 0xc0045fcf57 0xc0045fcf58}] [] [{kube-controller-manager Update v1 2023-09-06 10:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"749bc93a-28cd-4a79-b403-cc79ea40b982\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 10:02:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 10:03:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7d8xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7d8xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:02:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:03:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:03:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.67,StartTime:2023-09-06 10:02:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 10:02:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a99d97fb57f82648b1b192d486701cd764ee53f73bc2ea827c87b45b8cf30976,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:03:01.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2176" for this suite. 09/06/23 10:03:01.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:03:01.179
Sep  6 10:03:01.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:03:01.181
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:03:01.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:03:01.211
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/06/23 10:03:01.214
Sep  6 10:03:01.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-8054 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  6 10:03:01.276: INFO: stderr: ""
Sep  6 10:03:01.276: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 09/06/23 10:03:01.276
STEP: verifying the pod e2e-test-httpd-pod was created 09/06/23 10:03:16.332
Sep  6 10:03:16.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-8054 get pod e2e-test-httpd-pod -o json'
Sep  6 10:03:16.381: INFO: stderr: ""
Sep  6 10:03:16.381: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"ab90252dc2d5fc470da7e5d75cd7ef14d7292e3b189c48bc5ded4cdf2b829364\",\n            \"cni.projectcalico.org/podIP\": \"10.233.99.73/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.99.73/32\"\n        },\n        \"creationTimestamp\": \"2023-09-06T10:03:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8054\",\n        \"resourceVersion\": \"2903\",\n        \"uid\": \"9614c922-6e0a-419e-a0a1-e636275a1e2a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6wfp4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6wfp4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-06T10:03:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-06T10:03:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-06T10:03:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-06T10:03:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://9555fa2101502d833ade292c917ccf9e0d1275c4fbd14a79f7f1bdf8532c7d50\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-06T10:03:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.2.20.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.99.73\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.99.73\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-06T10:03:01Z\"\n    }\n}\n"
STEP: replace the image in the pod 09/06/23 10:03:16.381
Sep  6 10:03:16.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-8054 replace -f -'
Sep  6 10:03:17.745: INFO: stderr: ""
Sep  6 10:03:17.745: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/06/23 10:03:17.745
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Sep  6 10:03:17.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-8054 delete pods e2e-test-httpd-pod'
Sep  6 10:03:19.979: INFO: stderr: ""
Sep  6 10:03:19.979: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:03:19.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8054" for this suite. 09/06/23 10:03:19.984
------------------------------
• [SLOW TEST] [18.812 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:03:01.179
    Sep  6 10:03:01.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:03:01.181
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:03:01.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:03:01.211
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/06/23 10:03:01.214
    Sep  6 10:03:01.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-8054 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  6 10:03:01.276: INFO: stderr: ""
    Sep  6 10:03:01.276: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 09/06/23 10:03:01.276
    STEP: verifying the pod e2e-test-httpd-pod was created 09/06/23 10:03:16.332
    Sep  6 10:03:16.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-8054 get pod e2e-test-httpd-pod -o json'
    Sep  6 10:03:16.381: INFO: stderr: ""
    Sep  6 10:03:16.381: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"ab90252dc2d5fc470da7e5d75cd7ef14d7292e3b189c48bc5ded4cdf2b829364\",\n            \"cni.projectcalico.org/podIP\": \"10.233.99.73/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.99.73/32\"\n        },\n        \"creationTimestamp\": \"2023-09-06T10:03:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8054\",\n        \"resourceVersion\": \"2903\",\n        \"uid\": \"9614c922-6e0a-419e-a0a1-e636275a1e2a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6wfp4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6wfp4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-06T10:03:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-06T10:03:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-06T10:03:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-06T10:03:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://9555fa2101502d833ade292c917ccf9e0d1275c4fbd14a79f7f1bdf8532c7d50\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-06T10:03:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.2.20.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.99.73\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.99.73\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-06T10:03:01Z\"\n    }\n}\n"
    STEP: replace the image in the pod 09/06/23 10:03:16.381
    Sep  6 10:03:16.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-8054 replace -f -'
    Sep  6 10:03:17.745: INFO: stderr: ""
    Sep  6 10:03:17.745: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/06/23 10:03:17.745
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Sep  6 10:03:17.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-8054 delete pods e2e-test-httpd-pod'
    Sep  6 10:03:19.979: INFO: stderr: ""
    Sep  6 10:03:19.979: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:03:19.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8054" for this suite. 09/06/23 10:03:19.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:03:19.993
Sep  6 10:03:19.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename cronjob 09/06/23 10:03:19.994
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:03:20.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:03:20.019
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 09/06/23 10:03:20.022
STEP: creating 09/06/23 10:03:20.022
STEP: getting 09/06/23 10:03:20.029
STEP: listing 09/06/23 10:03:20.032
STEP: watching 09/06/23 10:03:20.036
Sep  6 10:03:20.036: INFO: starting watch
STEP: cluster-wide listing 09/06/23 10:03:20.037
STEP: cluster-wide watching 09/06/23 10:03:20.04
Sep  6 10:03:20.041: INFO: starting watch
STEP: patching 09/06/23 10:03:20.042
STEP: updating 09/06/23 10:03:20.048
Sep  6 10:03:20.062: INFO: waiting for watch events with expected annotations
Sep  6 10:03:20.062: INFO: saw patched and updated annotations
STEP: patching /status 09/06/23 10:03:20.062
STEP: updating /status 09/06/23 10:03:20.068
STEP: get /status 09/06/23 10:03:20.079
STEP: deleting 09/06/23 10:03:20.084
STEP: deleting a collection 09/06/23 10:03:20.1
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  6 10:03:20.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5490" for this suite. 09/06/23 10:03:20.115
------------------------------
• [0.130 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:03:19.993
    Sep  6 10:03:19.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename cronjob 09/06/23 10:03:19.994
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:03:20.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:03:20.019
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 09/06/23 10:03:20.022
    STEP: creating 09/06/23 10:03:20.022
    STEP: getting 09/06/23 10:03:20.029
    STEP: listing 09/06/23 10:03:20.032
    STEP: watching 09/06/23 10:03:20.036
    Sep  6 10:03:20.036: INFO: starting watch
    STEP: cluster-wide listing 09/06/23 10:03:20.037
    STEP: cluster-wide watching 09/06/23 10:03:20.04
    Sep  6 10:03:20.041: INFO: starting watch
    STEP: patching 09/06/23 10:03:20.042
    STEP: updating 09/06/23 10:03:20.048
    Sep  6 10:03:20.062: INFO: waiting for watch events with expected annotations
    Sep  6 10:03:20.062: INFO: saw patched and updated annotations
    STEP: patching /status 09/06/23 10:03:20.062
    STEP: updating /status 09/06/23 10:03:20.068
    STEP: get /status 09/06/23 10:03:20.079
    STEP: deleting 09/06/23 10:03:20.084
    STEP: deleting a collection 09/06/23 10:03:20.1
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:03:20.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5490" for this suite. 09/06/23 10:03:20.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:03:20.126
Sep  6 10:03:20.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replication-controller 09/06/23 10:03:20.13
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:03:20.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:03:20.173
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 09/06/23 10:03:20.181
STEP: waiting for RC to be added 09/06/23 10:03:20.191
STEP: waiting for available Replicas 09/06/23 10:03:20.193
STEP: patching ReplicationController 09/06/23 10:03:20.973
STEP: waiting for RC to be modified 09/06/23 10:03:20.987
STEP: patching ReplicationController status 09/06/23 10:03:20.987
STEP: waiting for RC to be modified 09/06/23 10:03:20.994
STEP: waiting for available Replicas 09/06/23 10:03:20.995
STEP: fetching ReplicationController status 09/06/23 10:03:21.003
STEP: patching ReplicationController scale 09/06/23 10:03:21.009
STEP: waiting for RC to be modified 09/06/23 10:03:21.018
STEP: waiting for ReplicationController's scale to be the max amount 09/06/23 10:03:21.019
STEP: fetching ReplicationController; ensuring that it's patched 09/06/23 10:03:22.089
STEP: updating ReplicationController status 09/06/23 10:03:22.095
STEP: waiting for RC to be modified 09/06/23 10:03:22.105
STEP: listing all ReplicationControllers 09/06/23 10:03:22.107
STEP: checking that ReplicationController has expected values 09/06/23 10:03:22.12
STEP: deleting ReplicationControllers by collection 09/06/23 10:03:22.12
STEP: waiting for ReplicationController to have a DELETED watchEvent 09/06/23 10:03:22.138
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  6 10:03:22.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7517" for this suite. 09/06/23 10:03:22.215
------------------------------
• [2.098 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:03:20.126
    Sep  6 10:03:20.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replication-controller 09/06/23 10:03:20.13
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:03:20.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:03:20.173
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 09/06/23 10:03:20.181
    STEP: waiting for RC to be added 09/06/23 10:03:20.191
    STEP: waiting for available Replicas 09/06/23 10:03:20.193
    STEP: patching ReplicationController 09/06/23 10:03:20.973
    STEP: waiting for RC to be modified 09/06/23 10:03:20.987
    STEP: patching ReplicationController status 09/06/23 10:03:20.987
    STEP: waiting for RC to be modified 09/06/23 10:03:20.994
    STEP: waiting for available Replicas 09/06/23 10:03:20.995
    STEP: fetching ReplicationController status 09/06/23 10:03:21.003
    STEP: patching ReplicationController scale 09/06/23 10:03:21.009
    STEP: waiting for RC to be modified 09/06/23 10:03:21.018
    STEP: waiting for ReplicationController's scale to be the max amount 09/06/23 10:03:21.019
    STEP: fetching ReplicationController; ensuring that it's patched 09/06/23 10:03:22.089
    STEP: updating ReplicationController status 09/06/23 10:03:22.095
    STEP: waiting for RC to be modified 09/06/23 10:03:22.105
    STEP: listing all ReplicationControllers 09/06/23 10:03:22.107
    STEP: checking that ReplicationController has expected values 09/06/23 10:03:22.12
    STEP: deleting ReplicationControllers by collection 09/06/23 10:03:22.12
    STEP: waiting for ReplicationController to have a DELETED watchEvent 09/06/23 10:03:22.138
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:03:22.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7517" for this suite. 09/06/23 10:03:22.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:03:22.224
Sep  6 10:03:22.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename job 09/06/23 10:03:22.225
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:03:22.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:03:22.26
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 09/06/23 10:03:22.27
STEP: Patching the Job 09/06/23 10:03:22.277
STEP: Watching for Job to be patched 09/06/23 10:03:22.305
Sep  6 10:03:22.307: INFO: Event ADDED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  6 10:03:22.307: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  6 10:03:22.307: INFO: Event MODIFIED found for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 09/06/23 10:03:22.307
STEP: Watching for Job to be updated 09/06/23 10:03:22.32
Sep  6 10:03:22.322: INFO: Event MODIFIED found for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  6 10:03:22.322: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 09/06/23 10:03:22.322
Sep  6 10:03:22.329: INFO: Job: e2e-wmttc as labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched]
STEP: Waiting for job to complete 09/06/23 10:03:22.329
STEP: Delete a job collection with a labelselector 09/06/23 10:04:14.334
STEP: Watching for Job to be deleted 09/06/23 10:04:14.342
Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  6 10:04:14.344: INFO: Event DELETED found for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 09/06/23 10:04:14.344
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  6 10:04:14.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4892" for this suite. 09/06/23 10:04:14.356
------------------------------
• [SLOW TEST] [52.160 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:03:22.224
    Sep  6 10:03:22.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename job 09/06/23 10:03:22.225
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:03:22.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:03:22.26
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 09/06/23 10:03:22.27
    STEP: Patching the Job 09/06/23 10:03:22.277
    STEP: Watching for Job to be patched 09/06/23 10:03:22.305
    Sep  6 10:03:22.307: INFO: Event ADDED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  6 10:03:22.307: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  6 10:03:22.307: INFO: Event MODIFIED found for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 09/06/23 10:03:22.307
    STEP: Watching for Job to be updated 09/06/23 10:03:22.32
    Sep  6 10:03:22.322: INFO: Event MODIFIED found for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  6 10:03:22.322: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 09/06/23 10:03:22.322
    Sep  6 10:03:22.329: INFO: Job: e2e-wmttc as labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched]
    STEP: Waiting for job to complete 09/06/23 10:03:22.329
    STEP: Delete a job collection with a labelselector 09/06/23 10:04:14.334
    STEP: Watching for Job to be deleted 09/06/23 10:04:14.342
    Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  6 10:04:14.344: INFO: Event MODIFIED observed for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  6 10:04:14.344: INFO: Event DELETED found for Job e2e-wmttc in namespace job-4892 with labels: map[e2e-job-label:e2e-wmttc e2e-wmttc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 09/06/23 10:04:14.344
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:04:14.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4892" for this suite. 09/06/23 10:04:14.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:04:14.385
Sep  6 10:04:14.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename svcaccounts 09/06/23 10:04:14.386
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:14.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:14.425
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 09/06/23 10:04:14.428
STEP: watching for the ServiceAccount to be added 09/06/23 10:04:14.438
STEP: patching the ServiceAccount 09/06/23 10:04:14.44
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/06/23 10:04:14.447
STEP: deleting the ServiceAccount 09/06/23 10:04:14.45
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  6 10:04:14.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4255" for this suite. 09/06/23 10:04:14.473
------------------------------
• [0.098 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:04:14.385
    Sep  6 10:04:14.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename svcaccounts 09/06/23 10:04:14.386
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:14.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:14.425
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 09/06/23 10:04:14.428
    STEP: watching for the ServiceAccount to be added 09/06/23 10:04:14.438
    STEP: patching the ServiceAccount 09/06/23 10:04:14.44
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/06/23 10:04:14.447
    STEP: deleting the ServiceAccount 09/06/23 10:04:14.45
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:04:14.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4255" for this suite. 09/06/23 10:04:14.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:04:14.485
Sep  6 10:04:14.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:04:14.485
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:14.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:14.508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:04:14.533
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:04:14.905
STEP: Deploying the webhook pod 09/06/23 10:04:14.913
STEP: Wait for the deployment to be ready 09/06/23 10:04:14.926
Sep  6 10:04:14.933: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/06/23 10:04:16.942
STEP: Verifying the service has paired with the endpoint 09/06/23 10:04:17.087
Sep  6 10:04:18.087: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 09/06/23 10:04:18.09
STEP: Creating a custom resource definition that should be denied by the webhook 09/06/23 10:04:18.133
Sep  6 10:04:18.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:04:18.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4373" for this suite. 09/06/23 10:04:18.329
STEP: Destroying namespace "webhook-4373-markers" for this suite. 09/06/23 10:04:18.343
------------------------------
• [3.878 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:04:14.485
    Sep  6 10:04:14.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:04:14.485
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:14.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:14.508
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:04:14.533
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:04:14.905
    STEP: Deploying the webhook pod 09/06/23 10:04:14.913
    STEP: Wait for the deployment to be ready 09/06/23 10:04:14.926
    Sep  6 10:04:14.933: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/06/23 10:04:16.942
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:04:17.087
    Sep  6 10:04:18.087: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 09/06/23 10:04:18.09
    STEP: Creating a custom resource definition that should be denied by the webhook 09/06/23 10:04:18.133
    Sep  6 10:04:18.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:04:18.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4373" for this suite. 09/06/23 10:04:18.329
    STEP: Destroying namespace "webhook-4373-markers" for this suite. 09/06/23 10:04:18.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:04:18.373
Sep  6 10:04:18.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename security-context-test 09/06/23 10:04:18.375
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:18.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:18.408
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Sep  6 10:04:18.420: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97" in namespace "security-context-test-774" to be "Succeeded or Failed"
Sep  6 10:04:18.423: INFO: Pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97": Phase="Pending", Reason="", readiness=false. Elapsed: 3.586095ms
Sep  6 10:04:20.427: INFO: Pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007662909s
Sep  6 10:04:22.427: INFO: Pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006964827s
Sep  6 10:04:22.427: INFO: Pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  6 10:04:22.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-774" for this suite. 09/06/23 10:04:22.43
------------------------------
• [4.063 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:04:18.373
    Sep  6 10:04:18.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename security-context-test 09/06/23 10:04:18.375
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:18.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:18.408
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Sep  6 10:04:18.420: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97" in namespace "security-context-test-774" to be "Succeeded or Failed"
    Sep  6 10:04:18.423: INFO: Pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97": Phase="Pending", Reason="", readiness=false. Elapsed: 3.586095ms
    Sep  6 10:04:20.427: INFO: Pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007662909s
    Sep  6 10:04:22.427: INFO: Pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006964827s
    Sep  6 10:04:22.427: INFO: Pod "busybox-readonly-false-93f959a8-3df2-4297-96d0-b212a9ce5c97" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:04:22.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-774" for this suite. 09/06/23 10:04:22.43
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:04:22.437
Sep  6 10:04:22.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:04:22.437
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:22.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:22.461
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-021a12f5-0a99-4c93-881a-b2f37ceaced1 09/06/23 10:04:22.467
STEP: Creating secret with name s-test-opt-upd-5135e721-fee2-4a37-9e1f-f6c08dbccceb 09/06/23 10:04:22.472
STEP: Creating the pod 09/06/23 10:04:22.477
Sep  6 10:04:22.489: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d" in namespace "projected-9971" to be "running and ready"
Sep  6 10:04:22.492: INFO: Pod "pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.74298ms
Sep  6 10:04:22.492: INFO: The phase of Pod pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:04:24.497: INFO: Pod "pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d": Phase="Running", Reason="", readiness=true. Elapsed: 2.00838493s
Sep  6 10:04:24.497: INFO: The phase of Pod pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d is Running (Ready = true)
Sep  6 10:04:24.497: INFO: Pod "pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-021a12f5-0a99-4c93-881a-b2f37ceaced1 09/06/23 10:04:24.524
STEP: Updating secret s-test-opt-upd-5135e721-fee2-4a37-9e1f-f6c08dbccceb 09/06/23 10:04:24.53
STEP: Creating secret with name s-test-opt-create-6fd03d4c-f4f5-406e-88dd-02d89bb34ed7 09/06/23 10:04:24.536
STEP: waiting to observe update in volume 09/06/23 10:04:24.541
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  6 10:04:26.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9971" for this suite. 09/06/23 10:04:26.565
------------------------------
• [4.139 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:04:22.437
    Sep  6 10:04:22.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:04:22.437
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:22.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:22.461
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-021a12f5-0a99-4c93-881a-b2f37ceaced1 09/06/23 10:04:22.467
    STEP: Creating secret with name s-test-opt-upd-5135e721-fee2-4a37-9e1f-f6c08dbccceb 09/06/23 10:04:22.472
    STEP: Creating the pod 09/06/23 10:04:22.477
    Sep  6 10:04:22.489: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d" in namespace "projected-9971" to be "running and ready"
    Sep  6 10:04:22.492: INFO: Pod "pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.74298ms
    Sep  6 10:04:22.492: INFO: The phase of Pod pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:04:24.497: INFO: Pod "pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d": Phase="Running", Reason="", readiness=true. Elapsed: 2.00838493s
    Sep  6 10:04:24.497: INFO: The phase of Pod pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d is Running (Ready = true)
    Sep  6 10:04:24.497: INFO: Pod "pod-projected-secrets-ba30bc75-3caa-4397-b1cc-f0fe4d35c72d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-021a12f5-0a99-4c93-881a-b2f37ceaced1 09/06/23 10:04:24.524
    STEP: Updating secret s-test-opt-upd-5135e721-fee2-4a37-9e1f-f6c08dbccceb 09/06/23 10:04:24.53
    STEP: Creating secret with name s-test-opt-create-6fd03d4c-f4f5-406e-88dd-02d89bb34ed7 09/06/23 10:04:24.536
    STEP: waiting to observe update in volume 09/06/23 10:04:24.541
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:04:26.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9971" for this suite. 09/06/23 10:04:26.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:04:26.576
Sep  6 10:04:26.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:04:26.577
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:26.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:26.602
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 09/06/23 10:04:26.605
Sep  6 10:04:26.617: INFO: Waiting up to 5m0s for pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2" in namespace "projected-3115" to be "running and ready"
Sep  6 10:04:26.628: INFO: Pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.139839ms
Sep  6 10:04:26.628: INFO: The phase of Pod annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:04:28.633: INFO: Pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016298452s
Sep  6 10:04:28.633: INFO: The phase of Pod annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2 is Running (Ready = true)
Sep  6 10:04:28.633: INFO: Pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2" satisfied condition "running and ready"
Sep  6 10:04:29.152: INFO: Successfully updated pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 10:04:33.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3115" for this suite. 09/06/23 10:04:33.217
------------------------------
• [SLOW TEST] [6.665 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:04:26.576
    Sep  6 10:04:26.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:04:26.577
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:26.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:26.602
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 09/06/23 10:04:26.605
    Sep  6 10:04:26.617: INFO: Waiting up to 5m0s for pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2" in namespace "projected-3115" to be "running and ready"
    Sep  6 10:04:26.628: INFO: Pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.139839ms
    Sep  6 10:04:26.628: INFO: The phase of Pod annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:04:28.633: INFO: Pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016298452s
    Sep  6 10:04:28.633: INFO: The phase of Pod annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2 is Running (Ready = true)
    Sep  6 10:04:28.633: INFO: Pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2" satisfied condition "running and ready"
    Sep  6 10:04:29.152: INFO: Successfully updated pod "annotationupdate664a7e9c-24d4-4309-b007-4d29a60c5eb2"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:04:33.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3115" for this suite. 09/06/23 10:04:33.217
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:04:33.242
Sep  6 10:04:33.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 10:04:33.243
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:33.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:33.37
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-129a737e-fd19-4494-a454-7ab7ab95c371 09/06/23 10:04:33.373
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:04:33.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3175" for this suite. 09/06/23 10:04:33.378
------------------------------
• [0.146 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:04:33.242
    Sep  6 10:04:33.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 10:04:33.243
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:33.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:33.37
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-129a737e-fd19-4494-a454-7ab7ab95c371 09/06/23 10:04:33.373
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:04:33.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3175" for this suite. 09/06/23 10:04:33.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:04:33.388
Sep  6 10:04:33.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-probe 09/06/23 10:04:33.389
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:33.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:33.417
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c in namespace container-probe-6650 09/06/23 10:04:33.42
Sep  6 10:04:33.427: INFO: Waiting up to 5m0s for pod "liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c" in namespace "container-probe-6650" to be "not pending"
Sep  6 10:04:33.431: INFO: Pod "liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012252ms
Sep  6 10:04:35.436: INFO: Pod "liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008848302s
Sep  6 10:04:35.436: INFO: Pod "liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c" satisfied condition "not pending"
Sep  6 10:04:35.436: INFO: Started pod liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c in namespace container-probe-6650
STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:04:35.436
Sep  6 10:04:35.440: INFO: Initial restart count of pod liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c is 0
STEP: deleting the pod 09/06/23 10:08:36.285
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  6 10:08:36.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6650" for this suite. 09/06/23 10:08:36.313
------------------------------
• [SLOW TEST] [242.933 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:04:33.388
    Sep  6 10:04:33.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-probe 09/06/23 10:04:33.389
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:04:33.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:04:33.417
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c in namespace container-probe-6650 09/06/23 10:04:33.42
    Sep  6 10:04:33.427: INFO: Waiting up to 5m0s for pod "liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c" in namespace "container-probe-6650" to be "not pending"
    Sep  6 10:04:33.431: INFO: Pod "liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012252ms
    Sep  6 10:04:35.436: INFO: Pod "liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008848302s
    Sep  6 10:04:35.436: INFO: Pod "liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c" satisfied condition "not pending"
    Sep  6 10:04:35.436: INFO: Started pod liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c in namespace container-probe-6650
    STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:04:35.436
    Sep  6 10:04:35.440: INFO: Initial restart count of pod liveness-0d1a30fd-e426-4e46-9766-9305e6fe008c is 0
    STEP: deleting the pod 09/06/23 10:08:36.285
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:08:36.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6650" for this suite. 09/06/23 10:08:36.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:08:36.322
Sep  6 10:08:36.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:08:36.323
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:08:36.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:08:36.35
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-c136ec26-e68a-4d3e-8b50-839bdd340ed5 09/06/23 10:08:36.357
STEP: Creating secret with name secret-projected-all-test-volume-2c77e4a8-d4cb-475d-8abe-c584f2124146 09/06/23 10:08:36.363
STEP: Creating a pod to test Check all projections for projected volume plugin 09/06/23 10:08:36.369
Sep  6 10:08:36.382: INFO: Waiting up to 5m0s for pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d" in namespace "projected-9390" to be "Succeeded or Failed"
Sep  6 10:08:36.390: INFO: Pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281353ms
Sep  6 10:08:38.404: INFO: Pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021285031s
Sep  6 10:08:40.402: INFO: Pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019671707s
STEP: Saw pod success 09/06/23 10:08:40.402
Sep  6 10:08:40.403: INFO: Pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d" satisfied condition "Succeeded or Failed"
Sep  6 10:08:40.413: INFO: Trying to get logs from node kube-3 pod projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d container projected-all-volume-test: <nil>
STEP: delete the pod 09/06/23 10:08:40.457
Sep  6 10:08:40.482: INFO: Waiting for pod projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d to disappear
Sep  6 10:08:40.486: INFO: Pod projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Sep  6 10:08:40.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9390" for this suite. 09/06/23 10:08:40.49
------------------------------
• [4.174 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:08:36.322
    Sep  6 10:08:36.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:08:36.323
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:08:36.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:08:36.35
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-c136ec26-e68a-4d3e-8b50-839bdd340ed5 09/06/23 10:08:36.357
    STEP: Creating secret with name secret-projected-all-test-volume-2c77e4a8-d4cb-475d-8abe-c584f2124146 09/06/23 10:08:36.363
    STEP: Creating a pod to test Check all projections for projected volume plugin 09/06/23 10:08:36.369
    Sep  6 10:08:36.382: INFO: Waiting up to 5m0s for pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d" in namespace "projected-9390" to be "Succeeded or Failed"
    Sep  6 10:08:36.390: INFO: Pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281353ms
    Sep  6 10:08:38.404: INFO: Pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021285031s
    Sep  6 10:08:40.402: INFO: Pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019671707s
    STEP: Saw pod success 09/06/23 10:08:40.402
    Sep  6 10:08:40.403: INFO: Pod "projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d" satisfied condition "Succeeded or Failed"
    Sep  6 10:08:40.413: INFO: Trying to get logs from node kube-3 pod projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d container projected-all-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:08:40.457
    Sep  6 10:08:40.482: INFO: Waiting for pod projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d to disappear
    Sep  6 10:08:40.486: INFO: Pod projected-volume-06e5926f-40d9-4bb0-acee-6d17cd26523d no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:08:40.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9390" for this suite. 09/06/23 10:08:40.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:08:40.497
Sep  6 10:08:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:08:40.498
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:08:40.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:08:40.523
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Sep  6 10:08:40.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9981 version'
Sep  6 10:08:40.568: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Sep  6 10:08:40.568: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:14:46Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:08:49Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:08:40.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9981" for this suite. 09/06/23 10:08:40.572
------------------------------
• [0.083 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:08:40.497
    Sep  6 10:08:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:08:40.498
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:08:40.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:08:40.523
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Sep  6 10:08:40.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9981 version'
    Sep  6 10:08:40.568: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Sep  6 10:08:40.568: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:14:46Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:08:49Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:08:40.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9981" for this suite. 09/06/23 10:08:40.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:08:40.582
Sep  6 10:08:40.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:08:40.583
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:08:40.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:08:40.602
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 09/06/23 10:08:40.604
Sep  6 10:08:40.610: INFO: Waiting up to 5m0s for pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb" in namespace "emptydir-9008" to be "Succeeded or Failed"
Sep  6 10:08:40.613: INFO: Pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.502719ms
Sep  6 10:08:42.624: INFO: Pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014239102s
Sep  6 10:08:44.627: INFO: Pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016972924s
STEP: Saw pod success 09/06/23 10:08:44.627
Sep  6 10:08:44.628: INFO: Pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb" satisfied condition "Succeeded or Failed"
Sep  6 10:08:44.636: INFO: Trying to get logs from node kube-3 pod pod-1f881fe5-9147-4138-9bda-c210e763edfb container test-container: <nil>
STEP: delete the pod 09/06/23 10:08:44.649
Sep  6 10:08:44.666: INFO: Waiting for pod pod-1f881fe5-9147-4138-9bda-c210e763edfb to disappear
Sep  6 10:08:44.670: INFO: Pod pod-1f881fe5-9147-4138-9bda-c210e763edfb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:08:44.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9008" for this suite. 09/06/23 10:08:44.674
------------------------------
• [4.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:08:40.582
    Sep  6 10:08:40.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:08:40.583
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:08:40.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:08:40.602
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/06/23 10:08:40.604
    Sep  6 10:08:40.610: INFO: Waiting up to 5m0s for pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb" in namespace "emptydir-9008" to be "Succeeded or Failed"
    Sep  6 10:08:40.613: INFO: Pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.502719ms
    Sep  6 10:08:42.624: INFO: Pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014239102s
    Sep  6 10:08:44.627: INFO: Pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016972924s
    STEP: Saw pod success 09/06/23 10:08:44.627
    Sep  6 10:08:44.628: INFO: Pod "pod-1f881fe5-9147-4138-9bda-c210e763edfb" satisfied condition "Succeeded or Failed"
    Sep  6 10:08:44.636: INFO: Trying to get logs from node kube-3 pod pod-1f881fe5-9147-4138-9bda-c210e763edfb container test-container: <nil>
    STEP: delete the pod 09/06/23 10:08:44.649
    Sep  6 10:08:44.666: INFO: Waiting for pod pod-1f881fe5-9147-4138-9bda-c210e763edfb to disappear
    Sep  6 10:08:44.670: INFO: Pod pod-1f881fe5-9147-4138-9bda-c210e763edfb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:08:44.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9008" for this suite. 09/06/23 10:08:44.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:08:44.689
Sep  6 10:08:44.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename endpointslice 09/06/23 10:08:44.69
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:08:44.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:08:44.712
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 09/06/23 10:08:49.825
STEP: referencing matching pods with named port 09/06/23 10:08:54.833
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/06/23 10:08:59.849
STEP: recreating EndpointSlices after they've been deleted 09/06/23 10:09:04.878
Sep  6 10:09:04.938: INFO: EndpointSlice for Service endpointslice-8159/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  6 10:09:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8159" for this suite. 09/06/23 10:09:14.974
------------------------------
• [SLOW TEST] [30.306 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:08:44.689
    Sep  6 10:08:44.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename endpointslice 09/06/23 10:08:44.69
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:08:44.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:08:44.712
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 09/06/23 10:08:49.825
    STEP: referencing matching pods with named port 09/06/23 10:08:54.833
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/06/23 10:08:59.849
    STEP: recreating EndpointSlices after they've been deleted 09/06/23 10:09:04.878
    Sep  6 10:09:04.938: INFO: EndpointSlice for Service endpointslice-8159/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:09:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8159" for this suite. 09/06/23 10:09:14.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:09:14.996
Sep  6 10:09:14.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:09:14.997
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:09:15.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:09:15.043
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 09/06/23 10:09:15.045
Sep  6 10:09:15.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 create -f -'
Sep  6 10:09:15.707: INFO: stderr: ""
Sep  6 10:09:15.707: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/06/23 10:09:15.707
Sep  6 10:09:15.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 10:09:15.800: INFO: stderr: ""
Sep  6 10:09:15.800: INFO: stdout: "update-demo-nautilus-ks4bq update-demo-nautilus-lbcg2 "
Sep  6 10:09:15.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:09:15.852: INFO: stderr: ""
Sep  6 10:09:15.852: INFO: stdout: ""
Sep  6 10:09:15.852: INFO: update-demo-nautilus-ks4bq is created but not running
Sep  6 10:09:20.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 10:09:21.346: INFO: stderr: ""
Sep  6 10:09:21.346: INFO: stdout: "update-demo-nautilus-ks4bq update-demo-nautilus-lbcg2 "
Sep  6 10:09:21.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:09:21.854: INFO: stderr: ""
Sep  6 10:09:21.854: INFO: stdout: ""
Sep  6 10:09:21.854: INFO: update-demo-nautilus-ks4bq is created but not running
Sep  6 10:09:26.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 10:09:28.021: INFO: stderr: ""
Sep  6 10:09:28.021: INFO: stdout: "update-demo-nautilus-ks4bq update-demo-nautilus-lbcg2 "
Sep  6 10:09:28.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:09:28.442: INFO: stderr: ""
Sep  6 10:09:28.442: INFO: stdout: ""
Sep  6 10:09:28.442: INFO: update-demo-nautilus-ks4bq is created but not running
Sep  6 10:09:33.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 10:09:33.590: INFO: stderr: ""
Sep  6 10:09:33.590: INFO: stdout: "update-demo-nautilus-ks4bq update-demo-nautilus-lbcg2 "
Sep  6 10:09:33.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:09:33.678: INFO: stderr: ""
Sep  6 10:09:33.678: INFO: stdout: "true"
Sep  6 10:09:33.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 10:09:33.728: INFO: stderr: ""
Sep  6 10:09:33.728: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  6 10:09:33.728: INFO: validating pod update-demo-nautilus-ks4bq
Sep  6 10:09:33.732: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 10:09:33.732: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 10:09:33.732: INFO: update-demo-nautilus-ks4bq is verified up and running
Sep  6 10:09:33.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-lbcg2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:09:33.779: INFO: stderr: ""
Sep  6 10:09:33.779: INFO: stdout: "true"
Sep  6 10:09:33.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-lbcg2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 10:09:33.831: INFO: stderr: ""
Sep  6 10:09:33.831: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  6 10:09:33.831: INFO: validating pod update-demo-nautilus-lbcg2
Sep  6 10:09:33.835: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 10:09:33.835: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 10:09:33.835: INFO: update-demo-nautilus-lbcg2 is verified up and running
STEP: using delete to clean up resources 09/06/23 10:09:33.835
Sep  6 10:09:33.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 delete --grace-period=0 --force -f -'
Sep  6 10:09:33.891: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:09:33.891: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 10:09:33.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get rc,svc -l name=update-demo --no-headers'
Sep  6 10:09:33.994: INFO: stderr: "No resources found in kubectl-3242 namespace.\n"
Sep  6 10:09:33.994: INFO: stdout: ""
Sep  6 10:09:33.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 10:09:34.061: INFO: stderr: ""
Sep  6 10:09:34.061: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:09:34.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3242" for this suite. 09/06/23 10:09:34.065
------------------------------
• [SLOW TEST] [26.449 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:09:14.996
    Sep  6 10:09:14.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:09:14.997
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:09:15.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:09:15.043
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 09/06/23 10:09:15.045
    Sep  6 10:09:15.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 create -f -'
    Sep  6 10:09:15.707: INFO: stderr: ""
    Sep  6 10:09:15.707: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/06/23 10:09:15.707
    Sep  6 10:09:15.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 10:09:15.800: INFO: stderr: ""
    Sep  6 10:09:15.800: INFO: stdout: "update-demo-nautilus-ks4bq update-demo-nautilus-lbcg2 "
    Sep  6 10:09:15.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 10:09:15.852: INFO: stderr: ""
    Sep  6 10:09:15.852: INFO: stdout: ""
    Sep  6 10:09:15.852: INFO: update-demo-nautilus-ks4bq is created but not running
    Sep  6 10:09:20.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 10:09:21.346: INFO: stderr: ""
    Sep  6 10:09:21.346: INFO: stdout: "update-demo-nautilus-ks4bq update-demo-nautilus-lbcg2 "
    Sep  6 10:09:21.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 10:09:21.854: INFO: stderr: ""
    Sep  6 10:09:21.854: INFO: stdout: ""
    Sep  6 10:09:21.854: INFO: update-demo-nautilus-ks4bq is created but not running
    Sep  6 10:09:26.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 10:09:28.021: INFO: stderr: ""
    Sep  6 10:09:28.021: INFO: stdout: "update-demo-nautilus-ks4bq update-demo-nautilus-lbcg2 "
    Sep  6 10:09:28.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 10:09:28.442: INFO: stderr: ""
    Sep  6 10:09:28.442: INFO: stdout: ""
    Sep  6 10:09:28.442: INFO: update-demo-nautilus-ks4bq is created but not running
    Sep  6 10:09:33.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 10:09:33.590: INFO: stderr: ""
    Sep  6 10:09:33.590: INFO: stdout: "update-demo-nautilus-ks4bq update-demo-nautilus-lbcg2 "
    Sep  6 10:09:33.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 10:09:33.678: INFO: stderr: ""
    Sep  6 10:09:33.678: INFO: stdout: "true"
    Sep  6 10:09:33.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-ks4bq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  6 10:09:33.728: INFO: stderr: ""
    Sep  6 10:09:33.728: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  6 10:09:33.728: INFO: validating pod update-demo-nautilus-ks4bq
    Sep  6 10:09:33.732: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  6 10:09:33.732: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  6 10:09:33.732: INFO: update-demo-nautilus-ks4bq is verified up and running
    Sep  6 10:09:33.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-lbcg2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 10:09:33.779: INFO: stderr: ""
    Sep  6 10:09:33.779: INFO: stdout: "true"
    Sep  6 10:09:33.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods update-demo-nautilus-lbcg2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  6 10:09:33.831: INFO: stderr: ""
    Sep  6 10:09:33.831: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  6 10:09:33.831: INFO: validating pod update-demo-nautilus-lbcg2
    Sep  6 10:09:33.835: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  6 10:09:33.835: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  6 10:09:33.835: INFO: update-demo-nautilus-lbcg2 is verified up and running
    STEP: using delete to clean up resources 09/06/23 10:09:33.835
    Sep  6 10:09:33.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 delete --grace-period=0 --force -f -'
    Sep  6 10:09:33.891: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 10:09:33.891: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  6 10:09:33.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get rc,svc -l name=update-demo --no-headers'
    Sep  6 10:09:33.994: INFO: stderr: "No resources found in kubectl-3242 namespace.\n"
    Sep  6 10:09:33.994: INFO: stdout: ""
    Sep  6 10:09:33.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-3242 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  6 10:09:34.061: INFO: stderr: ""
    Sep  6 10:09:34.061: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:09:34.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3242" for this suite. 09/06/23 10:09:34.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:09:41.447
Sep  6 10:09:41.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pod-network-test 09/06/23 10:09:41.449
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:09:41.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:09:41.764
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-976 09/06/23 10:09:41.768
STEP: creating a selector 09/06/23 10:09:41.768
STEP: Creating the service pods in kubernetes 09/06/23 10:09:41.768
Sep  6 10:09:41.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  6 10:09:41.835: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-976" to be "running and ready"
Sep  6 10:09:41.849: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.892468ms
Sep  6 10:09:41.849: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:09:43.853: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018259911s
Sep  6 10:09:43.853: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:09:45.853: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017750986s
Sep  6 10:09:45.853: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:09:48.185: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.350315521s
Sep  6 10:09:48.185: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:09:49.853: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018236341s
Sep  6 10:09:49.853: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:09:51.855: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020327883s
Sep  6 10:09:51.855: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:09:53.853: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018080431s
Sep  6 10:09:53.853: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 10:09:55.863: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.028155427s
Sep  6 10:09:55.863: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 10:09:57.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.030246493s
Sep  6 10:09:57.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 10:09:59.861: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.026438152s
Sep  6 10:09:59.861: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 10:10:02.269: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.434418439s
Sep  6 10:10:02.269: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 10:10:03.864: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.029172263s
Sep  6 10:10:03.864: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  6 10:10:03.864: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  6 10:10:03.875: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-976" to be "running and ready"
Sep  6 10:10:03.888: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.610519ms
Sep  6 10:10:03.888: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  6 10:10:03.888: INFO: Pod "netserver-1" satisfied condition "running and ready"
Sep  6 10:10:03.900: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-976" to be "running and ready"
Sep  6 10:10:03.907: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.547367ms
Sep  6 10:10:03.907: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Sep  6 10:10:03.907: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 09/06/23 10:10:03.915
Sep  6 10:10:03.989: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-976" to be "running"
Sep  6 10:10:04.000: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.778875ms
Sep  6 10:10:06.010: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020930191s
Sep  6 10:10:08.015: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025676108s
Sep  6 10:10:08.015: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  6 10:10:08.028: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep  6 10:10:08.028: INFO: Breadth first check of 10.233.120.68 on host 10.2.20.101...
Sep  6 10:10:08.038: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.90:9080/dial?request=hostname&protocol=udp&host=10.233.120.68&port=8081&tries=1'] Namespace:pod-network-test-976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:10:08.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:10:08.040: INFO: ExecWithOptions: Clientset creation
Sep  6 10:10:08.040: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.120.68%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  6 10:10:08.192: INFO: Waiting for responses: map[]
Sep  6 10:10:08.192: INFO: reached 10.233.120.68 after 0/1 tries
Sep  6 10:10:08.192: INFO: Breadth first check of 10.233.120.199 on host 10.2.20.102...
Sep  6 10:10:08.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.90:9080/dial?request=hostname&protocol=udp&host=10.233.120.199&port=8081&tries=1'] Namespace:pod-network-test-976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:10:08.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:10:08.198: INFO: ExecWithOptions: Clientset creation
Sep  6 10:10:08.198: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.120.199%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  6 10:10:08.295: INFO: Waiting for responses: map[]
Sep  6 10:10:08.295: INFO: reached 10.233.120.199 after 0/1 tries
Sep  6 10:10:08.295: INFO: Breadth first check of 10.233.99.89 on host 10.2.20.103...
Sep  6 10:10:08.298: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.90:9080/dial?request=hostname&protocol=udp&host=10.233.99.89&port=8081&tries=1'] Namespace:pod-network-test-976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:10:08.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:10:08.299: INFO: ExecWithOptions: Clientset creation
Sep  6 10:10:08.299: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.99.89%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  6 10:10:08.370: INFO: Waiting for responses: map[]
Sep  6 10:10:08.370: INFO: reached 10.233.99.89 after 0/1 tries
Sep  6 10:10:08.370: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  6 10:10:08.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-976" for this suite. 09/06/23 10:10:08.374
------------------------------
• [SLOW TEST] [26.936 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:09:41.447
    Sep  6 10:09:41.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pod-network-test 09/06/23 10:09:41.449
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:09:41.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:09:41.764
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-976 09/06/23 10:09:41.768
    STEP: creating a selector 09/06/23 10:09:41.768
    STEP: Creating the service pods in kubernetes 09/06/23 10:09:41.768
    Sep  6 10:09:41.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  6 10:09:41.835: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-976" to be "running and ready"
    Sep  6 10:09:41.849: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.892468ms
    Sep  6 10:09:41.849: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:09:43.853: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018259911s
    Sep  6 10:09:43.853: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:09:45.853: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017750986s
    Sep  6 10:09:45.853: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:09:48.185: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.350315521s
    Sep  6 10:09:48.185: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:09:49.853: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018236341s
    Sep  6 10:09:49.853: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:09:51.855: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020327883s
    Sep  6 10:09:51.855: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:09:53.853: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018080431s
    Sep  6 10:09:53.853: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 10:09:55.863: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.028155427s
    Sep  6 10:09:55.863: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 10:09:57.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.030246493s
    Sep  6 10:09:57.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 10:09:59.861: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.026438152s
    Sep  6 10:09:59.861: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 10:10:02.269: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.434418439s
    Sep  6 10:10:02.269: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 10:10:03.864: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.029172263s
    Sep  6 10:10:03.864: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  6 10:10:03.864: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  6 10:10:03.875: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-976" to be "running and ready"
    Sep  6 10:10:03.888: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.610519ms
    Sep  6 10:10:03.888: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  6 10:10:03.888: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Sep  6 10:10:03.900: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-976" to be "running and ready"
    Sep  6 10:10:03.907: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.547367ms
    Sep  6 10:10:03.907: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Sep  6 10:10:03.907: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 09/06/23 10:10:03.915
    Sep  6 10:10:03.989: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-976" to be "running"
    Sep  6 10:10:04.000: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.778875ms
    Sep  6 10:10:06.010: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020930191s
    Sep  6 10:10:08.015: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025676108s
    Sep  6 10:10:08.015: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  6 10:10:08.028: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Sep  6 10:10:08.028: INFO: Breadth first check of 10.233.120.68 on host 10.2.20.101...
    Sep  6 10:10:08.038: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.90:9080/dial?request=hostname&protocol=udp&host=10.233.120.68&port=8081&tries=1'] Namespace:pod-network-test-976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:10:08.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:10:08.040: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:10:08.040: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.120.68%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  6 10:10:08.192: INFO: Waiting for responses: map[]
    Sep  6 10:10:08.192: INFO: reached 10.233.120.68 after 0/1 tries
    Sep  6 10:10:08.192: INFO: Breadth first check of 10.233.120.199 on host 10.2.20.102...
    Sep  6 10:10:08.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.90:9080/dial?request=hostname&protocol=udp&host=10.233.120.199&port=8081&tries=1'] Namespace:pod-network-test-976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:10:08.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:10:08.198: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:10:08.198: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.120.199%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  6 10:10:08.295: INFO: Waiting for responses: map[]
    Sep  6 10:10:08.295: INFO: reached 10.233.120.199 after 0/1 tries
    Sep  6 10:10:08.295: INFO: Breadth first check of 10.233.99.89 on host 10.2.20.103...
    Sep  6 10:10:08.298: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.90:9080/dial?request=hostname&protocol=udp&host=10.233.99.89&port=8081&tries=1'] Namespace:pod-network-test-976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:10:08.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:10:08.299: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:10:08.299: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.99.89%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  6 10:10:08.370: INFO: Waiting for responses: map[]
    Sep  6 10:10:08.370: INFO: reached 10.233.99.89 after 0/1 tries
    Sep  6 10:10:08.370: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:10:08.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-976" for this suite. 09/06/23 10:10:08.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:10:08.382
Sep  6 10:10:08.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 10:10:08.383
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:10:08.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:10:08.405
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-ed7c8365-34cb-4c25-bedf-df82cf8c2e5f 09/06/23 10:10:08.406
STEP: Creating a pod to test consume secrets 09/06/23 10:10:08.411
Sep  6 10:10:08.418: INFO: Waiting up to 5m0s for pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14" in namespace "secrets-6874" to be "Succeeded or Failed"
Sep  6 10:10:08.434: INFO: Pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14": Phase="Pending", Reason="", readiness=false. Elapsed: 15.818091ms
Sep  6 10:10:10.440: INFO: Pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021575887s
Sep  6 10:10:12.444: INFO: Pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025940882s
STEP: Saw pod success 09/06/23 10:10:12.445
Sep  6 10:10:12.445: INFO: Pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14" satisfied condition "Succeeded or Failed"
Sep  6 10:10:12.450: INFO: Trying to get logs from node kube-3 pod pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14 container secret-env-test: <nil>
STEP: delete the pod 09/06/23 10:10:12.458
Sep  6 10:10:12.479: INFO: Waiting for pod pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14 to disappear
Sep  6 10:10:12.483: INFO: Pod pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 10:10:12.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6874" for this suite. 09/06/23 10:10:12.498
------------------------------
• [4.130 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:10:08.382
    Sep  6 10:10:08.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 10:10:08.383
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:10:08.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:10:08.405
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-ed7c8365-34cb-4c25-bedf-df82cf8c2e5f 09/06/23 10:10:08.406
    STEP: Creating a pod to test consume secrets 09/06/23 10:10:08.411
    Sep  6 10:10:08.418: INFO: Waiting up to 5m0s for pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14" in namespace "secrets-6874" to be "Succeeded or Failed"
    Sep  6 10:10:08.434: INFO: Pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14": Phase="Pending", Reason="", readiness=false. Elapsed: 15.818091ms
    Sep  6 10:10:10.440: INFO: Pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021575887s
    Sep  6 10:10:12.444: INFO: Pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025940882s
    STEP: Saw pod success 09/06/23 10:10:12.445
    Sep  6 10:10:12.445: INFO: Pod "pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14" satisfied condition "Succeeded or Failed"
    Sep  6 10:10:12.450: INFO: Trying to get logs from node kube-3 pod pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14 container secret-env-test: <nil>
    STEP: delete the pod 09/06/23 10:10:12.458
    Sep  6 10:10:12.479: INFO: Waiting for pod pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14 to disappear
    Sep  6 10:10:12.483: INFO: Pod pod-secrets-8e039ad0-e7a6-4024-92e2-eeff9fdc8b14 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:10:12.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6874" for this suite. 09/06/23 10:10:12.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:10:12.513
Sep  6 10:10:12.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 10:10:12.514
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:10:12.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:10:12.543
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-d391667b-eeb4-45fa-9ad3-227c17b60c86 09/06/23 10:10:12.545
STEP: Creating a pod to test consume configMaps 09/06/23 10:10:12.566
Sep  6 10:10:12.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e" in namespace "configmap-6257" to be "Succeeded or Failed"
Sep  6 10:10:12.593: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.29676ms
Sep  6 10:10:14.609: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029623827s
Sep  6 10:10:16.950: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370529871s
Sep  6 10:10:18.597: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017644809s
STEP: Saw pod success 09/06/23 10:10:18.597
Sep  6 10:10:18.598: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e" satisfied condition "Succeeded or Failed"
Sep  6 10:10:18.601: INFO: Trying to get logs from node kube-3 pod pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e container configmap-volume-test: <nil>
STEP: delete the pod 09/06/23 10:10:18.606
Sep  6 10:10:18.737: INFO: Waiting for pod pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e to disappear
Sep  6 10:10:18.739: INFO: Pod pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:10:18.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6257" for this suite. 09/06/23 10:10:18.743
------------------------------
• [SLOW TEST] [6.249 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:10:12.513
    Sep  6 10:10:12.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 10:10:12.514
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:10:12.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:10:12.543
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-d391667b-eeb4-45fa-9ad3-227c17b60c86 09/06/23 10:10:12.545
    STEP: Creating a pod to test consume configMaps 09/06/23 10:10:12.566
    Sep  6 10:10:12.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e" in namespace "configmap-6257" to be "Succeeded or Failed"
    Sep  6 10:10:12.593: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.29676ms
    Sep  6 10:10:14.609: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029623827s
    Sep  6 10:10:16.950: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370529871s
    Sep  6 10:10:18.597: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017644809s
    STEP: Saw pod success 09/06/23 10:10:18.597
    Sep  6 10:10:18.598: INFO: Pod "pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e" satisfied condition "Succeeded or Failed"
    Sep  6 10:10:18.601: INFO: Trying to get logs from node kube-3 pod pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e container configmap-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:10:18.606
    Sep  6 10:10:18.737: INFO: Waiting for pod pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e to disappear
    Sep  6 10:10:18.739: INFO: Pod pod-configmaps-7aa7ad35-cbd3-43c2-92c5-f379df87487e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:10:18.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6257" for this suite. 09/06/23 10:10:18.743
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:10:18.763
Sep  6 10:10:18.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename cronjob 09/06/23 10:10:18.763
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:10:18.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:10:18.947
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 09/06/23 10:10:18.95
STEP: Ensuring a job is scheduled 09/06/23 10:10:18.977
STEP: Ensuring exactly one is scheduled 09/06/23 10:11:00.983
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/06/23 10:11:00.986
STEP: Ensuring no more jobs are scheduled 09/06/23 10:11:00.989
STEP: Removing cronjob 09/06/23 10:16:01.006
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  6 10:16:01.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6784" for this suite. 09/06/23 10:16:01.042
------------------------------
• [SLOW TEST] [342.330 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:10:18.763
    Sep  6 10:10:18.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename cronjob 09/06/23 10:10:18.763
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:10:18.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:10:18.947
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 09/06/23 10:10:18.95
    STEP: Ensuring a job is scheduled 09/06/23 10:10:18.977
    STEP: Ensuring exactly one is scheduled 09/06/23 10:11:00.983
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/06/23 10:11:00.986
    STEP: Ensuring no more jobs are scheduled 09/06/23 10:11:00.989
    STEP: Removing cronjob 09/06/23 10:16:01.006
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:16:01.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6784" for this suite. 09/06/23 10:16:01.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:16:01.095
Sep  6 10:16:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:16:01.096
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:16:01.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:16:01.144
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:16:01.162
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:16:01.466
STEP: Deploying the webhook pod 09/06/23 10:16:01.474
STEP: Wait for the deployment to be ready 09/06/23 10:16:01.49
Sep  6 10:16:01.498: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 10:16:03.539
STEP: Verifying the service has paired with the endpoint 09/06/23 10:16:03.557
Sep  6 10:16:04.557: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 09/06/23 10:16:04.656
STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:16:04.693
STEP: Deleting the collection of validation webhooks 09/06/23 10:16:04.719
STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:16:04.769
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:16:04.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8175" for this suite. 09/06/23 10:16:04.845
STEP: Destroying namespace "webhook-8175-markers" for this suite. 09/06/23 10:16:04.864
------------------------------
• [3.787 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:16:01.095
    Sep  6 10:16:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:16:01.096
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:16:01.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:16:01.144
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:16:01.162
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:16:01.466
    STEP: Deploying the webhook pod 09/06/23 10:16:01.474
    STEP: Wait for the deployment to be ready 09/06/23 10:16:01.49
    Sep  6 10:16:01.498: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 10:16:03.539
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:16:03.557
    Sep  6 10:16:04.557: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 09/06/23 10:16:04.656
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:16:04.693
    STEP: Deleting the collection of validation webhooks 09/06/23 10:16:04.719
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:16:04.769
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:16:04.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8175" for this suite. 09/06/23 10:16:04.845
    STEP: Destroying namespace "webhook-8175-markers" for this suite. 09/06/23 10:16:04.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:16:04.882
Sep  6 10:16:04.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-probe 09/06/23 10:16:04.882
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:16:04.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:16:04.916
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c in namespace container-probe-3428 09/06/23 10:16:04.925
Sep  6 10:16:04.937: INFO: Waiting up to 5m0s for pod "liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c" in namespace "container-probe-3428" to be "not pending"
Sep  6 10:16:04.944: INFO: Pod "liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299876ms
Sep  6 10:16:06.950: INFO: Pod "liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012645087s
Sep  6 10:16:06.950: INFO: Pod "liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c" satisfied condition "not pending"
Sep  6 10:16:06.950: INFO: Started pod liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c in namespace container-probe-3428
STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:16:06.95
Sep  6 10:16:06.953: INFO: Initial restart count of pod liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c is 0
Sep  6 10:16:27.085: INFO: Restart count of pod container-probe-3428/liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c is now 1 (20.131547478s elapsed)
STEP: deleting the pod 09/06/23 10:16:27.085
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  6 10:16:27.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3428" for this suite. 09/06/23 10:16:27.118
------------------------------
• [SLOW TEST] [22.247 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:16:04.882
    Sep  6 10:16:04.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-probe 09/06/23 10:16:04.882
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:16:04.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:16:04.916
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c in namespace container-probe-3428 09/06/23 10:16:04.925
    Sep  6 10:16:04.937: INFO: Waiting up to 5m0s for pod "liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c" in namespace "container-probe-3428" to be "not pending"
    Sep  6 10:16:04.944: INFO: Pod "liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299876ms
    Sep  6 10:16:06.950: INFO: Pod "liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012645087s
    Sep  6 10:16:06.950: INFO: Pod "liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c" satisfied condition "not pending"
    Sep  6 10:16:06.950: INFO: Started pod liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c in namespace container-probe-3428
    STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:16:06.95
    Sep  6 10:16:06.953: INFO: Initial restart count of pod liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c is 0
    Sep  6 10:16:27.085: INFO: Restart count of pod container-probe-3428/liveness-182c8197-cdc4-4207-bc89-487c35f6ea6c is now 1 (20.131547478s elapsed)
    STEP: deleting the pod 09/06/23 10:16:27.085
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:16:27.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3428" for this suite. 09/06/23 10:16:27.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:16:27.132
Sep  6 10:16:27.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename ingressclass 09/06/23 10:16:27.133
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:16:27.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:16:27.158
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 09/06/23 10:16:27.161
STEP: getting /apis/networking.k8s.io 09/06/23 10:16:27.162
STEP: getting /apis/networking.k8s.iov1 09/06/23 10:16:27.164
STEP: creating 09/06/23 10:16:27.165
STEP: getting 09/06/23 10:16:27.178
STEP: listing 09/06/23 10:16:27.183
STEP: watching 09/06/23 10:16:27.187
Sep  6 10:16:27.187: INFO: starting watch
STEP: patching 09/06/23 10:16:27.189
STEP: updating 09/06/23 10:16:27.194
Sep  6 10:16:27.201: INFO: waiting for watch events with expected annotations
Sep  6 10:16:27.201: INFO: saw patched and updated annotations
STEP: deleting 09/06/23 10:16:27.201
STEP: deleting a collection 09/06/23 10:16:27.213
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Sep  6 10:16:27.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-940" for this suite. 09/06/23 10:16:27.232
------------------------------
• [0.107 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:16:27.132
    Sep  6 10:16:27.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename ingressclass 09/06/23 10:16:27.133
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:16:27.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:16:27.158
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 09/06/23 10:16:27.161
    STEP: getting /apis/networking.k8s.io 09/06/23 10:16:27.162
    STEP: getting /apis/networking.k8s.iov1 09/06/23 10:16:27.164
    STEP: creating 09/06/23 10:16:27.165
    STEP: getting 09/06/23 10:16:27.178
    STEP: listing 09/06/23 10:16:27.183
    STEP: watching 09/06/23 10:16:27.187
    Sep  6 10:16:27.187: INFO: starting watch
    STEP: patching 09/06/23 10:16:27.189
    STEP: updating 09/06/23 10:16:27.194
    Sep  6 10:16:27.201: INFO: waiting for watch events with expected annotations
    Sep  6 10:16:27.201: INFO: saw patched and updated annotations
    STEP: deleting 09/06/23 10:16:27.201
    STEP: deleting a collection 09/06/23 10:16:27.213
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:16:27.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-940" for this suite. 09/06/23 10:16:27.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:16:27.242
Sep  6 10:16:27.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename var-expansion 09/06/23 10:16:27.243
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:16:27.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:16:27.266
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 09/06/23 10:16:27.268
STEP: waiting for pod running 09/06/23 10:16:27.275
Sep  6 10:16:27.275: INFO: Waiting up to 2m0s for pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" in namespace "var-expansion-7194" to be "running"
Sep  6 10:16:27.279: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086776ms
Sep  6 10:16:29.289: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.013888535s
Sep  6 10:16:29.290: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" satisfied condition "running"
STEP: creating a file in subpath 09/06/23 10:16:29.29
Sep  6 10:16:29.300: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7194 PodName:var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:16:29.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:16:29.301: INFO: ExecWithOptions: Clientset creation
Sep  6 10:16:29.301: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7194/pods/var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 09/06/23 10:16:29.437
Sep  6 10:16:29.444: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7194 PodName:var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:16:29.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:16:29.445: INFO: ExecWithOptions: Clientset creation
Sep  6 10:16:29.445: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7194/pods/var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 09/06/23 10:16:29.526
Sep  6 10:16:30.055: INFO: Successfully updated pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa"
STEP: waiting for annotated pod running 09/06/23 10:16:30.055
Sep  6 10:16:30.056: INFO: Waiting up to 2m0s for pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" in namespace "var-expansion-7194" to be "running"
Sep  6 10:16:30.060: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.121221ms
Sep  6 10:16:30.060: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" satisfied condition "running"
STEP: deleting the pod gracefully 09/06/23 10:16:30.06
Sep  6 10:16:30.060: INFO: Deleting pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" in namespace "var-expansion-7194"
Sep  6 10:16:30.070: INFO: Wait up to 5m0s for pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  6 10:17:04.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7194" for this suite. 09/06/23 10:17:04.106
------------------------------
• [SLOW TEST] [36.885 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:16:27.242
    Sep  6 10:16:27.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename var-expansion 09/06/23 10:16:27.243
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:16:27.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:16:27.266
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 09/06/23 10:16:27.268
    STEP: waiting for pod running 09/06/23 10:16:27.275
    Sep  6 10:16:27.275: INFO: Waiting up to 2m0s for pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" in namespace "var-expansion-7194" to be "running"
    Sep  6 10:16:27.279: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086776ms
    Sep  6 10:16:29.289: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.013888535s
    Sep  6 10:16:29.290: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" satisfied condition "running"
    STEP: creating a file in subpath 09/06/23 10:16:29.29
    Sep  6 10:16:29.300: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7194 PodName:var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:16:29.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:16:29.301: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:16:29.301: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7194/pods/var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 09/06/23 10:16:29.437
    Sep  6 10:16:29.444: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7194 PodName:var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:16:29.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:16:29.445: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:16:29.445: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7194/pods/var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 09/06/23 10:16:29.526
    Sep  6 10:16:30.055: INFO: Successfully updated pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa"
    STEP: waiting for annotated pod running 09/06/23 10:16:30.055
    Sep  6 10:16:30.056: INFO: Waiting up to 2m0s for pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" in namespace "var-expansion-7194" to be "running"
    Sep  6 10:16:30.060: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.121221ms
    Sep  6 10:16:30.060: INFO: Pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" satisfied condition "running"
    STEP: deleting the pod gracefully 09/06/23 10:16:30.06
    Sep  6 10:16:30.060: INFO: Deleting pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" in namespace "var-expansion-7194"
    Sep  6 10:16:30.070: INFO: Wait up to 5m0s for pod "var-expansion-807b8eec-772b-4052-a66d-e677764ca3fa" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:17:04.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7194" for this suite. 09/06/23 10:17:04.106
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:17:04.128
Sep  6 10:17:04.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 10:17:04.131
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:04.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:04.169
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9800 09/06/23 10:17:04.171
STEP: changing the ExternalName service to type=NodePort 09/06/23 10:17:04.267
STEP: creating replication controller externalname-service in namespace services-9800 09/06/23 10:17:04.3
I0906 10:17:04.312187      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9800, replica count: 2
I0906 10:17:07.362926      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 10:17:07.363: INFO: Creating new exec pod
Sep  6 10:17:07.392: INFO: Waiting up to 5m0s for pod "execpodtk2rd" in namespace "services-9800" to be "running"
Sep  6 10:17:07.400: INFO: Pod "execpodtk2rd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.90343ms
Sep  6 10:17:09.438: INFO: Pod "execpodtk2rd": Phase="Running", Reason="", readiness=true. Elapsed: 2.045387327s
Sep  6 10:17:09.438: INFO: Pod "execpodtk2rd" satisfied condition "running"
Sep  6 10:17:10.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-9800 exec execpodtk2rd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  6 10:17:10.749: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  6 10:17:10.749: INFO: stdout: ""
Sep  6 10:17:10.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-9800 exec execpodtk2rd -- /bin/sh -x -c nc -v -z -w 2 10.233.44.93 80'
Sep  6 10:17:10.856: INFO: stderr: "+ nc -v -z -w 2 10.233.44.93 80\nConnection to 10.233.44.93 80 port [tcp/http] succeeded!\n"
Sep  6 10:17:10.856: INFO: stdout: ""
Sep  6 10:17:10.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-9800 exec execpodtk2rd -- /bin/sh -x -c nc -v -z -w 2 10.2.20.101 30549'
Sep  6 10:17:10.958: INFO: stderr: "+ nc -v -z -w 2 10.2.20.101 30549\nConnection to 10.2.20.101 30549 port [tcp/*] succeeded!\n"
Sep  6 10:17:10.958: INFO: stdout: ""
Sep  6 10:17:10.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-9800 exec execpodtk2rd -- /bin/sh -x -c nc -v -z -w 2 10.2.20.103 30549'
Sep  6 10:17:11.075: INFO: stderr: "+ nc -v -z -w 2 10.2.20.103 30549\nConnection to 10.2.20.103 30549 port [tcp/*] succeeded!\n"
Sep  6 10:17:11.075: INFO: stdout: ""
Sep  6 10:17:11.075: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 10:17:11.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9800" for this suite. 09/06/23 10:17:11.204
------------------------------
• [SLOW TEST] [7.111 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:17:04.128
    Sep  6 10:17:04.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 10:17:04.131
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:04.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:04.169
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9800 09/06/23 10:17:04.171
    STEP: changing the ExternalName service to type=NodePort 09/06/23 10:17:04.267
    STEP: creating replication controller externalname-service in namespace services-9800 09/06/23 10:17:04.3
    I0906 10:17:04.312187      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9800, replica count: 2
    I0906 10:17:07.362926      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 10:17:07.363: INFO: Creating new exec pod
    Sep  6 10:17:07.392: INFO: Waiting up to 5m0s for pod "execpodtk2rd" in namespace "services-9800" to be "running"
    Sep  6 10:17:07.400: INFO: Pod "execpodtk2rd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.90343ms
    Sep  6 10:17:09.438: INFO: Pod "execpodtk2rd": Phase="Running", Reason="", readiness=true. Elapsed: 2.045387327s
    Sep  6 10:17:09.438: INFO: Pod "execpodtk2rd" satisfied condition "running"
    Sep  6 10:17:10.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-9800 exec execpodtk2rd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  6 10:17:10.749: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  6 10:17:10.749: INFO: stdout: ""
    Sep  6 10:17:10.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-9800 exec execpodtk2rd -- /bin/sh -x -c nc -v -z -w 2 10.233.44.93 80'
    Sep  6 10:17:10.856: INFO: stderr: "+ nc -v -z -w 2 10.233.44.93 80\nConnection to 10.233.44.93 80 port [tcp/http] succeeded!\n"
    Sep  6 10:17:10.856: INFO: stdout: ""
    Sep  6 10:17:10.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-9800 exec execpodtk2rd -- /bin/sh -x -c nc -v -z -w 2 10.2.20.101 30549'
    Sep  6 10:17:10.958: INFO: stderr: "+ nc -v -z -w 2 10.2.20.101 30549\nConnection to 10.2.20.101 30549 port [tcp/*] succeeded!\n"
    Sep  6 10:17:10.958: INFO: stdout: ""
    Sep  6 10:17:10.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-9800 exec execpodtk2rd -- /bin/sh -x -c nc -v -z -w 2 10.2.20.103 30549'
    Sep  6 10:17:11.075: INFO: stderr: "+ nc -v -z -w 2 10.2.20.103 30549\nConnection to 10.2.20.103 30549 port [tcp/*] succeeded!\n"
    Sep  6 10:17:11.075: INFO: stdout: ""
    Sep  6 10:17:11.075: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:17:11.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9800" for this suite. 09/06/23 10:17:11.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:17:11.239
Sep  6 10:17:11.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir-wrapper 09/06/23 10:17:11.24
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:11.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:11.318
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Sep  6 10:17:11.337: INFO: Waiting up to 5m0s for pod "pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c" in namespace "emptydir-wrapper-3750" to be "running and ready"
Sep  6 10:17:11.344: INFO: Pod "pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.713125ms
Sep  6 10:17:11.344: INFO: The phase of Pod pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:17:13.358: INFO: Pod "pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c": Phase="Running", Reason="", readiness=true. Elapsed: 2.020780698s
Sep  6 10:17:13.358: INFO: The phase of Pod pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c is Running (Ready = true)
Sep  6 10:17:13.358: INFO: Pod "pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c" satisfied condition "running and ready"
STEP: Cleaning up the secret 09/06/23 10:17:13.37
STEP: Cleaning up the configmap 09/06/23 10:17:13.391
STEP: Cleaning up the pod 09/06/23 10:17:13.398
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:17:13.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3750" for this suite. 09/06/23 10:17:13.429
------------------------------
• [2.199 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:17:11.239
    Sep  6 10:17:11.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir-wrapper 09/06/23 10:17:11.24
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:11.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:11.318
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Sep  6 10:17:11.337: INFO: Waiting up to 5m0s for pod "pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c" in namespace "emptydir-wrapper-3750" to be "running and ready"
    Sep  6 10:17:11.344: INFO: Pod "pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.713125ms
    Sep  6 10:17:11.344: INFO: The phase of Pod pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:17:13.358: INFO: Pod "pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c": Phase="Running", Reason="", readiness=true. Elapsed: 2.020780698s
    Sep  6 10:17:13.358: INFO: The phase of Pod pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c is Running (Ready = true)
    Sep  6 10:17:13.358: INFO: Pod "pod-secrets-45ac46b4-5ab6-4d1c-a0d3-7008f9fa281c" satisfied condition "running and ready"
    STEP: Cleaning up the secret 09/06/23 10:17:13.37
    STEP: Cleaning up the configmap 09/06/23 10:17:13.391
    STEP: Cleaning up the pod 09/06/23 10:17:13.398
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:17:13.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3750" for this suite. 09/06/23 10:17:13.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:17:13.44
Sep  6 10:17:13.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename security-context-test 09/06/23 10:17:13.442
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:13.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:13.463
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Sep  6 10:17:13.478: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483" in namespace "security-context-test-475" to be "Succeeded or Failed"
Sep  6 10:17:13.482: INFO: Pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483": Phase="Pending", Reason="", readiness=false. Elapsed: 3.846941ms
Sep  6 10:17:15.486: INFO: Pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007970383s
Sep  6 10:17:17.491: INFO: Pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012485087s
Sep  6 10:17:17.491: INFO: Pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  6 10:17:17.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-475" for this suite. 09/06/23 10:17:17.498
------------------------------
• [4.068 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:17:13.44
    Sep  6 10:17:13.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename security-context-test 09/06/23 10:17:13.442
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:13.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:13.463
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Sep  6 10:17:13.478: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483" in namespace "security-context-test-475" to be "Succeeded or Failed"
    Sep  6 10:17:13.482: INFO: Pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483": Phase="Pending", Reason="", readiness=false. Elapsed: 3.846941ms
    Sep  6 10:17:15.486: INFO: Pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007970383s
    Sep  6 10:17:17.491: INFO: Pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012485087s
    Sep  6 10:17:17.491: INFO: Pod "busybox-user-65534-c58430c7-0fd8-4236-bd95-2e3d733aa483" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:17:17.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-475" for this suite. 09/06/23 10:17:17.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:17:17.511
Sep  6 10:17:17.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 10:17:17.512
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:17.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:17.549
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-0a49a3fa-40a6-4cf3-a4a8-0c080cdb543e 09/06/23 10:17:17.553
STEP: Creating a pod to test consume configMaps 09/06/23 10:17:17.563
Sep  6 10:17:17.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea" in namespace "configmap-3171" to be "Succeeded or Failed"
Sep  6 10:17:17.603: INFO: Pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea": Phase="Pending", Reason="", readiness=false. Elapsed: 12.97754ms
Sep  6 10:17:19.615: INFO: Pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024370802s
Sep  6 10:17:21.608: INFO: Pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017077626s
STEP: Saw pod success 09/06/23 10:17:21.608
Sep  6 10:17:21.608: INFO: Pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea" satisfied condition "Succeeded or Failed"
Sep  6 10:17:21.610: INFO: Trying to get logs from node kube-3 pod pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea container agnhost-container: <nil>
STEP: delete the pod 09/06/23 10:17:21.622
Sep  6 10:17:22.343: INFO: Waiting for pod pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea to disappear
Sep  6 10:17:22.348: INFO: Pod pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:17:22.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3171" for this suite. 09/06/23 10:17:22.354
------------------------------
• [4.855 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:17:17.511
    Sep  6 10:17:17.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 10:17:17.512
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:17.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:17.549
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-0a49a3fa-40a6-4cf3-a4a8-0c080cdb543e 09/06/23 10:17:17.553
    STEP: Creating a pod to test consume configMaps 09/06/23 10:17:17.563
    Sep  6 10:17:17.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea" in namespace "configmap-3171" to be "Succeeded or Failed"
    Sep  6 10:17:17.603: INFO: Pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea": Phase="Pending", Reason="", readiness=false. Elapsed: 12.97754ms
    Sep  6 10:17:19.615: INFO: Pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024370802s
    Sep  6 10:17:21.608: INFO: Pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017077626s
    STEP: Saw pod success 09/06/23 10:17:21.608
    Sep  6 10:17:21.608: INFO: Pod "pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea" satisfied condition "Succeeded or Failed"
    Sep  6 10:17:21.610: INFO: Trying to get logs from node kube-3 pod pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 10:17:21.622
    Sep  6 10:17:22.343: INFO: Waiting for pod pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea to disappear
    Sep  6 10:17:22.348: INFO: Pod pod-configmaps-3dd34930-f262-4498-a7c3-227c7b9c55ea no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:17:22.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3171" for this suite. 09/06/23 10:17:22.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:17:22.367
Sep  6 10:17:22.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename job 09/06/23 10:17:22.368
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:22.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:22.389
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 09/06/23 10:17:22.391
STEP: Ensuring active pods == parallelism 09/06/23 10:17:22.396
STEP: delete a job 09/06/23 10:17:26.412
STEP: deleting Job.batch foo in namespace job-2613, will wait for the garbage collector to delete the pods 09/06/23 10:17:26.412
Sep  6 10:17:26.511: INFO: Deleting Job.batch foo took: 33.327867ms
Sep  6 10:17:27.312: INFO: Terminating Job.batch foo pods took: 801.150631ms
STEP: Ensuring job was deleted 09/06/23 10:17:58.613
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  6 10:17:58.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2613" for this suite. 09/06/23 10:17:58.621
------------------------------
• [SLOW TEST] [36.262 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:17:22.367
    Sep  6 10:17:22.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename job 09/06/23 10:17:22.368
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:22.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:22.389
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 09/06/23 10:17:22.391
    STEP: Ensuring active pods == parallelism 09/06/23 10:17:22.396
    STEP: delete a job 09/06/23 10:17:26.412
    STEP: deleting Job.batch foo in namespace job-2613, will wait for the garbage collector to delete the pods 09/06/23 10:17:26.412
    Sep  6 10:17:26.511: INFO: Deleting Job.batch foo took: 33.327867ms
    Sep  6 10:17:27.312: INFO: Terminating Job.batch foo pods took: 801.150631ms
    STEP: Ensuring job was deleted 09/06/23 10:17:58.613
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:17:58.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2613" for this suite. 09/06/23 10:17:58.621
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:17:58.63
Sep  6 10:17:58.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:17:58.631
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:58.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:58.653
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 09/06/23 10:17:58.655
Sep  6 10:17:58.655: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-5366 proxy --unix-socket=/tmp/kubectl-proxy-unix2458053716/test'
STEP: retrieving proxy /api/ output 09/06/23 10:17:58.694
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:17:58.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5366" for this suite. 09/06/23 10:17:58.699
------------------------------
• [0.076 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:17:58.63
    Sep  6 10:17:58.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:17:58.631
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:58.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:58.653
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 09/06/23 10:17:58.655
    Sep  6 10:17:58.655: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-5366 proxy --unix-socket=/tmp/kubectl-proxy-unix2458053716/test'
    STEP: retrieving proxy /api/ output 09/06/23 10:17:58.694
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:17:58.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5366" for this suite. 09/06/23 10:17:58.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:17:58.708
Sep  6 10:17:58.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:17:58.708
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:58.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:58.737
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 09/06/23 10:17:58.739
Sep  6 10:17:58.746: INFO: Waiting up to 5m0s for pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537" in namespace "emptydir-7853" to be "Succeeded or Failed"
Sep  6 10:17:58.759: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537": Phase="Pending", Reason="", readiness=false. Elapsed: 12.39356ms
Sep  6 10:18:02.354: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537": Phase="Running", Reason="", readiness=true. Elapsed: 3.608000662s
Sep  6 10:18:02.773: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537": Phase="Running", Reason="", readiness=false. Elapsed: 4.027045666s
Sep  6 10:18:04.762: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016266688s
STEP: Saw pod success 09/06/23 10:18:04.763
Sep  6 10:18:04.763: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537" satisfied condition "Succeeded or Failed"
Sep  6 10:18:04.771: INFO: Trying to get logs from node kube-3 pod pod-e871cd96-cf1e-41ab-9d91-1b3668592537 container test-container: <nil>
STEP: delete the pod 09/06/23 10:18:04.777
Sep  6 10:18:04.828: INFO: Waiting for pod pod-e871cd96-cf1e-41ab-9d91-1b3668592537 to disappear
Sep  6 10:18:04.840: INFO: Pod pod-e871cd96-cf1e-41ab-9d91-1b3668592537 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:18:04.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7853" for this suite. 09/06/23 10:18:04.845
------------------------------
• [SLOW TEST] [6.150 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:17:58.708
    Sep  6 10:17:58.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:17:58.708
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:17:58.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:17:58.737
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 09/06/23 10:17:58.739
    Sep  6 10:17:58.746: INFO: Waiting up to 5m0s for pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537" in namespace "emptydir-7853" to be "Succeeded or Failed"
    Sep  6 10:17:58.759: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537": Phase="Pending", Reason="", readiness=false. Elapsed: 12.39356ms
    Sep  6 10:18:02.354: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537": Phase="Running", Reason="", readiness=true. Elapsed: 3.608000662s
    Sep  6 10:18:02.773: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537": Phase="Running", Reason="", readiness=false. Elapsed: 4.027045666s
    Sep  6 10:18:04.762: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016266688s
    STEP: Saw pod success 09/06/23 10:18:04.763
    Sep  6 10:18:04.763: INFO: Pod "pod-e871cd96-cf1e-41ab-9d91-1b3668592537" satisfied condition "Succeeded or Failed"
    Sep  6 10:18:04.771: INFO: Trying to get logs from node kube-3 pod pod-e871cd96-cf1e-41ab-9d91-1b3668592537 container test-container: <nil>
    STEP: delete the pod 09/06/23 10:18:04.777
    Sep  6 10:18:04.828: INFO: Waiting for pod pod-e871cd96-cf1e-41ab-9d91-1b3668592537 to disappear
    Sep  6 10:18:04.840: INFO: Pod pod-e871cd96-cf1e-41ab-9d91-1b3668592537 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:18:04.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7853" for this suite. 09/06/23 10:18:04.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:18:04.86
Sep  6 10:18:04.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 10:18:04.861
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:04.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:04.961
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 09/06/23 10:18:04.963
STEP: fetching the ConfigMap 09/06/23 10:18:04.984
STEP: patching the ConfigMap 09/06/23 10:18:04.989
STEP: listing all ConfigMaps in all namespaces with a label selector 09/06/23 10:18:05.027
STEP: deleting the ConfigMap by collection with a label selector 09/06/23 10:18:05.032
STEP: listing all ConfigMaps in test namespace 09/06/23 10:18:05.045
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:18:05.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4076" for this suite. 09/06/23 10:18:05.054
------------------------------
• [0.272 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:18:04.86
    Sep  6 10:18:04.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 10:18:04.861
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:04.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:04.961
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 09/06/23 10:18:04.963
    STEP: fetching the ConfigMap 09/06/23 10:18:04.984
    STEP: patching the ConfigMap 09/06/23 10:18:04.989
    STEP: listing all ConfigMaps in all namespaces with a label selector 09/06/23 10:18:05.027
    STEP: deleting the ConfigMap by collection with a label selector 09/06/23 10:18:05.032
    STEP: listing all ConfigMaps in test namespace 09/06/23 10:18:05.045
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:18:05.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4076" for this suite. 09/06/23 10:18:05.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:18:05.133
Sep  6 10:18:05.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename subpath 09/06/23 10:18:05.134
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:05.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:05.161
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/06/23 10:18:05.166
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-7g5n 09/06/23 10:18:05.18
STEP: Creating a pod to test atomic-volume-subpath 09/06/23 10:18:05.18
Sep  6 10:18:05.192: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7g5n" in namespace "subpath-4914" to be "Succeeded or Failed"
Sep  6 10:18:05.208: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Pending", Reason="", readiness=false. Elapsed: 15.263661ms
Sep  6 10:18:07.212: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 2.019409511s
Sep  6 10:18:09.221: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 4.028036809s
Sep  6 10:18:11.220: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 6.027751921s
Sep  6 10:18:13.213: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 8.02033819s
Sep  6 10:18:15.220: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 10.027650952s
Sep  6 10:18:17.213: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 12.020186113s
Sep  6 10:18:19.224: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 14.031592202s
Sep  6 10:18:21.218: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 16.02554974s
Sep  6 10:18:23.220: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 18.027813222s
Sep  6 10:18:25.221: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 20.028202564s
Sep  6 10:18:27.221: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=false. Elapsed: 22.028283968s
Sep  6 10:18:29.228: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.035301377s
STEP: Saw pod success 09/06/23 10:18:29.228
Sep  6 10:18:29.228: INFO: Pod "pod-subpath-test-secret-7g5n" satisfied condition "Succeeded or Failed"
Sep  6 10:18:29.236: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-secret-7g5n container test-container-subpath-secret-7g5n: <nil>
STEP: delete the pod 09/06/23 10:18:29.245
Sep  6 10:18:29.264: INFO: Waiting for pod pod-subpath-test-secret-7g5n to disappear
Sep  6 10:18:29.266: INFO: Pod pod-subpath-test-secret-7g5n no longer exists
STEP: Deleting pod pod-subpath-test-secret-7g5n 09/06/23 10:18:29.266
Sep  6 10:18:29.267: INFO: Deleting pod "pod-subpath-test-secret-7g5n" in namespace "subpath-4914"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  6 10:18:29.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4914" for this suite. 09/06/23 10:18:29.272
------------------------------
• [SLOW TEST] [24.145 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:18:05.133
    Sep  6 10:18:05.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename subpath 09/06/23 10:18:05.134
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:05.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:05.161
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/06/23 10:18:05.166
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-7g5n 09/06/23 10:18:05.18
    STEP: Creating a pod to test atomic-volume-subpath 09/06/23 10:18:05.18
    Sep  6 10:18:05.192: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7g5n" in namespace "subpath-4914" to be "Succeeded or Failed"
    Sep  6 10:18:05.208: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Pending", Reason="", readiness=false. Elapsed: 15.263661ms
    Sep  6 10:18:07.212: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 2.019409511s
    Sep  6 10:18:09.221: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 4.028036809s
    Sep  6 10:18:11.220: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 6.027751921s
    Sep  6 10:18:13.213: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 8.02033819s
    Sep  6 10:18:15.220: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 10.027650952s
    Sep  6 10:18:17.213: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 12.020186113s
    Sep  6 10:18:19.224: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 14.031592202s
    Sep  6 10:18:21.218: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 16.02554974s
    Sep  6 10:18:23.220: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 18.027813222s
    Sep  6 10:18:25.221: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=true. Elapsed: 20.028202564s
    Sep  6 10:18:27.221: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Running", Reason="", readiness=false. Elapsed: 22.028283968s
    Sep  6 10:18:29.228: INFO: Pod "pod-subpath-test-secret-7g5n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.035301377s
    STEP: Saw pod success 09/06/23 10:18:29.228
    Sep  6 10:18:29.228: INFO: Pod "pod-subpath-test-secret-7g5n" satisfied condition "Succeeded or Failed"
    Sep  6 10:18:29.236: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-secret-7g5n container test-container-subpath-secret-7g5n: <nil>
    STEP: delete the pod 09/06/23 10:18:29.245
    Sep  6 10:18:29.264: INFO: Waiting for pod pod-subpath-test-secret-7g5n to disappear
    Sep  6 10:18:29.266: INFO: Pod pod-subpath-test-secret-7g5n no longer exists
    STEP: Deleting pod pod-subpath-test-secret-7g5n 09/06/23 10:18:29.266
    Sep  6 10:18:29.267: INFO: Deleting pod "pod-subpath-test-secret-7g5n" in namespace "subpath-4914"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:18:29.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4914" for this suite. 09/06/23 10:18:29.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:18:29.281
Sep  6 10:18:29.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename job 09/06/23 10:18:29.282
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:29.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:29.302
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 09/06/23 10:18:29.304
STEP: Ensuring job reaches completions 09/06/23 10:18:29.31
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  6 10:18:41.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3470" for this suite. 09/06/23 10:18:41.337
------------------------------
• [SLOW TEST] [12.071 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:18:29.281
    Sep  6 10:18:29.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename job 09/06/23 10:18:29.282
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:29.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:29.302
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 09/06/23 10:18:29.304
    STEP: Ensuring job reaches completions 09/06/23 10:18:29.31
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:18:41.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3470" for this suite. 09/06/23 10:18:41.337
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:18:41.353
Sep  6 10:18:41.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:18:41.354
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:41.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:41.433
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 09/06/23 10:18:41.435
Sep  6 10:18:41.444: INFO: Waiting up to 5m0s for pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd" in namespace "projected-7086" to be "running and ready"
Sep  6 10:18:41.449: INFO: Pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.643527ms
Sep  6 10:18:41.449: INFO: The phase of Pod labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:18:43.463: INFO: Pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.019180202s
Sep  6 10:18:43.463: INFO: The phase of Pod labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd is Running (Ready = true)
Sep  6 10:18:43.463: INFO: Pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd" satisfied condition "running and ready"
Sep  6 10:18:44.037: INFO: Successfully updated pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 10:18:48.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7086" for this suite. 09/06/23 10:18:48.092
------------------------------
• [SLOW TEST] [6.753 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:18:41.353
    Sep  6 10:18:41.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:18:41.354
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:41.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:41.433
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 09/06/23 10:18:41.435
    Sep  6 10:18:41.444: INFO: Waiting up to 5m0s for pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd" in namespace "projected-7086" to be "running and ready"
    Sep  6 10:18:41.449: INFO: Pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.643527ms
    Sep  6 10:18:41.449: INFO: The phase of Pod labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:18:43.463: INFO: Pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.019180202s
    Sep  6 10:18:43.463: INFO: The phase of Pod labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd is Running (Ready = true)
    Sep  6 10:18:43.463: INFO: Pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd" satisfied condition "running and ready"
    Sep  6 10:18:44.037: INFO: Successfully updated pod "labelsupdate13044048-1b40-4c0e-99c6-76ee810304dd"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:18:48.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7086" for this suite. 09/06/23 10:18:48.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:18:48.106
Sep  6 10:18:48.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename daemonsets 09/06/23 10:18:48.107
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:48.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:48.128
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 09/06/23 10:18:48.151
STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 10:18:48.159
Sep  6 10:18:48.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 10:18:48.166: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 10:18:49.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 10:18:49.181: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 10:18:52.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:18:52.284: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:18:53.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:18:53.186: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:18:54.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:18:54.184: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:18:55.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:18:55.176: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:18:56.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:18:56.172: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:19:01.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:19:01.177: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:19:02.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:19:02.812: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:19:03.178: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:19:03.178: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:19:04.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 10:19:04.198: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 09/06/23 10:19:04.209
Sep  6 10:19:04.219: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 09/06/23 10:19:04.219
Sep  6 10:19:04.273: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 09/06/23 10:19:04.274
Sep  6 10:19:04.278: INFO: Observed &DaemonSet event: ADDED
Sep  6 10:19:04.279: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.280: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.281: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.281: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.282: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.283: INFO: Found daemon set daemon-set in namespace daemonsets-2541 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  6 10:19:04.283: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 09/06/23 10:19:04.284
STEP: watching for the daemon set status to be patched 09/06/23 10:19:04.317
Sep  6 10:19:04.320: INFO: Observed &DaemonSet event: ADDED
Sep  6 10:19:04.320: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.321: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.322: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.322: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.323: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.323: INFO: Observed daemon set daemon-set in namespace daemonsets-2541 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  6 10:19:04.323: INFO: Observed &DaemonSet event: MODIFIED
Sep  6 10:19:04.327: INFO: Found daemon set daemon-set in namespace daemonsets-2541 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep  6 10:19:04.327: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/06/23 10:19:04.333
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2541, will wait for the garbage collector to delete the pods 09/06/23 10:19:04.333
Sep  6 10:19:04.438: INFO: Deleting DaemonSet.extensions daemon-set took: 18.315486ms
Sep  6 10:19:04.539: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.012168ms
Sep  6 10:19:06.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 10:19:06.644: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  6 10:19:06.646: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6675"},"items":null}

Sep  6 10:19:06.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6675"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:19:06.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2541" for this suite. 09/06/23 10:19:06.671
------------------------------
• [SLOW TEST] [18.580 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:18:48.106
    Sep  6 10:18:48.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename daemonsets 09/06/23 10:18:48.107
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:18:48.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:18:48.128
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 09/06/23 10:18:48.151
    STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 10:18:48.159
    Sep  6 10:18:48.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 10:18:48.166: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 10:18:49.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 10:18:49.181: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 10:18:52.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:18:52.284: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:18:53.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:18:53.186: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:18:54.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:18:54.184: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:18:55.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:18:55.176: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:18:56.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:18:56.172: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:19:01.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:19:01.177: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:19:02.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:19:02.812: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:19:03.178: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:19:03.178: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:19:04.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 10:19:04.198: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 09/06/23 10:19:04.209
    Sep  6 10:19:04.219: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 09/06/23 10:19:04.219
    Sep  6 10:19:04.273: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 09/06/23 10:19:04.274
    Sep  6 10:19:04.278: INFO: Observed &DaemonSet event: ADDED
    Sep  6 10:19:04.279: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.280: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.281: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.281: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.282: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.283: INFO: Found daemon set daemon-set in namespace daemonsets-2541 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  6 10:19:04.283: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 09/06/23 10:19:04.284
    STEP: watching for the daemon set status to be patched 09/06/23 10:19:04.317
    Sep  6 10:19:04.320: INFO: Observed &DaemonSet event: ADDED
    Sep  6 10:19:04.320: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.321: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.322: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.322: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.323: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.323: INFO: Observed daemon set daemon-set in namespace daemonsets-2541 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  6 10:19:04.323: INFO: Observed &DaemonSet event: MODIFIED
    Sep  6 10:19:04.327: INFO: Found daemon set daemon-set in namespace daemonsets-2541 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Sep  6 10:19:04.327: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/06/23 10:19:04.333
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2541, will wait for the garbage collector to delete the pods 09/06/23 10:19:04.333
    Sep  6 10:19:04.438: INFO: Deleting DaemonSet.extensions daemon-set took: 18.315486ms
    Sep  6 10:19:04.539: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.012168ms
    Sep  6 10:19:06.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 10:19:06.644: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  6 10:19:06.646: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6675"},"items":null}

    Sep  6 10:19:06.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6675"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:19:06.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2541" for this suite. 09/06/23 10:19:06.671
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:19:06.686
Sep  6 10:19:06.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename var-expansion 09/06/23 10:19:06.687
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:06.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:06.732
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Sep  6 10:19:06.751: INFO: Waiting up to 2m0s for pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9" in namespace "var-expansion-2556" to be "container 0 failed with reason CreateContainerConfigError"
Sep  6 10:19:06.761: INFO: Pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.531906ms
Sep  6 10:19:09.550: INFO: Pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798292251s
Sep  6 10:19:09.550: INFO: Pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  6 10:19:09.550: INFO: Deleting pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9" in namespace "var-expansion-2556"
Sep  6 10:19:09.716: INFO: Wait up to 5m0s for pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  6 10:19:11.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2556" for this suite. 09/06/23 10:19:11.777
------------------------------
• [SLOW TEST] [5.097 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:19:06.686
    Sep  6 10:19:06.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename var-expansion 09/06/23 10:19:06.687
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:06.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:06.732
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Sep  6 10:19:06.751: INFO: Waiting up to 2m0s for pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9" in namespace "var-expansion-2556" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  6 10:19:06.761: INFO: Pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.531906ms
    Sep  6 10:19:09.550: INFO: Pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798292251s
    Sep  6 10:19:09.550: INFO: Pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  6 10:19:09.550: INFO: Deleting pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9" in namespace "var-expansion-2556"
    Sep  6 10:19:09.716: INFO: Wait up to 5m0s for pod "var-expansion-50c33bfe-76c0-4e43-9fc1-80cfb0c98ce9" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:19:11.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2556" for this suite. 09/06/23 10:19:11.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:19:11.784
Sep  6 10:19:11.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename lease-test 09/06/23 10:19:11.785
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:11.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:11.813
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Sep  6 10:19:11.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-2842" for this suite. 09/06/23 10:19:11.884
------------------------------
• [0.109 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:19:11.784
    Sep  6 10:19:11.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename lease-test 09/06/23 10:19:11.785
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:11.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:11.813
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:19:11.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-2842" for this suite. 09/06/23 10:19:11.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:19:11.894
Sep  6 10:19:11.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:19:11.895
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:11.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:11.921
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/06/23 10:19:11.923
Sep  6 10:19:11.938: INFO: Waiting up to 5m0s for pod "pod-de084379-290c-4355-baa3-7f738468c2bd" in namespace "emptydir-1519" to be "Succeeded or Failed"
Sep  6 10:19:11.944: INFO: Pod "pod-de084379-290c-4355-baa3-7f738468c2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116696ms
Sep  6 10:19:13.959: INFO: Pod "pod-de084379-290c-4355-baa3-7f738468c2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021091716s
Sep  6 10:19:15.947: INFO: Pod "pod-de084379-290c-4355-baa3-7f738468c2bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00947125s
STEP: Saw pod success 09/06/23 10:19:15.947
Sep  6 10:19:15.947: INFO: Pod "pod-de084379-290c-4355-baa3-7f738468c2bd" satisfied condition "Succeeded or Failed"
Sep  6 10:19:15.951: INFO: Trying to get logs from node kube-3 pod pod-de084379-290c-4355-baa3-7f738468c2bd container test-container: <nil>
STEP: delete the pod 09/06/23 10:19:15.959
Sep  6 10:19:15.973: INFO: Waiting for pod pod-de084379-290c-4355-baa3-7f738468c2bd to disappear
Sep  6 10:19:15.976: INFO: Pod pod-de084379-290c-4355-baa3-7f738468c2bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:19:15.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1519" for this suite. 09/06/23 10:19:15.978
------------------------------
• [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:19:11.894
    Sep  6 10:19:11.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:19:11.895
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:11.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:11.921
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/06/23 10:19:11.923
    Sep  6 10:19:11.938: INFO: Waiting up to 5m0s for pod "pod-de084379-290c-4355-baa3-7f738468c2bd" in namespace "emptydir-1519" to be "Succeeded or Failed"
    Sep  6 10:19:11.944: INFO: Pod "pod-de084379-290c-4355-baa3-7f738468c2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116696ms
    Sep  6 10:19:13.959: INFO: Pod "pod-de084379-290c-4355-baa3-7f738468c2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021091716s
    Sep  6 10:19:15.947: INFO: Pod "pod-de084379-290c-4355-baa3-7f738468c2bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00947125s
    STEP: Saw pod success 09/06/23 10:19:15.947
    Sep  6 10:19:15.947: INFO: Pod "pod-de084379-290c-4355-baa3-7f738468c2bd" satisfied condition "Succeeded or Failed"
    Sep  6 10:19:15.951: INFO: Trying to get logs from node kube-3 pod pod-de084379-290c-4355-baa3-7f738468c2bd container test-container: <nil>
    STEP: delete the pod 09/06/23 10:19:15.959
    Sep  6 10:19:15.973: INFO: Waiting for pod pod-de084379-290c-4355-baa3-7f738468c2bd to disappear
    Sep  6 10:19:15.976: INFO: Pod pod-de084379-290c-4355-baa3-7f738468c2bd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:19:15.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1519" for this suite. 09/06/23 10:19:15.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:19:15.988
Sep  6 10:19:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:19:15.989
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:16.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:16.01
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 09/06/23 10:19:16.012
Sep  6 10:19:16.020: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381" in namespace "downward-api-8015" to be "Succeeded or Failed"
Sep  6 10:19:16.030: INFO: Pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381": Phase="Pending", Reason="", readiness=false. Elapsed: 10.23135ms
Sep  6 10:19:18.044: INFO: Pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024568334s
Sep  6 10:19:20.039: INFO: Pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01898578s
STEP: Saw pod success 09/06/23 10:19:20.039
Sep  6 10:19:20.039: INFO: Pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381" satisfied condition "Succeeded or Failed"
Sep  6 10:19:20.047: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381 container client-container: <nil>
STEP: delete the pod 09/06/23 10:19:20.054
Sep  6 10:19:20.071: INFO: Waiting for pod downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381 to disappear
Sep  6 10:19:20.075: INFO: Pod downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 10:19:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8015" for this suite. 09/06/23 10:19:20.08
------------------------------
• [4.104 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:19:15.988
    Sep  6 10:19:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:19:15.989
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:16.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:16.01
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 09/06/23 10:19:16.012
    Sep  6 10:19:16.020: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381" in namespace "downward-api-8015" to be "Succeeded or Failed"
    Sep  6 10:19:16.030: INFO: Pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381": Phase="Pending", Reason="", readiness=false. Elapsed: 10.23135ms
    Sep  6 10:19:18.044: INFO: Pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024568334s
    Sep  6 10:19:20.039: INFO: Pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01898578s
    STEP: Saw pod success 09/06/23 10:19:20.039
    Sep  6 10:19:20.039: INFO: Pod "downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381" satisfied condition "Succeeded or Failed"
    Sep  6 10:19:20.047: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381 container client-container: <nil>
    STEP: delete the pod 09/06/23 10:19:20.054
    Sep  6 10:19:20.071: INFO: Waiting for pod downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381 to disappear
    Sep  6 10:19:20.075: INFO: Pod downwardapi-volume-d1a889b8-2cb7-40ef-b3ad-9cfb1d268381 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:19:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8015" for this suite. 09/06/23 10:19:20.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:19:20.093
Sep  6 10:19:20.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:19:20.095
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:20.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:20.12
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:19:20.144
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:19:20.58
STEP: Deploying the webhook pod 09/06/23 10:19:20.591
STEP: Wait for the deployment to be ready 09/06/23 10:19:20.606
Sep  6 10:19:20.614: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 10:19:23.875
STEP: Verifying the service has paired with the endpoint 09/06/23 10:19:24.01
Sep  6 10:19:25.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/06/23 10:19:25.032
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/06/23 10:19:25.092
STEP: Creating a dummy validating-webhook-configuration object 09/06/23 10:19:25.193
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/06/23 10:19:25.236
STEP: Creating a dummy mutating-webhook-configuration object 09/06/23 10:19:25.288
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/06/23 10:19:25.355
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:19:25.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2396" for this suite. 09/06/23 10:19:25.843
STEP: Destroying namespace "webhook-2396-markers" for this suite. 09/06/23 10:19:25.917
------------------------------
• [SLOW TEST] [5.850 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:19:20.093
    Sep  6 10:19:20.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:19:20.095
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:20.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:20.12
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:19:20.144
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:19:20.58
    STEP: Deploying the webhook pod 09/06/23 10:19:20.591
    STEP: Wait for the deployment to be ready 09/06/23 10:19:20.606
    Sep  6 10:19:20.614: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 10:19:23.875
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:19:24.01
    Sep  6 10:19:25.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/06/23 10:19:25.032
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/06/23 10:19:25.092
    STEP: Creating a dummy validating-webhook-configuration object 09/06/23 10:19:25.193
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/06/23 10:19:25.236
    STEP: Creating a dummy mutating-webhook-configuration object 09/06/23 10:19:25.288
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/06/23 10:19:25.355
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:19:25.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2396" for this suite. 09/06/23 10:19:25.843
    STEP: Destroying namespace "webhook-2396-markers" for this suite. 09/06/23 10:19:25.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:19:25.946
Sep  6 10:19:25.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename ephemeral-containers-test 09/06/23 10:19:25.947
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:26.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:26.069
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 09/06/23 10:19:26.072
Sep  6 10:19:26.084: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2464" to be "running and ready"
Sep  6 10:19:26.098: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039294ms
Sep  6 10:19:26.099: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:19:28.115: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.030253421s
Sep  6 10:19:28.115: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Sep  6 10:19:28.115: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 09/06/23 10:19:28.131
Sep  6 10:19:28.170: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2464" to be "container debugger running"
Sep  6 10:19:28.176: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.920428ms
Sep  6 10:19:30.181: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010419644s
Sep  6 10:19:32.184: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013598498s
Sep  6 10:19:32.184: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 09/06/23 10:19:32.184
Sep  6 10:19:32.184: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2464 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:19:32.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:19:32.185: INFO: ExecWithOptions: Clientset creation
Sep  6 10:19:32.185: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-2464/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Sep  6 10:19:32.262: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:19:32.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2464" for this suite. 09/06/23 10:19:32.271
------------------------------
• [SLOW TEST] [6.335 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:19:25.946
    Sep  6 10:19:25.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename ephemeral-containers-test 09/06/23 10:19:25.947
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:26.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:26.069
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 09/06/23 10:19:26.072
    Sep  6 10:19:26.084: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2464" to be "running and ready"
    Sep  6 10:19:26.098: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039294ms
    Sep  6 10:19:26.099: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:19:28.115: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.030253421s
    Sep  6 10:19:28.115: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Sep  6 10:19:28.115: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 09/06/23 10:19:28.131
    Sep  6 10:19:28.170: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2464" to be "container debugger running"
    Sep  6 10:19:28.176: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.920428ms
    Sep  6 10:19:30.181: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010419644s
    Sep  6 10:19:32.184: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013598498s
    Sep  6 10:19:32.184: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 09/06/23 10:19:32.184
    Sep  6 10:19:32.184: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2464 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:19:32.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:19:32.185: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:19:32.185: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-2464/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Sep  6 10:19:32.262: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:19:32.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2464" for this suite. 09/06/23 10:19:32.271
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:19:32.281
Sep  6 10:19:32.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:19:32.282
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:32.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:32.304
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-3d891b4a-9fee-4b0a-ad3f-c8cbf13161cb 09/06/23 10:19:32.305
STEP: Creating a pod to test consume secrets 09/06/23 10:19:32.309
Sep  6 10:19:32.318: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa" in namespace "projected-7034" to be "Succeeded or Failed"
Sep  6 10:19:32.322: INFO: Pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048411ms
Sep  6 10:19:34.326: INFO: Pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007612145s
Sep  6 10:19:36.331: INFO: Pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012614158s
STEP: Saw pod success 09/06/23 10:19:36.331
Sep  6 10:19:36.331: INFO: Pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa" satisfied condition "Succeeded or Failed"
Sep  6 10:19:36.339: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa container projected-secret-volume-test: <nil>
STEP: delete the pod 09/06/23 10:19:36.347
Sep  6 10:19:36.376: INFO: Waiting for pod pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa to disappear
Sep  6 10:19:36.383: INFO: Pod pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  6 10:19:36.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7034" for this suite. 09/06/23 10:19:36.388
------------------------------
• [4.116 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:19:32.281
    Sep  6 10:19:32.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:19:32.282
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:32.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:32.304
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-3d891b4a-9fee-4b0a-ad3f-c8cbf13161cb 09/06/23 10:19:32.305
    STEP: Creating a pod to test consume secrets 09/06/23 10:19:32.309
    Sep  6 10:19:32.318: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa" in namespace "projected-7034" to be "Succeeded or Failed"
    Sep  6 10:19:32.322: INFO: Pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048411ms
    Sep  6 10:19:34.326: INFO: Pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007612145s
    Sep  6 10:19:36.331: INFO: Pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012614158s
    STEP: Saw pod success 09/06/23 10:19:36.331
    Sep  6 10:19:36.331: INFO: Pod "pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa" satisfied condition "Succeeded or Failed"
    Sep  6 10:19:36.339: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:19:36.347
    Sep  6 10:19:36.376: INFO: Waiting for pod pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa to disappear
    Sep  6 10:19:36.383: INFO: Pod pod-projected-secrets-f4f0cd34-3fa6-4b3d-85b8-06d27d8dd9aa no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:19:36.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7034" for this suite. 09/06/23 10:19:36.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:19:36.4
Sep  6 10:19:36.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename dns 09/06/23 10:19:36.401
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:36.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:36.427
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/06/23 10:19:36.429
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/06/23 10:19:36.429
STEP: creating a pod to probe DNS 09/06/23 10:19:36.429
STEP: submitting the pod to kubernetes 09/06/23 10:19:36.429
Sep  6 10:19:36.439: INFO: Waiting up to 15m0s for pod "dns-test-69831867-a539-4344-8276-050abaa16b7c" in namespace "dns-6939" to be "running"
Sep  6 10:19:36.445: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560569ms
Sep  6 10:19:43.276: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.836586433s
Sep  6 10:19:44.464: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024377325s
Sep  6 10:19:46.449: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010220687s
Sep  6 10:19:48.746: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.306868784s
Sep  6 10:19:50.448: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009284768s
Sep  6 10:19:52.565: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.126033608s
Sep  6 10:19:54.450: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010580917s
Sep  6 10:19:56.516: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.077084985s
Sep  6 10:19:58.458: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019209614s
Sep  6 10:20:00.453: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.013928369s
Sep  6 10:20:02.449: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Running", Reason="", readiness=true. Elapsed: 26.009451703s
Sep  6 10:20:02.449: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c" satisfied condition "running"
STEP: retrieving the pod 09/06/23 10:20:02.449
STEP: looking for the results for each expected name from probers 09/06/23 10:20:02.452
Sep  6 10:20:02.465: INFO: DNS probes using dns-6939/dns-test-69831867-a539-4344-8276-050abaa16b7c succeeded

STEP: deleting the pod 09/06/23 10:20:02.465
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:02.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6939" for this suite. 09/06/23 10:20:02.488
------------------------------
• [SLOW TEST] [26.103 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:19:36.4
    Sep  6 10:19:36.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename dns 09/06/23 10:19:36.401
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:19:36.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:19:36.427
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/06/23 10:19:36.429
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/06/23 10:19:36.429
    STEP: creating a pod to probe DNS 09/06/23 10:19:36.429
    STEP: submitting the pod to kubernetes 09/06/23 10:19:36.429
    Sep  6 10:19:36.439: INFO: Waiting up to 15m0s for pod "dns-test-69831867-a539-4344-8276-050abaa16b7c" in namespace "dns-6939" to be "running"
    Sep  6 10:19:36.445: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560569ms
    Sep  6 10:19:43.276: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.836586433s
    Sep  6 10:19:44.464: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024377325s
    Sep  6 10:19:46.449: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010220687s
    Sep  6 10:19:48.746: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.306868784s
    Sep  6 10:19:50.448: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009284768s
    Sep  6 10:19:52.565: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.126033608s
    Sep  6 10:19:54.450: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010580917s
    Sep  6 10:19:56.516: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.077084985s
    Sep  6 10:19:58.458: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019209614s
    Sep  6 10:20:00.453: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.013928369s
    Sep  6 10:20:02.449: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c": Phase="Running", Reason="", readiness=true. Elapsed: 26.009451703s
    Sep  6 10:20:02.449: INFO: Pod "dns-test-69831867-a539-4344-8276-050abaa16b7c" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 10:20:02.449
    STEP: looking for the results for each expected name from probers 09/06/23 10:20:02.452
    Sep  6 10:20:02.465: INFO: DNS probes using dns-6939/dns-test-69831867-a539-4344-8276-050abaa16b7c succeeded

    STEP: deleting the pod 09/06/23 10:20:02.465
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:02.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6939" for this suite. 09/06/23 10:20:02.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:02.503
Sep  6 10:20:02.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:20:02.506
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:02.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:02.547
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 09/06/23 10:20:02.551
Sep  6 10:20:02.563: INFO: Waiting up to 5m0s for pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749" in namespace "emptydir-4595" to be "Succeeded or Failed"
Sep  6 10:20:02.569: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122564ms
Sep  6 10:20:04.581: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01824476s
Sep  6 10:20:09.207: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644105306s
Sep  6 10:20:10.578: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014799432s
STEP: Saw pod success 09/06/23 10:20:10.578
Sep  6 10:20:10.578: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749" satisfied condition "Succeeded or Failed"
Sep  6 10:20:10.582: INFO: Trying to get logs from node kube-3 pod pod-a14d44f6-9f65-4520-9b85-2d907269a749 container test-container: <nil>
STEP: delete the pod 09/06/23 10:20:10.59
Sep  6 10:20:10.679: INFO: Waiting for pod pod-a14d44f6-9f65-4520-9b85-2d907269a749 to disappear
Sep  6 10:20:10.682: INFO: Pod pod-a14d44f6-9f65-4520-9b85-2d907269a749 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:10.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4595" for this suite. 09/06/23 10:20:10.687
------------------------------
• [SLOW TEST] [8.199 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:02.503
    Sep  6 10:20:02.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:20:02.506
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:02.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:02.547
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/06/23 10:20:02.551
    Sep  6 10:20:02.563: INFO: Waiting up to 5m0s for pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749" in namespace "emptydir-4595" to be "Succeeded or Failed"
    Sep  6 10:20:02.569: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122564ms
    Sep  6 10:20:04.581: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01824476s
    Sep  6 10:20:09.207: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644105306s
    Sep  6 10:20:10.578: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014799432s
    STEP: Saw pod success 09/06/23 10:20:10.578
    Sep  6 10:20:10.578: INFO: Pod "pod-a14d44f6-9f65-4520-9b85-2d907269a749" satisfied condition "Succeeded or Failed"
    Sep  6 10:20:10.582: INFO: Trying to get logs from node kube-3 pod pod-a14d44f6-9f65-4520-9b85-2d907269a749 container test-container: <nil>
    STEP: delete the pod 09/06/23 10:20:10.59
    Sep  6 10:20:10.679: INFO: Waiting for pod pod-a14d44f6-9f65-4520-9b85-2d907269a749 to disappear
    Sep  6 10:20:10.682: INFO: Pod pod-a14d44f6-9f65-4520-9b85-2d907269a749 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:10.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4595" for this suite. 09/06/23 10:20:10.687
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:10.704
Sep  6 10:20:10.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-pred 09/06/23 10:20:10.705
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:10.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:10.777
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  6 10:20:10.781: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 10:20:10.813: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 10:20:10.822: INFO: 
Logging pods the apiserver thinks is on node kube-1 before test
Sep  6 10:20:10.829: INFO: calico-kube-controllers-6dfcdfb99-6q4ng from kube-system started at 2023-09-06 09:55:41 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 10:20:10.829: INFO: calico-node-pkqgc from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container calico-node ready: true, restart count 2
Sep  6 10:20:10.829: INFO: coredns-645b46f4b6-hq55k from kube-system started at 2023-09-06 09:55:53 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container coredns ready: true, restart count 0
Sep  6 10:20:10.829: INFO: kube-apiserver-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container kube-apiserver ready: true, restart count 2
Sep  6 10:20:10.829: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container kube-controller-manager ready: true, restart count 4
Sep  6 10:20:10.829: INFO: kube-proxy-fjqk6 from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 10:20:10.829: INFO: kube-scheduler-kube-1 from kube-system started at 2023-09-06 09:52:15 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container kube-scheduler ready: true, restart count 3
Sep  6 10:20:10.829: INFO: nodelocaldns-74qn2 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 10:20:10.829: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 10:20:10.829: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:20:10.829: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:20:10.829: INFO: 
Logging pods the apiserver thinks is on node kube-2 before test
Sep  6 10:20:10.847: INFO: calico-node-f57x2 from kube-system started at 2023-09-06 09:54:23 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container calico-node ready: true, restart count 2
Sep  6 10:20:10.847: INFO: coredns-645b46f4b6-9lpfv from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container coredns ready: true, restart count 0
Sep  6 10:20:10.847: INFO: dns-autoscaler-659b8c48cb-5h6w8 from kube-system started at 2023-09-06 09:55:57 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container autoscaler ready: true, restart count 0
Sep  6 10:20:10.847: INFO: kube-apiserver-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container kube-apiserver ready: true, restart count 1
Sep  6 10:20:10.847: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-09-06 09:53:08 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container kube-controller-manager ready: true, restart count 3
Sep  6 10:20:10.847: INFO: kube-proxy-7fxzk from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 10:20:10.847: INFO: kube-scheduler-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container kube-scheduler ready: true, restart count 3
Sep  6 10:20:10.847: INFO: nodelocaldns-jpj4c from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 10:20:10.847: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 10:20:10.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:20:10.847: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:20:10.847: INFO: 
Logging pods the apiserver thinks is on node kube-3 before test
Sep  6 10:20:10.855: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-2464 started at 2023-09-06 10:19:26 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.855: INFO: 	Container test-container-1 ready: true, restart count 0
Sep  6 10:20:10.855: INFO: calico-node-6w7db from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.855: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 10:20:10.855: INFO: kube-proxy-sfndk from kube-system started at 2023-09-06 09:54:02 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.855: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 10:20:10.855: INFO: nginx-proxy-kube-3 from kube-system started at 2023-09-06 09:53:42 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.855: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  6 10:20:10.855: INFO: nodelocaldns-c9bb4 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.855: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 10:20:10.855: INFO: sonobuoy from sonobuoy started at 2023-09-06 09:59:53 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:10.855: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 10:20:10.855: INFO: sonobuoy-e2e-job-c7c8c161973b4a54 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 10:20:10.855: INFO: 	Container e2e ready: true, restart count 0
Sep  6 10:20:10.855: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:20:10.855: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 10:20:10.855: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:20:10.855: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 09/06/23 10:20:10.856
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.178248086f4ed2cd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 09/06/23 10:20:10.917
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:11.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2591" for this suite. 09/06/23 10:20:11.941
------------------------------
• [1.258 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:10.704
    Sep  6 10:20:10.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-pred 09/06/23 10:20:10.705
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:10.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:10.777
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  6 10:20:10.781: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  6 10:20:10.813: INFO: Waiting for terminating namespaces to be deleted...
    Sep  6 10:20:10.822: INFO: 
    Logging pods the apiserver thinks is on node kube-1 before test
    Sep  6 10:20:10.829: INFO: calico-kube-controllers-6dfcdfb99-6q4ng from kube-system started at 2023-09-06 09:55:41 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  6 10:20:10.829: INFO: calico-node-pkqgc from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container calico-node ready: true, restart count 2
    Sep  6 10:20:10.829: INFO: coredns-645b46f4b6-hq55k from kube-system started at 2023-09-06 09:55:53 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container coredns ready: true, restart count 0
    Sep  6 10:20:10.829: INFO: kube-apiserver-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container kube-apiserver ready: true, restart count 2
    Sep  6 10:20:10.829: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Sep  6 10:20:10.829: INFO: kube-proxy-fjqk6 from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 10:20:10.829: INFO: kube-scheduler-kube-1 from kube-system started at 2023-09-06 09:52:15 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container kube-scheduler ready: true, restart count 3
    Sep  6 10:20:10.829: INFO: nodelocaldns-74qn2 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 10:20:10.829: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 10:20:10.829: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 10:20:10.829: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  6 10:20:10.829: INFO: 
    Logging pods the apiserver thinks is on node kube-2 before test
    Sep  6 10:20:10.847: INFO: calico-node-f57x2 from kube-system started at 2023-09-06 09:54:23 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container calico-node ready: true, restart count 2
    Sep  6 10:20:10.847: INFO: coredns-645b46f4b6-9lpfv from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container coredns ready: true, restart count 0
    Sep  6 10:20:10.847: INFO: dns-autoscaler-659b8c48cb-5h6w8 from kube-system started at 2023-09-06 09:55:57 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  6 10:20:10.847: INFO: kube-apiserver-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container kube-apiserver ready: true, restart count 1
    Sep  6 10:20:10.847: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-09-06 09:53:08 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container kube-controller-manager ready: true, restart count 3
    Sep  6 10:20:10.847: INFO: kube-proxy-7fxzk from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 10:20:10.847: INFO: kube-scheduler-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container kube-scheduler ready: true, restart count 3
    Sep  6 10:20:10.847: INFO: nodelocaldns-jpj4c from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 10:20:10.847: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 10:20:10.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 10:20:10.847: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  6 10:20:10.847: INFO: 
    Logging pods the apiserver thinks is on node kube-3 before test
    Sep  6 10:20:10.855: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-2464 started at 2023-09-06 10:19:26 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.855: INFO: 	Container test-container-1 ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: calico-node-6w7db from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.855: INFO: 	Container calico-node ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: kube-proxy-sfndk from kube-system started at 2023-09-06 09:54:02 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.855: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: nginx-proxy-kube-3 from kube-system started at 2023-09-06 09:53:42 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.855: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: nodelocaldns-c9bb4 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.855: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: sonobuoy from sonobuoy started at 2023-09-06 09:59:53 +0000 UTC (1 container statuses recorded)
    Sep  6 10:20:10.855: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: sonobuoy-e2e-job-c7c8c161973b4a54 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 10:20:10.855: INFO: 	Container e2e ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 10:20:10.855: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 10:20:10.855: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 09/06/23 10:20:10.856
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.178248086f4ed2cd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 09/06/23 10:20:10.917
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:11.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2591" for this suite. 09/06/23 10:20:11.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:11.964
Sep  6 10:20:11.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 10:20:11.966
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:11.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:11.997
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 09/06/23 10:20:11.999
STEP: Creating a ResourceQuota 09/06/23 10:20:17.004
STEP: Ensuring resource quota status is calculated 09/06/23 10:20:17.011
STEP: Creating a ReplicaSet 09/06/23 10:20:19.027
STEP: Ensuring resource quota status captures replicaset creation 09/06/23 10:20:19.722
STEP: Deleting a ReplicaSet 09/06/23 10:20:21.741
STEP: Ensuring resource quota status released usage 09/06/23 10:20:21.762
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:23.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5451" for this suite. 09/06/23 10:20:23.779
------------------------------
• [SLOW TEST] [11.823 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:11.964
    Sep  6 10:20:11.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 10:20:11.966
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:11.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:11.997
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 09/06/23 10:20:11.999
    STEP: Creating a ResourceQuota 09/06/23 10:20:17.004
    STEP: Ensuring resource quota status is calculated 09/06/23 10:20:17.011
    STEP: Creating a ReplicaSet 09/06/23 10:20:19.027
    STEP: Ensuring resource quota status captures replicaset creation 09/06/23 10:20:19.722
    STEP: Deleting a ReplicaSet 09/06/23 10:20:21.741
    STEP: Ensuring resource quota status released usage 09/06/23 10:20:21.762
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:23.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5451" for this suite. 09/06/23 10:20:23.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:23.79
Sep  6 10:20:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename discovery 09/06/23 10:20:23.79
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:23.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:23.817
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 09/06/23 10:20:23.82
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Sep  6 10:20:23.997: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  6 10:20:23.998: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  6 10:20:23.998: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep  6 10:20:23.998: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  6 10:20:23.998: INFO: Checking APIGroup: apps
Sep  6 10:20:23.999: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  6 10:20:23.999: INFO: Versions found [{apps/v1 v1}]
Sep  6 10:20:23.999: INFO: apps/v1 matches apps/v1
Sep  6 10:20:23.999: INFO: Checking APIGroup: events.k8s.io
Sep  6 10:20:24.000: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  6 10:20:24.000: INFO: Versions found [{events.k8s.io/v1 v1}]
Sep  6 10:20:24.000: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  6 10:20:24.000: INFO: Checking APIGroup: authentication.k8s.io
Sep  6 10:20:24.001: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  6 10:20:24.001: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep  6 10:20:24.001: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  6 10:20:24.001: INFO: Checking APIGroup: authorization.k8s.io
Sep  6 10:20:24.001: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  6 10:20:24.001: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep  6 10:20:24.001: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  6 10:20:24.001: INFO: Checking APIGroup: autoscaling
Sep  6 10:20:24.002: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep  6 10:20:24.002: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Sep  6 10:20:24.002: INFO: autoscaling/v2 matches autoscaling/v2
Sep  6 10:20:24.002: INFO: Checking APIGroup: batch
Sep  6 10:20:24.003: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  6 10:20:24.003: INFO: Versions found [{batch/v1 v1}]
Sep  6 10:20:24.003: INFO: batch/v1 matches batch/v1
Sep  6 10:20:24.003: INFO: Checking APIGroup: certificates.k8s.io
Sep  6 10:20:24.004: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  6 10:20:24.004: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep  6 10:20:24.004: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  6 10:20:24.004: INFO: Checking APIGroup: networking.k8s.io
Sep  6 10:20:24.004: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  6 10:20:24.004: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep  6 10:20:24.004: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  6 10:20:24.004: INFO: Checking APIGroup: policy
Sep  6 10:20:24.005: INFO: PreferredVersion.GroupVersion: policy/v1
Sep  6 10:20:24.005: INFO: Versions found [{policy/v1 v1}]
Sep  6 10:20:24.005: INFO: policy/v1 matches policy/v1
Sep  6 10:20:24.005: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  6 10:20:24.006: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  6 10:20:24.006: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep  6 10:20:24.006: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  6 10:20:24.006: INFO: Checking APIGroup: storage.k8s.io
Sep  6 10:20:24.007: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  6 10:20:24.007: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  6 10:20:24.007: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  6 10:20:24.007: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  6 10:20:24.008: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  6 10:20:24.008: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep  6 10:20:24.008: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  6 10:20:24.008: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  6 10:20:24.009: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  6 10:20:24.009: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep  6 10:20:24.009: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  6 10:20:24.009: INFO: Checking APIGroup: scheduling.k8s.io
Sep  6 10:20:24.009: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  6 10:20:24.009: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep  6 10:20:24.009: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  6 10:20:24.009: INFO: Checking APIGroup: coordination.k8s.io
Sep  6 10:20:24.010: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  6 10:20:24.010: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep  6 10:20:24.010: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  6 10:20:24.010: INFO: Checking APIGroup: node.k8s.io
Sep  6 10:20:24.011: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep  6 10:20:24.011: INFO: Versions found [{node.k8s.io/v1 v1}]
Sep  6 10:20:24.011: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep  6 10:20:24.011: INFO: Checking APIGroup: discovery.k8s.io
Sep  6 10:20:24.011: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep  6 10:20:24.011: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Sep  6 10:20:24.011: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep  6 10:20:24.011: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  6 10:20:24.012: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Sep  6 10:20:24.012: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Sep  6 10:20:24.012: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Sep  6 10:20:24.012: INFO: Checking APIGroup: crd.projectcalico.org
Sep  6 10:20:24.013: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep  6 10:20:24.013: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep  6 10:20:24.013: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:24.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-710" for this suite. 09/06/23 10:20:24.016
------------------------------
• [0.232 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:23.79
    Sep  6 10:20:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename discovery 09/06/23 10:20:23.79
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:23.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:23.817
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 09/06/23 10:20:23.82
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Sep  6 10:20:23.997: INFO: Checking APIGroup: apiregistration.k8s.io
    Sep  6 10:20:23.998: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Sep  6 10:20:23.998: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Sep  6 10:20:23.998: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Sep  6 10:20:23.998: INFO: Checking APIGroup: apps
    Sep  6 10:20:23.999: INFO: PreferredVersion.GroupVersion: apps/v1
    Sep  6 10:20:23.999: INFO: Versions found [{apps/v1 v1}]
    Sep  6 10:20:23.999: INFO: apps/v1 matches apps/v1
    Sep  6 10:20:23.999: INFO: Checking APIGroup: events.k8s.io
    Sep  6 10:20:24.000: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Sep  6 10:20:24.000: INFO: Versions found [{events.k8s.io/v1 v1}]
    Sep  6 10:20:24.000: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Sep  6 10:20:24.000: INFO: Checking APIGroup: authentication.k8s.io
    Sep  6 10:20:24.001: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Sep  6 10:20:24.001: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Sep  6 10:20:24.001: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Sep  6 10:20:24.001: INFO: Checking APIGroup: authorization.k8s.io
    Sep  6 10:20:24.001: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Sep  6 10:20:24.001: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Sep  6 10:20:24.001: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Sep  6 10:20:24.001: INFO: Checking APIGroup: autoscaling
    Sep  6 10:20:24.002: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Sep  6 10:20:24.002: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Sep  6 10:20:24.002: INFO: autoscaling/v2 matches autoscaling/v2
    Sep  6 10:20:24.002: INFO: Checking APIGroup: batch
    Sep  6 10:20:24.003: INFO: PreferredVersion.GroupVersion: batch/v1
    Sep  6 10:20:24.003: INFO: Versions found [{batch/v1 v1}]
    Sep  6 10:20:24.003: INFO: batch/v1 matches batch/v1
    Sep  6 10:20:24.003: INFO: Checking APIGroup: certificates.k8s.io
    Sep  6 10:20:24.004: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Sep  6 10:20:24.004: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Sep  6 10:20:24.004: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Sep  6 10:20:24.004: INFO: Checking APIGroup: networking.k8s.io
    Sep  6 10:20:24.004: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Sep  6 10:20:24.004: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Sep  6 10:20:24.004: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Sep  6 10:20:24.004: INFO: Checking APIGroup: policy
    Sep  6 10:20:24.005: INFO: PreferredVersion.GroupVersion: policy/v1
    Sep  6 10:20:24.005: INFO: Versions found [{policy/v1 v1}]
    Sep  6 10:20:24.005: INFO: policy/v1 matches policy/v1
    Sep  6 10:20:24.005: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Sep  6 10:20:24.006: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Sep  6 10:20:24.006: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Sep  6 10:20:24.006: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Sep  6 10:20:24.006: INFO: Checking APIGroup: storage.k8s.io
    Sep  6 10:20:24.007: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Sep  6 10:20:24.007: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Sep  6 10:20:24.007: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Sep  6 10:20:24.007: INFO: Checking APIGroup: admissionregistration.k8s.io
    Sep  6 10:20:24.008: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Sep  6 10:20:24.008: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Sep  6 10:20:24.008: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Sep  6 10:20:24.008: INFO: Checking APIGroup: apiextensions.k8s.io
    Sep  6 10:20:24.009: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Sep  6 10:20:24.009: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Sep  6 10:20:24.009: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Sep  6 10:20:24.009: INFO: Checking APIGroup: scheduling.k8s.io
    Sep  6 10:20:24.009: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Sep  6 10:20:24.009: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Sep  6 10:20:24.009: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Sep  6 10:20:24.009: INFO: Checking APIGroup: coordination.k8s.io
    Sep  6 10:20:24.010: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Sep  6 10:20:24.010: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Sep  6 10:20:24.010: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Sep  6 10:20:24.010: INFO: Checking APIGroup: node.k8s.io
    Sep  6 10:20:24.011: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Sep  6 10:20:24.011: INFO: Versions found [{node.k8s.io/v1 v1}]
    Sep  6 10:20:24.011: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Sep  6 10:20:24.011: INFO: Checking APIGroup: discovery.k8s.io
    Sep  6 10:20:24.011: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Sep  6 10:20:24.011: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Sep  6 10:20:24.011: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Sep  6 10:20:24.011: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Sep  6 10:20:24.012: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Sep  6 10:20:24.012: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Sep  6 10:20:24.012: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Sep  6 10:20:24.012: INFO: Checking APIGroup: crd.projectcalico.org
    Sep  6 10:20:24.013: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Sep  6 10:20:24.013: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Sep  6 10:20:24.013: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:24.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-710" for this suite. 09/06/23 10:20:24.016
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:24.022
Sep  6 10:20:24.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:20:24.025
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:24.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:24.046
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 09/06/23 10:20:24.048
Sep  6 10:20:24.048: INFO: namespace kubectl-9198
Sep  6 10:20:24.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9198 create -f -'
Sep  6 10:20:24.195: INFO: stderr: ""
Sep  6 10:20:24.196: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/06/23 10:20:24.196
Sep  6 10:20:25.205: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 10:20:25.205: INFO: Found 1 / 1
Sep  6 10:20:25.205: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 10:20:25.211: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 10:20:25.211: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 10:20:25.211: INFO: wait on agnhost-primary startup in kubectl-9198 
Sep  6 10:20:25.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9198 logs agnhost-primary-64zbw agnhost-primary'
Sep  6 10:20:25.322: INFO: stderr: ""
Sep  6 10:20:25.322: INFO: stdout: "Paused\n"
STEP: exposing RC 09/06/23 10:20:25.322
Sep  6 10:20:25.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9198 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  6 10:20:25.401: INFO: stderr: ""
Sep  6 10:20:25.401: INFO: stdout: "service/rm2 exposed\n"
Sep  6 10:20:25.414: INFO: Service rm2 in namespace kubectl-9198 found.
STEP: exposing service 09/06/23 10:20:27.444
Sep  6 10:20:27.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9198 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  6 10:20:27.646: INFO: stderr: ""
Sep  6 10:20:27.646: INFO: stdout: "service/rm3 exposed\n"
Sep  6 10:20:27.654: INFO: Service rm3 in namespace kubectl-9198 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:29.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9198" for this suite. 09/06/23 10:20:29.694
------------------------------
• [SLOW TEST] [5.690 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:24.022
    Sep  6 10:20:24.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:20:24.025
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:24.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:24.046
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 09/06/23 10:20:24.048
    Sep  6 10:20:24.048: INFO: namespace kubectl-9198
    Sep  6 10:20:24.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9198 create -f -'
    Sep  6 10:20:24.195: INFO: stderr: ""
    Sep  6 10:20:24.196: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/06/23 10:20:24.196
    Sep  6 10:20:25.205: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 10:20:25.205: INFO: Found 1 / 1
    Sep  6 10:20:25.205: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  6 10:20:25.211: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 10:20:25.211: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  6 10:20:25.211: INFO: wait on agnhost-primary startup in kubectl-9198 
    Sep  6 10:20:25.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9198 logs agnhost-primary-64zbw agnhost-primary'
    Sep  6 10:20:25.322: INFO: stderr: ""
    Sep  6 10:20:25.322: INFO: stdout: "Paused\n"
    STEP: exposing RC 09/06/23 10:20:25.322
    Sep  6 10:20:25.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9198 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Sep  6 10:20:25.401: INFO: stderr: ""
    Sep  6 10:20:25.401: INFO: stdout: "service/rm2 exposed\n"
    Sep  6 10:20:25.414: INFO: Service rm2 in namespace kubectl-9198 found.
    STEP: exposing service 09/06/23 10:20:27.444
    Sep  6 10:20:27.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9198 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Sep  6 10:20:27.646: INFO: stderr: ""
    Sep  6 10:20:27.646: INFO: stdout: "service/rm3 exposed\n"
    Sep  6 10:20:27.654: INFO: Service rm3 in namespace kubectl-9198 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:29.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9198" for this suite. 09/06/23 10:20:29.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:29.713
Sep  6 10:20:29.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 10:20:29.715
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:29.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:29.742
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-9648aa67-77ef-46f7-a72e-29b7c0ef5f3d 09/06/23 10:20:29.745
STEP: Creating a pod to test consume configMaps 09/06/23 10:20:29.75
Sep  6 10:20:29.764: INFO: Waiting up to 5m0s for pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796" in namespace "configmap-2247" to be "Succeeded or Failed"
Sep  6 10:20:29.775: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Pending", Reason="", readiness=false. Elapsed: 11.288764ms
Sep  6 10:20:39.320: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Running", Reason="", readiness=true. Elapsed: 9.556316629s
Sep  6 10:20:39.788: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Running", Reason="", readiness=true. Elapsed: 10.024364581s
Sep  6 10:20:41.788: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Running", Reason="", readiness=false. Elapsed: 12.024563331s
Sep  6 10:20:43.788: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.024489007s
STEP: Saw pod success 09/06/23 10:20:43.788
Sep  6 10:20:43.789: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796" satisfied condition "Succeeded or Failed"
Sep  6 10:20:43.799: INFO: Trying to get logs from node kube-3 pod pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796 container agnhost-container: <nil>
STEP: delete the pod 09/06/23 10:20:43.822
Sep  6 10:20:43.850: INFO: Waiting for pod pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796 to disappear
Sep  6 10:20:43.852: INFO: Pod pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:43.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2247" for this suite. 09/06/23 10:20:43.856
------------------------------
• [SLOW TEST] [14.150 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:29.713
    Sep  6 10:20:29.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 10:20:29.715
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:29.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:29.742
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-9648aa67-77ef-46f7-a72e-29b7c0ef5f3d 09/06/23 10:20:29.745
    STEP: Creating a pod to test consume configMaps 09/06/23 10:20:29.75
    Sep  6 10:20:29.764: INFO: Waiting up to 5m0s for pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796" in namespace "configmap-2247" to be "Succeeded or Failed"
    Sep  6 10:20:29.775: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Pending", Reason="", readiness=false. Elapsed: 11.288764ms
    Sep  6 10:20:39.320: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Running", Reason="", readiness=true. Elapsed: 9.556316629s
    Sep  6 10:20:39.788: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Running", Reason="", readiness=true. Elapsed: 10.024364581s
    Sep  6 10:20:41.788: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Running", Reason="", readiness=false. Elapsed: 12.024563331s
    Sep  6 10:20:43.788: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.024489007s
    STEP: Saw pod success 09/06/23 10:20:43.788
    Sep  6 10:20:43.789: INFO: Pod "pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796" satisfied condition "Succeeded or Failed"
    Sep  6 10:20:43.799: INFO: Trying to get logs from node kube-3 pod pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796 container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 10:20:43.822
    Sep  6 10:20:43.850: INFO: Waiting for pod pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796 to disappear
    Sep  6 10:20:43.852: INFO: Pod pod-configmaps-2eb8e113-89a5-4f63-8a0e-2d29c8a82796 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:43.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2247" for this suite. 09/06/23 10:20:43.856
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:43.863
Sep  6 10:20:43.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:20:43.864
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:43.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:43.89
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:20:43.907
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:20:44.506
STEP: Deploying the webhook pod 09/06/23 10:20:44.513
STEP: Wait for the deployment to be ready 09/06/23 10:20:44.529
Sep  6 10:20:44.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 10:20:46.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 20, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 20, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 20, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 20, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/06/23 10:20:48.596
STEP: Verifying the service has paired with the endpoint 09/06/23 10:20:48.628
Sep  6 10:20:49.629: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/06/23 10:20:49.642
STEP: create a configmap that should be updated by the webhook 09/06/23 10:20:49.669
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:49.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5921" for this suite. 09/06/23 10:20:49.777
STEP: Destroying namespace "webhook-5921-markers" for this suite. 09/06/23 10:20:49.806
------------------------------
• [SLOW TEST] [5.961 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:43.863
    Sep  6 10:20:43.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:20:43.864
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:43.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:43.89
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:20:43.907
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:20:44.506
    STEP: Deploying the webhook pod 09/06/23 10:20:44.513
    STEP: Wait for the deployment to be ready 09/06/23 10:20:44.529
    Sep  6 10:20:44.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  6 10:20:46.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 20, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 20, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 20, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 20, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/06/23 10:20:48.596
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:20:48.628
    Sep  6 10:20:49.629: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/06/23 10:20:49.642
    STEP: create a configmap that should be updated by the webhook 09/06/23 10:20:49.669
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:49.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5921" for this suite. 09/06/23 10:20:49.777
    STEP: Destroying namespace "webhook-5921-markers" for this suite. 09/06/23 10:20:49.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:49.826
Sep  6 10:20:49.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:20:49.826
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:49.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:49.865
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 09/06/23 10:20:49.872
Sep  6 10:20:49.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15" in namespace "projected-9935" to be "Succeeded or Failed"
Sep  6 10:20:49.901: INFO: Pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15": Phase="Pending", Reason="", readiness=false. Elapsed: 11.266859ms
Sep  6 10:20:51.908: INFO: Pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018065945s
Sep  6 10:20:53.915: INFO: Pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024632993s
STEP: Saw pod success 09/06/23 10:20:53.915
Sep  6 10:20:53.916: INFO: Pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15" satisfied condition "Succeeded or Failed"
Sep  6 10:20:53.931: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15 container client-container: <nil>
STEP: delete the pod 09/06/23 10:20:53.955
Sep  6 10:20:53.978: INFO: Waiting for pod downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15 to disappear
Sep  6 10:20:53.982: INFO: Pod downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:53.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9935" for this suite. 09/06/23 10:20:53.985
------------------------------
• [4.166 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:49.826
    Sep  6 10:20:49.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:20:49.826
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:49.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:49.865
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 09/06/23 10:20:49.872
    Sep  6 10:20:49.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15" in namespace "projected-9935" to be "Succeeded or Failed"
    Sep  6 10:20:49.901: INFO: Pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15": Phase="Pending", Reason="", readiness=false. Elapsed: 11.266859ms
    Sep  6 10:20:51.908: INFO: Pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018065945s
    Sep  6 10:20:53.915: INFO: Pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024632993s
    STEP: Saw pod success 09/06/23 10:20:53.915
    Sep  6 10:20:53.916: INFO: Pod "downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15" satisfied condition "Succeeded or Failed"
    Sep  6 10:20:53.931: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15 container client-container: <nil>
    STEP: delete the pod 09/06/23 10:20:53.955
    Sep  6 10:20:53.978: INFO: Waiting for pod downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15 to disappear
    Sep  6 10:20:53.982: INFO: Pod downwardapi-volume-d2fe1e17-c60e-484c-98d6-f94093fc5c15 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:53.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9935" for this suite. 09/06/23 10:20:53.985
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:53.992
Sep  6 10:20:53.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:20:53.993
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:54.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:54.014
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/06/23 10:20:54.016
Sep  6 10:20:54.024: INFO: Waiting up to 5m0s for pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175" in namespace "emptydir-9302" to be "Succeeded or Failed"
Sep  6 10:20:54.027: INFO: Pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935585ms
Sep  6 10:20:56.039: INFO: Pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014799762s
Sep  6 10:20:58.060: INFO: Pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035830281s
STEP: Saw pod success 09/06/23 10:20:58.06
Sep  6 10:20:58.061: INFO: Pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175" satisfied condition "Succeeded or Failed"
Sep  6 10:20:58.110: INFO: Trying to get logs from node kube-3 pod pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175 container test-container: <nil>
STEP: delete the pod 09/06/23 10:20:58.127
Sep  6 10:20:58.284: INFO: Waiting for pod pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175 to disappear
Sep  6 10:20:58.289: INFO: Pod pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:58.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9302" for this suite. 09/06/23 10:20:58.306
------------------------------
• [4.367 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:53.992
    Sep  6 10:20:53.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:20:53.993
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:54.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:54.014
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/06/23 10:20:54.016
    Sep  6 10:20:54.024: INFO: Waiting up to 5m0s for pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175" in namespace "emptydir-9302" to be "Succeeded or Failed"
    Sep  6 10:20:54.027: INFO: Pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935585ms
    Sep  6 10:20:56.039: INFO: Pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014799762s
    Sep  6 10:20:58.060: INFO: Pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035830281s
    STEP: Saw pod success 09/06/23 10:20:58.06
    Sep  6 10:20:58.061: INFO: Pod "pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175" satisfied condition "Succeeded or Failed"
    Sep  6 10:20:58.110: INFO: Trying to get logs from node kube-3 pod pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175 container test-container: <nil>
    STEP: delete the pod 09/06/23 10:20:58.127
    Sep  6 10:20:58.284: INFO: Waiting for pod pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175 to disappear
    Sep  6 10:20:58.289: INFO: Pod pod-6daef315-b4d0-4d2e-bd6c-9df4d8e4d175 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:58.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9302" for this suite. 09/06/23 10:20:58.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:58.382
Sep  6 10:20:58.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename namespaces 09/06/23 10:20:58.383
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:58.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:58.506
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 09/06/23 10:20:58.51
Sep  6 10:20:58.516: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 09/06/23 10:20:58.516
Sep  6 10:20:58.588: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 09/06/23 10:20:58.589
Sep  6 10:20:58.628: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:20:58.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-924" for this suite. 09/06/23 10:20:58.647
------------------------------
• [0.313 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:58.382
    Sep  6 10:20:58.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename namespaces 09/06/23 10:20:58.383
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:58.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:58.506
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 09/06/23 10:20:58.51
    Sep  6 10:20:58.516: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 09/06/23 10:20:58.516
    Sep  6 10:20:58.588: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 09/06/23 10:20:58.589
    Sep  6 10:20:58.628: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:20:58.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-924" for this suite. 09/06/23 10:20:58.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:20:58.704
Sep  6 10:20:58.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename disruption 09/06/23 10:20:58.706
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:58.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:58.821
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 09/06/23 10:20:58.89
STEP: Updating PodDisruptionBudget status 09/06/23 10:21:00.903
STEP: Waiting for all pods to be running 09/06/23 10:21:00.919
Sep  6 10:21:00.928: INFO: running pods: 0 < 1
STEP: locating a running pod 09/06/23 10:21:02.942
STEP: Waiting for the pdb to be processed 09/06/23 10:21:02.991
STEP: Patching PodDisruptionBudget status 09/06/23 10:21:03.01
STEP: Waiting for the pdb to be processed 09/06/23 10:21:03.022
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  6 10:21:03.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2264" for this suite. 09/06/23 10:21:03.03
------------------------------
• [4.332 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:20:58.704
    Sep  6 10:20:58.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename disruption 09/06/23 10:20:58.706
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:20:58.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:20:58.821
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 09/06/23 10:20:58.89
    STEP: Updating PodDisruptionBudget status 09/06/23 10:21:00.903
    STEP: Waiting for all pods to be running 09/06/23 10:21:00.919
    Sep  6 10:21:00.928: INFO: running pods: 0 < 1
    STEP: locating a running pod 09/06/23 10:21:02.942
    STEP: Waiting for the pdb to be processed 09/06/23 10:21:02.991
    STEP: Patching PodDisruptionBudget status 09/06/23 10:21:03.01
    STEP: Waiting for the pdb to be processed 09/06/23 10:21:03.022
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:21:03.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2264" for this suite. 09/06/23 10:21:03.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:21:03.037
Sep  6 10:21:03.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:21:03.038
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:03.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:03.076
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 09/06/23 10:21:03.079
Sep  6 10:21:03.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 create -f -'
Sep  6 10:21:03.239: INFO: stderr: ""
Sep  6 10:21:03.239: INFO: stdout: "pod/pause created\n"
Sep  6 10:21:03.239: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  6 10:21:03.239: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9991" to be "running and ready"
Sep  6 10:21:03.246: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.058445ms
Sep  6 10:21:03.247: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'kube-3' to be 'Running' but was 'Pending'
Sep  6 10:21:05.251: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011962184s
Sep  6 10:21:05.251: INFO: Pod "pause" satisfied condition "running and ready"
Sep  6 10:21:05.251: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 09/06/23 10:21:05.251
Sep  6 10:21:05.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 label pods pause testing-label=testing-label-value'
Sep  6 10:21:05.328: INFO: stderr: ""
Sep  6 10:21:05.328: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 09/06/23 10:21:05.328
Sep  6 10:21:05.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 get pod pause -L testing-label'
Sep  6 10:21:05.386: INFO: stderr: ""
Sep  6 10:21:05.386: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 09/06/23 10:21:05.386
Sep  6 10:21:05.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 label pods pause testing-label-'
Sep  6 10:21:05.449: INFO: stderr: ""
Sep  6 10:21:05.449: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 09/06/23 10:21:05.449
Sep  6 10:21:05.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 get pod pause -L testing-label'
Sep  6 10:21:05.508: INFO: stderr: ""
Sep  6 10:21:05.508: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 09/06/23 10:21:05.508
Sep  6 10:21:05.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 delete --grace-period=0 --force -f -'
Sep  6 10:21:05.579: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:21:05.579: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  6 10:21:05.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 get rc,svc -l name=pause --no-headers'
Sep  6 10:21:05.642: INFO: stderr: "No resources found in kubectl-9991 namespace.\n"
Sep  6 10:21:05.642: INFO: stdout: ""
Sep  6 10:21:05.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 10:21:05.704: INFO: stderr: ""
Sep  6 10:21:05.704: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:21:05.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9991" for this suite. 09/06/23 10:21:05.729
------------------------------
• [2.703 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:21:03.037
    Sep  6 10:21:03.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:21:03.038
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:03.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:03.076
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 09/06/23 10:21:03.079
    Sep  6 10:21:03.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 create -f -'
    Sep  6 10:21:03.239: INFO: stderr: ""
    Sep  6 10:21:03.239: INFO: stdout: "pod/pause created\n"
    Sep  6 10:21:03.239: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Sep  6 10:21:03.239: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9991" to be "running and ready"
    Sep  6 10:21:03.246: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.058445ms
    Sep  6 10:21:03.247: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'kube-3' to be 'Running' but was 'Pending'
    Sep  6 10:21:05.251: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011962184s
    Sep  6 10:21:05.251: INFO: Pod "pause" satisfied condition "running and ready"
    Sep  6 10:21:05.251: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 09/06/23 10:21:05.251
    Sep  6 10:21:05.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 label pods pause testing-label=testing-label-value'
    Sep  6 10:21:05.328: INFO: stderr: ""
    Sep  6 10:21:05.328: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 09/06/23 10:21:05.328
    Sep  6 10:21:05.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 get pod pause -L testing-label'
    Sep  6 10:21:05.386: INFO: stderr: ""
    Sep  6 10:21:05.386: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 09/06/23 10:21:05.386
    Sep  6 10:21:05.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 label pods pause testing-label-'
    Sep  6 10:21:05.449: INFO: stderr: ""
    Sep  6 10:21:05.449: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 09/06/23 10:21:05.449
    Sep  6 10:21:05.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 get pod pause -L testing-label'
    Sep  6 10:21:05.508: INFO: stderr: ""
    Sep  6 10:21:05.508: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 09/06/23 10:21:05.508
    Sep  6 10:21:05.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 delete --grace-period=0 --force -f -'
    Sep  6 10:21:05.579: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 10:21:05.579: INFO: stdout: "pod \"pause\" force deleted\n"
    Sep  6 10:21:05.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 get rc,svc -l name=pause --no-headers'
    Sep  6 10:21:05.642: INFO: stderr: "No resources found in kubectl-9991 namespace.\n"
    Sep  6 10:21:05.642: INFO: stdout: ""
    Sep  6 10:21:05.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9991 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  6 10:21:05.704: INFO: stderr: ""
    Sep  6 10:21:05.704: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:21:05.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9991" for this suite. 09/06/23 10:21:05.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:21:05.741
Sep  6 10:21:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename deployment 09/06/23 10:21:05.742
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:05.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:05.763
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Sep  6 10:21:05.765: INFO: Creating deployment "test-recreate-deployment"
Sep  6 10:21:05.770: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  6 10:21:05.775: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Sep  6 10:21:07.782: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  6 10:21:07.786: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  6 10:21:07.794: INFO: Updating deployment test-recreate-deployment
Sep  6 10:21:07.794: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  6 10:21:07.925: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6149  dacaf16d-324c-404e-8e7b-809720f999e8 7630 2 2023-09-06 10:21:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050ae7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-06 10:21:07 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-06 10:21:07 +0000 UTC,LastTransitionTime:2023-09-06 10:21:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 10:21:07.928: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-6149  75184884-9c2e-48e4-874b-eef645bc22bb 7629 1 2023-09-06 10:21:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment dacaf16d-324c-404e-8e7b-809720f999e8 0xc00500b1b0 0xc00500b1b1}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dacaf16d-324c-404e-8e7b-809720f999e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00500b248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:21:07.929: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  6 10:21:07.929: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-6149  89a43ef5-49da-40fa-a895-3fb6ec4b44f7 7618 2 2023-09-06 10:21:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment dacaf16d-324c-404e-8e7b-809720f999e8 0xc00500b0a7 0xc00500b0a8}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dacaf16d-324c-404e-8e7b-809720f999e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00500b158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:21:07.932: INFO: Pod "test-recreate-deployment-cff6dc657-rm8z8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-rm8z8 test-recreate-deployment-cff6dc657- deployment-6149  515f049e-2486-41b4-baba-98dedd71d312 7628 0 2023-09-06 10:21:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 75184884-9c2e-48e4-874b-eef645bc22bb 0xc004ff4700 0xc004ff4701}] [] [{kube-controller-manager Update v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75184884-9c2e-48e4-874b-eef645bc22bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jv4n2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jv4n2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 10:21:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  6 10:21:07.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6149" for this suite. 09/06/23 10:21:07.936
------------------------------
• [2.202 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:21:05.741
    Sep  6 10:21:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename deployment 09/06/23 10:21:05.742
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:05.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:05.763
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Sep  6 10:21:05.765: INFO: Creating deployment "test-recreate-deployment"
    Sep  6 10:21:05.770: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Sep  6 10:21:05.775: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Sep  6 10:21:07.782: INFO: Waiting deployment "test-recreate-deployment" to complete
    Sep  6 10:21:07.786: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Sep  6 10:21:07.794: INFO: Updating deployment test-recreate-deployment
    Sep  6 10:21:07.794: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  6 10:21:07.925: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6149  dacaf16d-324c-404e-8e7b-809720f999e8 7630 2 2023-09-06 10:21:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050ae7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-06 10:21:07 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-06 10:21:07 +0000 UTC,LastTransitionTime:2023-09-06 10:21:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  6 10:21:07.928: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-6149  75184884-9c2e-48e4-874b-eef645bc22bb 7629 1 2023-09-06 10:21:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment dacaf16d-324c-404e-8e7b-809720f999e8 0xc00500b1b0 0xc00500b1b1}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dacaf16d-324c-404e-8e7b-809720f999e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00500b248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:21:07.929: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Sep  6 10:21:07.929: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-6149  89a43ef5-49da-40fa-a895-3fb6ec4b44f7 7618 2 2023-09-06 10:21:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment dacaf16d-324c-404e-8e7b-809720f999e8 0xc00500b0a7 0xc00500b0a8}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dacaf16d-324c-404e-8e7b-809720f999e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00500b158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:21:07.932: INFO: Pod "test-recreate-deployment-cff6dc657-rm8z8" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-rm8z8 test-recreate-deployment-cff6dc657- deployment-6149  515f049e-2486-41b4-baba-98dedd71d312 7628 0 2023-09-06 10:21:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 75184884-9c2e-48e4-874b-eef645bc22bb 0xc004ff4700 0xc004ff4701}] [] [{kube-controller-manager Update v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75184884-9c2e-48e4-874b-eef645bc22bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 10:21:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jv4n2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jv4n2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:21:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 10:21:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:21:07.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6149" for this suite. 09/06/23 10:21:07.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:21:07.943
Sep  6 10:21:07.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename statefulset 09/06/23 10:21:07.944
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:07.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:07.969
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1140 09/06/23 10:21:07.971
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-1140 09/06/23 10:21:07.978
Sep  6 10:21:07.995: INFO: Found 0 stateful pods, waiting for 1
Sep  6 10:21:18.010: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 09/06/23 10:21:18.033
STEP: updating a scale subresource 09/06/23 10:21:18.046
STEP: verifying the statefulset Spec.Replicas was modified 09/06/23 10:21:18.063
STEP: Patch a scale subresource 09/06/23 10:21:18.066
STEP: verifying the statefulset Spec.Replicas was modified 09/06/23 10:21:18.078
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  6 10:21:18.094: INFO: Deleting all statefulset in ns statefulset-1140
Sep  6 10:21:18.102: INFO: Scaling statefulset ss to 0
Sep  6 10:21:28.151: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:21:28.161: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:21:28.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1140" for this suite. 09/06/23 10:21:28.199
------------------------------
• [SLOW TEST] [20.276 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:21:07.943
    Sep  6 10:21:07.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename statefulset 09/06/23 10:21:07.944
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:07.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:07.969
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1140 09/06/23 10:21:07.971
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-1140 09/06/23 10:21:07.978
    Sep  6 10:21:07.995: INFO: Found 0 stateful pods, waiting for 1
    Sep  6 10:21:18.010: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 09/06/23 10:21:18.033
    STEP: updating a scale subresource 09/06/23 10:21:18.046
    STEP: verifying the statefulset Spec.Replicas was modified 09/06/23 10:21:18.063
    STEP: Patch a scale subresource 09/06/23 10:21:18.066
    STEP: verifying the statefulset Spec.Replicas was modified 09/06/23 10:21:18.078
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  6 10:21:18.094: INFO: Deleting all statefulset in ns statefulset-1140
    Sep  6 10:21:18.102: INFO: Scaling statefulset ss to 0
    Sep  6 10:21:28.151: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 10:21:28.161: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:21:28.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1140" for this suite. 09/06/23 10:21:28.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:21:28.222
Sep  6 10:21:28.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:21:28.224
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:28.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:28.252
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-70d9b42f-b1d4-4102-8b9f-cd32d0b1b0f9 09/06/23 10:21:28.254
STEP: Creating a pod to test consume configMaps 09/06/23 10:21:28.259
Sep  6 10:21:28.268: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630" in namespace "projected-5560" to be "Succeeded or Failed"
Sep  6 10:21:28.272: INFO: Pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155887ms
Sep  6 10:21:30.276: INFO: Pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008718885s
Sep  6 10:21:32.291: INFO: Pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023323899s
STEP: Saw pod success 09/06/23 10:21:32.291
Sep  6 10:21:32.292: INFO: Pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630" satisfied condition "Succeeded or Failed"
Sep  6 10:21:32.301: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630 container agnhost-container: <nil>
STEP: delete the pod 09/06/23 10:21:32.32
Sep  6 10:21:32.349: INFO: Waiting for pod pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630 to disappear
Sep  6 10:21:32.352: INFO: Pod pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:21:32.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5560" for this suite. 09/06/23 10:21:32.355
------------------------------
• [4.143 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:21:28.222
    Sep  6 10:21:28.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:21:28.224
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:28.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:28.252
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-70d9b42f-b1d4-4102-8b9f-cd32d0b1b0f9 09/06/23 10:21:28.254
    STEP: Creating a pod to test consume configMaps 09/06/23 10:21:28.259
    Sep  6 10:21:28.268: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630" in namespace "projected-5560" to be "Succeeded or Failed"
    Sep  6 10:21:28.272: INFO: Pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155887ms
    Sep  6 10:21:30.276: INFO: Pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008718885s
    Sep  6 10:21:32.291: INFO: Pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023323899s
    STEP: Saw pod success 09/06/23 10:21:32.291
    Sep  6 10:21:32.292: INFO: Pod "pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630" satisfied condition "Succeeded or Failed"
    Sep  6 10:21:32.301: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630 container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 10:21:32.32
    Sep  6 10:21:32.349: INFO: Waiting for pod pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630 to disappear
    Sep  6 10:21:32.352: INFO: Pod pod-projected-configmaps-28741d8e-5e35-401c-8cbe-e27add158630 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:21:32.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5560" for this suite. 09/06/23 10:21:32.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:21:32.367
Sep  6 10:21:32.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename dns 09/06/23 10:21:32.369
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:32.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:32.392
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 09/06/23 10:21:32.393
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
 09/06/23 10:21:32.398
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
 09/06/23 10:21:32.398
STEP: creating a pod to probe DNS 09/06/23 10:21:32.398
STEP: submitting the pod to kubernetes 09/06/23 10:21:32.398
Sep  6 10:21:32.409: INFO: Waiting up to 15m0s for pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8" in namespace "dns-9034" to be "running"
Sep  6 10:21:32.415: INFO: Pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.225809ms
Sep  6 10:21:34.434: INFO: Pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024797364s
Sep  6 10:21:36.421: INFO: Pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8": Phase="Running", Reason="", readiness=true. Elapsed: 4.011799403s
Sep  6 10:21:36.421: INFO: Pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8" satisfied condition "running"
STEP: retrieving the pod 09/06/23 10:21:36.421
STEP: looking for the results for each expected name from probers 09/06/23 10:21:36.439
Sep  6 10:21:36.466: INFO: DNS probes using dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8 succeeded

STEP: deleting the pod 09/06/23 10:21:36.467
STEP: changing the externalName to bar.example.com 09/06/23 10:21:36.795
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
 09/06/23 10:21:36.988
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
 09/06/23 10:21:36.988
STEP: creating a second pod to probe DNS 09/06/23 10:21:36.988
STEP: submitting the pod to kubernetes 09/06/23 10:21:36.988
Sep  6 10:21:37.083: INFO: Waiting up to 15m0s for pod "dns-test-31224c33-e76f-475f-a211-8043a1933729" in namespace "dns-9034" to be "running"
Sep  6 10:21:37.107: INFO: Pod "dns-test-31224c33-e76f-475f-a211-8043a1933729": Phase="Pending", Reason="", readiness=false. Elapsed: 23.949747ms
Sep  6 10:21:39.111: INFO: Pod "dns-test-31224c33-e76f-475f-a211-8043a1933729": Phase="Running", Reason="", readiness=true. Elapsed: 2.028230659s
Sep  6 10:21:39.111: INFO: Pod "dns-test-31224c33-e76f-475f-a211-8043a1933729" satisfied condition "running"
STEP: retrieving the pod 09/06/23 10:21:39.111
STEP: looking for the results for each expected name from probers 09/06/23 10:21:39.119
Sep  6 10:21:39.125: INFO: File wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local from pod  dns-9034/dns-test-31224c33-e76f-475f-a211-8043a1933729 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 10:21:39.131: INFO: File jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local from pod  dns-9034/dns-test-31224c33-e76f-475f-a211-8043a1933729 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 10:21:39.131: INFO: Lookups using dns-9034/dns-test-31224c33-e76f-475f-a211-8043a1933729 failed for: [wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local]

Sep  6 10:21:44.139: INFO: DNS probes using dns-test-31224c33-e76f-475f-a211-8043a1933729 succeeded

STEP: deleting the pod 09/06/23 10:21:44.139
STEP: changing the service to type=ClusterIP 09/06/23 10:21:44.167
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
 09/06/23 10:21:44.201
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
 09/06/23 10:21:44.202
STEP: creating a third pod to probe DNS 09/06/23 10:21:44.202
STEP: submitting the pod to kubernetes 09/06/23 10:21:44.211
Sep  6 10:21:44.224: INFO: Waiting up to 15m0s for pod "dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf" in namespace "dns-9034" to be "running"
Sep  6 10:21:44.231: INFO: Pod "dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.882571ms
Sep  6 10:21:46.249: INFO: Pod "dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.024919996s
Sep  6 10:21:46.249: INFO: Pod "dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf" satisfied condition "running"
STEP: retrieving the pod 09/06/23 10:21:46.249
STEP: looking for the results for each expected name from probers 09/06/23 10:21:46.261
Sep  6 10:21:46.277: INFO: DNS probes using dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf succeeded

STEP: deleting the pod 09/06/23 10:21:46.278
STEP: deleting the test externalName service 09/06/23 10:21:46.301
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  6 10:21:46.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9034" for this suite. 09/06/23 10:21:46.327
------------------------------
• [SLOW TEST] [13.974 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:21:32.367
    Sep  6 10:21:32.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename dns 09/06/23 10:21:32.369
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:32.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:32.392
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 09/06/23 10:21:32.393
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
     09/06/23 10:21:32.398
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
     09/06/23 10:21:32.398
    STEP: creating a pod to probe DNS 09/06/23 10:21:32.398
    STEP: submitting the pod to kubernetes 09/06/23 10:21:32.398
    Sep  6 10:21:32.409: INFO: Waiting up to 15m0s for pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8" in namespace "dns-9034" to be "running"
    Sep  6 10:21:32.415: INFO: Pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.225809ms
    Sep  6 10:21:34.434: INFO: Pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024797364s
    Sep  6 10:21:36.421: INFO: Pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8": Phase="Running", Reason="", readiness=true. Elapsed: 4.011799403s
    Sep  6 10:21:36.421: INFO: Pod "dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 10:21:36.421
    STEP: looking for the results for each expected name from probers 09/06/23 10:21:36.439
    Sep  6 10:21:36.466: INFO: DNS probes using dns-test-3fac0965-5d00-40aa-af53-e91e9b22cef8 succeeded

    STEP: deleting the pod 09/06/23 10:21:36.467
    STEP: changing the externalName to bar.example.com 09/06/23 10:21:36.795
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
     09/06/23 10:21:36.988
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
     09/06/23 10:21:36.988
    STEP: creating a second pod to probe DNS 09/06/23 10:21:36.988
    STEP: submitting the pod to kubernetes 09/06/23 10:21:36.988
    Sep  6 10:21:37.083: INFO: Waiting up to 15m0s for pod "dns-test-31224c33-e76f-475f-a211-8043a1933729" in namespace "dns-9034" to be "running"
    Sep  6 10:21:37.107: INFO: Pod "dns-test-31224c33-e76f-475f-a211-8043a1933729": Phase="Pending", Reason="", readiness=false. Elapsed: 23.949747ms
    Sep  6 10:21:39.111: INFO: Pod "dns-test-31224c33-e76f-475f-a211-8043a1933729": Phase="Running", Reason="", readiness=true. Elapsed: 2.028230659s
    Sep  6 10:21:39.111: INFO: Pod "dns-test-31224c33-e76f-475f-a211-8043a1933729" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 10:21:39.111
    STEP: looking for the results for each expected name from probers 09/06/23 10:21:39.119
    Sep  6 10:21:39.125: INFO: File wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local from pod  dns-9034/dns-test-31224c33-e76f-475f-a211-8043a1933729 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  6 10:21:39.131: INFO: File jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local from pod  dns-9034/dns-test-31224c33-e76f-475f-a211-8043a1933729 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  6 10:21:39.131: INFO: Lookups using dns-9034/dns-test-31224c33-e76f-475f-a211-8043a1933729 failed for: [wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local]

    Sep  6 10:21:44.139: INFO: DNS probes using dns-test-31224c33-e76f-475f-a211-8043a1933729 succeeded

    STEP: deleting the pod 09/06/23 10:21:44.139
    STEP: changing the service to type=ClusterIP 09/06/23 10:21:44.167
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
     09/06/23 10:21:44.201
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9034.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9034.svc.cluster.local; sleep 1; done
     09/06/23 10:21:44.202
    STEP: creating a third pod to probe DNS 09/06/23 10:21:44.202
    STEP: submitting the pod to kubernetes 09/06/23 10:21:44.211
    Sep  6 10:21:44.224: INFO: Waiting up to 15m0s for pod "dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf" in namespace "dns-9034" to be "running"
    Sep  6 10:21:44.231: INFO: Pod "dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.882571ms
    Sep  6 10:21:46.249: INFO: Pod "dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.024919996s
    Sep  6 10:21:46.249: INFO: Pod "dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 10:21:46.249
    STEP: looking for the results for each expected name from probers 09/06/23 10:21:46.261
    Sep  6 10:21:46.277: INFO: DNS probes using dns-test-7981d7ff-3671-4866-9df2-8e1936ad55bf succeeded

    STEP: deleting the pod 09/06/23 10:21:46.278
    STEP: deleting the test externalName service 09/06/23 10:21:46.301
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:21:46.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9034" for this suite. 09/06/23 10:21:46.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:21:46.348
Sep  6 10:21:46.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename var-expansion 09/06/23 10:21:46.349
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:46.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:46.379
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 09/06/23 10:21:46.384
Sep  6 10:21:46.396: INFO: Waiting up to 5m0s for pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5" in namespace "var-expansion-3012" to be "Succeeded or Failed"
Sep  6 10:21:46.405: INFO: Pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.520141ms
Sep  6 10:21:48.419: INFO: Pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022075607s
Sep  6 10:21:50.417: INFO: Pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020700671s
STEP: Saw pod success 09/06/23 10:21:50.417
Sep  6 10:21:50.418: INFO: Pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5" satisfied condition "Succeeded or Failed"
Sep  6 10:21:50.429: INFO: Trying to get logs from node kube-3 pod var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5 container dapi-container: <nil>
STEP: delete the pod 09/06/23 10:21:50.451
Sep  6 10:21:50.476: INFO: Waiting for pod var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5 to disappear
Sep  6 10:21:50.480: INFO: Pod var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  6 10:21:50.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3012" for this suite. 09/06/23 10:21:50.483
------------------------------
• [4.142 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:21:46.348
    Sep  6 10:21:46.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename var-expansion 09/06/23 10:21:46.349
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:46.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:46.379
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 09/06/23 10:21:46.384
    Sep  6 10:21:46.396: INFO: Waiting up to 5m0s for pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5" in namespace "var-expansion-3012" to be "Succeeded or Failed"
    Sep  6 10:21:46.405: INFO: Pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.520141ms
    Sep  6 10:21:48.419: INFO: Pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022075607s
    Sep  6 10:21:50.417: INFO: Pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020700671s
    STEP: Saw pod success 09/06/23 10:21:50.417
    Sep  6 10:21:50.418: INFO: Pod "var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5" satisfied condition "Succeeded or Failed"
    Sep  6 10:21:50.429: INFO: Trying to get logs from node kube-3 pod var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5 container dapi-container: <nil>
    STEP: delete the pod 09/06/23 10:21:50.451
    Sep  6 10:21:50.476: INFO: Waiting for pod var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5 to disappear
    Sep  6 10:21:50.480: INFO: Pod var-expansion-c680fccc-3404-4e07-9577-f2a8ac238fa5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:21:50.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3012" for this suite. 09/06/23 10:21:50.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:21:50.491
Sep  6 10:21:50.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubelet-test 09/06/23 10:21:50.491
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:50.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:50.512
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:21:54.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6777" for this suite. 09/06/23 10:21:54.558
------------------------------
• [4.090 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:21:50.491
    Sep  6 10:21:50.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubelet-test 09/06/23 10:21:50.491
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:50.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:50.512
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:21:54.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6777" for this suite. 09/06/23 10:21:54.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:21:54.581
Sep  6 10:21:54.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename dns 09/06/23 10:21:54.582
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:54.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:54.605
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 09/06/23 10:21:54.607
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3049.svc.cluster.local;sleep 1; done
 09/06/23 10:21:54.614
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3049.svc.cluster.local;sleep 1; done
 09/06/23 10:21:54.614
STEP: creating a pod to probe DNS 09/06/23 10:21:54.614
STEP: submitting the pod to kubernetes 09/06/23 10:21:54.614
Sep  6 10:21:54.637: INFO: Waiting up to 15m0s for pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4" in namespace "dns-3049" to be "running"
Sep  6 10:21:54.646: INFO: Pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.163626ms
Sep  6 10:21:56.660: INFO: Pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022299581s
Sep  6 10:21:58.660: INFO: Pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4": Phase="Running", Reason="", readiness=true. Elapsed: 4.022275111s
Sep  6 10:21:58.660: INFO: Pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4" satisfied condition "running"
STEP: retrieving the pod 09/06/23 10:21:58.66
STEP: looking for the results for each expected name from probers 09/06/23 10:21:58.677
Sep  6 10:21:58.697: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
Sep  6 10:21:58.706: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
Sep  6 10:21:58.717: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
Sep  6 10:21:58.726: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
Sep  6 10:21:58.733: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
Sep  6 10:21:58.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
Sep  6 10:21:58.741: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
Sep  6 10:21:58.747: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
Sep  6 10:21:58.747: INFO: Lookups using dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3049.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3049.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local jessie_udp@dns-test-service-2.dns-3049.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3049.svc.cluster.local]

Sep  6 10:22:03.797: INFO: DNS probes using dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4 succeeded

STEP: deleting the pod 09/06/23 10:22:03.797
STEP: deleting the test headless service 09/06/23 10:22:03.826
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  6 10:22:03.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3049" for this suite. 09/06/23 10:22:03.891
------------------------------
• [SLOW TEST] [9.329 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:21:54.581
    Sep  6 10:21:54.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename dns 09/06/23 10:21:54.582
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:21:54.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:21:54.605
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 09/06/23 10:21:54.607
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3049.svc.cluster.local;sleep 1; done
     09/06/23 10:21:54.614
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3049.svc.cluster.local;sleep 1; done
     09/06/23 10:21:54.614
    STEP: creating a pod to probe DNS 09/06/23 10:21:54.614
    STEP: submitting the pod to kubernetes 09/06/23 10:21:54.614
    Sep  6 10:21:54.637: INFO: Waiting up to 15m0s for pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4" in namespace "dns-3049" to be "running"
    Sep  6 10:21:54.646: INFO: Pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.163626ms
    Sep  6 10:21:56.660: INFO: Pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022299581s
    Sep  6 10:21:58.660: INFO: Pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4": Phase="Running", Reason="", readiness=true. Elapsed: 4.022275111s
    Sep  6 10:21:58.660: INFO: Pod "dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 10:21:58.66
    STEP: looking for the results for each expected name from probers 09/06/23 10:21:58.677
    Sep  6 10:21:58.697: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
    Sep  6 10:21:58.706: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
    Sep  6 10:21:58.717: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
    Sep  6 10:21:58.726: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
    Sep  6 10:21:58.733: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
    Sep  6 10:21:58.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
    Sep  6 10:21:58.741: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
    Sep  6 10:21:58.747: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3049.svc.cluster.local from pod dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4: the server could not find the requested resource (get pods dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4)
    Sep  6 10:21:58.747: INFO: Lookups using dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3049.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3049.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3049.svc.cluster.local jessie_udp@dns-test-service-2.dns-3049.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3049.svc.cluster.local]

    Sep  6 10:22:03.797: INFO: DNS probes using dns-3049/dns-test-315db1df-5ee6-4624-9a7c-9061bf5b80d4 succeeded

    STEP: deleting the pod 09/06/23 10:22:03.797
    STEP: deleting the test headless service 09/06/23 10:22:03.826
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:22:03.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3049" for this suite. 09/06/23 10:22:03.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:22:03.919
Sep  6 10:22:03.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:22:03.92
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:03.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:03.947
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:22:03.962
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:22:04.186
STEP: Deploying the webhook pod 09/06/23 10:22:04.198
STEP: Wait for the deployment to be ready 09/06/23 10:22:04.21
Sep  6 10:22:04.217: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 10:22:06.27
STEP: Verifying the service has paired with the endpoint 09/06/23 10:22:06.284
Sep  6 10:22:07.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 09/06/23 10:22:07.292
STEP: create a pod 09/06/23 10:22:08.046
Sep  6 10:22:08.760: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6367" to be "running"
Sep  6 10:22:08.852: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 91.797816ms
Sep  6 10:22:10.861: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.100238418s
Sep  6 10:22:10.861: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 09/06/23 10:22:10.861
Sep  6 10:22:10.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=webhook-6367 attach --namespace=webhook-6367 to-be-attached-pod -i -c=container1'
Sep  6 10:22:10.977: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:22:10.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6367" for this suite. 09/06/23 10:22:11.06
STEP: Destroying namespace "webhook-6367-markers" for this suite. 09/06/23 10:22:11.083
------------------------------
• [SLOW TEST] [7.187 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:22:03.919
    Sep  6 10:22:03.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:22:03.92
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:03.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:03.947
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:22:03.962
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:22:04.186
    STEP: Deploying the webhook pod 09/06/23 10:22:04.198
    STEP: Wait for the deployment to be ready 09/06/23 10:22:04.21
    Sep  6 10:22:04.217: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 10:22:06.27
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:22:06.284
    Sep  6 10:22:07.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 09/06/23 10:22:07.292
    STEP: create a pod 09/06/23 10:22:08.046
    Sep  6 10:22:08.760: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6367" to be "running"
    Sep  6 10:22:08.852: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 91.797816ms
    Sep  6 10:22:10.861: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.100238418s
    Sep  6 10:22:10.861: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 09/06/23 10:22:10.861
    Sep  6 10:22:10.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=webhook-6367 attach --namespace=webhook-6367 to-be-attached-pod -i -c=container1'
    Sep  6 10:22:10.977: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:22:10.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6367" for this suite. 09/06/23 10:22:11.06
    STEP: Destroying namespace "webhook-6367-markers" for this suite. 09/06/23 10:22:11.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:22:11.11
Sep  6 10:22:11.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:22:11.111
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:11.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:11.148
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 09/06/23 10:22:11.152
Sep  6 10:22:11.168: INFO: Waiting up to 5m0s for pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea" in namespace "downward-api-5745" to be "Succeeded or Failed"
Sep  6 10:22:11.172: INFO: Pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83563ms
Sep  6 10:22:13.186: INFO: Pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018004468s
Sep  6 10:22:15.196: INFO: Pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028343604s
STEP: Saw pod success 09/06/23 10:22:15.196
Sep  6 10:22:15.197: INFO: Pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea" satisfied condition "Succeeded or Failed"
Sep  6 10:22:15.209: INFO: Trying to get logs from node kube-3 pod downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea container dapi-container: <nil>
STEP: delete the pod 09/06/23 10:22:15.224
Sep  6 10:22:15.247: INFO: Waiting for pod downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea to disappear
Sep  6 10:22:15.251: INFO: Pod downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  6 10:22:15.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5745" for this suite. 09/06/23 10:22:15.255
------------------------------
• [4.155 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:22:11.11
    Sep  6 10:22:11.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:22:11.111
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:11.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:11.148
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 09/06/23 10:22:11.152
    Sep  6 10:22:11.168: INFO: Waiting up to 5m0s for pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea" in namespace "downward-api-5745" to be "Succeeded or Failed"
    Sep  6 10:22:11.172: INFO: Pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83563ms
    Sep  6 10:22:13.186: INFO: Pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018004468s
    Sep  6 10:22:15.196: INFO: Pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028343604s
    STEP: Saw pod success 09/06/23 10:22:15.196
    Sep  6 10:22:15.197: INFO: Pod "downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea" satisfied condition "Succeeded or Failed"
    Sep  6 10:22:15.209: INFO: Trying to get logs from node kube-3 pod downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea container dapi-container: <nil>
    STEP: delete the pod 09/06/23 10:22:15.224
    Sep  6 10:22:15.247: INFO: Waiting for pod downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea to disappear
    Sep  6 10:22:15.251: INFO: Pod downward-api-da9c9b53-a7a3-4286-b65a-eeb26eb3c9ea no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:22:15.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5745" for this suite. 09/06/23 10:22:15.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:22:15.266
Sep  6 10:22:15.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename subpath 09/06/23 10:22:15.267
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:15.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:15.29
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/06/23 10:22:15.292
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-mjpq 09/06/23 10:22:15.301
STEP: Creating a pod to test atomic-volume-subpath 09/06/23 10:22:15.301
Sep  6 10:22:15.313: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mjpq" in namespace "subpath-4294" to be "Succeeded or Failed"
Sep  6 10:22:15.316: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918789ms
Sep  6 10:22:17.329: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 2.015647352s
Sep  6 10:22:19.326: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 4.012522168s
Sep  6 10:22:21.332: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 6.018712351s
Sep  6 10:22:23.329: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 8.015205444s
Sep  6 10:22:25.333: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 10.019561791s
Sep  6 10:22:27.331: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 12.017379659s
Sep  6 10:22:29.330: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 14.016524332s
Sep  6 10:22:31.331: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 16.017358483s
Sep  6 10:22:33.331: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 18.017910659s
Sep  6 10:22:35.488: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 20.17442463s
Sep  6 10:22:37.338: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=false. Elapsed: 22.024559116s
Sep  6 10:22:39.333: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.019531176s
STEP: Saw pod success 09/06/23 10:22:39.333
Sep  6 10:22:39.334: INFO: Pod "pod-subpath-test-configmap-mjpq" satisfied condition "Succeeded or Failed"
Sep  6 10:22:39.346: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-configmap-mjpq container test-container-subpath-configmap-mjpq: <nil>
STEP: delete the pod 09/06/23 10:22:39.371
Sep  6 10:22:39.609: INFO: Waiting for pod pod-subpath-test-configmap-mjpq to disappear
Sep  6 10:22:39.612: INFO: Pod pod-subpath-test-configmap-mjpq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mjpq 09/06/23 10:22:39.612
Sep  6 10:22:39.612: INFO: Deleting pod "pod-subpath-test-configmap-mjpq" in namespace "subpath-4294"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  6 10:22:39.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4294" for this suite. 09/06/23 10:22:39.619
------------------------------
• [SLOW TEST] [24.371 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:22:15.266
    Sep  6 10:22:15.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename subpath 09/06/23 10:22:15.267
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:15.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:15.29
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/06/23 10:22:15.292
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-mjpq 09/06/23 10:22:15.301
    STEP: Creating a pod to test atomic-volume-subpath 09/06/23 10:22:15.301
    Sep  6 10:22:15.313: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mjpq" in namespace "subpath-4294" to be "Succeeded or Failed"
    Sep  6 10:22:15.316: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918789ms
    Sep  6 10:22:17.329: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 2.015647352s
    Sep  6 10:22:19.326: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 4.012522168s
    Sep  6 10:22:21.332: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 6.018712351s
    Sep  6 10:22:23.329: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 8.015205444s
    Sep  6 10:22:25.333: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 10.019561791s
    Sep  6 10:22:27.331: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 12.017379659s
    Sep  6 10:22:29.330: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 14.016524332s
    Sep  6 10:22:31.331: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 16.017358483s
    Sep  6 10:22:33.331: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 18.017910659s
    Sep  6 10:22:35.488: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=true. Elapsed: 20.17442463s
    Sep  6 10:22:37.338: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Running", Reason="", readiness=false. Elapsed: 22.024559116s
    Sep  6 10:22:39.333: INFO: Pod "pod-subpath-test-configmap-mjpq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.019531176s
    STEP: Saw pod success 09/06/23 10:22:39.333
    Sep  6 10:22:39.334: INFO: Pod "pod-subpath-test-configmap-mjpq" satisfied condition "Succeeded or Failed"
    Sep  6 10:22:39.346: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-configmap-mjpq container test-container-subpath-configmap-mjpq: <nil>
    STEP: delete the pod 09/06/23 10:22:39.371
    Sep  6 10:22:39.609: INFO: Waiting for pod pod-subpath-test-configmap-mjpq to disappear
    Sep  6 10:22:39.612: INFO: Pod pod-subpath-test-configmap-mjpq no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-mjpq 09/06/23 10:22:39.612
    Sep  6 10:22:39.612: INFO: Deleting pod "pod-subpath-test-configmap-mjpq" in namespace "subpath-4294"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:22:39.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4294" for this suite. 09/06/23 10:22:39.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:22:39.64
Sep  6 10:22:39.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 10:22:39.64
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:39.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:39.736
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Sep  6 10:22:39.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:22:48.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2556" for this suite. 09/06/23 10:22:48.026
------------------------------
• [SLOW TEST] [8.412 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:22:39.64
    Sep  6 10:22:39.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 10:22:39.64
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:39.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:39.736
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Sep  6 10:22:39.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:22:48.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2556" for this suite. 09/06/23 10:22:48.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:22:48.056
Sep  6 10:22:48.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replicaset 09/06/23 10:22:48.057
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:48.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:48.125
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Sep  6 10:22:48.127: INFO: Creating ReplicaSet my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c
Sep  6 10:22:48.137: INFO: Pod name my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c: Found 0 pods out of 1
Sep  6 10:22:53.143: INFO: Pod name my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c: Found 1 pods out of 1
Sep  6 10:22:53.143: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c" is running
Sep  6 10:22:53.143: INFO: Waiting up to 5m0s for pod "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr" in namespace "replicaset-6499" to be "running"
Sep  6 10:22:53.147: INFO: Pod "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr": Phase="Running", Reason="", readiness=true. Elapsed: 3.230313ms
Sep  6 10:22:53.147: INFO: Pod "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr" satisfied condition "running"
Sep  6 10:22:53.147: INFO: Pod "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:22:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:22:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:22:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:22:48 +0000 UTC Reason: Message:}])
Sep  6 10:22:53.147: INFO: Trying to dial the pod
Sep  6 10:22:58.168: INFO: Controller my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c: Got expected result from replica 1 [my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr]: "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:22:58.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6499" for this suite. 09/06/23 10:22:58.175
------------------------------
• [SLOW TEST] [10.139 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:22:48.056
    Sep  6 10:22:48.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replicaset 09/06/23 10:22:48.057
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:48.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:48.125
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Sep  6 10:22:48.127: INFO: Creating ReplicaSet my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c
    Sep  6 10:22:48.137: INFO: Pod name my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c: Found 0 pods out of 1
    Sep  6 10:22:53.143: INFO: Pod name my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c: Found 1 pods out of 1
    Sep  6 10:22:53.143: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c" is running
    Sep  6 10:22:53.143: INFO: Waiting up to 5m0s for pod "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr" in namespace "replicaset-6499" to be "running"
    Sep  6 10:22:53.147: INFO: Pod "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr": Phase="Running", Reason="", readiness=true. Elapsed: 3.230313ms
    Sep  6 10:22:53.147: INFO: Pod "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr" satisfied condition "running"
    Sep  6 10:22:53.147: INFO: Pod "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:22:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:22:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:22:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:22:48 +0000 UTC Reason: Message:}])
    Sep  6 10:22:53.147: INFO: Trying to dial the pod
    Sep  6 10:22:58.168: INFO: Controller my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c: Got expected result from replica 1 [my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr]: "my-hostname-basic-18cf3f11-b742-4a72-aaf2-31c112afc68c-9sglr", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:22:58.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6499" for this suite. 09/06/23 10:22:58.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:22:58.197
Sep  6 10:22:58.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 10:22:58.198
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:58.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:58.22
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-c305aece-d03f-4b7b-969f-bc33ab10a159 09/06/23 10:22:58.251
STEP: Creating a pod to test consume secrets 09/06/23 10:22:58.257
Sep  6 10:22:58.266: INFO: Waiting up to 5m0s for pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70" in namespace "secrets-523" to be "Succeeded or Failed"
Sep  6 10:22:58.270: INFO: Pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.835406ms
Sep  6 10:23:00.275: INFO: Pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70": Phase="Running", Reason="", readiness=false. Elapsed: 2.008998638s
Sep  6 10:23:02.285: INFO: Pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01896592s
STEP: Saw pod success 09/06/23 10:23:02.285
Sep  6 10:23:02.285: INFO: Pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70" satisfied condition "Succeeded or Failed"
Sep  6 10:23:02.290: INFO: Trying to get logs from node kube-3 pod pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70 container secret-volume-test: <nil>
STEP: delete the pod 09/06/23 10:23:02.3
Sep  6 10:23:02.314: INFO: Waiting for pod pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70 to disappear
Sep  6 10:23:02.317: INFO: Pod pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 10:23:02.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-523" for this suite. 09/06/23 10:23:02.321
STEP: Destroying namespace "secret-namespace-7150" for this suite. 09/06/23 10:23:02.336
------------------------------
• [4.145 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:22:58.197
    Sep  6 10:22:58.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 10:22:58.198
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:22:58.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:22:58.22
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-c305aece-d03f-4b7b-969f-bc33ab10a159 09/06/23 10:22:58.251
    STEP: Creating a pod to test consume secrets 09/06/23 10:22:58.257
    Sep  6 10:22:58.266: INFO: Waiting up to 5m0s for pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70" in namespace "secrets-523" to be "Succeeded or Failed"
    Sep  6 10:22:58.270: INFO: Pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.835406ms
    Sep  6 10:23:00.275: INFO: Pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70": Phase="Running", Reason="", readiness=false. Elapsed: 2.008998638s
    Sep  6 10:23:02.285: INFO: Pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01896592s
    STEP: Saw pod success 09/06/23 10:23:02.285
    Sep  6 10:23:02.285: INFO: Pod "pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70" satisfied condition "Succeeded or Failed"
    Sep  6 10:23:02.290: INFO: Trying to get logs from node kube-3 pod pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70 container secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:23:02.3
    Sep  6 10:23:02.314: INFO: Waiting for pod pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70 to disappear
    Sep  6 10:23:02.317: INFO: Pod pod-secrets-5e2181be-0d9d-45e6-9182-60ccbe255c70 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:23:02.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-523" for this suite. 09/06/23 10:23:02.321
    STEP: Destroying namespace "secret-namespace-7150" for this suite. 09/06/23 10:23:02.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:23:02.343
Sep  6 10:23:02.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replication-controller 09/06/23 10:23:02.344
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:23:02.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:23:02.366
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 09/06/23 10:23:02.368
Sep  6 10:23:02.381: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7703" to be "running and ready"
Sep  6 10:23:02.388: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.917396ms
Sep  6 10:23:02.388: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:23:04.399: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.018287937s
Sep  6 10:23:04.399: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Sep  6 10:23:04.399: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 09/06/23 10:23:04.405
STEP: Then the orphan pod is adopted 09/06/23 10:23:04.414
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  6 10:23:05.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7703" for this suite. 09/06/23 10:23:05.446
------------------------------
• [3.112 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:23:02.343
    Sep  6 10:23:02.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replication-controller 09/06/23 10:23:02.344
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:23:02.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:23:02.366
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 09/06/23 10:23:02.368
    Sep  6 10:23:02.381: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7703" to be "running and ready"
    Sep  6 10:23:02.388: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.917396ms
    Sep  6 10:23:02.388: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:23:04.399: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.018287937s
    Sep  6 10:23:04.399: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Sep  6 10:23:04.399: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 09/06/23 10:23:04.405
    STEP: Then the orphan pod is adopted 09/06/23 10:23:04.414
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:23:05.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7703" for this suite. 09/06/23 10:23:05.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:23:05.455
Sep  6 10:23:05.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:23:05.456
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:23:05.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:23:05.485
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 09/06/23 10:23:05.488
Sep  6 10:23:05.496: INFO: Waiting up to 5m0s for pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874" in namespace "emptydir-4099" to be "Succeeded or Failed"
Sep  6 10:23:05.499: INFO: Pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874": Phase="Pending", Reason="", readiness=false. Elapsed: 3.483396ms
Sep  6 10:23:07.512: INFO: Pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016488512s
Sep  6 10:23:09.506: INFO: Pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010322648s
STEP: Saw pod success 09/06/23 10:23:09.506
Sep  6 10:23:09.506: INFO: Pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874" satisfied condition "Succeeded or Failed"
Sep  6 10:23:09.517: INFO: Trying to get logs from node kube-3 pod pod-86cf45fc-1c27-4938-b956-f1eabc3a8874 container test-container: <nil>
STEP: delete the pod 09/06/23 10:23:09.527
Sep  6 10:23:09.545: INFO: Waiting for pod pod-86cf45fc-1c27-4938-b956-f1eabc3a8874 to disappear
Sep  6 10:23:09.548: INFO: Pod pod-86cf45fc-1c27-4938-b956-f1eabc3a8874 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:23:09.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4099" for this suite. 09/06/23 10:23:09.552
------------------------------
• [4.103 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:23:05.455
    Sep  6 10:23:05.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:23:05.456
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:23:05.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:23:05.485
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 09/06/23 10:23:05.488
    Sep  6 10:23:05.496: INFO: Waiting up to 5m0s for pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874" in namespace "emptydir-4099" to be "Succeeded or Failed"
    Sep  6 10:23:05.499: INFO: Pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874": Phase="Pending", Reason="", readiness=false. Elapsed: 3.483396ms
    Sep  6 10:23:07.512: INFO: Pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016488512s
    Sep  6 10:23:09.506: INFO: Pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010322648s
    STEP: Saw pod success 09/06/23 10:23:09.506
    Sep  6 10:23:09.506: INFO: Pod "pod-86cf45fc-1c27-4938-b956-f1eabc3a8874" satisfied condition "Succeeded or Failed"
    Sep  6 10:23:09.517: INFO: Trying to get logs from node kube-3 pod pod-86cf45fc-1c27-4938-b956-f1eabc3a8874 container test-container: <nil>
    STEP: delete the pod 09/06/23 10:23:09.527
    Sep  6 10:23:09.545: INFO: Waiting for pod pod-86cf45fc-1c27-4938-b956-f1eabc3a8874 to disappear
    Sep  6 10:23:09.548: INFO: Pod pod-86cf45fc-1c27-4938-b956-f1eabc3a8874 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:23:09.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4099" for this suite. 09/06/23 10:23:09.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:23:09.559
Sep  6 10:23:09.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:23:09.559
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:23:09.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:23:09.584
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:23:09.598
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:23:10.027
STEP: Deploying the webhook pod 09/06/23 10:23:10.035
STEP: Wait for the deployment to be ready 09/06/23 10:23:10.052
Sep  6 10:23:10.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 10:23:12.1
STEP: Verifying the service has paired with the endpoint 09/06/23 10:23:12.113
Sep  6 10:23:13.115: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 09/06/23 10:23:13.125
STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/06/23 10:23:13.149
STEP: Creating a configMap that should not be mutated 09/06/23 10:23:13.157
STEP: Patching a mutating webhook configuration's rules to include the create operation 09/06/23 10:23:13.173
STEP: Creating a configMap that should be mutated 09/06/23 10:23:13.182
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:23:13.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7378" for this suite. 09/06/23 10:23:13.275
STEP: Destroying namespace "webhook-7378-markers" for this suite. 09/06/23 10:23:13.294
------------------------------
• [3.748 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:23:09.559
    Sep  6 10:23:09.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:23:09.559
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:23:09.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:23:09.584
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:23:09.598
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:23:10.027
    STEP: Deploying the webhook pod 09/06/23 10:23:10.035
    STEP: Wait for the deployment to be ready 09/06/23 10:23:10.052
    Sep  6 10:23:10.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 10:23:12.1
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:23:12.113
    Sep  6 10:23:13.115: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 09/06/23 10:23:13.125
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/06/23 10:23:13.149
    STEP: Creating a configMap that should not be mutated 09/06/23 10:23:13.157
    STEP: Patching a mutating webhook configuration's rules to include the create operation 09/06/23 10:23:13.173
    STEP: Creating a configMap that should be mutated 09/06/23 10:23:13.182
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:23:13.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7378" for this suite. 09/06/23 10:23:13.275
    STEP: Destroying namespace "webhook-7378-markers" for this suite. 09/06/23 10:23:13.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:23:13.308
Sep  6 10:23:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename cronjob 09/06/23 10:23:13.31
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:23:13.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:23:13.352
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 09/06/23 10:23:13.355
STEP: Ensuring no jobs are scheduled 09/06/23 10:23:13.362
STEP: Ensuring no job exists by listing jobs explicitly 09/06/23 10:28:13.369
STEP: Removing cronjob 09/06/23 10:28:13.372
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  6 10:28:13.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5141" for this suite. 09/06/23 10:28:13.384
------------------------------
• [SLOW TEST] [300.082 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:23:13.308
    Sep  6 10:23:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename cronjob 09/06/23 10:23:13.31
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:23:13.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:23:13.352
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 09/06/23 10:23:13.355
    STEP: Ensuring no jobs are scheduled 09/06/23 10:23:13.362
    STEP: Ensuring no job exists by listing jobs explicitly 09/06/23 10:28:13.369
    STEP: Removing cronjob 09/06/23 10:28:13.372
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:28:13.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5141" for this suite. 09/06/23 10:28:13.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:28:13.39
Sep  6 10:28:13.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename statefulset 09/06/23 10:28:13.391
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:28:13.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:28:13.415
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1857 09/06/23 10:28:13.416
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-1857 09/06/23 10:28:13.422
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1857 09/06/23 10:28:13.431
Sep  6 10:28:13.435: INFO: Found 0 stateful pods, waiting for 1
Sep  6 10:28:23.640: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/06/23 10:28:23.641
Sep  6 10:28:24.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:28:24.404: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:28:24.404: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:28:24.404: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:28:24.408: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 10:28:34.413: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:28:34.413: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:28:34.429: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  6 10:28:34.429: INFO: ss-0  kube-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
Sep  6 10:28:34.429: INFO: 
Sep  6 10:28:34.429: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  6 10:28:35.440: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995313001s
Sep  6 10:28:36.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98480628s
Sep  6 10:28:37.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968977695s
Sep  6 10:28:38.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963803004s
Sep  6 10:28:39.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958817183s
Sep  6 10:28:40.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.944752241s
Sep  6 10:28:41.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940654016s
Sep  6 10:28:42.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.925446126s
Sep  6 10:28:43.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.024518ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1857 09/06/23 10:28:44.517
Sep  6 10:28:44.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:44.655: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 10:28:44.655: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:28:44.655: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:28:44.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:44.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 10:28:44.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:28:44.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:28:44.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:45.083: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 10:28:45.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:28:45.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:28:45.086: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:28:45.086: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:28:45.086: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 09/06/23 10:28:45.086
Sep  6 10:28:45.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:28:45.197: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:28:45.197: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:28:45.197: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:28:45.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:28:45.315: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:28:45.315: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:28:45.315: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:28:45.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:28:45.455: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:28:45.455: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:28:45.455: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:28:45.455: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:28:45.458: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  6 10:28:55.485: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:28:55.485: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:28:55.485: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:28:55.531: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  6 10:28:55.531: INFO: ss-0  kube-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
Sep  6 10:28:55.531: INFO: ss-1  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
Sep  6 10:28:55.531: INFO: ss-2  kube-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
Sep  6 10:28:55.531: INFO: 
Sep  6 10:28:55.531: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 10:28:59.070: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  6 10:28:59.070: INFO: ss-0  kube-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
Sep  6 10:28:59.070: INFO: ss-1  kube-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
Sep  6 10:28:59.071: INFO: ss-2  kube-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
Sep  6 10:28:59.071: INFO: 
Sep  6 10:28:59.071: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 10:29:00.084: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  6 10:29:00.084: INFO: ss-0  kube-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
Sep  6 10:29:00.084: INFO: ss-1  kube-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
Sep  6 10:29:00.084: INFO: ss-2  kube-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
Sep  6 10:29:00.084: INFO: 
Sep  6 10:29:00.084: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 10:29:01.092: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  6 10:29:01.092: INFO: ss-0  kube-3  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
Sep  6 10:29:01.092: INFO: ss-1  kube-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
Sep  6 10:29:01.092: INFO: 
Sep  6 10:29:01.092: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  6 10:29:02.102: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.433612539s
Sep  6 10:29:03.110: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.423674714s
Sep  6 10:29:04.123: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.41637177s
Sep  6 10:29:05.127: INFO: Verifying statefulset ss doesn't scale past 0 for another 403.182862ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1857 09/06/23 10:29:06.127
Sep  6 10:29:06.146: INFO: Scaling statefulset ss to 0
Sep  6 10:29:06.187: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  6 10:29:06.194: INFO: Deleting all statefulset in ns statefulset-1857
Sep  6 10:29:06.198: INFO: Scaling statefulset ss to 0
Sep  6 10:29:06.210: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:29:06.213: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:29:06.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1857" for this suite. 09/06/23 10:29:06.237
------------------------------
• [SLOW TEST] [52.860 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:28:13.39
    Sep  6 10:28:13.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename statefulset 09/06/23 10:28:13.391
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:28:13.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:28:13.415
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1857 09/06/23 10:28:13.416
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-1857 09/06/23 10:28:13.422
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1857 09/06/23 10:28:13.431
    Sep  6 10:28:13.435: INFO: Found 0 stateful pods, waiting for 1
    Sep  6 10:28:23.640: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/06/23 10:28:23.641
    Sep  6 10:28:24.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 10:28:24.404: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 10:28:24.404: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 10:28:24.404: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 10:28:24.408: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  6 10:28:34.413: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  6 10:28:34.413: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 10:28:34.429: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Sep  6 10:28:34.429: INFO: ss-0  kube-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
    Sep  6 10:28:34.429: INFO: 
    Sep  6 10:28:34.429: INFO: StatefulSet ss has not reached scale 3, at 1
    Sep  6 10:28:35.440: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995313001s
    Sep  6 10:28:36.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98480628s
    Sep  6 10:28:37.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968977695s
    Sep  6 10:28:38.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963803004s
    Sep  6 10:28:39.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958817183s
    Sep  6 10:28:40.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.944752241s
    Sep  6 10:28:41.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940654016s
    Sep  6 10:28:42.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.925446126s
    Sep  6 10:28:43.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.024518ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1857 09/06/23 10:28:44.517
    Sep  6 10:28:44.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 10:28:44.655: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  6 10:28:44.655: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 10:28:44.655: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 10:28:44.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 10:28:44.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  6 10:28:44.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 10:28:44.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 10:28:44.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 10:28:45.083: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  6 10:28:45.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 10:28:45.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 10:28:45.086: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 10:28:45.086: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 10:28:45.086: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 09/06/23 10:28:45.086
    Sep  6 10:28:45.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 10:28:45.197: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 10:28:45.197: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 10:28:45.197: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 10:28:45.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 10:28:45.315: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 10:28:45.315: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 10:28:45.315: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 10:28:45.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-1857 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 10:28:45.455: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 10:28:45.455: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 10:28:45.455: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 10:28:45.455: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 10:28:45.458: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Sep  6 10:28:55.485: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  6 10:28:55.485: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  6 10:28:55.485: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  6 10:28:55.531: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Sep  6 10:28:55.531: INFO: ss-0  kube-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
    Sep  6 10:28:55.531: INFO: ss-1  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
    Sep  6 10:28:55.531: INFO: ss-2  kube-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
    Sep  6 10:28:55.531: INFO: 
    Sep  6 10:28:55.531: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  6 10:28:59.070: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Sep  6 10:28:59.070: INFO: ss-0  kube-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
    Sep  6 10:28:59.070: INFO: ss-1  kube-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
    Sep  6 10:28:59.071: INFO: ss-2  kube-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
    Sep  6 10:28:59.071: INFO: 
    Sep  6 10:28:59.071: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  6 10:29:00.084: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Sep  6 10:29:00.084: INFO: ss-0  kube-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
    Sep  6 10:29:00.084: INFO: ss-1  kube-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
    Sep  6 10:29:00.084: INFO: ss-2  kube-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
    Sep  6 10:29:00.084: INFO: 
    Sep  6 10:29:00.084: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  6 10:29:01.092: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Sep  6 10:29:01.092: INFO: ss-0  kube-3  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:13 +0000 UTC  }]
    Sep  6 10:29:01.092: INFO: ss-1  kube-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 10:28:34 +0000 UTC  }]
    Sep  6 10:29:01.092: INFO: 
    Sep  6 10:29:01.092: INFO: StatefulSet ss has not reached scale 0, at 2
    Sep  6 10:29:02.102: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.433612539s
    Sep  6 10:29:03.110: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.423674714s
    Sep  6 10:29:04.123: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.41637177s
    Sep  6 10:29:05.127: INFO: Verifying statefulset ss doesn't scale past 0 for another 403.182862ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1857 09/06/23 10:29:06.127
    Sep  6 10:29:06.146: INFO: Scaling statefulset ss to 0
    Sep  6 10:29:06.187: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  6 10:29:06.194: INFO: Deleting all statefulset in ns statefulset-1857
    Sep  6 10:29:06.198: INFO: Scaling statefulset ss to 0
    Sep  6 10:29:06.210: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 10:29:06.213: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:29:06.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1857" for this suite. 09/06/23 10:29:06.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:29:06.252
Sep  6 10:29:06.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:29:06.253
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:06.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:06.279
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 09/06/23 10:29:06.281
Sep  6 10:29:06.292: INFO: Waiting up to 5m0s for pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b" in namespace "emptydir-8792" to be "Succeeded or Failed"
Sep  6 10:29:06.297: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146144ms
Sep  6 10:29:08.314: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Running", Reason="", readiness=false. Elapsed: 2.021219043s
Sep  6 10:29:10.302: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Running", Reason="", readiness=false. Elapsed: 4.009503527s
Sep  6 10:29:12.310: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Running", Reason="", readiness=false. Elapsed: 6.018123507s
Sep  6 10:29:14.307: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01474696s
STEP: Saw pod success 09/06/23 10:29:14.307
Sep  6 10:29:14.308: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b" satisfied condition "Succeeded or Failed"
Sep  6 10:29:14.315: INFO: Trying to get logs from node kube-3 pod pod-fbc97fb4-f8c0-440c-8162-189db630e16b container test-container: <nil>
STEP: delete the pod 09/06/23 10:29:14.34
Sep  6 10:29:14.359: INFO: Waiting for pod pod-fbc97fb4-f8c0-440c-8162-189db630e16b to disappear
Sep  6 10:29:14.362: INFO: Pod pod-fbc97fb4-f8c0-440c-8162-189db630e16b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:29:14.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8792" for this suite. 09/06/23 10:29:14.365
------------------------------
• [SLOW TEST] [8.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:29:06.252
    Sep  6 10:29:06.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:29:06.253
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:06.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:06.279
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/06/23 10:29:06.281
    Sep  6 10:29:06.292: INFO: Waiting up to 5m0s for pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b" in namespace "emptydir-8792" to be "Succeeded or Failed"
    Sep  6 10:29:06.297: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146144ms
    Sep  6 10:29:08.314: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Running", Reason="", readiness=false. Elapsed: 2.021219043s
    Sep  6 10:29:10.302: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Running", Reason="", readiness=false. Elapsed: 4.009503527s
    Sep  6 10:29:12.310: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Running", Reason="", readiness=false. Elapsed: 6.018123507s
    Sep  6 10:29:14.307: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01474696s
    STEP: Saw pod success 09/06/23 10:29:14.307
    Sep  6 10:29:14.308: INFO: Pod "pod-fbc97fb4-f8c0-440c-8162-189db630e16b" satisfied condition "Succeeded or Failed"
    Sep  6 10:29:14.315: INFO: Trying to get logs from node kube-3 pod pod-fbc97fb4-f8c0-440c-8162-189db630e16b container test-container: <nil>
    STEP: delete the pod 09/06/23 10:29:14.34
    Sep  6 10:29:14.359: INFO: Waiting for pod pod-fbc97fb4-f8c0-440c-8162-189db630e16b to disappear
    Sep  6 10:29:14.362: INFO: Pod pod-fbc97fb4-f8c0-440c-8162-189db630e16b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:29:14.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8792" for this suite. 09/06/23 10:29:14.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:29:14.374
Sep  6 10:29:14.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 10:29:14.375
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:14.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:14.398
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Sep  6 10:29:14.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/06/23 10:29:20.96
Sep  6 10:29:20.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 --namespace=crd-publish-openapi-8884 create -f -'
Sep  6 10:29:21.498: INFO: stderr: ""
Sep  6 10:29:21.498: INFO: stdout: "e2e-test-crd-publish-openapi-8011-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  6 10:29:21.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 --namespace=crd-publish-openapi-8884 delete e2e-test-crd-publish-openapi-8011-crds test-cr'
Sep  6 10:29:21.561: INFO: stderr: ""
Sep  6 10:29:21.561: INFO: stdout: "e2e-test-crd-publish-openapi-8011-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  6 10:29:21.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 --namespace=crd-publish-openapi-8884 apply -f -'
Sep  6 10:29:21.708: INFO: stderr: ""
Sep  6 10:29:21.708: INFO: stdout: "e2e-test-crd-publish-openapi-8011-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  6 10:29:21.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 --namespace=crd-publish-openapi-8884 delete e2e-test-crd-publish-openapi-8011-crds test-cr'
Sep  6 10:29:21.766: INFO: stderr: ""
Sep  6 10:29:21.766: INFO: stdout: "e2e-test-crd-publish-openapi-8011-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 09/06/23 10:29:21.766
Sep  6 10:29:21.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 explain e2e-test-crd-publish-openapi-8011-crds'
Sep  6 10:29:21.899: INFO: stderr: ""
Sep  6 10:29:21.899: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8011-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:29:23.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8884" for this suite. 09/06/23 10:29:23.626
------------------------------
• [SLOW TEST] [9.259 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:29:14.374
    Sep  6 10:29:14.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 10:29:14.375
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:14.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:14.398
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Sep  6 10:29:14.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/06/23 10:29:20.96
    Sep  6 10:29:20.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 --namespace=crd-publish-openapi-8884 create -f -'
    Sep  6 10:29:21.498: INFO: stderr: ""
    Sep  6 10:29:21.498: INFO: stdout: "e2e-test-crd-publish-openapi-8011-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  6 10:29:21.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 --namespace=crd-publish-openapi-8884 delete e2e-test-crd-publish-openapi-8011-crds test-cr'
    Sep  6 10:29:21.561: INFO: stderr: ""
    Sep  6 10:29:21.561: INFO: stdout: "e2e-test-crd-publish-openapi-8011-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Sep  6 10:29:21.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 --namespace=crd-publish-openapi-8884 apply -f -'
    Sep  6 10:29:21.708: INFO: stderr: ""
    Sep  6 10:29:21.708: INFO: stdout: "e2e-test-crd-publish-openapi-8011-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  6 10:29:21.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 --namespace=crd-publish-openapi-8884 delete e2e-test-crd-publish-openapi-8011-crds test-cr'
    Sep  6 10:29:21.766: INFO: stderr: ""
    Sep  6 10:29:21.766: INFO: stdout: "e2e-test-crd-publish-openapi-8011-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 09/06/23 10:29:21.766
    Sep  6 10:29:21.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-8884 explain e2e-test-crd-publish-openapi-8011-crds'
    Sep  6 10:29:21.899: INFO: stderr: ""
    Sep  6 10:29:21.899: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8011-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:29:23.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8884" for this suite. 09/06/23 10:29:23.626
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:29:23.634
Sep  6 10:29:23.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename deployment 09/06/23 10:29:23.634
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:23.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:23.654
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Sep  6 10:29:23.664: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  6 10:29:28.678: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/06/23 10:29:28.679
Sep  6 10:29:28.679: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  6 10:29:30.690: INFO: Creating deployment "test-rollover-deployment"
Sep  6 10:29:30.711: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  6 10:29:32.723: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  6 10:29:32.731: INFO: Ensure that both replica sets have 1 created replica
Sep  6 10:29:32.739: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  6 10:29:32.757: INFO: Updating deployment test-rollover-deployment
Sep  6 10:29:32.757: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  6 10:29:34.777: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  6 10:29:34.784: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  6 10:29:34.791: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:29:34.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:29:36.819: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:29:36.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:29:38.824: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:29:38.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:29:40.814: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:29:40.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:29:42.798: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:29:42.798: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:29:44.799: INFO: 
Sep  6 10:29:44.799: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  6 10:29:44.807: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5797  e1f33aee-071b-4e14-b022-17a98cdfac6a 9941 2 2023-09-06 10:29:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00192ee88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-06 10:29:30 +0000 UTC,LastTransitionTime:2023-09-06 10:29:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-06 10:29:44 +0000 UTC,LastTransitionTime:2023-09-06 10:29:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 10:29:44.810: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-5797  497d6cb0-2864-41a2-bf76-91c2268aa919 9931 2 2023-09-06 10:29:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e1f33aee-071b-4e14-b022-17a98cdfac6a 0xc004987987 0xc004987988}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1f33aee-071b-4e14-b022-17a98cdfac6a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004987b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:29:44.810: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  6 10:29:44.810: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5797  5c99ba81-352b-4495-9410-3087b3040189 9940 2 2023-09-06 10:29:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e1f33aee-071b-4e14-b022-17a98cdfac6a 0xc0049876bf 0xc0049876d0}] [] [{e2e.test Update apps/v1 2023-09-06 10:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1f33aee-071b-4e14-b022-17a98cdfac6a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049878e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:29:44.810: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-5797  6f69145b-0a14-4263-bf33-4091ce4fc488 9892 2 2023-09-06 10:29:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e1f33aee-071b-4e14-b022-17a98cdfac6a 0xc004987c17 0xc004987c18}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1f33aee-071b-4e14-b022-17a98cdfac6a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004987de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:29:44.814: INFO: Pod "test-rollover-deployment-6c6df9974f-mkm9q" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-mkm9q test-rollover-deployment-6c6df9974f- deployment-5797  bae812eb-8a84-408c-8b3f-a32f549ed4e5 9905 0 2023-09-06 10:29:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:e8bb6d6b8811f1a2640b1bccb0e6370d17b12be87d6fc382c37e04c675b35eb1 cni.projectcalico.org/podIP:10.233.99.88/32 cni.projectcalico.org/podIPs:10.233.99.88/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 497d6cb0-2864-41a2-bf76-91c2268aa919 0xc003b2c4b7 0xc003b2c4b8}] [] [{kube-controller-manager Update v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"497d6cb0-2864-41a2-bf76-91c2268aa919\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 10:29:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 10:29:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zjqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zjqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:29:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:29:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:29:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:29:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.88,StartTime:2023-09-06 10:29:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 10:29:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://6b2036c8a00e86381ecbd9ef11a2c5c1f67f500b4c7d62fa3955681ffa086f71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  6 10:29:44.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5797" for this suite. 09/06/23 10:29:44.818
------------------------------
• [SLOW TEST] [21.194 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:29:23.634
    Sep  6 10:29:23.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename deployment 09/06/23 10:29:23.634
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:23.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:23.654
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Sep  6 10:29:23.664: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Sep  6 10:29:28.678: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/06/23 10:29:28.679
    Sep  6 10:29:28.679: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Sep  6 10:29:30.690: INFO: Creating deployment "test-rollover-deployment"
    Sep  6 10:29:30.711: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Sep  6 10:29:32.723: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Sep  6 10:29:32.731: INFO: Ensure that both replica sets have 1 created replica
    Sep  6 10:29:32.739: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Sep  6 10:29:32.757: INFO: Updating deployment test-rollover-deployment
    Sep  6 10:29:32.757: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Sep  6 10:29:34.777: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Sep  6 10:29:34.784: INFO: Make sure deployment "test-rollover-deployment" is complete
    Sep  6 10:29:34.791: INFO: all replica sets need to contain the pod-template-hash label
    Sep  6 10:29:34.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:29:36.819: INFO: all replica sets need to contain the pod-template-hash label
    Sep  6 10:29:36.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:29:38.824: INFO: all replica sets need to contain the pod-template-hash label
    Sep  6 10:29:38.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:29:40.814: INFO: all replica sets need to contain the pod-template-hash label
    Sep  6 10:29:40.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:29:42.798: INFO: all replica sets need to contain the pod-template-hash label
    Sep  6 10:29:42.798: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 29, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:29:44.799: INFO: 
    Sep  6 10:29:44.799: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  6 10:29:44.807: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5797  e1f33aee-071b-4e14-b022-17a98cdfac6a 9941 2 2023-09-06 10:29:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00192ee88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-06 10:29:30 +0000 UTC,LastTransitionTime:2023-09-06 10:29:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-06 10:29:44 +0000 UTC,LastTransitionTime:2023-09-06 10:29:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  6 10:29:44.810: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-5797  497d6cb0-2864-41a2-bf76-91c2268aa919 9931 2 2023-09-06 10:29:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e1f33aee-071b-4e14-b022-17a98cdfac6a 0xc004987987 0xc004987988}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1f33aee-071b-4e14-b022-17a98cdfac6a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004987b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:29:44.810: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Sep  6 10:29:44.810: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5797  5c99ba81-352b-4495-9410-3087b3040189 9940 2 2023-09-06 10:29:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e1f33aee-071b-4e14-b022-17a98cdfac6a 0xc0049876bf 0xc0049876d0}] [] [{e2e.test Update apps/v1 2023-09-06 10:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1f33aee-071b-4e14-b022-17a98cdfac6a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049878e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:29:44.810: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-5797  6f69145b-0a14-4263-bf33-4091ce4fc488 9892 2 2023-09-06 10:29:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e1f33aee-071b-4e14-b022-17a98cdfac6a 0xc004987c17 0xc004987c18}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1f33aee-071b-4e14-b022-17a98cdfac6a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004987de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:29:44.814: INFO: Pod "test-rollover-deployment-6c6df9974f-mkm9q" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-mkm9q test-rollover-deployment-6c6df9974f- deployment-5797  bae812eb-8a84-408c-8b3f-a32f549ed4e5 9905 0 2023-09-06 10:29:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:e8bb6d6b8811f1a2640b1bccb0e6370d17b12be87d6fc382c37e04c675b35eb1 cni.projectcalico.org/podIP:10.233.99.88/32 cni.projectcalico.org/podIPs:10.233.99.88/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 497d6cb0-2864-41a2-bf76-91c2268aa919 0xc003b2c4b7 0xc003b2c4b8}] [] [{kube-controller-manager Update v1 2023-09-06 10:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"497d6cb0-2864-41a2-bf76-91c2268aa919\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 10:29:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 10:29:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zjqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zjqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:29:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:29:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:29:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:29:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.88,StartTime:2023-09-06 10:29:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 10:29:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://6b2036c8a00e86381ecbd9ef11a2c5c1f67f500b4c7d62fa3955681ffa086f71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:29:44.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5797" for this suite. 09/06/23 10:29:44.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:29:44.829
Sep  6 10:29:44.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 10:29:44.829
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:44.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:44.849
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-afeb669b-9d22-499e-b956-73873eea0037 09/06/23 10:29:44.851
STEP: Creating a pod to test consume configMaps 09/06/23 10:29:44.857
Sep  6 10:29:44.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30" in namespace "configmap-6776" to be "Succeeded or Failed"
Sep  6 10:29:44.875: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.851021ms
Sep  6 10:29:47.907: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30": Phase="Running", Reason="", readiness=true. Elapsed: 3.03831366s
Sep  6 10:29:48.879: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30": Phase="Running", Reason="", readiness=false. Elapsed: 4.011019095s
Sep  6 10:29:50.880: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011567825s
STEP: Saw pod success 09/06/23 10:29:50.88
Sep  6 10:29:50.880: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30" satisfied condition "Succeeded or Failed"
Sep  6 10:29:50.891: INFO: Trying to get logs from node kube-3 pod pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30 container agnhost-container: <nil>
STEP: delete the pod 09/06/23 10:29:50.936
Sep  6 10:29:51.013: INFO: Waiting for pod pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30 to disappear
Sep  6 10:29:51.023: INFO: Pod pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:29:51.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6776" for this suite. 09/06/23 10:29:51.027
------------------------------
• [SLOW TEST] [6.267 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:29:44.829
    Sep  6 10:29:44.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 10:29:44.829
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:44.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:44.849
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-afeb669b-9d22-499e-b956-73873eea0037 09/06/23 10:29:44.851
    STEP: Creating a pod to test consume configMaps 09/06/23 10:29:44.857
    Sep  6 10:29:44.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30" in namespace "configmap-6776" to be "Succeeded or Failed"
    Sep  6 10:29:44.875: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.851021ms
    Sep  6 10:29:47.907: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30": Phase="Running", Reason="", readiness=true. Elapsed: 3.03831366s
    Sep  6 10:29:48.879: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30": Phase="Running", Reason="", readiness=false. Elapsed: 4.011019095s
    Sep  6 10:29:50.880: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011567825s
    STEP: Saw pod success 09/06/23 10:29:50.88
    Sep  6 10:29:50.880: INFO: Pod "pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30" satisfied condition "Succeeded or Failed"
    Sep  6 10:29:50.891: INFO: Trying to get logs from node kube-3 pod pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30 container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 10:29:50.936
    Sep  6 10:29:51.013: INFO: Waiting for pod pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30 to disappear
    Sep  6 10:29:51.023: INFO: Pod pod-configmaps-0b15b4b7-1507-4622-919b-94df49356f30 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:29:51.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6776" for this suite. 09/06/23 10:29:51.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:29:51.097
Sep  6 10:29:51.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename dns 09/06/23 10:29:51.099
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:51.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:51.128
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6606.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6606.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 09/06/23 10:29:51.131
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6606.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6606.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 09/06/23 10:29:51.132
STEP: creating a pod to probe /etc/hosts 09/06/23 10:29:51.132
STEP: submitting the pod to kubernetes 09/06/23 10:29:51.132
Sep  6 10:29:51.146: INFO: Waiting up to 15m0s for pod "dns-test-d7803853-c936-4fab-8b4f-3613262c9509" in namespace "dns-6606" to be "running"
Sep  6 10:29:51.154: INFO: Pod "dns-test-d7803853-c936-4fab-8b4f-3613262c9509": Phase="Pending", Reason="", readiness=false. Elapsed: 8.421418ms
Sep  6 10:29:53.162: INFO: Pod "dns-test-d7803853-c936-4fab-8b4f-3613262c9509": Phase="Running", Reason="", readiness=true. Elapsed: 2.016496502s
Sep  6 10:29:53.163: INFO: Pod "dns-test-d7803853-c936-4fab-8b4f-3613262c9509" satisfied condition "running"
STEP: retrieving the pod 09/06/23 10:29:53.163
STEP: looking for the results for each expected name from probers 09/06/23 10:29:53.174
Sep  6 10:29:53.202: INFO: DNS probes using dns-6606/dns-test-d7803853-c936-4fab-8b4f-3613262c9509 succeeded

STEP: deleting the pod 09/06/23 10:29:53.202
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  6 10:29:53.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6606" for this suite. 09/06/23 10:29:53.244
------------------------------
• [2.163 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:29:51.097
    Sep  6 10:29:51.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename dns 09/06/23 10:29:51.099
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:51.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:51.128
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6606.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6606.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     09/06/23 10:29:51.131
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6606.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6606.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     09/06/23 10:29:51.132
    STEP: creating a pod to probe /etc/hosts 09/06/23 10:29:51.132
    STEP: submitting the pod to kubernetes 09/06/23 10:29:51.132
    Sep  6 10:29:51.146: INFO: Waiting up to 15m0s for pod "dns-test-d7803853-c936-4fab-8b4f-3613262c9509" in namespace "dns-6606" to be "running"
    Sep  6 10:29:51.154: INFO: Pod "dns-test-d7803853-c936-4fab-8b4f-3613262c9509": Phase="Pending", Reason="", readiness=false. Elapsed: 8.421418ms
    Sep  6 10:29:53.162: INFO: Pod "dns-test-d7803853-c936-4fab-8b4f-3613262c9509": Phase="Running", Reason="", readiness=true. Elapsed: 2.016496502s
    Sep  6 10:29:53.163: INFO: Pod "dns-test-d7803853-c936-4fab-8b4f-3613262c9509" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 10:29:53.163
    STEP: looking for the results for each expected name from probers 09/06/23 10:29:53.174
    Sep  6 10:29:53.202: INFO: DNS probes using dns-6606/dns-test-d7803853-c936-4fab-8b4f-3613262c9509 succeeded

    STEP: deleting the pod 09/06/23 10:29:53.202
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:29:53.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6606" for this suite. 09/06/23 10:29:53.244
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:29:53.263
Sep  6 10:29:53.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 10:29:53.264
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:53.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:53.298
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 09/06/23 10:29:53.301
STEP: submitting the pod to kubernetes 09/06/23 10:29:53.301
Sep  6 10:29:53.314: INFO: Waiting up to 5m0s for pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370" in namespace "pods-8484" to be "running and ready"
Sep  6 10:29:53.319: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370": Phase="Pending", Reason="", readiness=false. Elapsed: 5.376556ms
Sep  6 10:29:53.319: INFO: The phase of Pod pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:29:55.341: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026950879s
Sep  6 10:29:55.341: INFO: The phase of Pod pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:29:57.340: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370": Phase="Running", Reason="", readiness=true. Elapsed: 4.026280348s
Sep  6 10:29:57.340: INFO: The phase of Pod pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370 is Running (Ready = true)
Sep  6 10:29:57.340: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/06/23 10:29:57.349
STEP: updating the pod 09/06/23 10:29:57.359
Sep  6 10:29:57.907: INFO: Successfully updated pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370"
Sep  6 10:29:57.907: INFO: Waiting up to 5m0s for pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370" in namespace "pods-8484" to be "running"
Sep  6 10:29:57.911: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370": Phase="Running", Reason="", readiness=true. Elapsed: 4.812768ms
Sep  6 10:29:57.912: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 09/06/23 10:29:57.912
Sep  6 10:29:57.915: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 10:29:57.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8484" for this suite. 09/06/23 10:29:57.919
------------------------------
• [4.664 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:29:53.263
    Sep  6 10:29:53.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 10:29:53.264
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:53.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:53.298
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 09/06/23 10:29:53.301
    STEP: submitting the pod to kubernetes 09/06/23 10:29:53.301
    Sep  6 10:29:53.314: INFO: Waiting up to 5m0s for pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370" in namespace "pods-8484" to be "running and ready"
    Sep  6 10:29:53.319: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370": Phase="Pending", Reason="", readiness=false. Elapsed: 5.376556ms
    Sep  6 10:29:53.319: INFO: The phase of Pod pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:29:55.341: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026950879s
    Sep  6 10:29:55.341: INFO: The phase of Pod pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:29:57.340: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370": Phase="Running", Reason="", readiness=true. Elapsed: 4.026280348s
    Sep  6 10:29:57.340: INFO: The phase of Pod pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370 is Running (Ready = true)
    Sep  6 10:29:57.340: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/06/23 10:29:57.349
    STEP: updating the pod 09/06/23 10:29:57.359
    Sep  6 10:29:57.907: INFO: Successfully updated pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370"
    Sep  6 10:29:57.907: INFO: Waiting up to 5m0s for pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370" in namespace "pods-8484" to be "running"
    Sep  6 10:29:57.911: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370": Phase="Running", Reason="", readiness=true. Elapsed: 4.812768ms
    Sep  6 10:29:57.912: INFO: Pod "pod-update-36f5a1e6-ac9b-49f1-b01f-cad2d2ba6370" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 09/06/23 10:29:57.912
    Sep  6 10:29:57.915: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:29:57.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8484" for this suite. 09/06/23 10:29:57.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:29:57.927
Sep  6 10:29:57.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubelet-test 09/06/23 10:29:57.928
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:57.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:57.95
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:29:57.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8710" for this suite. 09/06/23 10:29:58.002
------------------------------
• [0.083 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:29:57.927
    Sep  6 10:29:57.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubelet-test 09/06/23 10:29:57.928
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:57.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:57.95
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:29:57.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8710" for this suite. 09/06/23 10:29:58.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:29:58.013
Sep  6 10:29:58.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 10:29:58.014
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:58.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:58.04
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 09/06/23 10:29:58.043
STEP: Creating a ResourceQuota 09/06/23 10:30:03.047
STEP: Ensuring resource quota status is calculated 09/06/23 10:30:03.067
STEP: Creating a ReplicationController 09/06/23 10:30:05.077
STEP: Ensuring resource quota status captures replication controller creation 09/06/23 10:30:05.091
STEP: Deleting a ReplicationController 09/06/23 10:30:07.098
STEP: Ensuring resource quota status released usage 09/06/23 10:30:07.109
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 10:30:09.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9327" for this suite. 09/06/23 10:30:09.118
------------------------------
• [SLOW TEST] [11.117 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:29:58.013
    Sep  6 10:29:58.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 10:29:58.014
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:29:58.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:29:58.04
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 09/06/23 10:29:58.043
    STEP: Creating a ResourceQuota 09/06/23 10:30:03.047
    STEP: Ensuring resource quota status is calculated 09/06/23 10:30:03.067
    STEP: Creating a ReplicationController 09/06/23 10:30:05.077
    STEP: Ensuring resource quota status captures replication controller creation 09/06/23 10:30:05.091
    STEP: Deleting a ReplicationController 09/06/23 10:30:07.098
    STEP: Ensuring resource quota status released usage 09/06/23 10:30:07.109
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:30:09.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9327" for this suite. 09/06/23 10:30:09.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:30:09.131
Sep  6 10:30:09.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-preemption 09/06/23 10:30:09.133
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:30:09.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:30:09.169
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  6 10:30:09.190: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 10:31:09.247: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 09/06/23 10:31:09.253
Sep  6 10:31:09.291: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  6 10:31:09.299: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  6 10:31:09.330: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  6 10:31:09.349: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Sep  6 10:31:09.395: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Sep  6 10:31:09.423: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/06/23 10:31:09.423
Sep  6 10:31:09.423: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5341" to be "running"
Sep  6 10:31:09.440: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.202295ms
Sep  6 10:31:12.840: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416731175s
Sep  6 10:31:13.446: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023154365s
Sep  6 10:31:15.456: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.032865821s
Sep  6 10:31:15.456: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  6 10:31:15.456: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
Sep  6 10:31:15.467: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.622825ms
Sep  6 10:31:15.467: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  6 10:31:15.467: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
Sep  6 10:31:15.479: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.021541ms
Sep  6 10:31:15.479: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  6 10:31:15.479: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
Sep  6 10:31:15.488: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.880319ms
Sep  6 10:31:15.488: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  6 10:31:15.488: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
Sep  6 10:31:15.492: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.584703ms
Sep  6 10:31:15.492: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  6 10:31:15.492: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
Sep  6 10:31:15.495: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.393605ms
Sep  6 10:31:15.495: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 09/06/23 10:31:15.495
Sep  6 10:31:15.509: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Sep  6 10:31:15.512: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642043ms
Sep  6 10:31:17.522: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013014125s
Sep  6 10:31:19.523: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013761953s
Sep  6 10:31:21.519: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009710138s
Sep  6 10:31:23.529: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.020345517s
Sep  6 10:31:23.529: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:23.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5341" for this suite. 09/06/23 10:31:23.621
------------------------------
• [SLOW TEST] [74.502 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:30:09.131
    Sep  6 10:30:09.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-preemption 09/06/23 10:30:09.133
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:30:09.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:30:09.169
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  6 10:30:09.190: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  6 10:31:09.247: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 09/06/23 10:31:09.253
    Sep  6 10:31:09.291: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  6 10:31:09.299: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  6 10:31:09.330: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  6 10:31:09.349: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Sep  6 10:31:09.395: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Sep  6 10:31:09.423: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/06/23 10:31:09.423
    Sep  6 10:31:09.423: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5341" to be "running"
    Sep  6 10:31:09.440: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.202295ms
    Sep  6 10:31:12.840: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416731175s
    Sep  6 10:31:13.446: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023154365s
    Sep  6 10:31:15.456: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.032865821s
    Sep  6 10:31:15.456: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  6 10:31:15.456: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
    Sep  6 10:31:15.467: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.622825ms
    Sep  6 10:31:15.467: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  6 10:31:15.467: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
    Sep  6 10:31:15.479: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.021541ms
    Sep  6 10:31:15.479: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  6 10:31:15.479: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
    Sep  6 10:31:15.488: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.880319ms
    Sep  6 10:31:15.488: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  6 10:31:15.488: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
    Sep  6 10:31:15.492: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.584703ms
    Sep  6 10:31:15.492: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  6 10:31:15.492: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5341" to be "running"
    Sep  6 10:31:15.495: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.393605ms
    Sep  6 10:31:15.495: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 09/06/23 10:31:15.495
    Sep  6 10:31:15.509: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Sep  6 10:31:15.512: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642043ms
    Sep  6 10:31:17.522: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013014125s
    Sep  6 10:31:19.523: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013761953s
    Sep  6 10:31:21.519: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009710138s
    Sep  6 10:31:23.529: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.020345517s
    Sep  6 10:31:23.529: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:23.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5341" for this suite. 09/06/23 10:31:23.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:23.634
Sep  6 10:31:23.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename disruption 09/06/23 10:31:23.635
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:23.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:23.654
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 09/06/23 10:31:23.656
STEP: Waiting for the pdb to be processed 09/06/23 10:31:23.662
STEP: updating the pdb 09/06/23 10:31:25.685
STEP: Waiting for the pdb to be processed 09/06/23 10:31:25.71
STEP: patching the pdb 09/06/23 10:31:27.724
STEP: Waiting for the pdb to be processed 09/06/23 10:31:27.757
STEP: Waiting for the pdb to be deleted 09/06/23 10:31:36.112
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:36.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9520" for this suite. 09/06/23 10:31:36.167
------------------------------
• [SLOW TEST] [12.565 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:23.634
    Sep  6 10:31:23.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename disruption 09/06/23 10:31:23.635
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:23.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:23.654
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 09/06/23 10:31:23.656
    STEP: Waiting for the pdb to be processed 09/06/23 10:31:23.662
    STEP: updating the pdb 09/06/23 10:31:25.685
    STEP: Waiting for the pdb to be processed 09/06/23 10:31:25.71
    STEP: patching the pdb 09/06/23 10:31:27.724
    STEP: Waiting for the pdb to be processed 09/06/23 10:31:27.757
    STEP: Waiting for the pdb to be deleted 09/06/23 10:31:36.112
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:36.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9520" for this suite. 09/06/23 10:31:36.167
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:36.2
Sep  6 10:31:36.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 10:31:36.201
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:36.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:36.264
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 09/06/23 10:31:36.275
STEP: watching for the Service to be added 09/06/23 10:31:36.31
Sep  6 10:31:36.316: INFO: Found Service test-service-nbfjf in namespace services-9832 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep  6 10:31:36.316: INFO: Service test-service-nbfjf created
STEP: Getting /status 09/06/23 10:31:36.316
Sep  6 10:31:36.329: INFO: Service test-service-nbfjf has LoadBalancer: {[]}
STEP: patching the ServiceStatus 09/06/23 10:31:36.329
STEP: watching for the Service to be patched 09/06/23 10:31:36.367
Sep  6 10:31:36.371: INFO: observed Service test-service-nbfjf in namespace services-9832 with annotations: map[] & LoadBalancer: {[]}
Sep  6 10:31:36.371: INFO: Found Service test-service-nbfjf in namespace services-9832 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep  6 10:31:36.372: INFO: Service test-service-nbfjf has service status patched
STEP: updating the ServiceStatus 09/06/23 10:31:36.372
Sep  6 10:31:36.509: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 09/06/23 10:31:36.509
Sep  6 10:31:36.513: INFO: Observed Service test-service-nbfjf in namespace services-9832 with annotations: map[] & Conditions: {[]}
Sep  6 10:31:36.514: INFO: Observed event: &Service{ObjectMeta:{test-service-nbfjf  services-9832  12eb8179-db37-4f88-ad1d-71bb089b05dd 10542 0 2023-09-06 10:31:36 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-06 10:31:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-06 10:31:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.60.78,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.60.78],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep  6 10:31:36.522: INFO: Found Service test-service-nbfjf in namespace services-9832 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  6 10:31:36.522: INFO: Service test-service-nbfjf has service status updated
STEP: patching the service 09/06/23 10:31:36.522
STEP: watching for the Service to be patched 09/06/23 10:31:36.557
Sep  6 10:31:36.559: INFO: observed Service test-service-nbfjf in namespace services-9832 with labels: map[test-service-static:true]
Sep  6 10:31:36.559: INFO: observed Service test-service-nbfjf in namespace services-9832 with labels: map[test-service-static:true]
Sep  6 10:31:36.559: INFO: observed Service test-service-nbfjf in namespace services-9832 with labels: map[test-service-static:true]
Sep  6 10:31:36.559: INFO: Found Service test-service-nbfjf in namespace services-9832 with labels: map[test-service:patched test-service-static:true]
Sep  6 10:31:36.559: INFO: Service test-service-nbfjf patched
STEP: deleting the service 09/06/23 10:31:36.559
STEP: watching for the Service to be deleted 09/06/23 10:31:36.688
Sep  6 10:31:36.690: INFO: Observed event: ADDED
Sep  6 10:31:36.690: INFO: Observed event: MODIFIED
Sep  6 10:31:36.690: INFO: Observed event: MODIFIED
Sep  6 10:31:36.690: INFO: Observed event: MODIFIED
Sep  6 10:31:36.690: INFO: Found Service test-service-nbfjf in namespace services-9832 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep  6 10:31:36.690: INFO: Service test-service-nbfjf deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:36.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9832" for this suite. 09/06/23 10:31:36.748
------------------------------
• [0.573 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:36.2
    Sep  6 10:31:36.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 10:31:36.201
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:36.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:36.264
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 09/06/23 10:31:36.275
    STEP: watching for the Service to be added 09/06/23 10:31:36.31
    Sep  6 10:31:36.316: INFO: Found Service test-service-nbfjf in namespace services-9832 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Sep  6 10:31:36.316: INFO: Service test-service-nbfjf created
    STEP: Getting /status 09/06/23 10:31:36.316
    Sep  6 10:31:36.329: INFO: Service test-service-nbfjf has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 09/06/23 10:31:36.329
    STEP: watching for the Service to be patched 09/06/23 10:31:36.367
    Sep  6 10:31:36.371: INFO: observed Service test-service-nbfjf in namespace services-9832 with annotations: map[] & LoadBalancer: {[]}
    Sep  6 10:31:36.371: INFO: Found Service test-service-nbfjf in namespace services-9832 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Sep  6 10:31:36.372: INFO: Service test-service-nbfjf has service status patched
    STEP: updating the ServiceStatus 09/06/23 10:31:36.372
    Sep  6 10:31:36.509: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 09/06/23 10:31:36.509
    Sep  6 10:31:36.513: INFO: Observed Service test-service-nbfjf in namespace services-9832 with annotations: map[] & Conditions: {[]}
    Sep  6 10:31:36.514: INFO: Observed event: &Service{ObjectMeta:{test-service-nbfjf  services-9832  12eb8179-db37-4f88-ad1d-71bb089b05dd 10542 0 2023-09-06 10:31:36 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-06 10:31:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-06 10:31:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.60.78,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.60.78],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Sep  6 10:31:36.522: INFO: Found Service test-service-nbfjf in namespace services-9832 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  6 10:31:36.522: INFO: Service test-service-nbfjf has service status updated
    STEP: patching the service 09/06/23 10:31:36.522
    STEP: watching for the Service to be patched 09/06/23 10:31:36.557
    Sep  6 10:31:36.559: INFO: observed Service test-service-nbfjf in namespace services-9832 with labels: map[test-service-static:true]
    Sep  6 10:31:36.559: INFO: observed Service test-service-nbfjf in namespace services-9832 with labels: map[test-service-static:true]
    Sep  6 10:31:36.559: INFO: observed Service test-service-nbfjf in namespace services-9832 with labels: map[test-service-static:true]
    Sep  6 10:31:36.559: INFO: Found Service test-service-nbfjf in namespace services-9832 with labels: map[test-service:patched test-service-static:true]
    Sep  6 10:31:36.559: INFO: Service test-service-nbfjf patched
    STEP: deleting the service 09/06/23 10:31:36.559
    STEP: watching for the Service to be deleted 09/06/23 10:31:36.688
    Sep  6 10:31:36.690: INFO: Observed event: ADDED
    Sep  6 10:31:36.690: INFO: Observed event: MODIFIED
    Sep  6 10:31:36.690: INFO: Observed event: MODIFIED
    Sep  6 10:31:36.690: INFO: Observed event: MODIFIED
    Sep  6 10:31:36.690: INFO: Found Service test-service-nbfjf in namespace services-9832 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Sep  6 10:31:36.690: INFO: Service test-service-nbfjf deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:36.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9832" for this suite. 09/06/23 10:31:36.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:36.774
Sep  6 10:31:36.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:31:36.775
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:36.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:36.916
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-db3d296b-6284-47e1-94b3-3f1848183a1e 09/06/23 10:31:37.092
STEP: Creating configMap with name cm-test-opt-upd-c9cef18c-f42a-4b4a-ad99-224479920db7 09/06/23 10:31:37.249
STEP: Creating the pod 09/06/23 10:31:37.467
Sep  6 10:31:37.561: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e" in namespace "projected-1554" to be "running and ready"
Sep  6 10:31:37.731: INFO: Pod "pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 170.322283ms
Sep  6 10:31:37.731: INFO: The phase of Pod pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:31:39.746: INFO: Pod "pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.18489491s
Sep  6 10:31:39.746: INFO: The phase of Pod pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e is Running (Ready = true)
Sep  6 10:31:39.746: INFO: Pod "pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-db3d296b-6284-47e1-94b3-3f1848183a1e 09/06/23 10:31:39.812
STEP: Updating configmap cm-test-opt-upd-c9cef18c-f42a-4b4a-ad99-224479920db7 09/06/23 10:31:39.818
STEP: Creating configMap with name cm-test-opt-create-8e999130-5df0-4d9e-a996-3de156a4dde2 09/06/23 10:31:39.823
STEP: waiting to observe update in volume 09/06/23 10:31:39.827
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:41.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1554" for this suite. 09/06/23 10:31:41.912
------------------------------
• [SLOW TEST] [5.150 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:36.774
    Sep  6 10:31:36.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:31:36.775
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:36.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:36.916
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-db3d296b-6284-47e1-94b3-3f1848183a1e 09/06/23 10:31:37.092
    STEP: Creating configMap with name cm-test-opt-upd-c9cef18c-f42a-4b4a-ad99-224479920db7 09/06/23 10:31:37.249
    STEP: Creating the pod 09/06/23 10:31:37.467
    Sep  6 10:31:37.561: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e" in namespace "projected-1554" to be "running and ready"
    Sep  6 10:31:37.731: INFO: Pod "pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 170.322283ms
    Sep  6 10:31:37.731: INFO: The phase of Pod pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:31:39.746: INFO: Pod "pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.18489491s
    Sep  6 10:31:39.746: INFO: The phase of Pod pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e is Running (Ready = true)
    Sep  6 10:31:39.746: INFO: Pod "pod-projected-configmaps-df114a87-b95d-424b-a92c-0b08b2d87d4e" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-db3d296b-6284-47e1-94b3-3f1848183a1e 09/06/23 10:31:39.812
    STEP: Updating configmap cm-test-opt-upd-c9cef18c-f42a-4b4a-ad99-224479920db7 09/06/23 10:31:39.818
    STEP: Creating configMap with name cm-test-opt-create-8e999130-5df0-4d9e-a996-3de156a4dde2 09/06/23 10:31:39.823
    STEP: waiting to observe update in volume 09/06/23 10:31:39.827
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:41.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1554" for this suite. 09/06/23 10:31:41.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:41.925
Sep  6 10:31:41.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename csistoragecapacity 09/06/23 10:31:41.926
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:41.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:41.944
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 09/06/23 10:31:41.946
STEP: getting /apis/storage.k8s.io 09/06/23 10:31:41.949
STEP: getting /apis/storage.k8s.io/v1 09/06/23 10:31:41.95
STEP: creating 09/06/23 10:31:41.951
STEP: watching 09/06/23 10:31:41.968
Sep  6 10:31:41.969: INFO: starting watch
STEP: getting 09/06/23 10:31:41.975
STEP: listing in namespace 09/06/23 10:31:41.979
STEP: listing across namespaces 09/06/23 10:31:41.982
STEP: patching 09/06/23 10:31:41.986
STEP: updating 09/06/23 10:31:41.992
Sep  6 10:31:41.999: INFO: waiting for watch events with expected annotations in namespace
Sep  6 10:31:41.999: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 09/06/23 10:31:41.999
STEP: deleting a collection 09/06/23 10:31:42.016
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:42.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-8697" for this suite. 09/06/23 10:31:42.039
------------------------------
• [0.121 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:41.925
    Sep  6 10:31:41.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename csistoragecapacity 09/06/23 10:31:41.926
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:41.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:41.944
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 09/06/23 10:31:41.946
    STEP: getting /apis/storage.k8s.io 09/06/23 10:31:41.949
    STEP: getting /apis/storage.k8s.io/v1 09/06/23 10:31:41.95
    STEP: creating 09/06/23 10:31:41.951
    STEP: watching 09/06/23 10:31:41.968
    Sep  6 10:31:41.969: INFO: starting watch
    STEP: getting 09/06/23 10:31:41.975
    STEP: listing in namespace 09/06/23 10:31:41.979
    STEP: listing across namespaces 09/06/23 10:31:41.982
    STEP: patching 09/06/23 10:31:41.986
    STEP: updating 09/06/23 10:31:41.992
    Sep  6 10:31:41.999: INFO: waiting for watch events with expected annotations in namespace
    Sep  6 10:31:41.999: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 09/06/23 10:31:41.999
    STEP: deleting a collection 09/06/23 10:31:42.016
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:42.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-8697" for this suite. 09/06/23 10:31:42.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:42.046
Sep  6 10:31:42.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename events 09/06/23 10:31:42.047
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:42.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:42.073
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 09/06/23 10:31:42.076
STEP: get a list of Events with a label in the current namespace 09/06/23 10:31:42.093
STEP: delete a list of events 09/06/23 10:31:42.096
Sep  6 10:31:42.097: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/06/23 10:31:42.124
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:42.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3626" for this suite. 09/06/23 10:31:42.134
------------------------------
• [0.100 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:42.046
    Sep  6 10:31:42.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename events 09/06/23 10:31:42.047
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:42.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:42.073
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 09/06/23 10:31:42.076
    STEP: get a list of Events with a label in the current namespace 09/06/23 10:31:42.093
    STEP: delete a list of events 09/06/23 10:31:42.096
    Sep  6 10:31:42.097: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/06/23 10:31:42.124
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:42.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3626" for this suite. 09/06/23 10:31:42.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:42.148
Sep  6 10:31:42.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:31:42.149
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:42.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:42.179
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 09/06/23 10:31:42.181
Sep  6 10:31:42.203: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808" in namespace "downward-api-7716" to be "Succeeded or Failed"
Sep  6 10:31:42.208: INFO: Pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808": Phase="Pending", Reason="", readiness=false. Elapsed: 5.75831ms
Sep  6 10:31:44.221: INFO: Pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018246493s
Sep  6 10:31:46.214: INFO: Pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011558519s
STEP: Saw pod success 09/06/23 10:31:46.214
Sep  6 10:31:46.214: INFO: Pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808" satisfied condition "Succeeded or Failed"
Sep  6 10:31:46.219: INFO: Trying to get logs from node kube-2 pod downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808 container client-container: <nil>
STEP: delete the pod 09/06/23 10:31:46.235
Sep  6 10:31:46.254: INFO: Waiting for pod downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808 to disappear
Sep  6 10:31:46.260: INFO: Pod downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:46.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7716" for this suite. 09/06/23 10:31:46.264
------------------------------
• [4.124 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:42.148
    Sep  6 10:31:42.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:31:42.149
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:42.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:42.179
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 09/06/23 10:31:42.181
    Sep  6 10:31:42.203: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808" in namespace "downward-api-7716" to be "Succeeded or Failed"
    Sep  6 10:31:42.208: INFO: Pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808": Phase="Pending", Reason="", readiness=false. Elapsed: 5.75831ms
    Sep  6 10:31:44.221: INFO: Pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018246493s
    Sep  6 10:31:46.214: INFO: Pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011558519s
    STEP: Saw pod success 09/06/23 10:31:46.214
    Sep  6 10:31:46.214: INFO: Pod "downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808" satisfied condition "Succeeded or Failed"
    Sep  6 10:31:46.219: INFO: Trying to get logs from node kube-2 pod downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808 container client-container: <nil>
    STEP: delete the pod 09/06/23 10:31:46.235
    Sep  6 10:31:46.254: INFO: Waiting for pod downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808 to disappear
    Sep  6 10:31:46.260: INFO: Pod downwardapi-volume-5190f7cb-31ed-47fa-a9cf-61681054d808 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:46.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7716" for this suite. 09/06/23 10:31:46.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:46.277
Sep  6 10:31:46.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:31:46.277
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:46.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:46.302
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 09/06/23 10:31:46.305
Sep  6 10:31:46.316: INFO: Waiting up to 5m0s for pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776" in namespace "downward-api-4097" to be "Succeeded or Failed"
Sep  6 10:31:46.324: INFO: Pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108294ms
Sep  6 10:31:48.593: INFO: Pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277098812s
Sep  6 10:31:50.329: INFO: Pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012661666s
STEP: Saw pod success 09/06/23 10:31:50.329
Sep  6 10:31:50.329: INFO: Pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776" satisfied condition "Succeeded or Failed"
Sep  6 10:31:50.332: INFO: Trying to get logs from node kube-3 pod downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776 container dapi-container: <nil>
STEP: delete the pod 09/06/23 10:31:50.338
Sep  6 10:31:50.363: INFO: Waiting for pod downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776 to disappear
Sep  6 10:31:50.365: INFO: Pod downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:50.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4097" for this suite. 09/06/23 10:31:50.368
------------------------------
• [4.099 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:46.277
    Sep  6 10:31:46.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:31:46.277
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:46.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:46.302
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 09/06/23 10:31:46.305
    Sep  6 10:31:46.316: INFO: Waiting up to 5m0s for pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776" in namespace "downward-api-4097" to be "Succeeded or Failed"
    Sep  6 10:31:46.324: INFO: Pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108294ms
    Sep  6 10:31:48.593: INFO: Pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277098812s
    Sep  6 10:31:50.329: INFO: Pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012661666s
    STEP: Saw pod success 09/06/23 10:31:50.329
    Sep  6 10:31:50.329: INFO: Pod "downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776" satisfied condition "Succeeded or Failed"
    Sep  6 10:31:50.332: INFO: Trying to get logs from node kube-3 pod downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776 container dapi-container: <nil>
    STEP: delete the pod 09/06/23 10:31:50.338
    Sep  6 10:31:50.363: INFO: Waiting for pod downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776 to disappear
    Sep  6 10:31:50.365: INFO: Pod downward-api-45f0eaa8-6e08-4647-ae04-f87ce8b3d776 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:50.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4097" for this suite. 09/06/23 10:31:50.368
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:50.377
Sep  6 10:31:50.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:31:50.378
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:50.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:50.399
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/06/23 10:31:50.401
Sep  6 10:31:50.414: INFO: Waiting up to 5m0s for pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745" in namespace "emptydir-5283" to be "Succeeded or Failed"
Sep  6 10:31:50.420: INFO: Pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745": Phase="Pending", Reason="", readiness=false. Elapsed: 5.847309ms
Sep  6 10:31:52.430: INFO: Pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745": Phase="Running", Reason="", readiness=false. Elapsed: 2.015360143s
Sep  6 10:31:54.424: INFO: Pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009991995s
STEP: Saw pod success 09/06/23 10:31:54.424
Sep  6 10:31:54.424: INFO: Pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745" satisfied condition "Succeeded or Failed"
Sep  6 10:31:54.429: INFO: Trying to get logs from node kube-3 pod pod-63dbe5cf-750b-47d5-af17-d8997a0bb745 container test-container: <nil>
STEP: delete the pod 09/06/23 10:31:54.438
Sep  6 10:31:54.463: INFO: Waiting for pod pod-63dbe5cf-750b-47d5-af17-d8997a0bb745 to disappear
Sep  6 10:31:54.466: INFO: Pod pod-63dbe5cf-750b-47d5-af17-d8997a0bb745 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:54.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5283" for this suite. 09/06/23 10:31:54.47
------------------------------
• [4.098 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:50.377
    Sep  6 10:31:50.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:31:50.378
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:50.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:50.399
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/06/23 10:31:50.401
    Sep  6 10:31:50.414: INFO: Waiting up to 5m0s for pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745" in namespace "emptydir-5283" to be "Succeeded or Failed"
    Sep  6 10:31:50.420: INFO: Pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745": Phase="Pending", Reason="", readiness=false. Elapsed: 5.847309ms
    Sep  6 10:31:52.430: INFO: Pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745": Phase="Running", Reason="", readiness=false. Elapsed: 2.015360143s
    Sep  6 10:31:54.424: INFO: Pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009991995s
    STEP: Saw pod success 09/06/23 10:31:54.424
    Sep  6 10:31:54.424: INFO: Pod "pod-63dbe5cf-750b-47d5-af17-d8997a0bb745" satisfied condition "Succeeded or Failed"
    Sep  6 10:31:54.429: INFO: Trying to get logs from node kube-3 pod pod-63dbe5cf-750b-47d5-af17-d8997a0bb745 container test-container: <nil>
    STEP: delete the pod 09/06/23 10:31:54.438
    Sep  6 10:31:54.463: INFO: Waiting for pod pod-63dbe5cf-750b-47d5-af17-d8997a0bb745 to disappear
    Sep  6 10:31:54.466: INFO: Pod pod-63dbe5cf-750b-47d5-af17-d8997a0bb745 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:54.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5283" for this suite. 09/06/23 10:31:54.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:54.477
Sep  6 10:31:54.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 10:31:54.477
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:54.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:54.501
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 09/06/23 10:31:54.503
STEP: Getting a ResourceQuota 09/06/23 10:31:54.509
STEP: Listing all ResourceQuotas with LabelSelector 09/06/23 10:31:54.515
STEP: Patching the ResourceQuota 09/06/23 10:31:54.519
STEP: Deleting a Collection of ResourceQuotas 09/06/23 10:31:54.528
STEP: Verifying the deleted ResourceQuota 09/06/23 10:31:54.539
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:54.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3700" for this suite. 09/06/23 10:31:54.545
------------------------------
• [0.075 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:54.477
    Sep  6 10:31:54.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 10:31:54.477
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:54.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:54.501
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 09/06/23 10:31:54.503
    STEP: Getting a ResourceQuota 09/06/23 10:31:54.509
    STEP: Listing all ResourceQuotas with LabelSelector 09/06/23 10:31:54.515
    STEP: Patching the ResourceQuota 09/06/23 10:31:54.519
    STEP: Deleting a Collection of ResourceQuotas 09/06/23 10:31:54.528
    STEP: Verifying the deleted ResourceQuota 09/06/23 10:31:54.539
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:54.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3700" for this suite. 09/06/23 10:31:54.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:54.553
Sep  6 10:31:54.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:31:54.554
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:54.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:54.572
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-a2f26fb8-21a8-4328-9bed-5edaf95d6f78 09/06/23 10:31:54.574
STEP: Creating a pod to test consume secrets 09/06/23 10:31:54.578
Sep  6 10:31:54.596: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a" in namespace "projected-8043" to be "Succeeded or Failed"
Sep  6 10:31:54.606: INFO: Pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.274908ms
Sep  6 10:31:56.615: INFO: Pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018416244s
Sep  6 10:31:58.611: INFO: Pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015100436s
STEP: Saw pod success 09/06/23 10:31:58.611
Sep  6 10:31:58.612: INFO: Pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a" satisfied condition "Succeeded or Failed"
Sep  6 10:31:58.614: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a container projected-secret-volume-test: <nil>
STEP: delete the pod 09/06/23 10:31:58.619
Sep  6 10:31:58.637: INFO: Waiting for pod pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a to disappear
Sep  6 10:31:58.640: INFO: Pod pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  6 10:31:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8043" for this suite. 09/06/23 10:31:58.643
------------------------------
• [4.097 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:54.553
    Sep  6 10:31:54.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:31:54.554
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:54.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:54.572
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-a2f26fb8-21a8-4328-9bed-5edaf95d6f78 09/06/23 10:31:54.574
    STEP: Creating a pod to test consume secrets 09/06/23 10:31:54.578
    Sep  6 10:31:54.596: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a" in namespace "projected-8043" to be "Succeeded or Failed"
    Sep  6 10:31:54.606: INFO: Pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.274908ms
    Sep  6 10:31:56.615: INFO: Pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018416244s
    Sep  6 10:31:58.611: INFO: Pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015100436s
    STEP: Saw pod success 09/06/23 10:31:58.611
    Sep  6 10:31:58.612: INFO: Pod "pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a" satisfied condition "Succeeded or Failed"
    Sep  6 10:31:58.614: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:31:58.619
    Sep  6 10:31:58.637: INFO: Waiting for pod pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a to disappear
    Sep  6 10:31:58.640: INFO: Pod pod-projected-secrets-069cce87-d0aa-4b40-8cf7-02ab791c759a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:31:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8043" for this suite. 09/06/23 10:31:58.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:31:58.652
Sep  6 10:31:58.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename security-context-test 09/06/23 10:31:58.653
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:58.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:58.678
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Sep  6 10:31:58.687: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489" in namespace "security-context-test-6978" to be "Succeeded or Failed"
Sep  6 10:31:58.699: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": Phase="Pending", Reason="", readiness=false. Elapsed: 11.536308ms
Sep  6 10:32:00.711: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023606335s
Sep  6 10:32:02.990: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": Phase="Pending", Reason="", readiness=false. Elapsed: 4.302634703s
Sep  6 10:32:04.716: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028603704s
Sep  6 10:32:04.716: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489" satisfied condition "Succeeded or Failed"
Sep  6 10:32:04.736: INFO: Got logs for pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:04.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6978" for this suite. 09/06/23 10:32:04.744
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:31:58.652
    Sep  6 10:31:58.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename security-context-test 09/06/23 10:31:58.653
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:31:58.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:31:58.678
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Sep  6 10:31:58.687: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489" in namespace "security-context-test-6978" to be "Succeeded or Failed"
    Sep  6 10:31:58.699: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": Phase="Pending", Reason="", readiness=false. Elapsed: 11.536308ms
    Sep  6 10:32:00.711: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023606335s
    Sep  6 10:32:02.990: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": Phase="Pending", Reason="", readiness=false. Elapsed: 4.302634703s
    Sep  6 10:32:04.716: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028603704s
    Sep  6 10:32:04.716: INFO: Pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489" satisfied condition "Succeeded or Failed"
    Sep  6 10:32:04.736: INFO: Got logs for pod "busybox-privileged-false-b94bd35b-d7eb-4003-be0e-dd18e31ef489": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:04.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6978" for this suite. 09/06/23 10:32:04.744
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:04.755
Sep  6 10:32:04.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename statefulset 09/06/23 10:32:04.758
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:04.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:04.779
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1855 09/06/23 10:32:04.781
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Sep  6 10:32:04.800: INFO: Found 0 stateful pods, waiting for 1
Sep  6 10:32:14.816: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 09/06/23 10:32:14.84
W0906 10:32:14.874349      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  6 10:32:14.948: INFO: Found 1 stateful pods, waiting for 2
Sep  6 10:32:24.964: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:32:24.964: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 09/06/23 10:32:24.978
STEP: Delete all of the StatefulSets 09/06/23 10:32:24.984
STEP: Verify that StatefulSets have been deleted 09/06/23 10:32:24.997
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  6 10:32:25.001: INFO: Deleting all statefulset in ns statefulset-1855
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:25.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1855" for this suite. 09/06/23 10:32:25.038
------------------------------
• [SLOW TEST] [20.308 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:04.755
    Sep  6 10:32:04.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename statefulset 09/06/23 10:32:04.758
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:04.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:04.779
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1855 09/06/23 10:32:04.781
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Sep  6 10:32:04.800: INFO: Found 0 stateful pods, waiting for 1
    Sep  6 10:32:14.816: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 09/06/23 10:32:14.84
    W0906 10:32:14.874349      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  6 10:32:14.948: INFO: Found 1 stateful pods, waiting for 2
    Sep  6 10:32:24.964: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 10:32:24.964: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 09/06/23 10:32:24.978
    STEP: Delete all of the StatefulSets 09/06/23 10:32:24.984
    STEP: Verify that StatefulSets have been deleted 09/06/23 10:32:24.997
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  6 10:32:25.001: INFO: Deleting all statefulset in ns statefulset-1855
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:25.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1855" for this suite. 09/06/23 10:32:25.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:25.066
Sep  6 10:32:25.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 10:32:25.068
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:25.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:25.13
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-2af3f4b9-870d-4518-b1f5-e6b7695f9d02 09/06/23 10:32:25.134
STEP: Creating a pod to test consume configMaps 09/06/23 10:32:25.145
Sep  6 10:32:25.162: INFO: Waiting up to 5m0s for pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef" in namespace "configmap-6372" to be "Succeeded or Failed"
Sep  6 10:32:25.168: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.829934ms
Sep  6 10:32:27.186: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023508012s
Sep  6 10:32:29.258: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095479453s
Sep  6 10:32:31.172: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00952468s
STEP: Saw pod success 09/06/23 10:32:31.172
Sep  6 10:32:31.172: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef" satisfied condition "Succeeded or Failed"
Sep  6 10:32:31.175: INFO: Trying to get logs from node kube-3 pod pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef container agnhost-container: <nil>
STEP: delete the pod 09/06/23 10:32:31.181
Sep  6 10:32:31.201: INFO: Waiting for pod pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef to disappear
Sep  6 10:32:31.205: INFO: Pod pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6372" for this suite. 09/06/23 10:32:31.209
------------------------------
• [SLOW TEST] [6.150 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:25.066
    Sep  6 10:32:25.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 10:32:25.068
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:25.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:25.13
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-2af3f4b9-870d-4518-b1f5-e6b7695f9d02 09/06/23 10:32:25.134
    STEP: Creating a pod to test consume configMaps 09/06/23 10:32:25.145
    Sep  6 10:32:25.162: INFO: Waiting up to 5m0s for pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef" in namespace "configmap-6372" to be "Succeeded or Failed"
    Sep  6 10:32:25.168: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.829934ms
    Sep  6 10:32:27.186: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023508012s
    Sep  6 10:32:29.258: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095479453s
    Sep  6 10:32:31.172: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00952468s
    STEP: Saw pod success 09/06/23 10:32:31.172
    Sep  6 10:32:31.172: INFO: Pod "pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef" satisfied condition "Succeeded or Failed"
    Sep  6 10:32:31.175: INFO: Trying to get logs from node kube-3 pod pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 10:32:31.181
    Sep  6 10:32:31.201: INFO: Waiting for pod pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef to disappear
    Sep  6 10:32:31.205: INFO: Pod pod-configmaps-995f9023-f157-4805-9214-2b83628a45ef no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6372" for this suite. 09/06/23 10:32:31.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:31.217
Sep  6 10:32:31.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:32:31.217
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:31.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:31.237
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 09/06/23 10:32:31.239
Sep  6 10:32:31.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb" in namespace "downward-api-7978" to be "Succeeded or Failed"
Sep  6 10:32:31.256: INFO: Pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.464767ms
Sep  6 10:32:33.268: INFO: Pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019809035s
Sep  6 10:32:35.271: INFO: Pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022806734s
STEP: Saw pod success 09/06/23 10:32:35.271
Sep  6 10:32:35.271: INFO: Pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb" satisfied condition "Succeeded or Failed"
Sep  6 10:32:35.280: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb container client-container: <nil>
STEP: delete the pod 09/06/23 10:32:35.298
Sep  6 10:32:35.324: INFO: Waiting for pod downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb to disappear
Sep  6 10:32:35.327: INFO: Pod downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:35.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7978" for this suite. 09/06/23 10:32:35.331
------------------------------
• [4.120 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:31.217
    Sep  6 10:32:31.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:32:31.217
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:31.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:31.237
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 09/06/23 10:32:31.239
    Sep  6 10:32:31.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb" in namespace "downward-api-7978" to be "Succeeded or Failed"
    Sep  6 10:32:31.256: INFO: Pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.464767ms
    Sep  6 10:32:33.268: INFO: Pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019809035s
    Sep  6 10:32:35.271: INFO: Pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022806734s
    STEP: Saw pod success 09/06/23 10:32:35.271
    Sep  6 10:32:35.271: INFO: Pod "downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb" satisfied condition "Succeeded or Failed"
    Sep  6 10:32:35.280: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb container client-container: <nil>
    STEP: delete the pod 09/06/23 10:32:35.298
    Sep  6 10:32:35.324: INFO: Waiting for pod downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb to disappear
    Sep  6 10:32:35.327: INFO: Pod downwardapi-volume-cef56cb7-401f-4209-b3f4-432fb73709eb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:35.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7978" for this suite. 09/06/23 10:32:35.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:35.341
Sep  6 10:32:35.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:32:35.342
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:35.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:35.356
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:32:35.379
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:32:35.611
STEP: Deploying the webhook pod 09/06/23 10:32:35.619
STEP: Wait for the deployment to be ready 09/06/23 10:32:35.634
Sep  6 10:32:35.646: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 10:32:37.656
STEP: Verifying the service has paired with the endpoint 09/06/23 10:32:37.674
Sep  6 10:32:38.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 09/06/23 10:32:38.773
STEP: Creating a configMap that should be mutated 09/06/23 10:32:38.784
STEP: Deleting the collection of validation webhooks 09/06/23 10:32:38.815
STEP: Creating a configMap that should not be mutated 09/06/23 10:32:38.877
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:38.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9865" for this suite. 09/06/23 10:32:38.96
STEP: Destroying namespace "webhook-9865-markers" for this suite. 09/06/23 10:32:38.97
------------------------------
• [3.655 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:35.341
    Sep  6 10:32:35.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:32:35.342
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:35.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:35.356
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:32:35.379
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:32:35.611
    STEP: Deploying the webhook pod 09/06/23 10:32:35.619
    STEP: Wait for the deployment to be ready 09/06/23 10:32:35.634
    Sep  6 10:32:35.646: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 10:32:37.656
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:32:37.674
    Sep  6 10:32:38.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 09/06/23 10:32:38.773
    STEP: Creating a configMap that should be mutated 09/06/23 10:32:38.784
    STEP: Deleting the collection of validation webhooks 09/06/23 10:32:38.815
    STEP: Creating a configMap that should not be mutated 09/06/23 10:32:38.877
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:38.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9865" for this suite. 09/06/23 10:32:38.96
    STEP: Destroying namespace "webhook-9865-markers" for this suite. 09/06/23 10:32:38.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:39.007
Sep  6 10:32:39.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:32:39.008
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:39.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:39.04
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 09/06/23 10:32:39.047
Sep  6 10:32:39.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  6 10:32:39.129: INFO: stderr: ""
Sep  6 10:32:39.129: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 09/06/23 10:32:39.129
Sep  6 10:32:39.129: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  6 10:32:39.129: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-601" to be "running and ready, or succeeded"
Sep  6 10:32:39.135: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.18254ms
Sep  6 10:32:39.135: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'kube-3' to be 'Running' but was 'Pending'
Sep  6 10:32:41.147: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.017488699s
Sep  6 10:32:41.147: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  6 10:32:41.147: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 09/06/23 10:32:41.147
Sep  6 10:32:41.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator'
Sep  6 10:32:41.335: INFO: stderr: ""
Sep  6 10:32:41.335: INFO: stdout: "I0906 10:32:39.873263       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/zvd 330\nI0906 10:32:40.073419       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/cmn 476\nI0906 10:32:40.274293       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/w9q 371\nI0906 10:32:40.473664       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/spc 486\nI0906 10:32:40.674008       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/5s6 242\nI0906 10:32:40.873338       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/tr5 424\nI0906 10:32:41.073897       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/84n 505\nI0906 10:32:41.273447       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z7p 587\n"
STEP: limiting log lines 09/06/23 10:32:41.335
Sep  6 10:32:41.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --tail=1'
Sep  6 10:32:41.445: INFO: stderr: ""
Sep  6 10:32:41.445: INFO: stdout: "I0906 10:32:41.273447       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z7p 587\n"
Sep  6 10:32:41.445: INFO: got output "I0906 10:32:41.273447       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z7p 587\n"
STEP: limiting log bytes 09/06/23 10:32:41.445
Sep  6 10:32:41.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --limit-bytes=1'
Sep  6 10:32:41.515: INFO: stderr: ""
Sep  6 10:32:41.515: INFO: stdout: "I"
Sep  6 10:32:41.515: INFO: got output "I"
STEP: exposing timestamps 09/06/23 10:32:41.515
Sep  6 10:32:41.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  6 10:32:41.575: INFO: stderr: ""
Sep  6 10:32:41.575: INFO: stdout: "2023-09-06T10:32:41.473600720Z I0906 10:32:41.473396       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dsl 326\n"
Sep  6 10:32:41.575: INFO: got output "2023-09-06T10:32:41.473600720Z I0906 10:32:41.473396       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dsl 326\n"
STEP: restricting to a time range 09/06/23 10:32:41.575
Sep  6 10:32:44.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --since=1s'
Sep  6 10:32:44.159: INFO: stderr: ""
Sep  6 10:32:44.159: INFO: stdout: "I0906 10:32:43.274208       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/844 467\nI0906 10:32:43.473663       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/9cq 491\nI0906 10:32:43.674058       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/z7x 385\nI0906 10:32:43.873328       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/vwj 203\nI0906 10:32:44.073623       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mkcg 363\n"
Sep  6 10:32:44.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --since=24h'
Sep  6 10:32:44.239: INFO: stderr: ""
Sep  6 10:32:44.239: INFO: stdout: "I0906 10:32:39.873263       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/zvd 330\nI0906 10:32:40.073419       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/cmn 476\nI0906 10:32:40.274293       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/w9q 371\nI0906 10:32:40.473664       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/spc 486\nI0906 10:32:40.674008       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/5s6 242\nI0906 10:32:40.873338       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/tr5 424\nI0906 10:32:41.073897       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/84n 505\nI0906 10:32:41.273447       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z7p 587\nI0906 10:32:41.473396       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dsl 326\nI0906 10:32:41.673941       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/9r24 396\nI0906 10:32:41.873937       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/zbzl 574\nI0906 10:32:42.074115       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/xsdw 208\nI0906 10:32:42.273666       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/rq2 370\nI0906 10:32:42.475172       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/zv7t 565\nI0906 10:32:42.673380       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/5hf9 272\nI0906 10:32:42.874096       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/7s5p 275\nI0906 10:32:43.073549       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/tpwf 522\nI0906 10:32:43.274208       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/844 467\nI0906 10:32:43.473663       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/9cq 491\nI0906 10:32:43.674058       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/z7x 385\nI0906 10:32:43.873328       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/vwj 203\nI0906 10:32:44.073623       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mkcg 363\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Sep  6 10:32:44.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 delete pod logs-generator'
Sep  6 10:32:45.585: INFO: stderr: ""
Sep  6 10:32:45.585: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:45.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-601" for this suite. 09/06/23 10:32:45.591
------------------------------
• [SLOW TEST] [6.760 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:39.007
    Sep  6 10:32:39.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:32:39.008
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:39.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:39.04
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 09/06/23 10:32:39.047
    Sep  6 10:32:39.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Sep  6 10:32:39.129: INFO: stderr: ""
    Sep  6 10:32:39.129: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 09/06/23 10:32:39.129
    Sep  6 10:32:39.129: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Sep  6 10:32:39.129: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-601" to be "running and ready, or succeeded"
    Sep  6 10:32:39.135: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.18254ms
    Sep  6 10:32:39.135: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'kube-3' to be 'Running' but was 'Pending'
    Sep  6 10:32:41.147: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.017488699s
    Sep  6 10:32:41.147: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Sep  6 10:32:41.147: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 09/06/23 10:32:41.147
    Sep  6 10:32:41.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator'
    Sep  6 10:32:41.335: INFO: stderr: ""
    Sep  6 10:32:41.335: INFO: stdout: "I0906 10:32:39.873263       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/zvd 330\nI0906 10:32:40.073419       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/cmn 476\nI0906 10:32:40.274293       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/w9q 371\nI0906 10:32:40.473664       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/spc 486\nI0906 10:32:40.674008       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/5s6 242\nI0906 10:32:40.873338       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/tr5 424\nI0906 10:32:41.073897       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/84n 505\nI0906 10:32:41.273447       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z7p 587\n"
    STEP: limiting log lines 09/06/23 10:32:41.335
    Sep  6 10:32:41.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --tail=1'
    Sep  6 10:32:41.445: INFO: stderr: ""
    Sep  6 10:32:41.445: INFO: stdout: "I0906 10:32:41.273447       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z7p 587\n"
    Sep  6 10:32:41.445: INFO: got output "I0906 10:32:41.273447       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z7p 587\n"
    STEP: limiting log bytes 09/06/23 10:32:41.445
    Sep  6 10:32:41.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --limit-bytes=1'
    Sep  6 10:32:41.515: INFO: stderr: ""
    Sep  6 10:32:41.515: INFO: stdout: "I"
    Sep  6 10:32:41.515: INFO: got output "I"
    STEP: exposing timestamps 09/06/23 10:32:41.515
    Sep  6 10:32:41.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --tail=1 --timestamps'
    Sep  6 10:32:41.575: INFO: stderr: ""
    Sep  6 10:32:41.575: INFO: stdout: "2023-09-06T10:32:41.473600720Z I0906 10:32:41.473396       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dsl 326\n"
    Sep  6 10:32:41.575: INFO: got output "2023-09-06T10:32:41.473600720Z I0906 10:32:41.473396       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dsl 326\n"
    STEP: restricting to a time range 09/06/23 10:32:41.575
    Sep  6 10:32:44.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --since=1s'
    Sep  6 10:32:44.159: INFO: stderr: ""
    Sep  6 10:32:44.159: INFO: stdout: "I0906 10:32:43.274208       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/844 467\nI0906 10:32:43.473663       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/9cq 491\nI0906 10:32:43.674058       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/z7x 385\nI0906 10:32:43.873328       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/vwj 203\nI0906 10:32:44.073623       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mkcg 363\n"
    Sep  6 10:32:44.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 logs logs-generator logs-generator --since=24h'
    Sep  6 10:32:44.239: INFO: stderr: ""
    Sep  6 10:32:44.239: INFO: stdout: "I0906 10:32:39.873263       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/zvd 330\nI0906 10:32:40.073419       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/cmn 476\nI0906 10:32:40.274293       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/w9q 371\nI0906 10:32:40.473664       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/spc 486\nI0906 10:32:40.674008       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/5s6 242\nI0906 10:32:40.873338       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/tr5 424\nI0906 10:32:41.073897       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/84n 505\nI0906 10:32:41.273447       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z7p 587\nI0906 10:32:41.473396       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dsl 326\nI0906 10:32:41.673941       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/9r24 396\nI0906 10:32:41.873937       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/zbzl 574\nI0906 10:32:42.074115       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/xsdw 208\nI0906 10:32:42.273666       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/rq2 370\nI0906 10:32:42.475172       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/zv7t 565\nI0906 10:32:42.673380       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/5hf9 272\nI0906 10:32:42.874096       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/7s5p 275\nI0906 10:32:43.073549       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/tpwf 522\nI0906 10:32:43.274208       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/844 467\nI0906 10:32:43.473663       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/9cq 491\nI0906 10:32:43.674058       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/z7x 385\nI0906 10:32:43.873328       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/vwj 203\nI0906 10:32:44.073623       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mkcg 363\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Sep  6 10:32:44.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-601 delete pod logs-generator'
    Sep  6 10:32:45.585: INFO: stderr: ""
    Sep  6 10:32:45.585: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:45.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-601" for this suite. 09/06/23 10:32:45.591
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:45.768
Sep  6 10:32:45.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 10:32:45.77
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:47.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:47.02
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 09/06/23 10:32:47.03
Sep  6 10:32:47.030: INFO: Creating e2e-svc-a-j8pj9
Sep  6 10:32:47.259: INFO: Creating e2e-svc-b-vrjj6
Sep  6 10:32:47.336: INFO: Creating e2e-svc-c-nzsch
STEP: deleting service collection 09/06/23 10:32:47.464
Sep  6 10:32:47.608: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:47.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3477" for this suite. 09/06/23 10:32:47.66
------------------------------
• [1.997 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:45.768
    Sep  6 10:32:45.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 10:32:45.77
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:47.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:47.02
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 09/06/23 10:32:47.03
    Sep  6 10:32:47.030: INFO: Creating e2e-svc-a-j8pj9
    Sep  6 10:32:47.259: INFO: Creating e2e-svc-b-vrjj6
    Sep  6 10:32:47.336: INFO: Creating e2e-svc-c-nzsch
    STEP: deleting service collection 09/06/23 10:32:47.464
    Sep  6 10:32:47.608: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:47.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3477" for this suite. 09/06/23 10:32:47.66
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:47.766
Sep  6 10:32:47.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 10:32:47.766
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:47.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:47.818
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 09/06/23 10:32:47.82
Sep  6 10:32:47.953: INFO: Waiting up to 5m0s for pod "pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1" in namespace "pods-5297" to be "running and ready"
Sep  6 10:32:47.974: INFO: Pod "pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.280257ms
Sep  6 10:32:47.975: INFO: The phase of Pod pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:32:49.980: INFO: Pod "pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026436425s
Sep  6 10:32:49.980: INFO: The phase of Pod pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1 is Running (Ready = true)
Sep  6 10:32:49.980: INFO: Pod "pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1" satisfied condition "running and ready"
Sep  6 10:32:49.985: INFO: Pod pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1 has hostIP: 10.2.20.103
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:49.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5297" for this suite. 09/06/23 10:32:49.988
------------------------------
• [2.229 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:47.766
    Sep  6 10:32:47.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 10:32:47.766
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:47.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:47.818
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 09/06/23 10:32:47.82
    Sep  6 10:32:47.953: INFO: Waiting up to 5m0s for pod "pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1" in namespace "pods-5297" to be "running and ready"
    Sep  6 10:32:47.974: INFO: Pod "pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.280257ms
    Sep  6 10:32:47.975: INFO: The phase of Pod pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:32:49.980: INFO: Pod "pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026436425s
    Sep  6 10:32:49.980: INFO: The phase of Pod pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1 is Running (Ready = true)
    Sep  6 10:32:49.980: INFO: Pod "pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1" satisfied condition "running and ready"
    Sep  6 10:32:49.985: INFO: Pod pod-hostip-904f3ca6-f727-422b-a15a-b4f74666d1f1 has hostIP: 10.2.20.103
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:49.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5297" for this suite. 09/06/23 10:32:49.988
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:49.995
Sep  6 10:32:49.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:32:49.996
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:50.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:50.013
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 09/06/23 10:32:50.015
Sep  6 10:32:50.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f" in namespace "downward-api-1064" to be "Succeeded or Failed"
Sep  6 10:32:50.038: INFO: Pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.380803ms
Sep  6 10:32:52.043: INFO: Pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01776263s
Sep  6 10:32:54.042: INFO: Pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016851779s
STEP: Saw pod success 09/06/23 10:32:54.042
Sep  6 10:32:54.043: INFO: Pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f" satisfied condition "Succeeded or Failed"
Sep  6 10:32:54.046: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f container client-container: <nil>
STEP: delete the pod 09/06/23 10:32:54.056
Sep  6 10:32:54.076: INFO: Waiting for pod downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f to disappear
Sep  6 10:32:54.080: INFO: Pod downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:54.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1064" for this suite. 09/06/23 10:32:54.087
------------------------------
• [4.100 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:49.995
    Sep  6 10:32:49.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:32:49.996
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:50.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:50.013
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 09/06/23 10:32:50.015
    Sep  6 10:32:50.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f" in namespace "downward-api-1064" to be "Succeeded or Failed"
    Sep  6 10:32:50.038: INFO: Pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.380803ms
    Sep  6 10:32:52.043: INFO: Pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01776263s
    Sep  6 10:32:54.042: INFO: Pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016851779s
    STEP: Saw pod success 09/06/23 10:32:54.042
    Sep  6 10:32:54.043: INFO: Pod "downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f" satisfied condition "Succeeded or Failed"
    Sep  6 10:32:54.046: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f container client-container: <nil>
    STEP: delete the pod 09/06/23 10:32:54.056
    Sep  6 10:32:54.076: INFO: Waiting for pod downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f to disappear
    Sep  6 10:32:54.080: INFO: Pod downwardapi-volume-dcb33acf-a1f7-40d7-aab4-4af35aab298f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:54.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1064" for this suite. 09/06/23 10:32:54.087
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:54.096
Sep  6 10:32:54.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:32:54.097
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:54.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:54.119
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-64a7648a-0264-4eb0-84d3-c8cd6eeb8fb4 09/06/23 10:32:54.124
STEP: Creating the pod 09/06/23 10:32:54.128
Sep  6 10:32:54.139: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7" in namespace "projected-6921" to be "running and ready"
Sep  6 10:32:54.142: INFO: Pod "pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.466403ms
Sep  6 10:32:54.143: INFO: The phase of Pod pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:32:56.161: INFO: Pod "pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7": Phase="Running", Reason="", readiness=true. Elapsed: 2.021747449s
Sep  6 10:32:56.161: INFO: The phase of Pod pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7 is Running (Ready = true)
Sep  6 10:32:56.161: INFO: Pod "pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-64a7648a-0264-4eb0-84d3-c8cd6eeb8fb4 09/06/23 10:32:56.207
STEP: waiting to observe update in volume 09/06/23 10:32:56.225
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:32:58.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6921" for this suite. 09/06/23 10:32:58.269
------------------------------
• [4.186 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:54.096
    Sep  6 10:32:54.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:32:54.097
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:54.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:54.119
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-64a7648a-0264-4eb0-84d3-c8cd6eeb8fb4 09/06/23 10:32:54.124
    STEP: Creating the pod 09/06/23 10:32:54.128
    Sep  6 10:32:54.139: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7" in namespace "projected-6921" to be "running and ready"
    Sep  6 10:32:54.142: INFO: Pod "pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.466403ms
    Sep  6 10:32:54.143: INFO: The phase of Pod pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:32:56.161: INFO: Pod "pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7": Phase="Running", Reason="", readiness=true. Elapsed: 2.021747449s
    Sep  6 10:32:56.161: INFO: The phase of Pod pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7 is Running (Ready = true)
    Sep  6 10:32:56.161: INFO: Pod "pod-projected-configmaps-bf60f69a-1742-426a-8093-c3fc30dd57c7" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-64a7648a-0264-4eb0-84d3-c8cd6eeb8fb4 09/06/23 10:32:56.207
    STEP: waiting to observe update in volume 09/06/23 10:32:56.225
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:32:58.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6921" for this suite. 09/06/23 10:32:58.269
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:32:58.282
Sep  6 10:32:58.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename svc-latency 09/06/23 10:32:58.283
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:58.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:58.305
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Sep  6 10:32:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8755 09/06/23 10:32:58.307
I0906 10:32:58.314459      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8755, replica count: 1
I0906 10:32:59.365698      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 10:33:00.366786      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 10:33:00.652: INFO: Created: latency-svc-lfpjw
Sep  6 10:33:00.785: INFO: Got endpoints: latency-svc-lfpjw [317.496683ms]
Sep  6 10:33:00.897: INFO: Created: latency-svc-cbhv2
Sep  6 10:33:00.990: INFO: Created: latency-svc-fmjjb
Sep  6 10:33:01.128: INFO: Got endpoints: latency-svc-cbhv2 [342.522397ms]
Sep  6 10:33:01.128: INFO: Got endpoints: latency-svc-fmjjb [343.056279ms]
Sep  6 10:33:01.211: INFO: Created: latency-svc-zgvsh
Sep  6 10:33:01.363: INFO: Got endpoints: latency-svc-zgvsh [577.462468ms]
Sep  6 10:33:01.383: INFO: Created: latency-svc-f479d
Sep  6 10:33:01.439: INFO: Got endpoints: latency-svc-f479d [653.477479ms]
Sep  6 10:33:01.498: INFO: Created: latency-svc-jlf6v
Sep  6 10:33:01.506: INFO: Created: latency-svc-vz86k
Sep  6 10:33:01.570: INFO: Got endpoints: latency-svc-jlf6v [784.354036ms]
Sep  6 10:33:01.596: INFO: Got endpoints: latency-svc-vz86k [810.010171ms]
Sep  6 10:33:01.621: INFO: Created: latency-svc-rfnwj
Sep  6 10:33:01.687: INFO: Created: latency-svc-7dbrj
Sep  6 10:33:01.690: INFO: Got endpoints: latency-svc-rfnwj [904.66856ms]
Sep  6 10:33:01.696: INFO: Got endpoints: latency-svc-7dbrj [910.45058ms]
Sep  6 10:33:01.701: INFO: Created: latency-svc-vmj88
Sep  6 10:33:01.713: INFO: Got endpoints: latency-svc-vmj88 [927.484735ms]
Sep  6 10:33:01.715: INFO: Created: latency-svc-6pt2f
Sep  6 10:33:01.727: INFO: Got endpoints: latency-svc-6pt2f [941.115922ms]
Sep  6 10:33:01.739: INFO: Created: latency-svc-wvv6v
Sep  6 10:33:01.749: INFO: Created: latency-svc-w2zx2
Sep  6 10:33:01.750: INFO: Got endpoints: latency-svc-wvv6v [965.169813ms]
Sep  6 10:33:01.760: INFO: Got endpoints: latency-svc-w2zx2 [973.837262ms]
Sep  6 10:33:01.781: INFO: Created: latency-svc-dnp7f
Sep  6 10:33:01.792: INFO: Got endpoints: latency-svc-dnp7f [1.006061664s]
Sep  6 10:33:01.801: INFO: Created: latency-svc-nb6lk
Sep  6 10:33:01.814: INFO: Got endpoints: latency-svc-nb6lk [1.027761166s]
Sep  6 10:33:01.818: INFO: Created: latency-svc-6gd2l
Sep  6 10:33:01.836: INFO: Got endpoints: latency-svc-6gd2l [1.049980302s]
Sep  6 10:33:01.837: INFO: Created: latency-svc-f7g87
Sep  6 10:33:01.852: INFO: Got endpoints: latency-svc-f7g87 [724.059951ms]
Sep  6 10:33:01.859: INFO: Created: latency-svc-6h4j4
Sep  6 10:33:01.870: INFO: Got endpoints: latency-svc-6h4j4 [741.255973ms]
Sep  6 10:33:01.878: INFO: Created: latency-svc-pshbb
Sep  6 10:33:01.900: INFO: Got endpoints: latency-svc-pshbb [536.828972ms]
Sep  6 10:33:01.908: INFO: Created: latency-svc-w5pcw
Sep  6 10:33:01.923: INFO: Got endpoints: latency-svc-w5pcw [484.18207ms]
Sep  6 10:33:01.932: INFO: Created: latency-svc-q7984
Sep  6 10:33:01.945: INFO: Got endpoints: latency-svc-q7984 [374.78735ms]
Sep  6 10:33:01.948: INFO: Created: latency-svc-z5dbj
Sep  6 10:33:01.962: INFO: Got endpoints: latency-svc-z5dbj [366.442169ms]
Sep  6 10:33:01.974: INFO: Created: latency-svc-cvgmw
Sep  6 10:33:01.986: INFO: Got endpoints: latency-svc-cvgmw [295.16995ms]
Sep  6 10:33:01.993: INFO: Created: latency-svc-q5px9
Sep  6 10:33:02.011: INFO: Got endpoints: latency-svc-q5px9 [314.502037ms]
Sep  6 10:33:02.014: INFO: Created: latency-svc-2fct2
Sep  6 10:33:02.031: INFO: Got endpoints: latency-svc-2fct2 [317.098557ms]
Sep  6 10:33:02.042: INFO: Created: latency-svc-q8vpt
Sep  6 10:33:02.056: INFO: Got endpoints: latency-svc-q8vpt [329.499972ms]
Sep  6 10:33:02.058: INFO: Created: latency-svc-kcfgn
Sep  6 10:33:02.073: INFO: Got endpoints: latency-svc-kcfgn [322.145329ms]
Sep  6 10:33:02.081: INFO: Created: latency-svc-z4wql
Sep  6 10:33:02.092: INFO: Got endpoints: latency-svc-z4wql [332.492324ms]
Sep  6 10:33:02.094: INFO: Created: latency-svc-7g7fk
Sep  6 10:33:02.112: INFO: Got endpoints: latency-svc-7g7fk [320.175673ms]
Sep  6 10:33:02.117: INFO: Created: latency-svc-pm8b4
Sep  6 10:33:02.130: INFO: Got endpoints: latency-svc-pm8b4 [316.407336ms]
Sep  6 10:33:02.136: INFO: Created: latency-svc-glfxj
Sep  6 10:33:02.148: INFO: Got endpoints: latency-svc-glfxj [311.887523ms]
Sep  6 10:33:02.149: INFO: Created: latency-svc-tjcgk
Sep  6 10:33:02.163: INFO: Got endpoints: latency-svc-tjcgk [309.779648ms]
Sep  6 10:33:02.172: INFO: Created: latency-svc-8qbzc
Sep  6 10:33:02.185: INFO: Got endpoints: latency-svc-8qbzc [315.587507ms]
Sep  6 10:33:02.192: INFO: Created: latency-svc-92vpz
Sep  6 10:33:02.199: INFO: Created: latency-svc-bhz5l
Sep  6 10:33:02.205: INFO: Got endpoints: latency-svc-92vpz [304.662142ms]
Sep  6 10:33:02.214: INFO: Got endpoints: latency-svc-bhz5l [291.069041ms]
Sep  6 10:33:02.222: INFO: Created: latency-svc-xbz44
Sep  6 10:33:02.235: INFO: Got endpoints: latency-svc-xbz44 [290.467142ms]
Sep  6 10:33:02.246: INFO: Created: latency-svc-dqtbk
Sep  6 10:33:02.258: INFO: Created: latency-svc-nfg42
Sep  6 10:33:02.262: INFO: Got endpoints: latency-svc-dqtbk [300.250589ms]
Sep  6 10:33:02.276: INFO: Got endpoints: latency-svc-nfg42 [289.470088ms]
Sep  6 10:33:02.284: INFO: Created: latency-svc-shzl7
Sep  6 10:33:02.293: INFO: Got endpoints: latency-svc-shzl7 [282.274779ms]
Sep  6 10:33:02.442: INFO: Created: latency-svc-s6pjm
Sep  6 10:33:02.451: INFO: Created: latency-svc-gvjl4
Sep  6 10:33:02.451: INFO: Created: latency-svc-rh7cb
Sep  6 10:33:02.451: INFO: Created: latency-svc-mjkrd
Sep  6 10:33:02.451: INFO: Created: latency-svc-4dbt7
Sep  6 10:33:02.456: INFO: Created: latency-svc-j999h
Sep  6 10:33:02.456: INFO: Created: latency-svc-dfjjs
Sep  6 10:33:02.456: INFO: Created: latency-svc-t2fcn
Sep  6 10:33:02.456: INFO: Created: latency-svc-dspcs
Sep  6 10:33:02.456: INFO: Created: latency-svc-8mfnv
Sep  6 10:33:02.466: INFO: Created: latency-svc-2mm5w
Sep  6 10:33:02.467: INFO: Created: latency-svc-qvxsk
Sep  6 10:33:02.467: INFO: Created: latency-svc-phkxf
Sep  6 10:33:02.467: INFO: Created: latency-svc-gt9rf
Sep  6 10:33:02.474: INFO: Created: latency-svc-4ck7f
Sep  6 10:33:02.485: INFO: Got endpoints: latency-svc-s6pjm [411.9807ms]
Sep  6 10:33:02.485: INFO: Got endpoints: latency-svc-dspcs [279.908905ms]
Sep  6 10:33:02.491: INFO: Got endpoints: latency-svc-j999h [305.393622ms]
Sep  6 10:33:02.497: INFO: Got endpoints: latency-svc-mjkrd [440.509976ms]
Sep  6 10:33:02.497: INFO: Got endpoints: latency-svc-4dbt7 [348.752506ms]
Sep  6 10:33:02.497: INFO: Got endpoints: latency-svc-t2fcn [261.328808ms]
Sep  6 10:33:02.512: INFO: Got endpoints: latency-svc-dfjjs [349.407201ms]
Sep  6 10:33:02.512: INFO: Got endpoints: latency-svc-qvxsk [218.696094ms]
Sep  6 10:33:02.512: INFO: Got endpoints: latency-svc-8mfnv [399.694002ms]
Sep  6 10:33:02.520: INFO: Got endpoints: latency-svc-2mm5w [305.874103ms]
Sep  6 10:33:02.529: INFO: Got endpoints: latency-svc-phkxf [436.344735ms]
Sep  6 10:33:02.535: INFO: Got endpoints: latency-svc-gt9rf [503.16416ms]
Sep  6 10:33:02.538: INFO: Got endpoints: latency-svc-gvjl4 [408.245876ms]
Sep  6 10:33:02.539: INFO: Got endpoints: latency-svc-rh7cb [276.825645ms]
Sep  6 10:33:02.540: INFO: Got endpoints: latency-svc-4ck7f [264.078807ms]
Sep  6 10:33:02.558: INFO: Created: latency-svc-4jsj4
Sep  6 10:33:02.574: INFO: Created: latency-svc-hh4gf
Sep  6 10:33:02.577: INFO: Got endpoints: latency-svc-4jsj4 [92.213797ms]
Sep  6 10:33:02.589: INFO: Created: latency-svc-vphq5
Sep  6 10:33:02.604: INFO: Got endpoints: latency-svc-hh4gf [113.481242ms]
Sep  6 10:33:02.611: INFO: Created: latency-svc-b4mtx
Sep  6 10:33:02.627: INFO: Created: latency-svc-764jw
Sep  6 10:33:02.649: INFO: Got endpoints: latency-svc-vphq5 [150.561588ms]
Sep  6 10:33:02.656: INFO: Created: latency-svc-8szv5
Sep  6 10:33:02.667: INFO: Created: latency-svc-frgr4
Sep  6 10:33:02.679: INFO: Created: latency-svc-6l2pc
Sep  6 10:33:02.699: INFO: Created: latency-svc-88gzj
Sep  6 10:33:02.704: INFO: Got endpoints: latency-svc-b4mtx [202.454045ms]
Sep  6 10:33:02.721: INFO: Created: latency-svc-222tc
Sep  6 10:33:02.736: INFO: Created: latency-svc-jccj2
Sep  6 10:33:02.736: INFO: Created: latency-svc-bhfb6
Sep  6 10:33:02.755: INFO: Created: latency-svc-x4vbj
Sep  6 10:33:02.771: INFO: Got endpoints: latency-svc-764jw [270.125725ms]
Sep  6 10:33:02.785: INFO: Created: latency-svc-xgsff
Sep  6 10:33:02.796: INFO: Got endpoints: latency-svc-8szv5 [283.801532ms]
Sep  6 10:33:02.802: INFO: Created: latency-svc-vf2lb
Sep  6 10:33:02.819: INFO: Created: latency-svc-h4nkg
Sep  6 10:33:02.831: INFO: Created: latency-svc-9hpkg
Sep  6 10:33:02.841: INFO: Created: latency-svc-9cl89
Sep  6 10:33:02.857: INFO: Got endpoints: latency-svc-frgr4 [344.690444ms]
Sep  6 10:33:02.863: INFO: Created: latency-svc-pl8tk
Sep  6 10:33:02.876: INFO: Created: latency-svc-5n7nv
Sep  6 10:33:02.885: INFO: Created: latency-svc-z4sgq
Sep  6 10:33:02.900: INFO: Created: latency-svc-8ddbv
Sep  6 10:33:02.903: INFO: Got endpoints: latency-svc-6l2pc [390.738029ms]
Sep  6 10:33:02.925: INFO: Created: latency-svc-hkczl
Sep  6 10:33:02.949: INFO: Got endpoints: latency-svc-88gzj [428.226295ms]
Sep  6 10:33:02.966: INFO: Created: latency-svc-tbx4m
Sep  6 10:33:02.996: INFO: Got endpoints: latency-svc-222tc [467.196613ms]
Sep  6 10:33:03.023: INFO: Created: latency-svc-v4bpx
Sep  6 10:33:03.054: INFO: Got endpoints: latency-svc-jccj2 [569.402658ms]
Sep  6 10:33:03.090: INFO: Created: latency-svc-hxn5g
Sep  6 10:33:03.108: INFO: Got endpoints: latency-svc-bhfb6 [573.126173ms]
Sep  6 10:33:03.127: INFO: Created: latency-svc-jqpzz
Sep  6 10:33:03.147: INFO: Got endpoints: latency-svc-x4vbj [607.86356ms]
Sep  6 10:33:03.173: INFO: Created: latency-svc-mgtn2
Sep  6 10:33:03.195: INFO: Got endpoints: latency-svc-xgsff [656.675398ms]
Sep  6 10:33:03.211: INFO: Created: latency-svc-kzpr7
Sep  6 10:33:03.251: INFO: Got endpoints: latency-svc-vf2lb [711.02534ms]
Sep  6 10:33:03.266: INFO: Created: latency-svc-85rck
Sep  6 10:33:03.303: INFO: Got endpoints: latency-svc-h4nkg [726.37995ms]
Sep  6 10:33:03.324: INFO: Created: latency-svc-bdhrt
Sep  6 10:33:03.349: INFO: Got endpoints: latency-svc-9hpkg [744.391426ms]
Sep  6 10:33:03.368: INFO: Created: latency-svc-npgvs
Sep  6 10:33:03.404: INFO: Got endpoints: latency-svc-9cl89 [755.35427ms]
Sep  6 10:33:03.431: INFO: Created: latency-svc-48jln
Sep  6 10:33:03.470: INFO: Got endpoints: latency-svc-pl8tk [766.208127ms]
Sep  6 10:33:03.491: INFO: Created: latency-svc-59pcm
Sep  6 10:33:03.511: INFO: Got endpoints: latency-svc-5n7nv [740.063771ms]
Sep  6 10:33:03.535: INFO: Created: latency-svc-6f5nb
Sep  6 10:33:03.549: INFO: Got endpoints: latency-svc-z4sgq [753.437204ms]
Sep  6 10:33:03.569: INFO: Created: latency-svc-bcrzp
Sep  6 10:33:03.603: INFO: Got endpoints: latency-svc-8ddbv [746.530264ms]
Sep  6 10:33:03.652: INFO: Created: latency-svc-b5mjq
Sep  6 10:33:03.655: INFO: Got endpoints: latency-svc-hkczl [752.361986ms]
Sep  6 10:33:03.687: INFO: Created: latency-svc-lwxqv
Sep  6 10:33:03.698: INFO: Got endpoints: latency-svc-tbx4m [749.475992ms]
Sep  6 10:33:03.731: INFO: Created: latency-svc-nqbzv
Sep  6 10:33:03.754: INFO: Got endpoints: latency-svc-v4bpx [758.379711ms]
Sep  6 10:33:03.779: INFO: Created: latency-svc-dvr96
Sep  6 10:33:03.798: INFO: Got endpoints: latency-svc-hxn5g [744.137056ms]
Sep  6 10:33:03.821: INFO: Created: latency-svc-l6h8p
Sep  6 10:33:03.851: INFO: Got endpoints: latency-svc-jqpzz [743.437234ms]
Sep  6 10:33:03.876: INFO: Created: latency-svc-hrtwc
Sep  6 10:33:03.907: INFO: Got endpoints: latency-svc-mgtn2 [759.605103ms]
Sep  6 10:33:03.930: INFO: Created: latency-svc-dwbtb
Sep  6 10:33:03.951: INFO: Got endpoints: latency-svc-kzpr7 [755.788766ms]
Sep  6 10:33:03.976: INFO: Created: latency-svc-xs575
Sep  6 10:33:04.002: INFO: Got endpoints: latency-svc-85rck [751.448442ms]
Sep  6 10:33:04.022: INFO: Created: latency-svc-vnk59
Sep  6 10:33:04.048: INFO: Got endpoints: latency-svc-bdhrt [744.64982ms]
Sep  6 10:33:04.073: INFO: Created: latency-svc-4mgs2
Sep  6 10:33:04.100: INFO: Got endpoints: latency-svc-npgvs [751.604954ms]
Sep  6 10:33:04.122: INFO: Created: latency-svc-p8ks9
Sep  6 10:33:04.162: INFO: Got endpoints: latency-svc-48jln [757.994391ms]
Sep  6 10:33:04.182: INFO: Created: latency-svc-bwnqq
Sep  6 10:33:04.197: INFO: Got endpoints: latency-svc-59pcm [726.856452ms]
Sep  6 10:33:04.212: INFO: Created: latency-svc-xbsgr
Sep  6 10:33:04.247: INFO: Got endpoints: latency-svc-6f5nb [735.617731ms]
Sep  6 10:33:04.267: INFO: Created: latency-svc-z4xhz
Sep  6 10:33:04.301: INFO: Got endpoints: latency-svc-bcrzp [751.105612ms]
Sep  6 10:33:04.315: INFO: Created: latency-svc-vh9lh
Sep  6 10:33:04.352: INFO: Got endpoints: latency-svc-b5mjq [748.583458ms]
Sep  6 10:33:04.370: INFO: Created: latency-svc-xnrpk
Sep  6 10:33:04.397: INFO: Got endpoints: latency-svc-lwxqv [741.780368ms]
Sep  6 10:33:04.415: INFO: Created: latency-svc-ph4hp
Sep  6 10:33:04.449: INFO: Got endpoints: latency-svc-nqbzv [750.929183ms]
Sep  6 10:33:04.464: INFO: Created: latency-svc-gh7fr
Sep  6 10:33:04.500: INFO: Got endpoints: latency-svc-dvr96 [745.997108ms]
Sep  6 10:33:04.523: INFO: Created: latency-svc-jjdpm
Sep  6 10:33:04.549: INFO: Got endpoints: latency-svc-l6h8p [750.323314ms]
Sep  6 10:33:04.567: INFO: Created: latency-svc-tvqsm
Sep  6 10:33:04.599: INFO: Got endpoints: latency-svc-hrtwc [747.667393ms]
Sep  6 10:33:04.616: INFO: Created: latency-svc-2llbb
Sep  6 10:33:04.648: INFO: Got endpoints: latency-svc-dwbtb [739.759387ms]
Sep  6 10:33:04.692: INFO: Created: latency-svc-vkwp8
Sep  6 10:33:04.950: INFO: Got endpoints: latency-svc-4mgs2 [901.613042ms]
Sep  6 10:33:04.950: INFO: Got endpoints: latency-svc-xs575 [998.593013ms]
Sep  6 10:33:04.953: INFO: Got endpoints: latency-svc-vnk59 [951.090839ms]
Sep  6 10:33:04.986: INFO: Got endpoints: latency-svc-bwnqq [823.372418ms]
Sep  6 10:33:05.141: INFO: Got endpoints: latency-svc-p8ks9 [1.040539898s]
Sep  6 10:33:05.145: INFO: Got endpoints: latency-svc-xbsgr [948.609205ms]
Sep  6 10:33:05.184: INFO: Got endpoints: latency-svc-vh9lh [883.679431ms]
Sep  6 10:33:05.225: INFO: Got endpoints: latency-svc-xnrpk [872.785464ms]
Sep  6 10:33:05.464: INFO: Got endpoints: latency-svc-z4xhz [1.21724352s]
Sep  6 10:33:05.475: INFO: Got endpoints: latency-svc-jjdpm [974.410316ms]
Sep  6 10:33:05.475: INFO: Got endpoints: latency-svc-ph4hp [1.078084513s]
Sep  6 10:33:05.476: INFO: Got endpoints: latency-svc-gh7fr [1.027094017s]
Sep  6 10:33:05.715: INFO: Got endpoints: latency-svc-tvqsm [1.16619449s]
Sep  6 10:33:05.715: INFO: Got endpoints: latency-svc-vkwp8 [1.067491899s]
Sep  6 10:33:05.716: INFO: Got endpoints: latency-svc-2llbb [1.116531535s]
Sep  6 10:33:05.785: INFO: Created: latency-svc-87sz7
Sep  6 10:33:05.833: INFO: Created: latency-svc-27nkt
Sep  6 10:33:05.960: INFO: Created: latency-svc-pb82t
Sep  6 10:33:06.005: INFO: Got endpoints: latency-svc-87sz7 [1.05153811s]
Sep  6 10:33:06.006: INFO: Got endpoints: latency-svc-27nkt [1.055994659s]
Sep  6 10:33:06.037: INFO: Got endpoints: latency-svc-pb82t [1.083122752s]
Sep  6 10:33:06.100: INFO: Created: latency-svc-k4v8g
Sep  6 10:33:06.119: INFO: Created: latency-svc-hhwhl
Sep  6 10:33:06.125: INFO: Got endpoints: latency-svc-k4v8g [1.139349478s]
Sep  6 10:33:06.139: INFO: Got endpoints: latency-svc-hhwhl [997.960839ms]
Sep  6 10:33:06.145: INFO: Created: latency-svc-qt679
Sep  6 10:33:06.162: INFO: Got endpoints: latency-svc-qt679 [1.016116587s]
Sep  6 10:33:06.178: INFO: Created: latency-svc-zdzd5
Sep  6 10:33:06.195: INFO: Created: latency-svc-m7gmw
Sep  6 10:33:06.196: INFO: Got endpoints: latency-svc-zdzd5 [1.011533687s]
Sep  6 10:33:06.218: INFO: Got endpoints: latency-svc-m7gmw [992.194743ms]
Sep  6 10:33:06.218: INFO: Created: latency-svc-n5rzm
Sep  6 10:33:06.232: INFO: Got endpoints: latency-svc-n5rzm [767.439233ms]
Sep  6 10:33:06.247: INFO: Created: latency-svc-8hb68
Sep  6 10:33:06.270: INFO: Created: latency-svc-4784g
Sep  6 10:33:06.283: INFO: Got endpoints: latency-svc-4784g [807.15206ms]
Sep  6 10:33:06.284: INFO: Got endpoints: latency-svc-8hb68 [808.563448ms]
Sep  6 10:33:06.286: INFO: Created: latency-svc-g9b7l
Sep  6 10:33:06.308: INFO: Got endpoints: latency-svc-g9b7l [831.073863ms]
Sep  6 10:33:06.318: INFO: Created: latency-svc-f8vnj
Sep  6 10:33:06.328: INFO: Got endpoints: latency-svc-f8vnj [612.687641ms]
Sep  6 10:33:06.334: INFO: Created: latency-svc-xhxw7
Sep  6 10:33:06.351: INFO: Created: latency-svc-xxbc4
Sep  6 10:33:06.353: INFO: Got endpoints: latency-svc-xhxw7 [637.472634ms]
Sep  6 10:33:06.371: INFO: Got endpoints: latency-svc-xxbc4 [654.987407ms]
Sep  6 10:33:06.380: INFO: Created: latency-svc-wtbmg
Sep  6 10:33:06.395: INFO: Created: latency-svc-4cvlb
Sep  6 10:33:06.398: INFO: Got endpoints: latency-svc-wtbmg [392.665059ms]
Sep  6 10:33:06.412: INFO: Got endpoints: latency-svc-4cvlb [406.61405ms]
Sep  6 10:33:06.421: INFO: Created: latency-svc-jwtjf
Sep  6 10:33:06.433: INFO: Got endpoints: latency-svc-jwtjf [396.734126ms]
Sep  6 10:33:06.450: INFO: Created: latency-svc-hszdw
Sep  6 10:33:06.453: INFO: Created: latency-svc-54776
Sep  6 10:33:06.457: INFO: Got endpoints: latency-svc-hszdw [331.763042ms]
Sep  6 10:33:06.469: INFO: Got endpoints: latency-svc-54776 [330.308741ms]
Sep  6 10:33:06.476: INFO: Created: latency-svc-gfjr5
Sep  6 10:33:06.486: INFO: Got endpoints: latency-svc-gfjr5 [324.012592ms]
Sep  6 10:33:06.499: INFO: Created: latency-svc-mfdmm
Sep  6 10:33:06.509: INFO: Created: latency-svc-nfnmd
Sep  6 10:33:06.514: INFO: Got endpoints: latency-svc-mfdmm [316.353791ms]
Sep  6 10:33:06.527: INFO: Created: latency-svc-ln46z
Sep  6 10:33:06.542: INFO: Created: latency-svc-4l6sn
Sep  6 10:33:06.568: INFO: Got endpoints: latency-svc-nfnmd [350.247307ms]
Sep  6 10:33:06.575: INFO: Created: latency-svc-qmd2l
Sep  6 10:33:06.592: INFO: Created: latency-svc-wbnfw
Sep  6 10:33:06.602: INFO: Got endpoints: latency-svc-ln46z [370.508547ms]
Sep  6 10:33:06.607: INFO: Created: latency-svc-x2k6f
Sep  6 10:33:06.629: INFO: Created: latency-svc-bxfmq
Sep  6 10:33:06.643: INFO: Created: latency-svc-wvvdp
Sep  6 10:33:06.654: INFO: Got endpoints: latency-svc-4l6sn [370.834096ms]
Sep  6 10:33:06.659: INFO: Created: latency-svc-s6q2m
Sep  6 10:33:06.678: INFO: Created: latency-svc-bddsd
Sep  6 10:33:06.686: INFO: Created: latency-svc-wxqmc
Sep  6 10:33:06.705: INFO: Got endpoints: latency-svc-qmd2l [421.910573ms]
Sep  6 10:33:06.714: INFO: Created: latency-svc-pnfpt
Sep  6 10:33:06.719: INFO: Created: latency-svc-8vp6k
Sep  6 10:33:06.731: INFO: Created: latency-svc-wfm9k
Sep  6 10:33:06.754: INFO: Got endpoints: latency-svc-wbnfw [445.783479ms]
Sep  6 10:33:06.763: INFO: Created: latency-svc-6nf2f
Sep  6 10:33:06.763: INFO: Created: latency-svc-xt8zx
Sep  6 10:33:06.791: INFO: Created: latency-svc-89cjm
Sep  6 10:33:06.804: INFO: Got endpoints: latency-svc-x2k6f [476.308402ms]
Sep  6 10:33:06.812: INFO: Created: latency-svc-m9n5g
Sep  6 10:33:06.822: INFO: Created: latency-svc-w52m7
Sep  6 10:33:06.832: INFO: Created: latency-svc-gt25n
Sep  6 10:33:06.852: INFO: Got endpoints: latency-svc-bxfmq [499.427461ms]
Sep  6 10:33:06.853: INFO: Created: latency-svc-j7dhn
Sep  6 10:33:06.871: INFO: Created: latency-svc-5zsk7
Sep  6 10:33:06.897: INFO: Got endpoints: latency-svc-wvvdp [526.328959ms]
Sep  6 10:33:06.911: INFO: Created: latency-svc-jsnlw
Sep  6 10:33:06.953: INFO: Got endpoints: latency-svc-s6q2m [555.30973ms]
Sep  6 10:33:06.969: INFO: Created: latency-svc-5wc8c
Sep  6 10:33:07.000: INFO: Got endpoints: latency-svc-bddsd [587.67514ms]
Sep  6 10:33:07.017: INFO: Created: latency-svc-gkwpj
Sep  6 10:33:07.046: INFO: Got endpoints: latency-svc-wxqmc [613.009209ms]
Sep  6 10:33:07.072: INFO: Created: latency-svc-p7lk5
Sep  6 10:33:07.097: INFO: Got endpoints: latency-svc-pnfpt [639.60718ms]
Sep  6 10:33:07.116: INFO: Created: latency-svc-cpswz
Sep  6 10:33:07.152: INFO: Got endpoints: latency-svc-8vp6k [682.577639ms]
Sep  6 10:33:07.176: INFO: Created: latency-svc-fg6mm
Sep  6 10:33:07.198: INFO: Got endpoints: latency-svc-wfm9k [712.099784ms]
Sep  6 10:33:07.214: INFO: Created: latency-svc-hz98b
Sep  6 10:33:07.247: INFO: Got endpoints: latency-svc-xt8zx [732.481443ms]
Sep  6 10:33:07.265: INFO: Created: latency-svc-v6gvc
Sep  6 10:33:07.310: INFO: Got endpoints: latency-svc-6nf2f [742.344654ms]
Sep  6 10:33:07.325: INFO: Created: latency-svc-cdmlg
Sep  6 10:33:07.352: INFO: Got endpoints: latency-svc-89cjm [749.427194ms]
Sep  6 10:33:07.368: INFO: Created: latency-svc-fcpwb
Sep  6 10:33:07.401: INFO: Got endpoints: latency-svc-m9n5g [746.211977ms]
Sep  6 10:33:07.423: INFO: Created: latency-svc-65fr8
Sep  6 10:33:07.447: INFO: Got endpoints: latency-svc-w52m7 [741.954903ms]
Sep  6 10:33:07.469: INFO: Created: latency-svc-cbnvq
Sep  6 10:33:07.499: INFO: Got endpoints: latency-svc-gt25n [745.195659ms]
Sep  6 10:33:07.522: INFO: Created: latency-svc-wbqqt
Sep  6 10:33:07.548: INFO: Got endpoints: latency-svc-j7dhn [743.378107ms]
Sep  6 10:33:07.572: INFO: Created: latency-svc-wgkgz
Sep  6 10:33:07.604: INFO: Got endpoints: latency-svc-5zsk7 [751.098781ms]
Sep  6 10:33:07.623: INFO: Created: latency-svc-857hs
Sep  6 10:33:07.646: INFO: Got endpoints: latency-svc-jsnlw [749.266005ms]
Sep  6 10:33:07.683: INFO: Created: latency-svc-qz2v2
Sep  6 10:33:07.698: INFO: Got endpoints: latency-svc-5wc8c [744.060344ms]
Sep  6 10:33:07.714: INFO: Created: latency-svc-mz24z
Sep  6 10:33:07.751: INFO: Got endpoints: latency-svc-gkwpj [750.66297ms]
Sep  6 10:33:07.766: INFO: Created: latency-svc-vlrl9
Sep  6 10:33:07.799: INFO: Got endpoints: latency-svc-p7lk5 [752.190411ms]
Sep  6 10:33:07.820: INFO: Created: latency-svc-mrbd7
Sep  6 10:33:07.851: INFO: Got endpoints: latency-svc-cpswz [754.013579ms]
Sep  6 10:33:07.870: INFO: Created: latency-svc-pm9n6
Sep  6 10:33:07.899: INFO: Got endpoints: latency-svc-fg6mm [747.103971ms]
Sep  6 10:33:07.915: INFO: Created: latency-svc-lcnqn
Sep  6 10:33:07.952: INFO: Got endpoints: latency-svc-hz98b [753.300039ms]
Sep  6 10:33:07.968: INFO: Created: latency-svc-w89wg
Sep  6 10:33:07.997: INFO: Got endpoints: latency-svc-v6gvc [749.763661ms]
Sep  6 10:33:08.018: INFO: Created: latency-svc-k94c7
Sep  6 10:33:08.047: INFO: Got endpoints: latency-svc-cdmlg [736.529089ms]
Sep  6 10:33:08.063: INFO: Created: latency-svc-68jb6
Sep  6 10:33:08.104: INFO: Got endpoints: latency-svc-fcpwb [752.735792ms]
Sep  6 10:33:08.124: INFO: Created: latency-svc-ldpr9
Sep  6 10:33:08.149: INFO: Got endpoints: latency-svc-65fr8 [747.867109ms]
Sep  6 10:33:08.169: INFO: Created: latency-svc-p6r6g
Sep  6 10:33:08.202: INFO: Got endpoints: latency-svc-cbnvq [754.530463ms]
Sep  6 10:33:08.219: INFO: Created: latency-svc-j59k9
Sep  6 10:33:08.250: INFO: Got endpoints: latency-svc-wbqqt [750.903629ms]
Sep  6 10:33:08.265: INFO: Created: latency-svc-k5wv6
Sep  6 10:33:08.298: INFO: Got endpoints: latency-svc-wgkgz [750.685309ms]
Sep  6 10:33:08.317: INFO: Created: latency-svc-d5nxm
Sep  6 10:33:08.350: INFO: Got endpoints: latency-svc-857hs [746.884708ms]
Sep  6 10:33:08.366: INFO: Created: latency-svc-kq7ch
Sep  6 10:33:08.401: INFO: Got endpoints: latency-svc-qz2v2 [754.862149ms]
Sep  6 10:33:08.415: INFO: Created: latency-svc-hzh47
Sep  6 10:33:08.448: INFO: Got endpoints: latency-svc-mz24z [750.015262ms]
Sep  6 10:33:08.467: INFO: Created: latency-svc-ckrdb
Sep  6 10:33:08.497: INFO: Got endpoints: latency-svc-vlrl9 [746.276541ms]
Sep  6 10:33:08.512: INFO: Created: latency-svc-s7j5l
Sep  6 10:33:08.548: INFO: Got endpoints: latency-svc-mrbd7 [748.900442ms]
Sep  6 10:33:08.575: INFO: Created: latency-svc-slbnh
Sep  6 10:33:08.601: INFO: Got endpoints: latency-svc-pm9n6 [750.392301ms]
Sep  6 10:33:08.628: INFO: Created: latency-svc-gtq5b
Sep  6 10:33:08.653: INFO: Got endpoints: latency-svc-lcnqn [753.601506ms]
Sep  6 10:33:08.671: INFO: Created: latency-svc-5flxv
Sep  6 10:33:08.696: INFO: Got endpoints: latency-svc-w89wg [744.097348ms]
Sep  6 10:33:08.717: INFO: Created: latency-svc-cjj9f
Sep  6 10:33:08.751: INFO: Got endpoints: latency-svc-k94c7 [754.349579ms]
Sep  6 10:33:08.769: INFO: Created: latency-svc-wb9wt
Sep  6 10:33:08.801: INFO: Got endpoints: latency-svc-68jb6 [753.604818ms]
Sep  6 10:33:08.818: INFO: Created: latency-svc-fbz49
Sep  6 10:33:08.849: INFO: Got endpoints: latency-svc-ldpr9 [743.952456ms]
Sep  6 10:33:08.864: INFO: Created: latency-svc-t8bxl
Sep  6 10:33:08.905: INFO: Got endpoints: latency-svc-p6r6g [756.217761ms]
Sep  6 10:33:08.924: INFO: Created: latency-svc-xvr27
Sep  6 10:33:08.949: INFO: Got endpoints: latency-svc-j59k9 [747.005347ms]
Sep  6 10:33:08.967: INFO: Created: latency-svc-lzlh9
Sep  6 10:33:09.003: INFO: Got endpoints: latency-svc-k5wv6 [752.723762ms]
Sep  6 10:33:09.023: INFO: Created: latency-svc-bxn8t
Sep  6 10:33:09.052: INFO: Got endpoints: latency-svc-d5nxm [753.069799ms]
Sep  6 10:33:09.071: INFO: Created: latency-svc-kjcdc
Sep  6 10:33:09.100: INFO: Got endpoints: latency-svc-kq7ch [749.670397ms]
Sep  6 10:33:09.118: INFO: Created: latency-svc-s2nrb
Sep  6 10:33:09.152: INFO: Got endpoints: latency-svc-hzh47 [750.89819ms]
Sep  6 10:33:09.197: INFO: Got endpoints: latency-svc-ckrdb [749.26711ms]
Sep  6 10:33:09.256: INFO: Got endpoints: latency-svc-s7j5l [758.438277ms]
Sep  6 10:33:09.296: INFO: Got endpoints: latency-svc-slbnh [748.251961ms]
Sep  6 10:33:09.348: INFO: Got endpoints: latency-svc-gtq5b [746.561116ms]
Sep  6 10:33:09.395: INFO: Got endpoints: latency-svc-5flxv [741.617322ms]
Sep  6 10:33:09.453: INFO: Got endpoints: latency-svc-cjj9f [756.367092ms]
Sep  6 10:33:09.496: INFO: Got endpoints: latency-svc-wb9wt [745.135046ms]
Sep  6 10:33:09.550: INFO: Got endpoints: latency-svc-fbz49 [749.42086ms]
Sep  6 10:33:09.596: INFO: Got endpoints: latency-svc-t8bxl [746.831851ms]
Sep  6 10:33:09.657: INFO: Got endpoints: latency-svc-xvr27 [751.65994ms]
Sep  6 10:33:09.699: INFO: Got endpoints: latency-svc-lzlh9 [749.26273ms]
Sep  6 10:33:09.749: INFO: Got endpoints: latency-svc-bxn8t [746.389831ms]
Sep  6 10:33:09.803: INFO: Got endpoints: latency-svc-kjcdc [751.045111ms]
Sep  6 10:33:09.848: INFO: Got endpoints: latency-svc-s2nrb [747.098916ms]
Sep  6 10:33:09.848: INFO: Latencies: [92.213797ms 113.481242ms 150.561588ms 202.454045ms 218.696094ms 261.328808ms 264.078807ms 270.125725ms 276.825645ms 279.908905ms 282.274779ms 283.801532ms 289.470088ms 290.467142ms 291.069041ms 295.16995ms 300.250589ms 304.662142ms 305.393622ms 305.874103ms 309.779648ms 311.887523ms 314.502037ms 315.587507ms 316.353791ms 316.407336ms 317.098557ms 320.175673ms 322.145329ms 324.012592ms 329.499972ms 330.308741ms 331.763042ms 332.492324ms 342.522397ms 343.056279ms 344.690444ms 348.752506ms 349.407201ms 350.247307ms 366.442169ms 370.508547ms 370.834096ms 374.78735ms 390.738029ms 392.665059ms 396.734126ms 399.694002ms 406.61405ms 408.245876ms 411.9807ms 421.910573ms 428.226295ms 436.344735ms 440.509976ms 445.783479ms 467.196613ms 476.308402ms 484.18207ms 499.427461ms 503.16416ms 526.328959ms 536.828972ms 555.30973ms 569.402658ms 573.126173ms 577.462468ms 587.67514ms 607.86356ms 612.687641ms 613.009209ms 637.472634ms 639.60718ms 653.477479ms 654.987407ms 656.675398ms 682.577639ms 711.02534ms 712.099784ms 724.059951ms 726.37995ms 726.856452ms 732.481443ms 735.617731ms 736.529089ms 739.759387ms 740.063771ms 741.255973ms 741.617322ms 741.780368ms 741.954903ms 742.344654ms 743.378107ms 743.437234ms 743.952456ms 744.060344ms 744.097348ms 744.137056ms 744.391426ms 744.64982ms 745.135046ms 745.195659ms 745.997108ms 746.211977ms 746.276541ms 746.389831ms 746.530264ms 746.561116ms 746.831851ms 746.884708ms 747.005347ms 747.098916ms 747.103971ms 747.667393ms 747.867109ms 748.251961ms 748.583458ms 748.900442ms 749.26273ms 749.266005ms 749.26711ms 749.42086ms 749.427194ms 749.475992ms 749.670397ms 749.763661ms 750.015262ms 750.323314ms 750.392301ms 750.66297ms 750.685309ms 750.89819ms 750.903629ms 750.929183ms 751.045111ms 751.098781ms 751.105612ms 751.448442ms 751.604954ms 751.65994ms 752.190411ms 752.361986ms 752.723762ms 752.735792ms 753.069799ms 753.300039ms 753.437204ms 753.601506ms 753.604818ms 754.013579ms 754.349579ms 754.530463ms 754.862149ms 755.35427ms 755.788766ms 756.217761ms 756.367092ms 757.994391ms 758.379711ms 758.438277ms 759.605103ms 766.208127ms 767.439233ms 784.354036ms 807.15206ms 808.563448ms 810.010171ms 823.372418ms 831.073863ms 872.785464ms 883.679431ms 901.613042ms 904.66856ms 910.45058ms 927.484735ms 941.115922ms 948.609205ms 951.090839ms 965.169813ms 973.837262ms 974.410316ms 992.194743ms 997.960839ms 998.593013ms 1.006061664s 1.011533687s 1.016116587s 1.027094017s 1.027761166s 1.040539898s 1.049980302s 1.05153811s 1.055994659s 1.067491899s 1.078084513s 1.083122752s 1.116531535s 1.139349478s 1.16619449s 1.21724352s]
Sep  6 10:33:09.848: INFO: 50 %ile: 745.135046ms
Sep  6 10:33:09.848: INFO: 90 %ile: 974.410316ms
Sep  6 10:33:09.848: INFO: 99 %ile: 1.16619449s
Sep  6 10:33:09.848: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Sep  6 10:33:09.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-8755" for this suite. 09/06/23 10:33:09.855
------------------------------
• [SLOW TEST] [11.584 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:32:58.282
    Sep  6 10:32:58.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename svc-latency 09/06/23 10:32:58.283
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:32:58.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:32:58.305
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Sep  6 10:32:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-8755 09/06/23 10:32:58.307
    I0906 10:32:58.314459      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8755, replica count: 1
    I0906 10:32:59.365698      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0906 10:33:00.366786      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 10:33:00.652: INFO: Created: latency-svc-lfpjw
    Sep  6 10:33:00.785: INFO: Got endpoints: latency-svc-lfpjw [317.496683ms]
    Sep  6 10:33:00.897: INFO: Created: latency-svc-cbhv2
    Sep  6 10:33:00.990: INFO: Created: latency-svc-fmjjb
    Sep  6 10:33:01.128: INFO: Got endpoints: latency-svc-cbhv2 [342.522397ms]
    Sep  6 10:33:01.128: INFO: Got endpoints: latency-svc-fmjjb [343.056279ms]
    Sep  6 10:33:01.211: INFO: Created: latency-svc-zgvsh
    Sep  6 10:33:01.363: INFO: Got endpoints: latency-svc-zgvsh [577.462468ms]
    Sep  6 10:33:01.383: INFO: Created: latency-svc-f479d
    Sep  6 10:33:01.439: INFO: Got endpoints: latency-svc-f479d [653.477479ms]
    Sep  6 10:33:01.498: INFO: Created: latency-svc-jlf6v
    Sep  6 10:33:01.506: INFO: Created: latency-svc-vz86k
    Sep  6 10:33:01.570: INFO: Got endpoints: latency-svc-jlf6v [784.354036ms]
    Sep  6 10:33:01.596: INFO: Got endpoints: latency-svc-vz86k [810.010171ms]
    Sep  6 10:33:01.621: INFO: Created: latency-svc-rfnwj
    Sep  6 10:33:01.687: INFO: Created: latency-svc-7dbrj
    Sep  6 10:33:01.690: INFO: Got endpoints: latency-svc-rfnwj [904.66856ms]
    Sep  6 10:33:01.696: INFO: Got endpoints: latency-svc-7dbrj [910.45058ms]
    Sep  6 10:33:01.701: INFO: Created: latency-svc-vmj88
    Sep  6 10:33:01.713: INFO: Got endpoints: latency-svc-vmj88 [927.484735ms]
    Sep  6 10:33:01.715: INFO: Created: latency-svc-6pt2f
    Sep  6 10:33:01.727: INFO: Got endpoints: latency-svc-6pt2f [941.115922ms]
    Sep  6 10:33:01.739: INFO: Created: latency-svc-wvv6v
    Sep  6 10:33:01.749: INFO: Created: latency-svc-w2zx2
    Sep  6 10:33:01.750: INFO: Got endpoints: latency-svc-wvv6v [965.169813ms]
    Sep  6 10:33:01.760: INFO: Got endpoints: latency-svc-w2zx2 [973.837262ms]
    Sep  6 10:33:01.781: INFO: Created: latency-svc-dnp7f
    Sep  6 10:33:01.792: INFO: Got endpoints: latency-svc-dnp7f [1.006061664s]
    Sep  6 10:33:01.801: INFO: Created: latency-svc-nb6lk
    Sep  6 10:33:01.814: INFO: Got endpoints: latency-svc-nb6lk [1.027761166s]
    Sep  6 10:33:01.818: INFO: Created: latency-svc-6gd2l
    Sep  6 10:33:01.836: INFO: Got endpoints: latency-svc-6gd2l [1.049980302s]
    Sep  6 10:33:01.837: INFO: Created: latency-svc-f7g87
    Sep  6 10:33:01.852: INFO: Got endpoints: latency-svc-f7g87 [724.059951ms]
    Sep  6 10:33:01.859: INFO: Created: latency-svc-6h4j4
    Sep  6 10:33:01.870: INFO: Got endpoints: latency-svc-6h4j4 [741.255973ms]
    Sep  6 10:33:01.878: INFO: Created: latency-svc-pshbb
    Sep  6 10:33:01.900: INFO: Got endpoints: latency-svc-pshbb [536.828972ms]
    Sep  6 10:33:01.908: INFO: Created: latency-svc-w5pcw
    Sep  6 10:33:01.923: INFO: Got endpoints: latency-svc-w5pcw [484.18207ms]
    Sep  6 10:33:01.932: INFO: Created: latency-svc-q7984
    Sep  6 10:33:01.945: INFO: Got endpoints: latency-svc-q7984 [374.78735ms]
    Sep  6 10:33:01.948: INFO: Created: latency-svc-z5dbj
    Sep  6 10:33:01.962: INFO: Got endpoints: latency-svc-z5dbj [366.442169ms]
    Sep  6 10:33:01.974: INFO: Created: latency-svc-cvgmw
    Sep  6 10:33:01.986: INFO: Got endpoints: latency-svc-cvgmw [295.16995ms]
    Sep  6 10:33:01.993: INFO: Created: latency-svc-q5px9
    Sep  6 10:33:02.011: INFO: Got endpoints: latency-svc-q5px9 [314.502037ms]
    Sep  6 10:33:02.014: INFO: Created: latency-svc-2fct2
    Sep  6 10:33:02.031: INFO: Got endpoints: latency-svc-2fct2 [317.098557ms]
    Sep  6 10:33:02.042: INFO: Created: latency-svc-q8vpt
    Sep  6 10:33:02.056: INFO: Got endpoints: latency-svc-q8vpt [329.499972ms]
    Sep  6 10:33:02.058: INFO: Created: latency-svc-kcfgn
    Sep  6 10:33:02.073: INFO: Got endpoints: latency-svc-kcfgn [322.145329ms]
    Sep  6 10:33:02.081: INFO: Created: latency-svc-z4wql
    Sep  6 10:33:02.092: INFO: Got endpoints: latency-svc-z4wql [332.492324ms]
    Sep  6 10:33:02.094: INFO: Created: latency-svc-7g7fk
    Sep  6 10:33:02.112: INFO: Got endpoints: latency-svc-7g7fk [320.175673ms]
    Sep  6 10:33:02.117: INFO: Created: latency-svc-pm8b4
    Sep  6 10:33:02.130: INFO: Got endpoints: latency-svc-pm8b4 [316.407336ms]
    Sep  6 10:33:02.136: INFO: Created: latency-svc-glfxj
    Sep  6 10:33:02.148: INFO: Got endpoints: latency-svc-glfxj [311.887523ms]
    Sep  6 10:33:02.149: INFO: Created: latency-svc-tjcgk
    Sep  6 10:33:02.163: INFO: Got endpoints: latency-svc-tjcgk [309.779648ms]
    Sep  6 10:33:02.172: INFO: Created: latency-svc-8qbzc
    Sep  6 10:33:02.185: INFO: Got endpoints: latency-svc-8qbzc [315.587507ms]
    Sep  6 10:33:02.192: INFO: Created: latency-svc-92vpz
    Sep  6 10:33:02.199: INFO: Created: latency-svc-bhz5l
    Sep  6 10:33:02.205: INFO: Got endpoints: latency-svc-92vpz [304.662142ms]
    Sep  6 10:33:02.214: INFO: Got endpoints: latency-svc-bhz5l [291.069041ms]
    Sep  6 10:33:02.222: INFO: Created: latency-svc-xbz44
    Sep  6 10:33:02.235: INFO: Got endpoints: latency-svc-xbz44 [290.467142ms]
    Sep  6 10:33:02.246: INFO: Created: latency-svc-dqtbk
    Sep  6 10:33:02.258: INFO: Created: latency-svc-nfg42
    Sep  6 10:33:02.262: INFO: Got endpoints: latency-svc-dqtbk [300.250589ms]
    Sep  6 10:33:02.276: INFO: Got endpoints: latency-svc-nfg42 [289.470088ms]
    Sep  6 10:33:02.284: INFO: Created: latency-svc-shzl7
    Sep  6 10:33:02.293: INFO: Got endpoints: latency-svc-shzl7 [282.274779ms]
    Sep  6 10:33:02.442: INFO: Created: latency-svc-s6pjm
    Sep  6 10:33:02.451: INFO: Created: latency-svc-gvjl4
    Sep  6 10:33:02.451: INFO: Created: latency-svc-rh7cb
    Sep  6 10:33:02.451: INFO: Created: latency-svc-mjkrd
    Sep  6 10:33:02.451: INFO: Created: latency-svc-4dbt7
    Sep  6 10:33:02.456: INFO: Created: latency-svc-j999h
    Sep  6 10:33:02.456: INFO: Created: latency-svc-dfjjs
    Sep  6 10:33:02.456: INFO: Created: latency-svc-t2fcn
    Sep  6 10:33:02.456: INFO: Created: latency-svc-dspcs
    Sep  6 10:33:02.456: INFO: Created: latency-svc-8mfnv
    Sep  6 10:33:02.466: INFO: Created: latency-svc-2mm5w
    Sep  6 10:33:02.467: INFO: Created: latency-svc-qvxsk
    Sep  6 10:33:02.467: INFO: Created: latency-svc-phkxf
    Sep  6 10:33:02.467: INFO: Created: latency-svc-gt9rf
    Sep  6 10:33:02.474: INFO: Created: latency-svc-4ck7f
    Sep  6 10:33:02.485: INFO: Got endpoints: latency-svc-s6pjm [411.9807ms]
    Sep  6 10:33:02.485: INFO: Got endpoints: latency-svc-dspcs [279.908905ms]
    Sep  6 10:33:02.491: INFO: Got endpoints: latency-svc-j999h [305.393622ms]
    Sep  6 10:33:02.497: INFO: Got endpoints: latency-svc-mjkrd [440.509976ms]
    Sep  6 10:33:02.497: INFO: Got endpoints: latency-svc-4dbt7 [348.752506ms]
    Sep  6 10:33:02.497: INFO: Got endpoints: latency-svc-t2fcn [261.328808ms]
    Sep  6 10:33:02.512: INFO: Got endpoints: latency-svc-dfjjs [349.407201ms]
    Sep  6 10:33:02.512: INFO: Got endpoints: latency-svc-qvxsk [218.696094ms]
    Sep  6 10:33:02.512: INFO: Got endpoints: latency-svc-8mfnv [399.694002ms]
    Sep  6 10:33:02.520: INFO: Got endpoints: latency-svc-2mm5w [305.874103ms]
    Sep  6 10:33:02.529: INFO: Got endpoints: latency-svc-phkxf [436.344735ms]
    Sep  6 10:33:02.535: INFO: Got endpoints: latency-svc-gt9rf [503.16416ms]
    Sep  6 10:33:02.538: INFO: Got endpoints: latency-svc-gvjl4 [408.245876ms]
    Sep  6 10:33:02.539: INFO: Got endpoints: latency-svc-rh7cb [276.825645ms]
    Sep  6 10:33:02.540: INFO: Got endpoints: latency-svc-4ck7f [264.078807ms]
    Sep  6 10:33:02.558: INFO: Created: latency-svc-4jsj4
    Sep  6 10:33:02.574: INFO: Created: latency-svc-hh4gf
    Sep  6 10:33:02.577: INFO: Got endpoints: latency-svc-4jsj4 [92.213797ms]
    Sep  6 10:33:02.589: INFO: Created: latency-svc-vphq5
    Sep  6 10:33:02.604: INFO: Got endpoints: latency-svc-hh4gf [113.481242ms]
    Sep  6 10:33:02.611: INFO: Created: latency-svc-b4mtx
    Sep  6 10:33:02.627: INFO: Created: latency-svc-764jw
    Sep  6 10:33:02.649: INFO: Got endpoints: latency-svc-vphq5 [150.561588ms]
    Sep  6 10:33:02.656: INFO: Created: latency-svc-8szv5
    Sep  6 10:33:02.667: INFO: Created: latency-svc-frgr4
    Sep  6 10:33:02.679: INFO: Created: latency-svc-6l2pc
    Sep  6 10:33:02.699: INFO: Created: latency-svc-88gzj
    Sep  6 10:33:02.704: INFO: Got endpoints: latency-svc-b4mtx [202.454045ms]
    Sep  6 10:33:02.721: INFO: Created: latency-svc-222tc
    Sep  6 10:33:02.736: INFO: Created: latency-svc-jccj2
    Sep  6 10:33:02.736: INFO: Created: latency-svc-bhfb6
    Sep  6 10:33:02.755: INFO: Created: latency-svc-x4vbj
    Sep  6 10:33:02.771: INFO: Got endpoints: latency-svc-764jw [270.125725ms]
    Sep  6 10:33:02.785: INFO: Created: latency-svc-xgsff
    Sep  6 10:33:02.796: INFO: Got endpoints: latency-svc-8szv5 [283.801532ms]
    Sep  6 10:33:02.802: INFO: Created: latency-svc-vf2lb
    Sep  6 10:33:02.819: INFO: Created: latency-svc-h4nkg
    Sep  6 10:33:02.831: INFO: Created: latency-svc-9hpkg
    Sep  6 10:33:02.841: INFO: Created: latency-svc-9cl89
    Sep  6 10:33:02.857: INFO: Got endpoints: latency-svc-frgr4 [344.690444ms]
    Sep  6 10:33:02.863: INFO: Created: latency-svc-pl8tk
    Sep  6 10:33:02.876: INFO: Created: latency-svc-5n7nv
    Sep  6 10:33:02.885: INFO: Created: latency-svc-z4sgq
    Sep  6 10:33:02.900: INFO: Created: latency-svc-8ddbv
    Sep  6 10:33:02.903: INFO: Got endpoints: latency-svc-6l2pc [390.738029ms]
    Sep  6 10:33:02.925: INFO: Created: latency-svc-hkczl
    Sep  6 10:33:02.949: INFO: Got endpoints: latency-svc-88gzj [428.226295ms]
    Sep  6 10:33:02.966: INFO: Created: latency-svc-tbx4m
    Sep  6 10:33:02.996: INFO: Got endpoints: latency-svc-222tc [467.196613ms]
    Sep  6 10:33:03.023: INFO: Created: latency-svc-v4bpx
    Sep  6 10:33:03.054: INFO: Got endpoints: latency-svc-jccj2 [569.402658ms]
    Sep  6 10:33:03.090: INFO: Created: latency-svc-hxn5g
    Sep  6 10:33:03.108: INFO: Got endpoints: latency-svc-bhfb6 [573.126173ms]
    Sep  6 10:33:03.127: INFO: Created: latency-svc-jqpzz
    Sep  6 10:33:03.147: INFO: Got endpoints: latency-svc-x4vbj [607.86356ms]
    Sep  6 10:33:03.173: INFO: Created: latency-svc-mgtn2
    Sep  6 10:33:03.195: INFO: Got endpoints: latency-svc-xgsff [656.675398ms]
    Sep  6 10:33:03.211: INFO: Created: latency-svc-kzpr7
    Sep  6 10:33:03.251: INFO: Got endpoints: latency-svc-vf2lb [711.02534ms]
    Sep  6 10:33:03.266: INFO: Created: latency-svc-85rck
    Sep  6 10:33:03.303: INFO: Got endpoints: latency-svc-h4nkg [726.37995ms]
    Sep  6 10:33:03.324: INFO: Created: latency-svc-bdhrt
    Sep  6 10:33:03.349: INFO: Got endpoints: latency-svc-9hpkg [744.391426ms]
    Sep  6 10:33:03.368: INFO: Created: latency-svc-npgvs
    Sep  6 10:33:03.404: INFO: Got endpoints: latency-svc-9cl89 [755.35427ms]
    Sep  6 10:33:03.431: INFO: Created: latency-svc-48jln
    Sep  6 10:33:03.470: INFO: Got endpoints: latency-svc-pl8tk [766.208127ms]
    Sep  6 10:33:03.491: INFO: Created: latency-svc-59pcm
    Sep  6 10:33:03.511: INFO: Got endpoints: latency-svc-5n7nv [740.063771ms]
    Sep  6 10:33:03.535: INFO: Created: latency-svc-6f5nb
    Sep  6 10:33:03.549: INFO: Got endpoints: latency-svc-z4sgq [753.437204ms]
    Sep  6 10:33:03.569: INFO: Created: latency-svc-bcrzp
    Sep  6 10:33:03.603: INFO: Got endpoints: latency-svc-8ddbv [746.530264ms]
    Sep  6 10:33:03.652: INFO: Created: latency-svc-b5mjq
    Sep  6 10:33:03.655: INFO: Got endpoints: latency-svc-hkczl [752.361986ms]
    Sep  6 10:33:03.687: INFO: Created: latency-svc-lwxqv
    Sep  6 10:33:03.698: INFO: Got endpoints: latency-svc-tbx4m [749.475992ms]
    Sep  6 10:33:03.731: INFO: Created: latency-svc-nqbzv
    Sep  6 10:33:03.754: INFO: Got endpoints: latency-svc-v4bpx [758.379711ms]
    Sep  6 10:33:03.779: INFO: Created: latency-svc-dvr96
    Sep  6 10:33:03.798: INFO: Got endpoints: latency-svc-hxn5g [744.137056ms]
    Sep  6 10:33:03.821: INFO: Created: latency-svc-l6h8p
    Sep  6 10:33:03.851: INFO: Got endpoints: latency-svc-jqpzz [743.437234ms]
    Sep  6 10:33:03.876: INFO: Created: latency-svc-hrtwc
    Sep  6 10:33:03.907: INFO: Got endpoints: latency-svc-mgtn2 [759.605103ms]
    Sep  6 10:33:03.930: INFO: Created: latency-svc-dwbtb
    Sep  6 10:33:03.951: INFO: Got endpoints: latency-svc-kzpr7 [755.788766ms]
    Sep  6 10:33:03.976: INFO: Created: latency-svc-xs575
    Sep  6 10:33:04.002: INFO: Got endpoints: latency-svc-85rck [751.448442ms]
    Sep  6 10:33:04.022: INFO: Created: latency-svc-vnk59
    Sep  6 10:33:04.048: INFO: Got endpoints: latency-svc-bdhrt [744.64982ms]
    Sep  6 10:33:04.073: INFO: Created: latency-svc-4mgs2
    Sep  6 10:33:04.100: INFO: Got endpoints: latency-svc-npgvs [751.604954ms]
    Sep  6 10:33:04.122: INFO: Created: latency-svc-p8ks9
    Sep  6 10:33:04.162: INFO: Got endpoints: latency-svc-48jln [757.994391ms]
    Sep  6 10:33:04.182: INFO: Created: latency-svc-bwnqq
    Sep  6 10:33:04.197: INFO: Got endpoints: latency-svc-59pcm [726.856452ms]
    Sep  6 10:33:04.212: INFO: Created: latency-svc-xbsgr
    Sep  6 10:33:04.247: INFO: Got endpoints: latency-svc-6f5nb [735.617731ms]
    Sep  6 10:33:04.267: INFO: Created: latency-svc-z4xhz
    Sep  6 10:33:04.301: INFO: Got endpoints: latency-svc-bcrzp [751.105612ms]
    Sep  6 10:33:04.315: INFO: Created: latency-svc-vh9lh
    Sep  6 10:33:04.352: INFO: Got endpoints: latency-svc-b5mjq [748.583458ms]
    Sep  6 10:33:04.370: INFO: Created: latency-svc-xnrpk
    Sep  6 10:33:04.397: INFO: Got endpoints: latency-svc-lwxqv [741.780368ms]
    Sep  6 10:33:04.415: INFO: Created: latency-svc-ph4hp
    Sep  6 10:33:04.449: INFO: Got endpoints: latency-svc-nqbzv [750.929183ms]
    Sep  6 10:33:04.464: INFO: Created: latency-svc-gh7fr
    Sep  6 10:33:04.500: INFO: Got endpoints: latency-svc-dvr96 [745.997108ms]
    Sep  6 10:33:04.523: INFO: Created: latency-svc-jjdpm
    Sep  6 10:33:04.549: INFO: Got endpoints: latency-svc-l6h8p [750.323314ms]
    Sep  6 10:33:04.567: INFO: Created: latency-svc-tvqsm
    Sep  6 10:33:04.599: INFO: Got endpoints: latency-svc-hrtwc [747.667393ms]
    Sep  6 10:33:04.616: INFO: Created: latency-svc-2llbb
    Sep  6 10:33:04.648: INFO: Got endpoints: latency-svc-dwbtb [739.759387ms]
    Sep  6 10:33:04.692: INFO: Created: latency-svc-vkwp8
    Sep  6 10:33:04.950: INFO: Got endpoints: latency-svc-4mgs2 [901.613042ms]
    Sep  6 10:33:04.950: INFO: Got endpoints: latency-svc-xs575 [998.593013ms]
    Sep  6 10:33:04.953: INFO: Got endpoints: latency-svc-vnk59 [951.090839ms]
    Sep  6 10:33:04.986: INFO: Got endpoints: latency-svc-bwnqq [823.372418ms]
    Sep  6 10:33:05.141: INFO: Got endpoints: latency-svc-p8ks9 [1.040539898s]
    Sep  6 10:33:05.145: INFO: Got endpoints: latency-svc-xbsgr [948.609205ms]
    Sep  6 10:33:05.184: INFO: Got endpoints: latency-svc-vh9lh [883.679431ms]
    Sep  6 10:33:05.225: INFO: Got endpoints: latency-svc-xnrpk [872.785464ms]
    Sep  6 10:33:05.464: INFO: Got endpoints: latency-svc-z4xhz [1.21724352s]
    Sep  6 10:33:05.475: INFO: Got endpoints: latency-svc-jjdpm [974.410316ms]
    Sep  6 10:33:05.475: INFO: Got endpoints: latency-svc-ph4hp [1.078084513s]
    Sep  6 10:33:05.476: INFO: Got endpoints: latency-svc-gh7fr [1.027094017s]
    Sep  6 10:33:05.715: INFO: Got endpoints: latency-svc-tvqsm [1.16619449s]
    Sep  6 10:33:05.715: INFO: Got endpoints: latency-svc-vkwp8 [1.067491899s]
    Sep  6 10:33:05.716: INFO: Got endpoints: latency-svc-2llbb [1.116531535s]
    Sep  6 10:33:05.785: INFO: Created: latency-svc-87sz7
    Sep  6 10:33:05.833: INFO: Created: latency-svc-27nkt
    Sep  6 10:33:05.960: INFO: Created: latency-svc-pb82t
    Sep  6 10:33:06.005: INFO: Got endpoints: latency-svc-87sz7 [1.05153811s]
    Sep  6 10:33:06.006: INFO: Got endpoints: latency-svc-27nkt [1.055994659s]
    Sep  6 10:33:06.037: INFO: Got endpoints: latency-svc-pb82t [1.083122752s]
    Sep  6 10:33:06.100: INFO: Created: latency-svc-k4v8g
    Sep  6 10:33:06.119: INFO: Created: latency-svc-hhwhl
    Sep  6 10:33:06.125: INFO: Got endpoints: latency-svc-k4v8g [1.139349478s]
    Sep  6 10:33:06.139: INFO: Got endpoints: latency-svc-hhwhl [997.960839ms]
    Sep  6 10:33:06.145: INFO: Created: latency-svc-qt679
    Sep  6 10:33:06.162: INFO: Got endpoints: latency-svc-qt679 [1.016116587s]
    Sep  6 10:33:06.178: INFO: Created: latency-svc-zdzd5
    Sep  6 10:33:06.195: INFO: Created: latency-svc-m7gmw
    Sep  6 10:33:06.196: INFO: Got endpoints: latency-svc-zdzd5 [1.011533687s]
    Sep  6 10:33:06.218: INFO: Got endpoints: latency-svc-m7gmw [992.194743ms]
    Sep  6 10:33:06.218: INFO: Created: latency-svc-n5rzm
    Sep  6 10:33:06.232: INFO: Got endpoints: latency-svc-n5rzm [767.439233ms]
    Sep  6 10:33:06.247: INFO: Created: latency-svc-8hb68
    Sep  6 10:33:06.270: INFO: Created: latency-svc-4784g
    Sep  6 10:33:06.283: INFO: Got endpoints: latency-svc-4784g [807.15206ms]
    Sep  6 10:33:06.284: INFO: Got endpoints: latency-svc-8hb68 [808.563448ms]
    Sep  6 10:33:06.286: INFO: Created: latency-svc-g9b7l
    Sep  6 10:33:06.308: INFO: Got endpoints: latency-svc-g9b7l [831.073863ms]
    Sep  6 10:33:06.318: INFO: Created: latency-svc-f8vnj
    Sep  6 10:33:06.328: INFO: Got endpoints: latency-svc-f8vnj [612.687641ms]
    Sep  6 10:33:06.334: INFO: Created: latency-svc-xhxw7
    Sep  6 10:33:06.351: INFO: Created: latency-svc-xxbc4
    Sep  6 10:33:06.353: INFO: Got endpoints: latency-svc-xhxw7 [637.472634ms]
    Sep  6 10:33:06.371: INFO: Got endpoints: latency-svc-xxbc4 [654.987407ms]
    Sep  6 10:33:06.380: INFO: Created: latency-svc-wtbmg
    Sep  6 10:33:06.395: INFO: Created: latency-svc-4cvlb
    Sep  6 10:33:06.398: INFO: Got endpoints: latency-svc-wtbmg [392.665059ms]
    Sep  6 10:33:06.412: INFO: Got endpoints: latency-svc-4cvlb [406.61405ms]
    Sep  6 10:33:06.421: INFO: Created: latency-svc-jwtjf
    Sep  6 10:33:06.433: INFO: Got endpoints: latency-svc-jwtjf [396.734126ms]
    Sep  6 10:33:06.450: INFO: Created: latency-svc-hszdw
    Sep  6 10:33:06.453: INFO: Created: latency-svc-54776
    Sep  6 10:33:06.457: INFO: Got endpoints: latency-svc-hszdw [331.763042ms]
    Sep  6 10:33:06.469: INFO: Got endpoints: latency-svc-54776 [330.308741ms]
    Sep  6 10:33:06.476: INFO: Created: latency-svc-gfjr5
    Sep  6 10:33:06.486: INFO: Got endpoints: latency-svc-gfjr5 [324.012592ms]
    Sep  6 10:33:06.499: INFO: Created: latency-svc-mfdmm
    Sep  6 10:33:06.509: INFO: Created: latency-svc-nfnmd
    Sep  6 10:33:06.514: INFO: Got endpoints: latency-svc-mfdmm [316.353791ms]
    Sep  6 10:33:06.527: INFO: Created: latency-svc-ln46z
    Sep  6 10:33:06.542: INFO: Created: latency-svc-4l6sn
    Sep  6 10:33:06.568: INFO: Got endpoints: latency-svc-nfnmd [350.247307ms]
    Sep  6 10:33:06.575: INFO: Created: latency-svc-qmd2l
    Sep  6 10:33:06.592: INFO: Created: latency-svc-wbnfw
    Sep  6 10:33:06.602: INFO: Got endpoints: latency-svc-ln46z [370.508547ms]
    Sep  6 10:33:06.607: INFO: Created: latency-svc-x2k6f
    Sep  6 10:33:06.629: INFO: Created: latency-svc-bxfmq
    Sep  6 10:33:06.643: INFO: Created: latency-svc-wvvdp
    Sep  6 10:33:06.654: INFO: Got endpoints: latency-svc-4l6sn [370.834096ms]
    Sep  6 10:33:06.659: INFO: Created: latency-svc-s6q2m
    Sep  6 10:33:06.678: INFO: Created: latency-svc-bddsd
    Sep  6 10:33:06.686: INFO: Created: latency-svc-wxqmc
    Sep  6 10:33:06.705: INFO: Got endpoints: latency-svc-qmd2l [421.910573ms]
    Sep  6 10:33:06.714: INFO: Created: latency-svc-pnfpt
    Sep  6 10:33:06.719: INFO: Created: latency-svc-8vp6k
    Sep  6 10:33:06.731: INFO: Created: latency-svc-wfm9k
    Sep  6 10:33:06.754: INFO: Got endpoints: latency-svc-wbnfw [445.783479ms]
    Sep  6 10:33:06.763: INFO: Created: latency-svc-6nf2f
    Sep  6 10:33:06.763: INFO: Created: latency-svc-xt8zx
    Sep  6 10:33:06.791: INFO: Created: latency-svc-89cjm
    Sep  6 10:33:06.804: INFO: Got endpoints: latency-svc-x2k6f [476.308402ms]
    Sep  6 10:33:06.812: INFO: Created: latency-svc-m9n5g
    Sep  6 10:33:06.822: INFO: Created: latency-svc-w52m7
    Sep  6 10:33:06.832: INFO: Created: latency-svc-gt25n
    Sep  6 10:33:06.852: INFO: Got endpoints: latency-svc-bxfmq [499.427461ms]
    Sep  6 10:33:06.853: INFO: Created: latency-svc-j7dhn
    Sep  6 10:33:06.871: INFO: Created: latency-svc-5zsk7
    Sep  6 10:33:06.897: INFO: Got endpoints: latency-svc-wvvdp [526.328959ms]
    Sep  6 10:33:06.911: INFO: Created: latency-svc-jsnlw
    Sep  6 10:33:06.953: INFO: Got endpoints: latency-svc-s6q2m [555.30973ms]
    Sep  6 10:33:06.969: INFO: Created: latency-svc-5wc8c
    Sep  6 10:33:07.000: INFO: Got endpoints: latency-svc-bddsd [587.67514ms]
    Sep  6 10:33:07.017: INFO: Created: latency-svc-gkwpj
    Sep  6 10:33:07.046: INFO: Got endpoints: latency-svc-wxqmc [613.009209ms]
    Sep  6 10:33:07.072: INFO: Created: latency-svc-p7lk5
    Sep  6 10:33:07.097: INFO: Got endpoints: latency-svc-pnfpt [639.60718ms]
    Sep  6 10:33:07.116: INFO: Created: latency-svc-cpswz
    Sep  6 10:33:07.152: INFO: Got endpoints: latency-svc-8vp6k [682.577639ms]
    Sep  6 10:33:07.176: INFO: Created: latency-svc-fg6mm
    Sep  6 10:33:07.198: INFO: Got endpoints: latency-svc-wfm9k [712.099784ms]
    Sep  6 10:33:07.214: INFO: Created: latency-svc-hz98b
    Sep  6 10:33:07.247: INFO: Got endpoints: latency-svc-xt8zx [732.481443ms]
    Sep  6 10:33:07.265: INFO: Created: latency-svc-v6gvc
    Sep  6 10:33:07.310: INFO: Got endpoints: latency-svc-6nf2f [742.344654ms]
    Sep  6 10:33:07.325: INFO: Created: latency-svc-cdmlg
    Sep  6 10:33:07.352: INFO: Got endpoints: latency-svc-89cjm [749.427194ms]
    Sep  6 10:33:07.368: INFO: Created: latency-svc-fcpwb
    Sep  6 10:33:07.401: INFO: Got endpoints: latency-svc-m9n5g [746.211977ms]
    Sep  6 10:33:07.423: INFO: Created: latency-svc-65fr8
    Sep  6 10:33:07.447: INFO: Got endpoints: latency-svc-w52m7 [741.954903ms]
    Sep  6 10:33:07.469: INFO: Created: latency-svc-cbnvq
    Sep  6 10:33:07.499: INFO: Got endpoints: latency-svc-gt25n [745.195659ms]
    Sep  6 10:33:07.522: INFO: Created: latency-svc-wbqqt
    Sep  6 10:33:07.548: INFO: Got endpoints: latency-svc-j7dhn [743.378107ms]
    Sep  6 10:33:07.572: INFO: Created: latency-svc-wgkgz
    Sep  6 10:33:07.604: INFO: Got endpoints: latency-svc-5zsk7 [751.098781ms]
    Sep  6 10:33:07.623: INFO: Created: latency-svc-857hs
    Sep  6 10:33:07.646: INFO: Got endpoints: latency-svc-jsnlw [749.266005ms]
    Sep  6 10:33:07.683: INFO: Created: latency-svc-qz2v2
    Sep  6 10:33:07.698: INFO: Got endpoints: latency-svc-5wc8c [744.060344ms]
    Sep  6 10:33:07.714: INFO: Created: latency-svc-mz24z
    Sep  6 10:33:07.751: INFO: Got endpoints: latency-svc-gkwpj [750.66297ms]
    Sep  6 10:33:07.766: INFO: Created: latency-svc-vlrl9
    Sep  6 10:33:07.799: INFO: Got endpoints: latency-svc-p7lk5 [752.190411ms]
    Sep  6 10:33:07.820: INFO: Created: latency-svc-mrbd7
    Sep  6 10:33:07.851: INFO: Got endpoints: latency-svc-cpswz [754.013579ms]
    Sep  6 10:33:07.870: INFO: Created: latency-svc-pm9n6
    Sep  6 10:33:07.899: INFO: Got endpoints: latency-svc-fg6mm [747.103971ms]
    Sep  6 10:33:07.915: INFO: Created: latency-svc-lcnqn
    Sep  6 10:33:07.952: INFO: Got endpoints: latency-svc-hz98b [753.300039ms]
    Sep  6 10:33:07.968: INFO: Created: latency-svc-w89wg
    Sep  6 10:33:07.997: INFO: Got endpoints: latency-svc-v6gvc [749.763661ms]
    Sep  6 10:33:08.018: INFO: Created: latency-svc-k94c7
    Sep  6 10:33:08.047: INFO: Got endpoints: latency-svc-cdmlg [736.529089ms]
    Sep  6 10:33:08.063: INFO: Created: latency-svc-68jb6
    Sep  6 10:33:08.104: INFO: Got endpoints: latency-svc-fcpwb [752.735792ms]
    Sep  6 10:33:08.124: INFO: Created: latency-svc-ldpr9
    Sep  6 10:33:08.149: INFO: Got endpoints: latency-svc-65fr8 [747.867109ms]
    Sep  6 10:33:08.169: INFO: Created: latency-svc-p6r6g
    Sep  6 10:33:08.202: INFO: Got endpoints: latency-svc-cbnvq [754.530463ms]
    Sep  6 10:33:08.219: INFO: Created: latency-svc-j59k9
    Sep  6 10:33:08.250: INFO: Got endpoints: latency-svc-wbqqt [750.903629ms]
    Sep  6 10:33:08.265: INFO: Created: latency-svc-k5wv6
    Sep  6 10:33:08.298: INFO: Got endpoints: latency-svc-wgkgz [750.685309ms]
    Sep  6 10:33:08.317: INFO: Created: latency-svc-d5nxm
    Sep  6 10:33:08.350: INFO: Got endpoints: latency-svc-857hs [746.884708ms]
    Sep  6 10:33:08.366: INFO: Created: latency-svc-kq7ch
    Sep  6 10:33:08.401: INFO: Got endpoints: latency-svc-qz2v2 [754.862149ms]
    Sep  6 10:33:08.415: INFO: Created: latency-svc-hzh47
    Sep  6 10:33:08.448: INFO: Got endpoints: latency-svc-mz24z [750.015262ms]
    Sep  6 10:33:08.467: INFO: Created: latency-svc-ckrdb
    Sep  6 10:33:08.497: INFO: Got endpoints: latency-svc-vlrl9 [746.276541ms]
    Sep  6 10:33:08.512: INFO: Created: latency-svc-s7j5l
    Sep  6 10:33:08.548: INFO: Got endpoints: latency-svc-mrbd7 [748.900442ms]
    Sep  6 10:33:08.575: INFO: Created: latency-svc-slbnh
    Sep  6 10:33:08.601: INFO: Got endpoints: latency-svc-pm9n6 [750.392301ms]
    Sep  6 10:33:08.628: INFO: Created: latency-svc-gtq5b
    Sep  6 10:33:08.653: INFO: Got endpoints: latency-svc-lcnqn [753.601506ms]
    Sep  6 10:33:08.671: INFO: Created: latency-svc-5flxv
    Sep  6 10:33:08.696: INFO: Got endpoints: latency-svc-w89wg [744.097348ms]
    Sep  6 10:33:08.717: INFO: Created: latency-svc-cjj9f
    Sep  6 10:33:08.751: INFO: Got endpoints: latency-svc-k94c7 [754.349579ms]
    Sep  6 10:33:08.769: INFO: Created: latency-svc-wb9wt
    Sep  6 10:33:08.801: INFO: Got endpoints: latency-svc-68jb6 [753.604818ms]
    Sep  6 10:33:08.818: INFO: Created: latency-svc-fbz49
    Sep  6 10:33:08.849: INFO: Got endpoints: latency-svc-ldpr9 [743.952456ms]
    Sep  6 10:33:08.864: INFO: Created: latency-svc-t8bxl
    Sep  6 10:33:08.905: INFO: Got endpoints: latency-svc-p6r6g [756.217761ms]
    Sep  6 10:33:08.924: INFO: Created: latency-svc-xvr27
    Sep  6 10:33:08.949: INFO: Got endpoints: latency-svc-j59k9 [747.005347ms]
    Sep  6 10:33:08.967: INFO: Created: latency-svc-lzlh9
    Sep  6 10:33:09.003: INFO: Got endpoints: latency-svc-k5wv6 [752.723762ms]
    Sep  6 10:33:09.023: INFO: Created: latency-svc-bxn8t
    Sep  6 10:33:09.052: INFO: Got endpoints: latency-svc-d5nxm [753.069799ms]
    Sep  6 10:33:09.071: INFO: Created: latency-svc-kjcdc
    Sep  6 10:33:09.100: INFO: Got endpoints: latency-svc-kq7ch [749.670397ms]
    Sep  6 10:33:09.118: INFO: Created: latency-svc-s2nrb
    Sep  6 10:33:09.152: INFO: Got endpoints: latency-svc-hzh47 [750.89819ms]
    Sep  6 10:33:09.197: INFO: Got endpoints: latency-svc-ckrdb [749.26711ms]
    Sep  6 10:33:09.256: INFO: Got endpoints: latency-svc-s7j5l [758.438277ms]
    Sep  6 10:33:09.296: INFO: Got endpoints: latency-svc-slbnh [748.251961ms]
    Sep  6 10:33:09.348: INFO: Got endpoints: latency-svc-gtq5b [746.561116ms]
    Sep  6 10:33:09.395: INFO: Got endpoints: latency-svc-5flxv [741.617322ms]
    Sep  6 10:33:09.453: INFO: Got endpoints: latency-svc-cjj9f [756.367092ms]
    Sep  6 10:33:09.496: INFO: Got endpoints: latency-svc-wb9wt [745.135046ms]
    Sep  6 10:33:09.550: INFO: Got endpoints: latency-svc-fbz49 [749.42086ms]
    Sep  6 10:33:09.596: INFO: Got endpoints: latency-svc-t8bxl [746.831851ms]
    Sep  6 10:33:09.657: INFO: Got endpoints: latency-svc-xvr27 [751.65994ms]
    Sep  6 10:33:09.699: INFO: Got endpoints: latency-svc-lzlh9 [749.26273ms]
    Sep  6 10:33:09.749: INFO: Got endpoints: latency-svc-bxn8t [746.389831ms]
    Sep  6 10:33:09.803: INFO: Got endpoints: latency-svc-kjcdc [751.045111ms]
    Sep  6 10:33:09.848: INFO: Got endpoints: latency-svc-s2nrb [747.098916ms]
    Sep  6 10:33:09.848: INFO: Latencies: [92.213797ms 113.481242ms 150.561588ms 202.454045ms 218.696094ms 261.328808ms 264.078807ms 270.125725ms 276.825645ms 279.908905ms 282.274779ms 283.801532ms 289.470088ms 290.467142ms 291.069041ms 295.16995ms 300.250589ms 304.662142ms 305.393622ms 305.874103ms 309.779648ms 311.887523ms 314.502037ms 315.587507ms 316.353791ms 316.407336ms 317.098557ms 320.175673ms 322.145329ms 324.012592ms 329.499972ms 330.308741ms 331.763042ms 332.492324ms 342.522397ms 343.056279ms 344.690444ms 348.752506ms 349.407201ms 350.247307ms 366.442169ms 370.508547ms 370.834096ms 374.78735ms 390.738029ms 392.665059ms 396.734126ms 399.694002ms 406.61405ms 408.245876ms 411.9807ms 421.910573ms 428.226295ms 436.344735ms 440.509976ms 445.783479ms 467.196613ms 476.308402ms 484.18207ms 499.427461ms 503.16416ms 526.328959ms 536.828972ms 555.30973ms 569.402658ms 573.126173ms 577.462468ms 587.67514ms 607.86356ms 612.687641ms 613.009209ms 637.472634ms 639.60718ms 653.477479ms 654.987407ms 656.675398ms 682.577639ms 711.02534ms 712.099784ms 724.059951ms 726.37995ms 726.856452ms 732.481443ms 735.617731ms 736.529089ms 739.759387ms 740.063771ms 741.255973ms 741.617322ms 741.780368ms 741.954903ms 742.344654ms 743.378107ms 743.437234ms 743.952456ms 744.060344ms 744.097348ms 744.137056ms 744.391426ms 744.64982ms 745.135046ms 745.195659ms 745.997108ms 746.211977ms 746.276541ms 746.389831ms 746.530264ms 746.561116ms 746.831851ms 746.884708ms 747.005347ms 747.098916ms 747.103971ms 747.667393ms 747.867109ms 748.251961ms 748.583458ms 748.900442ms 749.26273ms 749.266005ms 749.26711ms 749.42086ms 749.427194ms 749.475992ms 749.670397ms 749.763661ms 750.015262ms 750.323314ms 750.392301ms 750.66297ms 750.685309ms 750.89819ms 750.903629ms 750.929183ms 751.045111ms 751.098781ms 751.105612ms 751.448442ms 751.604954ms 751.65994ms 752.190411ms 752.361986ms 752.723762ms 752.735792ms 753.069799ms 753.300039ms 753.437204ms 753.601506ms 753.604818ms 754.013579ms 754.349579ms 754.530463ms 754.862149ms 755.35427ms 755.788766ms 756.217761ms 756.367092ms 757.994391ms 758.379711ms 758.438277ms 759.605103ms 766.208127ms 767.439233ms 784.354036ms 807.15206ms 808.563448ms 810.010171ms 823.372418ms 831.073863ms 872.785464ms 883.679431ms 901.613042ms 904.66856ms 910.45058ms 927.484735ms 941.115922ms 948.609205ms 951.090839ms 965.169813ms 973.837262ms 974.410316ms 992.194743ms 997.960839ms 998.593013ms 1.006061664s 1.011533687s 1.016116587s 1.027094017s 1.027761166s 1.040539898s 1.049980302s 1.05153811s 1.055994659s 1.067491899s 1.078084513s 1.083122752s 1.116531535s 1.139349478s 1.16619449s 1.21724352s]
    Sep  6 10:33:09.848: INFO: 50 %ile: 745.135046ms
    Sep  6 10:33:09.848: INFO: 90 %ile: 974.410316ms
    Sep  6 10:33:09.848: INFO: 99 %ile: 1.16619449s
    Sep  6 10:33:09.848: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:33:09.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-8755" for this suite. 09/06/23 10:33:09.855
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:33:09.868
Sep  6 10:33:09.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:33:09.873
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:33:09.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:33:09.929
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-902aac30-0cff-4909-b924-5bb4fb916d8c 09/06/23 10:33:09.933
STEP: Creating a pod to test consume configMaps 09/06/23 10:33:09.944
Sep  6 10:33:09.958: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b" in namespace "projected-6720" to be "Succeeded or Failed"
Sep  6 10:33:09.976: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.748336ms
Sep  6 10:33:11.990: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b": Phase="Running", Reason="", readiness=true. Elapsed: 2.031739465s
Sep  6 10:33:14.064: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b": Phase="Running", Reason="", readiness=false. Elapsed: 4.105130587s
Sep  6 10:33:15.985: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026261567s
STEP: Saw pod success 09/06/23 10:33:15.985
Sep  6 10:33:15.985: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b" satisfied condition "Succeeded or Failed"
Sep  6 10:33:15.991: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b container agnhost-container: <nil>
STEP: delete the pod 09/06/23 10:33:16.006
Sep  6 10:33:16.032: INFO: Waiting for pod pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b to disappear
Sep  6 10:33:16.041: INFO: Pod pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:33:16.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6720" for this suite. 09/06/23 10:33:16.048
------------------------------
• [SLOW TEST] [6.196 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:33:09.868
    Sep  6 10:33:09.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:33:09.873
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:33:09.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:33:09.929
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-902aac30-0cff-4909-b924-5bb4fb916d8c 09/06/23 10:33:09.933
    STEP: Creating a pod to test consume configMaps 09/06/23 10:33:09.944
    Sep  6 10:33:09.958: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b" in namespace "projected-6720" to be "Succeeded or Failed"
    Sep  6 10:33:09.976: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.748336ms
    Sep  6 10:33:11.990: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b": Phase="Running", Reason="", readiness=true. Elapsed: 2.031739465s
    Sep  6 10:33:14.064: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b": Phase="Running", Reason="", readiness=false. Elapsed: 4.105130587s
    Sep  6 10:33:15.985: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026261567s
    STEP: Saw pod success 09/06/23 10:33:15.985
    Sep  6 10:33:15.985: INFO: Pod "pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b" satisfied condition "Succeeded or Failed"
    Sep  6 10:33:15.991: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 10:33:16.006
    Sep  6 10:33:16.032: INFO: Waiting for pod pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b to disappear
    Sep  6 10:33:16.041: INFO: Pod pod-projected-configmaps-2c39896b-0bce-4be3-b9b8-1dfbe0e5e92b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:33:16.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6720" for this suite. 09/06/23 10:33:16.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:33:16.066
Sep  6 10:33:16.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename aggregator 09/06/23 10:33:16.069
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:33:16.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:33:16.104
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Sep  6 10:33:16.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 09/06/23 10:33:16.112
Sep  6 10:33:16.672: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  6 10:33:18.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:20.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:23.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:24.767: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:29.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:33.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:35.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:37.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:38.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:40.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:43.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:44.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:46.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:48.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:50.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:52.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:54.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:56.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:33:58.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:34:00.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:34:02.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:34:04.767: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:34:06.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:34:09.050: INFO: Waited 195.909771ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 09/06/23 10:34:09.153
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/06/23 10:34:09.156
STEP: List APIServices 09/06/23 10:34:09.186
Sep  6 10:34:09.192: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Sep  6 10:34:09.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-9925" for this suite. 09/06/23 10:34:09.833
------------------------------
• [SLOW TEST] [53.775 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:33:16.066
    Sep  6 10:33:16.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename aggregator 09/06/23 10:33:16.069
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:33:16.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:33:16.104
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Sep  6 10:33:16.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 09/06/23 10:33:16.112
    Sep  6 10:33:16.672: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Sep  6 10:33:18.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:20.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:23.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:24.767: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:29.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:33.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:35.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:37.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:38.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:40.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:43.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:44.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:46.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:48.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:50.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:52.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:54.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:56.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:33:58.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:34:00.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:34:02.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:34:04.767: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:34:06.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 33, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:34:09.050: INFO: Waited 195.909771ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 09/06/23 10:34:09.153
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/06/23 10:34:09.156
    STEP: List APIServices 09/06/23 10:34:09.186
    Sep  6 10:34:09.192: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:34:09.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-9925" for this suite. 09/06/23 10:34:09.833
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:34:09.842
Sep  6 10:34:09.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename daemonsets 09/06/23 10:34:09.843
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:34:09.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:34:09.868
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Sep  6 10:34:09.904: INFO: Create a RollingUpdate DaemonSet
Sep  6 10:34:09.910: INFO: Check that daemon pods launch on every node of the cluster
Sep  6 10:34:09.923: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 10:34:09.924: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 10:34:10.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  6 10:34:10.946: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:34:11.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 10:34:11.941: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Sep  6 10:34:11.941: INFO: Update the DaemonSet to trigger a rollout
Sep  6 10:34:11.973: INFO: Updating DaemonSet daemon-set
Sep  6 10:34:14.999: INFO: Roll back the DaemonSet before rollout is complete
Sep  6 10:34:15.011: INFO: Updating DaemonSet daemon-set
Sep  6 10:34:15.011: INFO: Make sure DaemonSet rollback is complete
Sep  6 10:34:15.014: INFO: Wrong image for pod: daemon-set-7qh7s. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Sep  6 10:34:15.014: INFO: Pod daemon-set-7qh7s is not available
Sep  6 10:34:19.036: INFO: Pod daemon-set-2bwpm is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/06/23 10:34:19.073
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4797, will wait for the garbage collector to delete the pods 09/06/23 10:34:19.073
Sep  6 10:34:19.138: INFO: Deleting DaemonSet.extensions daemon-set took: 12.260138ms
Sep  6 10:34:19.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 301.694886ms
Sep  6 10:34:22.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 10:34:22.146: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  6 10:34:22.149: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13514"},"items":null}

Sep  6 10:34:22.154: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13514"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:34:22.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4797" for this suite. 09/06/23 10:34:22.173
------------------------------
• [SLOW TEST] [12.339 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:34:09.842
    Sep  6 10:34:09.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename daemonsets 09/06/23 10:34:09.843
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:34:09.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:34:09.868
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Sep  6 10:34:09.904: INFO: Create a RollingUpdate DaemonSet
    Sep  6 10:34:09.910: INFO: Check that daemon pods launch on every node of the cluster
    Sep  6 10:34:09.923: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 10:34:09.924: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 10:34:10.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  6 10:34:10.946: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:34:11.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 10:34:11.941: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Sep  6 10:34:11.941: INFO: Update the DaemonSet to trigger a rollout
    Sep  6 10:34:11.973: INFO: Updating DaemonSet daemon-set
    Sep  6 10:34:14.999: INFO: Roll back the DaemonSet before rollout is complete
    Sep  6 10:34:15.011: INFO: Updating DaemonSet daemon-set
    Sep  6 10:34:15.011: INFO: Make sure DaemonSet rollback is complete
    Sep  6 10:34:15.014: INFO: Wrong image for pod: daemon-set-7qh7s. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Sep  6 10:34:15.014: INFO: Pod daemon-set-7qh7s is not available
    Sep  6 10:34:19.036: INFO: Pod daemon-set-2bwpm is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/06/23 10:34:19.073
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4797, will wait for the garbage collector to delete the pods 09/06/23 10:34:19.073
    Sep  6 10:34:19.138: INFO: Deleting DaemonSet.extensions daemon-set took: 12.260138ms
    Sep  6 10:34:19.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 301.694886ms
    Sep  6 10:34:22.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 10:34:22.146: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  6 10:34:22.149: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13514"},"items":null}

    Sep  6 10:34:22.154: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13514"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:34:22.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4797" for this suite. 09/06/23 10:34:22.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:34:22.182
Sep  6 10:34:22.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 10:34:22.182
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:34:22.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:34:22.199
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-7112 09/06/23 10:34:22.201
STEP: creating service affinity-nodeport in namespace services-7112 09/06/23 10:34:22.201
STEP: creating replication controller affinity-nodeport in namespace services-7112 09/06/23 10:34:22.214
I0906 10:34:23.951140      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7112, replica count: 3
I0906 10:34:27.002506      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 10:34:27.012: INFO: Creating new exec pod
Sep  6 10:34:27.020: INFO: Waiting up to 5m0s for pod "execpod-affinityc76bl" in namespace "services-7112" to be "running"
Sep  6 10:34:27.024: INFO: Pod "execpod-affinityc76bl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.291969ms
Sep  6 10:34:29.039: INFO: Pod "execpod-affinityc76bl": Phase="Running", Reason="", readiness=true. Elapsed: 2.018076576s
Sep  6 10:34:29.039: INFO: Pod "execpod-affinityc76bl" satisfied condition "running"
Sep  6 10:34:30.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Sep  6 10:34:30.236: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep  6 10:34:30.236: INFO: stdout: ""
Sep  6 10:34:30.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c nc -v -z -w 2 10.233.52.95 80'
Sep  6 10:34:30.354: INFO: stderr: "+ nc -v -z -w 2 10.233.52.95 80\nConnection to 10.233.52.95 80 port [tcp/http] succeeded!\n"
Sep  6 10:34:30.354: INFO: stdout: ""
Sep  6 10:34:30.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c nc -v -z -w 2 10.2.20.101 30095'
Sep  6 10:34:30.458: INFO: stderr: "+ nc -v -z -w 2 10.2.20.101 30095\nConnection to 10.2.20.101 30095 port [tcp/*] succeeded!\n"
Sep  6 10:34:30.458: INFO: stdout: ""
Sep  6 10:34:30.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c nc -v -z -w 2 10.2.20.103 30095'
Sep  6 10:34:30.570: INFO: stderr: "+ nc -v -z -w 2 10.2.20.103 30095\nConnection to 10.2.20.103 30095 port [tcp/*] succeeded!\n"
Sep  6 10:34:30.570: INFO: stdout: ""
Sep  6 10:34:30.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:30095/ ; done'
Sep  6 10:34:30.729: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n"
Sep  6 10:34:30.729: INFO: stdout: "\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77"
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
Sep  6 10:34:30.730: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7112, will wait for the garbage collector to delete the pods 09/06/23 10:34:30.748
Sep  6 10:34:30.817: INFO: Deleting ReplicationController affinity-nodeport took: 7.350035ms
Sep  6 10:34:30.918: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.813745ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 10:34:33.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7112" for this suite. 09/06/23 10:34:33.781
------------------------------
• [SLOW TEST] [11.612 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:34:22.182
    Sep  6 10:34:22.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 10:34:22.182
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:34:22.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:34:22.199
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-7112 09/06/23 10:34:22.201
    STEP: creating service affinity-nodeport in namespace services-7112 09/06/23 10:34:22.201
    STEP: creating replication controller affinity-nodeport in namespace services-7112 09/06/23 10:34:22.214
    I0906 10:34:23.951140      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7112, replica count: 3
    I0906 10:34:27.002506      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 10:34:27.012: INFO: Creating new exec pod
    Sep  6 10:34:27.020: INFO: Waiting up to 5m0s for pod "execpod-affinityc76bl" in namespace "services-7112" to be "running"
    Sep  6 10:34:27.024: INFO: Pod "execpod-affinityc76bl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.291969ms
    Sep  6 10:34:29.039: INFO: Pod "execpod-affinityc76bl": Phase="Running", Reason="", readiness=true. Elapsed: 2.018076576s
    Sep  6 10:34:29.039: INFO: Pod "execpod-affinityc76bl" satisfied condition "running"
    Sep  6 10:34:30.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Sep  6 10:34:30.236: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Sep  6 10:34:30.236: INFO: stdout: ""
    Sep  6 10:34:30.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c nc -v -z -w 2 10.233.52.95 80'
    Sep  6 10:34:30.354: INFO: stderr: "+ nc -v -z -w 2 10.233.52.95 80\nConnection to 10.233.52.95 80 port [tcp/http] succeeded!\n"
    Sep  6 10:34:30.354: INFO: stdout: ""
    Sep  6 10:34:30.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c nc -v -z -w 2 10.2.20.101 30095'
    Sep  6 10:34:30.458: INFO: stderr: "+ nc -v -z -w 2 10.2.20.101 30095\nConnection to 10.2.20.101 30095 port [tcp/*] succeeded!\n"
    Sep  6 10:34:30.458: INFO: stdout: ""
    Sep  6 10:34:30.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c nc -v -z -w 2 10.2.20.103 30095'
    Sep  6 10:34:30.570: INFO: stderr: "+ nc -v -z -w 2 10.2.20.103 30095\nConnection to 10.2.20.103 30095 port [tcp/*] succeeded!\n"
    Sep  6 10:34:30.570: INFO: stdout: ""
    Sep  6 10:34:30.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7112 exec execpod-affinityc76bl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:30095/ ; done'
    Sep  6 10:34:30.729: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30095/\n"
    Sep  6 10:34:30.729: INFO: stdout: "\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77\naffinity-nodeport-ptg77"
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.729: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.730: INFO: Received response from host: affinity-nodeport-ptg77
    Sep  6 10:34:30.730: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7112, will wait for the garbage collector to delete the pods 09/06/23 10:34:30.748
    Sep  6 10:34:30.817: INFO: Deleting ReplicationController affinity-nodeport took: 7.350035ms
    Sep  6 10:34:30.918: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.813745ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:34:33.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7112" for this suite. 09/06/23 10:34:33.781
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:34:33.794
Sep  6 10:34:33.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename statefulset 09/06/23 10:34:33.794
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:34:33.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:34:33.821
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2115 09/06/23 10:34:33.826
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-2115 09/06/23 10:34:33.844
Sep  6 10:34:33.864: INFO: Found 0 stateful pods, waiting for 1
Sep  6 10:34:43.880: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 09/06/23 10:34:43.909
STEP: Getting /status 09/06/23 10:34:43.926
Sep  6 10:34:43.931: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 09/06/23 10:34:43.931
Sep  6 10:34:43.947: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 09/06/23 10:34:43.947
Sep  6 10:34:43.949: INFO: Observed &StatefulSet event: ADDED
Sep  6 10:34:43.949: INFO: Found Statefulset ss in namespace statefulset-2115 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  6 10:34:43.949: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 09/06/23 10:34:43.949
Sep  6 10:34:43.949: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  6 10:34:43.957: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 09/06/23 10:34:43.957
Sep  6 10:34:43.959: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  6 10:34:43.959: INFO: Deleting all statefulset in ns statefulset-2115
Sep  6 10:34:43.962: INFO: Scaling statefulset ss to 0
Sep  6 10:34:53.983: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:34:53.987: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:34:54.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2115" for this suite. 09/06/23 10:34:54.006
------------------------------
• [SLOW TEST] [20.220 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:34:33.794
    Sep  6 10:34:33.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename statefulset 09/06/23 10:34:33.794
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:34:33.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:34:33.821
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2115 09/06/23 10:34:33.826
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-2115 09/06/23 10:34:33.844
    Sep  6 10:34:33.864: INFO: Found 0 stateful pods, waiting for 1
    Sep  6 10:34:43.880: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 09/06/23 10:34:43.909
    STEP: Getting /status 09/06/23 10:34:43.926
    Sep  6 10:34:43.931: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 09/06/23 10:34:43.931
    Sep  6 10:34:43.947: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 09/06/23 10:34:43.947
    Sep  6 10:34:43.949: INFO: Observed &StatefulSet event: ADDED
    Sep  6 10:34:43.949: INFO: Found Statefulset ss in namespace statefulset-2115 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  6 10:34:43.949: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 09/06/23 10:34:43.949
    Sep  6 10:34:43.949: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  6 10:34:43.957: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 09/06/23 10:34:43.957
    Sep  6 10:34:43.959: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  6 10:34:43.959: INFO: Deleting all statefulset in ns statefulset-2115
    Sep  6 10:34:43.962: INFO: Scaling statefulset ss to 0
    Sep  6 10:34:53.983: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 10:34:53.987: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:34:54.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2115" for this suite. 09/06/23 10:34:54.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:34:54.014
Sep  6 10:34:54.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename job 09/06/23 10:34:54.015
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:34:54.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:34:54.04
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 09/06/23 10:34:54.042
STEP: Ensuring job reaches completions 09/06/23 10:34:54.048
STEP: Ensuring pods with index for job exist 09/06/23 10:35:02.061
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  6 10:35:02.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4857" for this suite. 09/06/23 10:35:02.092
------------------------------
• [SLOW TEST] [8.096 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:34:54.014
    Sep  6 10:34:54.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename job 09/06/23 10:34:54.015
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:34:54.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:34:54.04
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 09/06/23 10:34:54.042
    STEP: Ensuring job reaches completions 09/06/23 10:34:54.048
    STEP: Ensuring pods with index for job exist 09/06/23 10:35:02.061
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:35:02.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4857" for this suite. 09/06/23 10:35:02.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:35:02.113
Sep  6 10:35:02.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename csiinlinevolumes 09/06/23 10:35:02.115
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:02.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:02.145
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 09/06/23 10:35:02.147
STEP: getting 09/06/23 10:35:02.166
STEP: listing 09/06/23 10:35:02.171
STEP: deleting 09/06/23 10:35:02.174
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:35:02.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-485" for this suite. 09/06/23 10:35:02.194
------------------------------
• [0.088 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:35:02.113
    Sep  6 10:35:02.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename csiinlinevolumes 09/06/23 10:35:02.115
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:02.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:02.145
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 09/06/23 10:35:02.147
    STEP: getting 09/06/23 10:35:02.166
    STEP: listing 09/06/23 10:35:02.171
    STEP: deleting 09/06/23 10:35:02.174
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:35:02.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-485" for this suite. 09/06/23 10:35:02.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:35:02.201
Sep  6 10:35:02.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename init-container 09/06/23 10:35:02.202
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:02.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:02.224
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 09/06/23 10:35:02.226
Sep  6 10:35:02.226: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:35:06.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4658" for this suite. 09/06/23 10:35:06.343
------------------------------
• [4.150 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:35:02.201
    Sep  6 10:35:02.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename init-container 09/06/23 10:35:02.202
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:02.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:02.224
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 09/06/23 10:35:02.226
    Sep  6 10:35:02.226: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:35:06.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4658" for this suite. 09/06/23 10:35:06.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:35:06.355
Sep  6 10:35:06.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:35:06.356
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:06.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:06.375
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-c171a8f8-4176-441c-8e03-04a3b5ea5df0 09/06/23 10:35:06.378
STEP: Creating a pod to test consume configMaps 09/06/23 10:35:06.383
Sep  6 10:35:06.394: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9" in namespace "projected-3948" to be "Succeeded or Failed"
Sep  6 10:35:06.405: INFO: Pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.027448ms
Sep  6 10:35:08.559: INFO: Pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9": Phase="Running", Reason="", readiness=false. Elapsed: 2.164532921s
Sep  6 10:35:10.409: INFO: Pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014429493s
STEP: Saw pod success 09/06/23 10:35:10.409
Sep  6 10:35:10.409: INFO: Pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9" satisfied condition "Succeeded or Failed"
Sep  6 10:35:10.412: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9 container agnhost-container: <nil>
STEP: delete the pod 09/06/23 10:35:10.425
Sep  6 10:35:10.441: INFO: Waiting for pod pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9 to disappear
Sep  6 10:35:10.444: INFO: Pod pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:35:10.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3948" for this suite. 09/06/23 10:35:10.448
------------------------------
• [4.101 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:35:06.355
    Sep  6 10:35:06.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:35:06.356
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:06.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:06.375
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-c171a8f8-4176-441c-8e03-04a3b5ea5df0 09/06/23 10:35:06.378
    STEP: Creating a pod to test consume configMaps 09/06/23 10:35:06.383
    Sep  6 10:35:06.394: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9" in namespace "projected-3948" to be "Succeeded or Failed"
    Sep  6 10:35:06.405: INFO: Pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.027448ms
    Sep  6 10:35:08.559: INFO: Pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9": Phase="Running", Reason="", readiness=false. Elapsed: 2.164532921s
    Sep  6 10:35:10.409: INFO: Pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014429493s
    STEP: Saw pod success 09/06/23 10:35:10.409
    Sep  6 10:35:10.409: INFO: Pod "pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9" satisfied condition "Succeeded or Failed"
    Sep  6 10:35:10.412: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9 container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 10:35:10.425
    Sep  6 10:35:10.441: INFO: Waiting for pod pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9 to disappear
    Sep  6 10:35:10.444: INFO: Pod pod-projected-configmaps-3560d27a-33a9-4456-81e8-fda79abd81d9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:35:10.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3948" for this suite. 09/06/23 10:35:10.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:35:10.456
Sep  6 10:35:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:35:10.457
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:10.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:10.474
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:35:10.497
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:35:11.177
STEP: Deploying the webhook pod 09/06/23 10:35:11.184
STEP: Wait for the deployment to be ready 09/06/23 10:35:11.2
Sep  6 10:35:11.208: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 10:35:13.238
STEP: Verifying the service has paired with the endpoint 09/06/23 10:35:13.257
Sep  6 10:35:14.259: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 09/06/23 10:35:14.268
STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:35:14.299
STEP: Updating a validating webhook configuration's rules to not include the create operation 09/06/23 10:35:14.309
STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:35:14.323
STEP: Patching a validating webhook configuration's rules to include the create operation 09/06/23 10:35:14.334
STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:35:14.342
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:35:14.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8663" for this suite. 09/06/23 10:35:14.427
STEP: Destroying namespace "webhook-8663-markers" for this suite. 09/06/23 10:35:14.446
------------------------------
• [4.005 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:35:10.456
    Sep  6 10:35:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:35:10.457
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:10.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:10.474
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:35:10.497
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:35:11.177
    STEP: Deploying the webhook pod 09/06/23 10:35:11.184
    STEP: Wait for the deployment to be ready 09/06/23 10:35:11.2
    Sep  6 10:35:11.208: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 10:35:13.238
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:35:13.257
    Sep  6 10:35:14.259: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 09/06/23 10:35:14.268
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:35:14.299
    STEP: Updating a validating webhook configuration's rules to not include the create operation 09/06/23 10:35:14.309
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:35:14.323
    STEP: Patching a validating webhook configuration's rules to include the create operation 09/06/23 10:35:14.334
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/06/23 10:35:14.342
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:35:14.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8663" for this suite. 09/06/23 10:35:14.427
    STEP: Destroying namespace "webhook-8663-markers" for this suite. 09/06/23 10:35:14.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:35:14.461
Sep  6 10:35:14.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 10:35:14.462
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:14.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:14.493
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 10:35:14.516
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:35:14.978
STEP: Deploying the webhook pod 09/06/23 10:35:14.984
STEP: Wait for the deployment to be ready 09/06/23 10:35:14.997
Sep  6 10:35:15.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 10:35:17.045
STEP: Verifying the service has paired with the endpoint 09/06/23 10:35:17.077
Sep  6 10:35:18.077: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Sep  6 10:35:18.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7226-crds.webhook.example.com via the AdmissionRegistration API 09/06/23 10:35:24.057
STEP: Creating a custom resource that should be mutated by the webhook 09/06/23 10:35:24.09
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:35:26.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9441" for this suite. 09/06/23 10:35:26.809
STEP: Destroying namespace "webhook-9441-markers" for this suite. 09/06/23 10:35:26.819
------------------------------
• [SLOW TEST] [12.392 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:35:14.461
    Sep  6 10:35:14.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 10:35:14.462
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:14.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:14.493
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 10:35:14.516
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 10:35:14.978
    STEP: Deploying the webhook pod 09/06/23 10:35:14.984
    STEP: Wait for the deployment to be ready 09/06/23 10:35:14.997
    Sep  6 10:35:15.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 10:35:17.045
    STEP: Verifying the service has paired with the endpoint 09/06/23 10:35:17.077
    Sep  6 10:35:18.077: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Sep  6 10:35:18.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7226-crds.webhook.example.com via the AdmissionRegistration API 09/06/23 10:35:24.057
    STEP: Creating a custom resource that should be mutated by the webhook 09/06/23 10:35:24.09
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:35:26.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9441" for this suite. 09/06/23 10:35:26.809
    STEP: Destroying namespace "webhook-9441-markers" for this suite. 09/06/23 10:35:26.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:35:26.855
Sep  6 10:35:26.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-runtime 09/06/23 10:35:26.856
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:26.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:26.891
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 09/06/23 10:35:26.897
STEP: wait for the container to reach Failed 09/06/23 10:35:26.911
STEP: get the container status 09/06/23 10:35:30.977
STEP: the container should be terminated 09/06/23 10:35:30.98
STEP: the termination message should be set 09/06/23 10:35:30.98
Sep  6 10:35:30.980: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/06/23 10:35:30.98
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  6 10:35:31.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7620" for this suite. 09/06/23 10:35:31.005
------------------------------
• [4.156 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:35:26.855
    Sep  6 10:35:26.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-runtime 09/06/23 10:35:26.856
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:26.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:26.891
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 09/06/23 10:35:26.897
    STEP: wait for the container to reach Failed 09/06/23 10:35:26.911
    STEP: get the container status 09/06/23 10:35:30.977
    STEP: the container should be terminated 09/06/23 10:35:30.98
    STEP: the termination message should be set 09/06/23 10:35:30.98
    Sep  6 10:35:30.980: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/06/23 10:35:30.98
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:35:31.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7620" for this suite. 09/06/23 10:35:31.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:35:31.011
Sep  6 10:35:31.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-probe 09/06/23 10:35:31.012
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:31.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:31.032
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  6 10:36:31.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-713" for this suite. 09/06/23 10:36:31.053
------------------------------
• [SLOW TEST] [60.053 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:35:31.011
    Sep  6 10:35:31.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-probe 09/06/23 10:35:31.012
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:35:31.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:35:31.032
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:36:31.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-713" for this suite. 09/06/23 10:36:31.053
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:36:31.064
Sep  6 10:36:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 10:36:31.065
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:31.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:31.088
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 09/06/23 10:36:31.093
STEP: waiting for available Endpoint 09/06/23 10:36:31.097
STEP: listing all Endpoints 09/06/23 10:36:31.098
STEP: updating the Endpoint 09/06/23 10:36:31.102
STEP: fetching the Endpoint 09/06/23 10:36:31.11
STEP: patching the Endpoint 09/06/23 10:36:31.112
STEP: fetching the Endpoint 09/06/23 10:36:31.12
STEP: deleting the Endpoint by Collection 09/06/23 10:36:31.123
STEP: waiting for Endpoint deletion 09/06/23 10:36:31.13
STEP: fetching the Endpoint 09/06/23 10:36:31.131
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 10:36:31.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-790" for this suite. 09/06/23 10:36:31.138
------------------------------
• [0.080 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:36:31.064
    Sep  6 10:36:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 10:36:31.065
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:31.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:31.088
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 09/06/23 10:36:31.093
    STEP: waiting for available Endpoint 09/06/23 10:36:31.097
    STEP: listing all Endpoints 09/06/23 10:36:31.098
    STEP: updating the Endpoint 09/06/23 10:36:31.102
    STEP: fetching the Endpoint 09/06/23 10:36:31.11
    STEP: patching the Endpoint 09/06/23 10:36:31.112
    STEP: fetching the Endpoint 09/06/23 10:36:31.12
    STEP: deleting the Endpoint by Collection 09/06/23 10:36:31.123
    STEP: waiting for Endpoint deletion 09/06/23 10:36:31.13
    STEP: fetching the Endpoint 09/06/23 10:36:31.131
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:36:31.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-790" for this suite. 09/06/23 10:36:31.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:36:31.148
Sep  6 10:36:31.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 10:36:31.149
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:31.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:31.175
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-dfd324ab-bb4b-4ebb-8ada-72cdec79dfd5 09/06/23 10:36:31.178
STEP: Creating a pod to test consume secrets 09/06/23 10:36:31.182
Sep  6 10:36:31.191: INFO: Waiting up to 5m0s for pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4" in namespace "secrets-4847" to be "Succeeded or Failed"
Sep  6 10:36:31.196: INFO: Pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.535937ms
Sep  6 10:36:33.206: INFO: Pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014827392s
Sep  6 10:36:35.215: INFO: Pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02317625s
STEP: Saw pod success 09/06/23 10:36:35.215
Sep  6 10:36:35.216: INFO: Pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4" satisfied condition "Succeeded or Failed"
Sep  6 10:36:35.226: INFO: Trying to get logs from node kube-3 pod pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4 container secret-volume-test: <nil>
STEP: delete the pod 09/06/23 10:36:35.239
Sep  6 10:36:35.304: INFO: Waiting for pod pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4 to disappear
Sep  6 10:36:35.312: INFO: Pod pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 10:36:35.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4847" for this suite. 09/06/23 10:36:35.317
------------------------------
• [4.209 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:36:31.148
    Sep  6 10:36:31.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 10:36:31.149
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:31.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:31.175
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-dfd324ab-bb4b-4ebb-8ada-72cdec79dfd5 09/06/23 10:36:31.178
    STEP: Creating a pod to test consume secrets 09/06/23 10:36:31.182
    Sep  6 10:36:31.191: INFO: Waiting up to 5m0s for pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4" in namespace "secrets-4847" to be "Succeeded or Failed"
    Sep  6 10:36:31.196: INFO: Pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.535937ms
    Sep  6 10:36:33.206: INFO: Pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014827392s
    Sep  6 10:36:35.215: INFO: Pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02317625s
    STEP: Saw pod success 09/06/23 10:36:35.215
    Sep  6 10:36:35.216: INFO: Pod "pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4" satisfied condition "Succeeded or Failed"
    Sep  6 10:36:35.226: INFO: Trying to get logs from node kube-3 pod pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4 container secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:36:35.239
    Sep  6 10:36:35.304: INFO: Waiting for pod pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4 to disappear
    Sep  6 10:36:35.312: INFO: Pod pod-secrets-6458b00e-ed72-4365-a650-88cf598494f4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:36:35.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4847" for this suite. 09/06/23 10:36:35.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:36:35.368
Sep  6 10:36:35.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:36:35.369
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:35.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:35.39
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/06/23 10:36:35.392
Sep  6 10:36:35.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-4644 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  6 10:36:35.461: INFO: stderr: ""
Sep  6 10:36:35.461: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 09/06/23 10:36:35.461
Sep  6 10:36:35.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-4644 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Sep  6 10:36:36.226: INFO: stderr: ""
Sep  6 10:36:36.226: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/06/23 10:36:36.226
Sep  6 10:36:36.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-4644 delete pods e2e-test-httpd-pod'
Sep  6 10:36:38.839: INFO: stderr: ""
Sep  6 10:36:38.839: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:36:38.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4644" for this suite. 09/06/23 10:36:38.843
------------------------------
• [3.482 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:36:35.368
    Sep  6 10:36:35.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:36:35.369
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:35.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:35.39
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/06/23 10:36:35.392
    Sep  6 10:36:35.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-4644 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  6 10:36:35.461: INFO: stderr: ""
    Sep  6 10:36:35.461: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 09/06/23 10:36:35.461
    Sep  6 10:36:35.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-4644 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Sep  6 10:36:36.226: INFO: stderr: ""
    Sep  6 10:36:36.226: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/06/23 10:36:36.226
    Sep  6 10:36:36.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-4644 delete pods e2e-test-httpd-pod'
    Sep  6 10:36:38.839: INFO: stderr: ""
    Sep  6 10:36:38.839: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:36:38.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4644" for this suite. 09/06/23 10:36:38.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:36:38.85
Sep  6 10:36:38.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename events 09/06/23 10:36:38.851
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:38.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:38.87
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 09/06/23 10:36:38.872
Sep  6 10:36:38.877: INFO: created test-event-1
Sep  6 10:36:38.881: INFO: created test-event-2
Sep  6 10:36:38.886: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 09/06/23 10:36:38.886
STEP: delete collection of events 09/06/23 10:36:38.889
Sep  6 10:36:38.889: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/06/23 10:36:38.912
Sep  6 10:36:38.912: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  6 10:36:38.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7763" for this suite. 09/06/23 10:36:38.917
------------------------------
• [0.079 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:36:38.85
    Sep  6 10:36:38.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename events 09/06/23 10:36:38.851
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:38.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:38.87
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 09/06/23 10:36:38.872
    Sep  6 10:36:38.877: INFO: created test-event-1
    Sep  6 10:36:38.881: INFO: created test-event-2
    Sep  6 10:36:38.886: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 09/06/23 10:36:38.886
    STEP: delete collection of events 09/06/23 10:36:38.889
    Sep  6 10:36:38.889: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/06/23 10:36:38.912
    Sep  6 10:36:38.912: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:36:38.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7763" for this suite. 09/06/23 10:36:38.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:36:38.929
Sep  6 10:36:38.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename daemonsets 09/06/23 10:36:38.93
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:38.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:38.948
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Sep  6 10:36:38.974: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 10:36:38.979
Sep  6 10:36:38.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 10:36:38.989: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 10:36:39.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 10:36:39.998: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 10:36:41.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:36:41.014: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 10:36:42.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 10:36:42.010: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 09/06/23 10:36:42.047
STEP: Check that daemon pods images are updated. 09/06/23 10:36:42.181
Sep  6 10:36:42.214: INFO: Wrong image for pod: daemon-set-7rk9l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  6 10:36:42.214: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  6 10:36:43.360: INFO: Wrong image for pod: daemon-set-7rk9l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  6 10:36:43.360: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  6 10:36:44.352: INFO: Pod daemon-set-4gnds is not available
Sep  6 10:36:44.352: INFO: Wrong image for pod: daemon-set-7rk9l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  6 10:36:44.352: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  6 10:36:45.353: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  6 10:36:46.355: INFO: Pod daemon-set-54rnf is not available
Sep  6 10:36:46.355: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  6 10:36:48.354: INFO: Pod daemon-set-4pps7 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 09/06/23 10:36:48.358
Sep  6 10:36:48.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 10:36:48.365: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 10:36:49.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 10:36:49.399: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/06/23 10:36:49.455
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3743, will wait for the garbage collector to delete the pods 09/06/23 10:36:49.455
Sep  6 10:36:49.524: INFO: Deleting DaemonSet.extensions daemon-set took: 12.048846ms
Sep  6 10:36:49.625: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.536316ms
Sep  6 10:36:53.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 10:36:53.032: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  6 10:36:53.036: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14765"},"items":null}

Sep  6 10:36:53.040: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14765"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:36:53.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3743" for this suite. 09/06/23 10:36:53.066
------------------------------
• [SLOW TEST] [14.144 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:36:38.929
    Sep  6 10:36:38.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename daemonsets 09/06/23 10:36:38.93
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:38.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:38.948
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Sep  6 10:36:38.974: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 10:36:38.979
    Sep  6 10:36:38.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 10:36:38.989: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 10:36:39.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 10:36:39.998: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 10:36:41.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:36:41.014: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 10:36:42.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 10:36:42.010: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 09/06/23 10:36:42.047
    STEP: Check that daemon pods images are updated. 09/06/23 10:36:42.181
    Sep  6 10:36:42.214: INFO: Wrong image for pod: daemon-set-7rk9l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  6 10:36:42.214: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  6 10:36:43.360: INFO: Wrong image for pod: daemon-set-7rk9l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  6 10:36:43.360: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  6 10:36:44.352: INFO: Pod daemon-set-4gnds is not available
    Sep  6 10:36:44.352: INFO: Wrong image for pod: daemon-set-7rk9l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  6 10:36:44.352: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  6 10:36:45.353: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  6 10:36:46.355: INFO: Pod daemon-set-54rnf is not available
    Sep  6 10:36:46.355: INFO: Wrong image for pod: daemon-set-lwcd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  6 10:36:48.354: INFO: Pod daemon-set-4pps7 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 09/06/23 10:36:48.358
    Sep  6 10:36:48.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 10:36:48.365: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 10:36:49.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 10:36:49.399: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/06/23 10:36:49.455
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3743, will wait for the garbage collector to delete the pods 09/06/23 10:36:49.455
    Sep  6 10:36:49.524: INFO: Deleting DaemonSet.extensions daemon-set took: 12.048846ms
    Sep  6 10:36:49.625: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.536316ms
    Sep  6 10:36:53.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 10:36:53.032: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  6 10:36:53.036: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14765"},"items":null}

    Sep  6 10:36:53.040: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14765"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:36:53.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3743" for this suite. 09/06/23 10:36:53.066
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:36:53.074
Sep  6 10:36:53.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:36:53.074
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:53.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:53.096
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-82e2888f-5eed-45f9-8e05-c2ab517cb5f3 09/06/23 10:36:53.102
STEP: Creating a pod to test consume secrets 09/06/23 10:36:53.109
Sep  6 10:36:53.119: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8" in namespace "projected-580" to be "Succeeded or Failed"
Sep  6 10:36:53.128: INFO: Pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.850839ms
Sep  6 10:36:55.132: INFO: Pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.012522493s
Sep  6 10:36:57.141: INFO: Pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021522885s
STEP: Saw pod success 09/06/23 10:36:57.141
Sep  6 10:36:57.141: INFO: Pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8" satisfied condition "Succeeded or Failed"
Sep  6 10:36:57.153: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/06/23 10:36:57.175
Sep  6 10:36:57.202: INFO: Waiting for pod pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8 to disappear
Sep  6 10:36:57.204: INFO: Pod pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  6 10:36:57.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-580" for this suite. 09/06/23 10:36:57.208
------------------------------
• [4.141 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:36:53.074
    Sep  6 10:36:53.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:36:53.074
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:53.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:53.096
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-82e2888f-5eed-45f9-8e05-c2ab517cb5f3 09/06/23 10:36:53.102
    STEP: Creating a pod to test consume secrets 09/06/23 10:36:53.109
    Sep  6 10:36:53.119: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8" in namespace "projected-580" to be "Succeeded or Failed"
    Sep  6 10:36:53.128: INFO: Pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.850839ms
    Sep  6 10:36:55.132: INFO: Pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.012522493s
    Sep  6 10:36:57.141: INFO: Pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021522885s
    STEP: Saw pod success 09/06/23 10:36:57.141
    Sep  6 10:36:57.141: INFO: Pod "pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8" satisfied condition "Succeeded or Failed"
    Sep  6 10:36:57.153: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:36:57.175
    Sep  6 10:36:57.202: INFO: Waiting for pod pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8 to disappear
    Sep  6 10:36:57.204: INFO: Pod pod-projected-secrets-396e9551-1a74-4c4b-80da-c9324d8179d8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:36:57.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-580" for this suite. 09/06/23 10:36:57.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:36:57.215
Sep  6 10:36:57.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:36:57.216
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:57.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:57.236
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-fcfa4304-fc52-493d-9622-05bc25407f4d 09/06/23 10:36:57.238
STEP: Creating a pod to test consume configMaps 09/06/23 10:36:57.243
Sep  6 10:36:57.254: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af" in namespace "projected-6261" to be "Succeeded or Failed"
Sep  6 10:36:57.258: INFO: Pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684466ms
Sep  6 10:36:59.272: INFO: Pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018169643s
Sep  6 10:37:01.271: INFO: Pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017133903s
STEP: Saw pod success 09/06/23 10:37:01.271
Sep  6 10:37:01.273: INFO: Pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af" satisfied condition "Succeeded or Failed"
Sep  6 10:37:01.287: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af container agnhost-container: <nil>
STEP: delete the pod 09/06/23 10:37:01.309
Sep  6 10:37:01.340: INFO: Waiting for pod pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af to disappear
Sep  6 10:37:01.343: INFO: Pod pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:01.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6261" for this suite. 09/06/23 10:37:01.348
------------------------------
• [4.140 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:36:57.215
    Sep  6 10:36:57.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:36:57.216
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:36:57.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:36:57.236
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-fcfa4304-fc52-493d-9622-05bc25407f4d 09/06/23 10:36:57.238
    STEP: Creating a pod to test consume configMaps 09/06/23 10:36:57.243
    Sep  6 10:36:57.254: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af" in namespace "projected-6261" to be "Succeeded or Failed"
    Sep  6 10:36:57.258: INFO: Pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684466ms
    Sep  6 10:36:59.272: INFO: Pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018169643s
    Sep  6 10:37:01.271: INFO: Pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017133903s
    STEP: Saw pod success 09/06/23 10:37:01.271
    Sep  6 10:37:01.273: INFO: Pod "pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af" satisfied condition "Succeeded or Failed"
    Sep  6 10:37:01.287: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 10:37:01.309
    Sep  6 10:37:01.340: INFO: Waiting for pod pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af to disappear
    Sep  6 10:37:01.343: INFO: Pod pod-projected-configmaps-7b22b18d-0087-4530-9a3c-062bed5eb9af no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:01.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6261" for this suite. 09/06/23 10:37:01.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:01.356
Sep  6 10:37:01.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename controllerrevisions 09/06/23 10:37:01.357
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:01.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:01.385
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-52rgg-daemon-set" 09/06/23 10:37:01.412
STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 10:37:01.421
Sep  6 10:37:01.432: INFO: Number of nodes with available pods controlled by daemonset e2e-52rgg-daemon-set: 0
Sep  6 10:37:01.432: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 10:37:02.446: INFO: Number of nodes with available pods controlled by daemonset e2e-52rgg-daemon-set: 0
Sep  6 10:37:02.446: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 10:37:03.461: INFO: Number of nodes with available pods controlled by daemonset e2e-52rgg-daemon-set: 3
Sep  6 10:37:03.461: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-52rgg-daemon-set
STEP: Confirm DaemonSet "e2e-52rgg-daemon-set" successfully created with "daemonset-name=e2e-52rgg-daemon-set" label 09/06/23 10:37:03.469
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-52rgg-daemon-set" 09/06/23 10:37:03.484
Sep  6 10:37:03.497: INFO: Located ControllerRevision: "e2e-52rgg-daemon-set-7b87d87ffd"
STEP: Patching ControllerRevision "e2e-52rgg-daemon-set-7b87d87ffd" 09/06/23 10:37:03.504
Sep  6 10:37:03.526: INFO: e2e-52rgg-daemon-set-7b87d87ffd has been patched
STEP: Create a new ControllerRevision 09/06/23 10:37:03.526
Sep  6 10:37:03.535: INFO: Created ControllerRevision: e2e-52rgg-daemon-set-565449799b
STEP: Confirm that there are two ControllerRevisions 09/06/23 10:37:03.535
Sep  6 10:37:03.535: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  6 10:37:03.538: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-52rgg-daemon-set-7b87d87ffd" 09/06/23 10:37:03.538
STEP: Confirm that there is only one ControllerRevision 09/06/23 10:37:03.543
Sep  6 10:37:03.543: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  6 10:37:03.545: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-52rgg-daemon-set-565449799b" 09/06/23 10:37:03.548
Sep  6 10:37:03.555: INFO: e2e-52rgg-daemon-set-565449799b has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 09/06/23 10:37:03.556
W0906 10:37:03.596369      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 09/06/23 10:37:03.596
Sep  6 10:37:03.596: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  6 10:37:04.616: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  6 10:37:04.620: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-52rgg-daemon-set-565449799b=updated" 09/06/23 10:37:04.62
STEP: Confirm that there is only one ControllerRevision 09/06/23 10:37:04.645
Sep  6 10:37:04.645: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  6 10:37:04.648: INFO: Found 1 ControllerRevisions
Sep  6 10:37:04.650: INFO: ControllerRevision "e2e-52rgg-daemon-set-54cb4c7d57" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-52rgg-daemon-set" 09/06/23 10:37:04.652
STEP: deleting DaemonSet.extensions e2e-52rgg-daemon-set in namespace controllerrevisions-4881, will wait for the garbage collector to delete the pods 09/06/23 10:37:04.653
Sep  6 10:37:04.719: INFO: Deleting DaemonSet.extensions e2e-52rgg-daemon-set took: 12.239872ms
Sep  6 10:37:05.019: INFO: Terminating DaemonSet.extensions e2e-52rgg-daemon-set pods took: 300.816126ms
Sep  6 10:37:08.123: INFO: Number of nodes with available pods controlled by daemonset e2e-52rgg-daemon-set: 0
Sep  6 10:37:08.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-52rgg-daemon-set
Sep  6 10:37:08.125: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14994"},"items":null}

Sep  6 10:37:08.127: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14994"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:08.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-4881" for this suite. 09/06/23 10:37:08.142
------------------------------
• [SLOW TEST] [6.793 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:01.356
    Sep  6 10:37:01.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename controllerrevisions 09/06/23 10:37:01.357
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:01.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:01.385
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-52rgg-daemon-set" 09/06/23 10:37:01.412
    STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 10:37:01.421
    Sep  6 10:37:01.432: INFO: Number of nodes with available pods controlled by daemonset e2e-52rgg-daemon-set: 0
    Sep  6 10:37:01.432: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 10:37:02.446: INFO: Number of nodes with available pods controlled by daemonset e2e-52rgg-daemon-set: 0
    Sep  6 10:37:02.446: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 10:37:03.461: INFO: Number of nodes with available pods controlled by daemonset e2e-52rgg-daemon-set: 3
    Sep  6 10:37:03.461: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-52rgg-daemon-set
    STEP: Confirm DaemonSet "e2e-52rgg-daemon-set" successfully created with "daemonset-name=e2e-52rgg-daemon-set" label 09/06/23 10:37:03.469
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-52rgg-daemon-set" 09/06/23 10:37:03.484
    Sep  6 10:37:03.497: INFO: Located ControllerRevision: "e2e-52rgg-daemon-set-7b87d87ffd"
    STEP: Patching ControllerRevision "e2e-52rgg-daemon-set-7b87d87ffd" 09/06/23 10:37:03.504
    Sep  6 10:37:03.526: INFO: e2e-52rgg-daemon-set-7b87d87ffd has been patched
    STEP: Create a new ControllerRevision 09/06/23 10:37:03.526
    Sep  6 10:37:03.535: INFO: Created ControllerRevision: e2e-52rgg-daemon-set-565449799b
    STEP: Confirm that there are two ControllerRevisions 09/06/23 10:37:03.535
    Sep  6 10:37:03.535: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  6 10:37:03.538: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-52rgg-daemon-set-7b87d87ffd" 09/06/23 10:37:03.538
    STEP: Confirm that there is only one ControllerRevision 09/06/23 10:37:03.543
    Sep  6 10:37:03.543: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  6 10:37:03.545: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-52rgg-daemon-set-565449799b" 09/06/23 10:37:03.548
    Sep  6 10:37:03.555: INFO: e2e-52rgg-daemon-set-565449799b has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 09/06/23 10:37:03.556
    W0906 10:37:03.596369      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 09/06/23 10:37:03.596
    Sep  6 10:37:03.596: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  6 10:37:04.616: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  6 10:37:04.620: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-52rgg-daemon-set-565449799b=updated" 09/06/23 10:37:04.62
    STEP: Confirm that there is only one ControllerRevision 09/06/23 10:37:04.645
    Sep  6 10:37:04.645: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  6 10:37:04.648: INFO: Found 1 ControllerRevisions
    Sep  6 10:37:04.650: INFO: ControllerRevision "e2e-52rgg-daemon-set-54cb4c7d57" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-52rgg-daemon-set" 09/06/23 10:37:04.652
    STEP: deleting DaemonSet.extensions e2e-52rgg-daemon-set in namespace controllerrevisions-4881, will wait for the garbage collector to delete the pods 09/06/23 10:37:04.653
    Sep  6 10:37:04.719: INFO: Deleting DaemonSet.extensions e2e-52rgg-daemon-set took: 12.239872ms
    Sep  6 10:37:05.019: INFO: Terminating DaemonSet.extensions e2e-52rgg-daemon-set pods took: 300.816126ms
    Sep  6 10:37:08.123: INFO: Number of nodes with available pods controlled by daemonset e2e-52rgg-daemon-set: 0
    Sep  6 10:37:08.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-52rgg-daemon-set
    Sep  6 10:37:08.125: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14994"},"items":null}

    Sep  6 10:37:08.127: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14994"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:08.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-4881" for this suite. 09/06/23 10:37:08.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:08.15
Sep  6 10:37:08.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:37:08.151
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:08.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:08.17
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 09/06/23 10:37:08.173
Sep  6 10:37:08.181: INFO: Waiting up to 5m0s for pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b" in namespace "downward-api-1301" to be "Succeeded or Failed"
Sep  6 10:37:08.187: INFO: Pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.192078ms
Sep  6 10:37:10.191: INFO: Pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b": Phase="Running", Reason="", readiness=false. Elapsed: 2.009952369s
Sep  6 10:37:12.204: INFO: Pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022988229s
STEP: Saw pod success 09/06/23 10:37:12.204
Sep  6 10:37:12.204: INFO: Pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b" satisfied condition "Succeeded or Failed"
Sep  6 10:37:12.213: INFO: Trying to get logs from node kube-3 pod downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b container dapi-container: <nil>
STEP: delete the pod 09/06/23 10:37:12.226
Sep  6 10:37:12.248: INFO: Waiting for pod downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b to disappear
Sep  6 10:37:12.251: INFO: Pod downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:12.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1301" for this suite. 09/06/23 10:37:12.255
------------------------------
• [4.110 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:08.15
    Sep  6 10:37:08.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:37:08.151
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:08.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:08.17
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 09/06/23 10:37:08.173
    Sep  6 10:37:08.181: INFO: Waiting up to 5m0s for pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b" in namespace "downward-api-1301" to be "Succeeded or Failed"
    Sep  6 10:37:08.187: INFO: Pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.192078ms
    Sep  6 10:37:10.191: INFO: Pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b": Phase="Running", Reason="", readiness=false. Elapsed: 2.009952369s
    Sep  6 10:37:12.204: INFO: Pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022988229s
    STEP: Saw pod success 09/06/23 10:37:12.204
    Sep  6 10:37:12.204: INFO: Pod "downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b" satisfied condition "Succeeded or Failed"
    Sep  6 10:37:12.213: INFO: Trying to get logs from node kube-3 pod downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b container dapi-container: <nil>
    STEP: delete the pod 09/06/23 10:37:12.226
    Sep  6 10:37:12.248: INFO: Waiting for pod downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b to disappear
    Sep  6 10:37:12.251: INFO: Pod downward-api-3efefda8-41c2-4670-9ba8-278c52cfde5b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:12.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1301" for this suite. 09/06/23 10:37:12.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:12.263
Sep  6 10:37:12.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:37:12.264
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:12.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:12.282
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 09/06/23 10:37:12.284
Sep  6 10:37:12.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f" in namespace "projected-7766" to be "Succeeded or Failed"
Sep  6 10:37:12.299: INFO: Pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.439414ms
Sep  6 10:37:14.305: INFO: Pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012439339s
Sep  6 10:37:16.309: INFO: Pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016638318s
STEP: Saw pod success 09/06/23 10:37:16.309
Sep  6 10:37:16.309: INFO: Pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f" satisfied condition "Succeeded or Failed"
Sep  6 10:37:16.320: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f container client-container: <nil>
STEP: delete the pod 09/06/23 10:37:16.333
Sep  6 10:37:16.360: INFO: Waiting for pod downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f to disappear
Sep  6 10:37:16.363: INFO: Pod downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:16.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7766" for this suite. 09/06/23 10:37:16.367
------------------------------
• [4.110 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:12.263
    Sep  6 10:37:12.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:37:12.264
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:12.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:12.282
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 09/06/23 10:37:12.284
    Sep  6 10:37:12.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f" in namespace "projected-7766" to be "Succeeded or Failed"
    Sep  6 10:37:12.299: INFO: Pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.439414ms
    Sep  6 10:37:14.305: INFO: Pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012439339s
    Sep  6 10:37:16.309: INFO: Pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016638318s
    STEP: Saw pod success 09/06/23 10:37:16.309
    Sep  6 10:37:16.309: INFO: Pod "downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f" satisfied condition "Succeeded or Failed"
    Sep  6 10:37:16.320: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f container client-container: <nil>
    STEP: delete the pod 09/06/23 10:37:16.333
    Sep  6 10:37:16.360: INFO: Waiting for pod downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f to disappear
    Sep  6 10:37:16.363: INFO: Pod downwardapi-volume-cca2307d-6124-4639-a032-126000686c5f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:16.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7766" for this suite. 09/06/23 10:37:16.367
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:16.373
Sep  6 10:37:16.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename security-context-test 09/06/23 10:37:16.374
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:16.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:16.392
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Sep  6 10:37:16.401: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2" in namespace "security-context-test-8265" to be "Succeeded or Failed"
Sep  6 10:37:16.411: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.440838ms
Sep  6 10:37:18.417: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015762848s
Sep  6 10:37:20.421: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019795757s
Sep  6 10:37:22.416: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01496343s
Sep  6 10:37:24.425: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024457452s
Sep  6 10:37:24.425: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:24.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8265" for this suite. 09/06/23 10:37:24.476
------------------------------
• [SLOW TEST] [8.116 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:16.373
    Sep  6 10:37:16.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename security-context-test 09/06/23 10:37:16.374
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:16.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:16.392
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Sep  6 10:37:16.401: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2" in namespace "security-context-test-8265" to be "Succeeded or Failed"
    Sep  6 10:37:16.411: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.440838ms
    Sep  6 10:37:18.417: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015762848s
    Sep  6 10:37:20.421: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019795757s
    Sep  6 10:37:22.416: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01496343s
    Sep  6 10:37:24.425: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024457452s
    Sep  6 10:37:24.425: INFO: Pod "alpine-nnp-false-3baf3e3c-8bdb-4f55-b15b-b7fbfb3e23a2" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:24.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8265" for this suite. 09/06/23 10:37:24.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:24.49
Sep  6 10:37:24.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:37:24.491
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:24.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:24.521
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 09/06/23 10:37:24.523
Sep  6 10:37:24.523: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6600 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 09/06/23 10:37:24.566
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:24.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6600" for this suite. 09/06/23 10:37:24.578
------------------------------
• [0.094 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:24.49
    Sep  6 10:37:24.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:37:24.491
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:24.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:24.521
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 09/06/23 10:37:24.523
    Sep  6 10:37:24.523: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6600 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 09/06/23 10:37:24.566
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:24.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6600" for this suite. 09/06/23 10:37:24.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:24.584
Sep  6 10:37:24.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 10:37:24.586
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:24.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:24.609
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/06/23 10:37:24.61
Sep  6 10:37:24.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:37:31.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:42.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3524" for this suite. 09/06/23 10:37:42.498
------------------------------
• [SLOW TEST] [17.920 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:24.584
    Sep  6 10:37:24.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 10:37:24.586
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:24.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:24.609
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/06/23 10:37:24.61
    Sep  6 10:37:24.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:37:31.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:42.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3524" for this suite. 09/06/23 10:37:42.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:42.505
Sep  6 10:37:42.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 10:37:42.506
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:42.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:42.532
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:42.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-282" for this suite. 09/06/23 10:37:42.574
------------------------------
• [0.077 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:42.505
    Sep  6 10:37:42.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 10:37:42.506
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:42.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:42.532
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:42.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-282" for this suite. 09/06/23 10:37:42.574
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:42.582
Sep  6 10:37:42.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 10:37:42.584
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:42.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:42.614
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 09/06/23 10:37:42.616
Sep  6 10:37:42.616: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  6 10:37:42.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
Sep  6 10:37:43.091: INFO: stderr: ""
Sep  6 10:37:43.091: INFO: stdout: "service/agnhost-replica created\n"
Sep  6 10:37:43.091: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  6 10:37:43.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
Sep  6 10:37:43.899: INFO: stderr: ""
Sep  6 10:37:43.899: INFO: stdout: "service/agnhost-primary created\n"
Sep  6 10:37:43.899: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  6 10:37:43.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
Sep  6 10:37:46.587: INFO: stderr: ""
Sep  6 10:37:46.587: INFO: stdout: "service/frontend created\n"
Sep  6 10:37:46.587: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  6 10:37:46.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
Sep  6 10:37:47.123: INFO: stderr: ""
Sep  6 10:37:47.123: INFO: stdout: "deployment.apps/frontend created\n"
Sep  6 10:37:47.123: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  6 10:37:47.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
Sep  6 10:37:47.749: INFO: stderr: ""
Sep  6 10:37:47.749: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  6 10:37:47.749: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  6 10:37:47.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
Sep  6 10:37:48.001: INFO: stderr: ""
Sep  6 10:37:48.001: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 09/06/23 10:37:48.001
Sep  6 10:37:48.001: INFO: Waiting for all frontend pods to be Running.
Sep  6 10:37:53.052: INFO: Waiting for frontend to serve content.
Sep  6 10:37:54.134: INFO: Trying to add a new entry to the guestbook.
Sep  6 10:37:54.176: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 09/06/23 10:37:54.197
Sep  6 10:37:54.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
Sep  6 10:37:54.330: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:37:54.330: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 09/06/23 10:37:54.33
Sep  6 10:37:54.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
Sep  6 10:37:54.427: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:37:54.427: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/06/23 10:37:54.427
Sep  6 10:37:54.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
Sep  6 10:37:54.531: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:37:54.531: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/06/23 10:37:54.531
Sep  6 10:37:54.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
Sep  6 10:37:54.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:37:54.614: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/06/23 10:37:54.614
Sep  6 10:37:54.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
Sep  6 10:37:54.779: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:37:54.779: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/06/23 10:37:54.779
Sep  6 10:37:54.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
Sep  6 10:37:54.882: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:37:54.882: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 10:37:54.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6580" for this suite. 09/06/23 10:37:54.913
------------------------------
• [SLOW TEST] [12.362 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:42.582
    Sep  6 10:37:42.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 10:37:42.584
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:42.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:42.614
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 09/06/23 10:37:42.616
    Sep  6 10:37:42.616: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Sep  6 10:37:42.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
    Sep  6 10:37:43.091: INFO: stderr: ""
    Sep  6 10:37:43.091: INFO: stdout: "service/agnhost-replica created\n"
    Sep  6 10:37:43.091: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Sep  6 10:37:43.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
    Sep  6 10:37:43.899: INFO: stderr: ""
    Sep  6 10:37:43.899: INFO: stdout: "service/agnhost-primary created\n"
    Sep  6 10:37:43.899: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Sep  6 10:37:43.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
    Sep  6 10:37:46.587: INFO: stderr: ""
    Sep  6 10:37:46.587: INFO: stdout: "service/frontend created\n"
    Sep  6 10:37:46.587: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Sep  6 10:37:46.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
    Sep  6 10:37:47.123: INFO: stderr: ""
    Sep  6 10:37:47.123: INFO: stdout: "deployment.apps/frontend created\n"
    Sep  6 10:37:47.123: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  6 10:37:47.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
    Sep  6 10:37:47.749: INFO: stderr: ""
    Sep  6 10:37:47.749: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Sep  6 10:37:47.749: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  6 10:37:47.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 create -f -'
    Sep  6 10:37:48.001: INFO: stderr: ""
    Sep  6 10:37:48.001: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 09/06/23 10:37:48.001
    Sep  6 10:37:48.001: INFO: Waiting for all frontend pods to be Running.
    Sep  6 10:37:53.052: INFO: Waiting for frontend to serve content.
    Sep  6 10:37:54.134: INFO: Trying to add a new entry to the guestbook.
    Sep  6 10:37:54.176: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 09/06/23 10:37:54.197
    Sep  6 10:37:54.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
    Sep  6 10:37:54.330: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 10:37:54.330: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 09/06/23 10:37:54.33
    Sep  6 10:37:54.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
    Sep  6 10:37:54.427: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 10:37:54.427: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/06/23 10:37:54.427
    Sep  6 10:37:54.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
    Sep  6 10:37:54.531: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 10:37:54.531: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/06/23 10:37:54.531
    Sep  6 10:37:54.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
    Sep  6 10:37:54.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 10:37:54.614: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/06/23 10:37:54.614
    Sep  6 10:37:54.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
    Sep  6 10:37:54.779: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 10:37:54.779: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/06/23 10:37:54.779
    Sep  6 10:37:54.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-6580 delete --grace-period=0 --force -f -'
    Sep  6 10:37:54.882: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 10:37:54.882: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:37:54.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6580" for this suite. 09/06/23 10:37:54.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:37:54.947
Sep  6 10:37:54.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 10:37:54.948
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:55.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:55.018
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/06/23 10:37:55.032
Sep  6 10:37:55.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:38:01.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:38:13.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5816" for this suite. 09/06/23 10:38:13.827
------------------------------
• [SLOW TEST] [18.891 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:37:54.947
    Sep  6 10:37:54.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 10:37:54.948
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:37:55.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:37:55.018
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/06/23 10:37:55.032
    Sep  6 10:37:55.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:38:01.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:38:13.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5816" for this suite. 09/06/23 10:38:13.827
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:38:13.839
Sep  6 10:38:13.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename limitrange 09/06/23 10:38:13.84
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:38:13.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:38:13.861
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-pfgq7" in namespace "limitrange-9636" 09/06/23 10:38:13.863
STEP: Creating another limitRange in another namespace 09/06/23 10:38:13.868
Sep  6 10:38:13.888: INFO: Namespace "e2e-limitrange-pfgq7-4984" created
Sep  6 10:38:13.888: INFO: Creating LimitRange "e2e-limitrange-pfgq7" in namespace "e2e-limitrange-pfgq7-4984"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-pfgq7" 09/06/23 10:38:13.893
Sep  6 10:38:13.898: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-pfgq7" in "limitrange-9636" namespace 09/06/23 10:38:13.898
Sep  6 10:38:13.904: INFO: LimitRange "e2e-limitrange-pfgq7" has been patched
STEP: Delete LimitRange "e2e-limitrange-pfgq7" by Collection with labelSelector: "e2e-limitrange-pfgq7=patched" 09/06/23 10:38:13.904
STEP: Confirm that the limitRange "e2e-limitrange-pfgq7" has been deleted 09/06/23 10:38:13.913
Sep  6 10:38:13.913: INFO: Requesting list of LimitRange to confirm quantity
Sep  6 10:38:13.916: INFO: Found 0 LimitRange with label "e2e-limitrange-pfgq7=patched"
Sep  6 10:38:13.916: INFO: LimitRange "e2e-limitrange-pfgq7" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-pfgq7" 09/06/23 10:38:13.916
Sep  6 10:38:13.919: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  6 10:38:13.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-9636" for this suite. 09/06/23 10:38:13.922
STEP: Destroying namespace "e2e-limitrange-pfgq7-4984" for this suite. 09/06/23 10:38:13.929
------------------------------
• [0.098 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:38:13.839
    Sep  6 10:38:13.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename limitrange 09/06/23 10:38:13.84
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:38:13.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:38:13.861
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-pfgq7" in namespace "limitrange-9636" 09/06/23 10:38:13.863
    STEP: Creating another limitRange in another namespace 09/06/23 10:38:13.868
    Sep  6 10:38:13.888: INFO: Namespace "e2e-limitrange-pfgq7-4984" created
    Sep  6 10:38:13.888: INFO: Creating LimitRange "e2e-limitrange-pfgq7" in namespace "e2e-limitrange-pfgq7-4984"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-pfgq7" 09/06/23 10:38:13.893
    Sep  6 10:38:13.898: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-pfgq7" in "limitrange-9636" namespace 09/06/23 10:38:13.898
    Sep  6 10:38:13.904: INFO: LimitRange "e2e-limitrange-pfgq7" has been patched
    STEP: Delete LimitRange "e2e-limitrange-pfgq7" by Collection with labelSelector: "e2e-limitrange-pfgq7=patched" 09/06/23 10:38:13.904
    STEP: Confirm that the limitRange "e2e-limitrange-pfgq7" has been deleted 09/06/23 10:38:13.913
    Sep  6 10:38:13.913: INFO: Requesting list of LimitRange to confirm quantity
    Sep  6 10:38:13.916: INFO: Found 0 LimitRange with label "e2e-limitrange-pfgq7=patched"
    Sep  6 10:38:13.916: INFO: LimitRange "e2e-limitrange-pfgq7" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-pfgq7" 09/06/23 10:38:13.916
    Sep  6 10:38:13.919: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:38:13.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-9636" for this suite. 09/06/23 10:38:13.922
    STEP: Destroying namespace "e2e-limitrange-pfgq7-4984" for this suite. 09/06/23 10:38:13.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:38:13.938
Sep  6 10:38:13.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:38:13.939
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:38:13.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:38:13.958
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 09/06/23 10:38:13.961
Sep  6 10:38:13.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5" in namespace "projected-3234" to be "Succeeded or Failed"
Sep  6 10:38:13.975: INFO: Pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.71077ms
Sep  6 10:38:15.989: INFO: Pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018761522s
Sep  6 10:38:17.987: INFO: Pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016852999s
STEP: Saw pod success 09/06/23 10:38:17.987
Sep  6 10:38:17.987: INFO: Pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5" satisfied condition "Succeeded or Failed"
Sep  6 10:38:18.001: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5 container client-container: <nil>
STEP: delete the pod 09/06/23 10:38:18.028
Sep  6 10:38:18.054: INFO: Waiting for pod downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5 to disappear
Sep  6 10:38:18.058: INFO: Pod downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 10:38:18.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3234" for this suite. 09/06/23 10:38:18.061
------------------------------
• [4.129 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:38:13.938
    Sep  6 10:38:13.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:38:13.939
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:38:13.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:38:13.958
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 09/06/23 10:38:13.961
    Sep  6 10:38:13.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5" in namespace "projected-3234" to be "Succeeded or Failed"
    Sep  6 10:38:13.975: INFO: Pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.71077ms
    Sep  6 10:38:15.989: INFO: Pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018761522s
    Sep  6 10:38:17.987: INFO: Pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016852999s
    STEP: Saw pod success 09/06/23 10:38:17.987
    Sep  6 10:38:17.987: INFO: Pod "downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5" satisfied condition "Succeeded or Failed"
    Sep  6 10:38:18.001: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5 container client-container: <nil>
    STEP: delete the pod 09/06/23 10:38:18.028
    Sep  6 10:38:18.054: INFO: Waiting for pod downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5 to disappear
    Sep  6 10:38:18.058: INFO: Pod downwardapi-volume-702dcf9d-b2d3-499e-8d3a-c9f043c2e4c5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:38:18.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3234" for this suite. 09/06/23 10:38:18.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:38:18.069
Sep  6 10:38:18.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename gc 09/06/23 10:38:18.07
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:38:18.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:38:18.088
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 09/06/23 10:38:18.093
STEP: delete the rc 09/06/23 10:38:23.236
STEP: wait for the rc to be deleted 09/06/23 10:38:23.392
Sep  6 10:38:26.032: INFO: 84 pods remaining
Sep  6 10:38:26.032: INFO: 79 pods has nil DeletionTimestamp
Sep  6 10:38:26.032: INFO: 
Sep  6 10:38:35.563: INFO: 68 pods remaining
Sep  6 10:38:35.563: INFO: 65 pods has nil DeletionTimestamp
Sep  6 10:38:35.563: INFO: 
Sep  6 10:38:36.490: INFO: 45 pods remaining
Sep  6 10:38:36.490: INFO: 44 pods has nil DeletionTimestamp
Sep  6 10:38:36.490: INFO: 
Sep  6 10:38:37.994: INFO: 35 pods remaining
Sep  6 10:38:37.994: INFO: 31 pods has nil DeletionTimestamp
Sep  6 10:38:37.994: INFO: 
Sep  6 10:38:38.494: INFO: 20 pods remaining
Sep  6 10:38:38.494: INFO: 16 pods has nil DeletionTimestamp
Sep  6 10:38:38.494: INFO: 
Sep  6 10:38:40.101: INFO: 8 pods remaining
Sep  6 10:38:40.101: INFO: 5 pods has nil DeletionTimestamp
Sep  6 10:38:40.101: INFO: 
Sep  6 10:38:40.454: INFO: 0 pods remaining
Sep  6 10:38:40.454: INFO: 0 pods has nil DeletionTimestamp
Sep  6 10:38:40.454: INFO: 
STEP: Gathering metrics 09/06/23 10:38:41.418
Sep  6 10:38:41.492: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Sep  6 10:38:43.572: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.079948585s
Sep  6 10:38:43.572: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Sep  6 10:38:43.572: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Sep  6 10:38:43.655: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  6 10:38:43.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6108" for this suite. 09/06/23 10:38:44.997
------------------------------
• [SLOW TEST] [26.975 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:38:18.069
    Sep  6 10:38:18.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename gc 09/06/23 10:38:18.07
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:38:18.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:38:18.088
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 09/06/23 10:38:18.093
    STEP: delete the rc 09/06/23 10:38:23.236
    STEP: wait for the rc to be deleted 09/06/23 10:38:23.392
    Sep  6 10:38:26.032: INFO: 84 pods remaining
    Sep  6 10:38:26.032: INFO: 79 pods has nil DeletionTimestamp
    Sep  6 10:38:26.032: INFO: 
    Sep  6 10:38:35.563: INFO: 68 pods remaining
    Sep  6 10:38:35.563: INFO: 65 pods has nil DeletionTimestamp
    Sep  6 10:38:35.563: INFO: 
    Sep  6 10:38:36.490: INFO: 45 pods remaining
    Sep  6 10:38:36.490: INFO: 44 pods has nil DeletionTimestamp
    Sep  6 10:38:36.490: INFO: 
    Sep  6 10:38:37.994: INFO: 35 pods remaining
    Sep  6 10:38:37.994: INFO: 31 pods has nil DeletionTimestamp
    Sep  6 10:38:37.994: INFO: 
    Sep  6 10:38:38.494: INFO: 20 pods remaining
    Sep  6 10:38:38.494: INFO: 16 pods has nil DeletionTimestamp
    Sep  6 10:38:38.494: INFO: 
    Sep  6 10:38:40.101: INFO: 8 pods remaining
    Sep  6 10:38:40.101: INFO: 5 pods has nil DeletionTimestamp
    Sep  6 10:38:40.101: INFO: 
    Sep  6 10:38:40.454: INFO: 0 pods remaining
    Sep  6 10:38:40.454: INFO: 0 pods has nil DeletionTimestamp
    Sep  6 10:38:40.454: INFO: 
    STEP: Gathering metrics 09/06/23 10:38:41.418
    Sep  6 10:38:41.492: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Sep  6 10:38:43.572: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.079948585s
    Sep  6 10:38:43.572: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Sep  6 10:38:43.572: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Sep  6 10:38:43.655: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:38:43.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6108" for this suite. 09/06/23 10:38:44.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:38:45.049
Sep  6 10:38:45.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-probe 09/06/23 10:38:45.05
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:38:45.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:38:45.375
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53 in namespace container-probe-8452 09/06/23 10:38:45.44
Sep  6 10:38:46.044: INFO: Waiting up to 5m0s for pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53" in namespace "container-probe-8452" to be "not pending"
Sep  6 10:38:46.138: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 94.043245ms
Sep  6 10:38:48.159: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114913752s
Sep  6 10:38:50.189: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.144714109s
Sep  6 10:38:52.154: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.109992235s
Sep  6 10:38:54.146: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101995203s
Sep  6 10:38:56.154: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110000686s
Sep  6 10:38:58.144: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Running", Reason="", readiness=true. Elapsed: 12.100060546s
Sep  6 10:38:58.144: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53" satisfied condition "not pending"
Sep  6 10:38:58.144: INFO: Started pod test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53 in namespace container-probe-8452
STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:38:58.145
Sep  6 10:38:58.148: INFO: Initial restart count of pod test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53 is 0
STEP: deleting the pod 09/06/23 10:42:59.791
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  6 10:42:59.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8452" for this suite. 09/06/23 10:42:59.836
------------------------------
• [SLOW TEST] [254.804 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:38:45.049
    Sep  6 10:38:45.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-probe 09/06/23 10:38:45.05
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:38:45.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:38:45.375
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53 in namespace container-probe-8452 09/06/23 10:38:45.44
    Sep  6 10:38:46.044: INFO: Waiting up to 5m0s for pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53" in namespace "container-probe-8452" to be "not pending"
    Sep  6 10:38:46.138: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 94.043245ms
    Sep  6 10:38:48.159: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114913752s
    Sep  6 10:38:50.189: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.144714109s
    Sep  6 10:38:52.154: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.109992235s
    Sep  6 10:38:54.146: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101995203s
    Sep  6 10:38:56.154: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110000686s
    Sep  6 10:38:58.144: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53": Phase="Running", Reason="", readiness=true. Elapsed: 12.100060546s
    Sep  6 10:38:58.144: INFO: Pod "test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53" satisfied condition "not pending"
    Sep  6 10:38:58.144: INFO: Started pod test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53 in namespace container-probe-8452
    STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:38:58.145
    Sep  6 10:38:58.148: INFO: Initial restart count of pod test-webserver-2a8c6fbc-92b1-4578-85a2-d3c7cc40ce53 is 0
    STEP: deleting the pod 09/06/23 10:42:59.791
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:42:59.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8452" for this suite. 09/06/23 10:42:59.836
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:42:59.853
Sep  6 10:42:59.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubelet-test 09/06/23 10:42:59.854
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:42:59.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:42:59.886
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Sep  6 10:42:59.899: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36" in namespace "kubelet-test-9391" to be "running and ready"
Sep  6 10:42:59.912: INFO: Pod "busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36": Phase="Pending", Reason="", readiness=false. Elapsed: 12.586525ms
Sep  6 10:42:59.912: INFO: The phase of Pod busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:43:01.923: INFO: Pod "busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36": Phase="Running", Reason="", readiness=true. Elapsed: 2.02407264s
Sep  6 10:43:01.923: INFO: The phase of Pod busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36 is Running (Ready = true)
Sep  6 10:43:01.923: INFO: Pod "busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:43:01.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9391" for this suite. 09/06/23 10:43:01.969
------------------------------
• [2.123 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:42:59.853
    Sep  6 10:42:59.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubelet-test 09/06/23 10:42:59.854
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:42:59.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:42:59.886
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Sep  6 10:42:59.899: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36" in namespace "kubelet-test-9391" to be "running and ready"
    Sep  6 10:42:59.912: INFO: Pod "busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36": Phase="Pending", Reason="", readiness=false. Elapsed: 12.586525ms
    Sep  6 10:42:59.912: INFO: The phase of Pod busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:43:01.923: INFO: Pod "busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36": Phase="Running", Reason="", readiness=true. Elapsed: 2.02407264s
    Sep  6 10:43:01.923: INFO: The phase of Pod busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36 is Running (Ready = true)
    Sep  6 10:43:01.923: INFO: Pod "busybox-readonly-fsd794a433-a4a1-464c-ad8f-755be2efae36" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:43:01.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9391" for this suite. 09/06/23 10:43:01.969
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:43:01.977
Sep  6 10:43:01.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:43:01.978
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:43:01.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:43:02
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-0e47350f-c042-421e-a462-fb5ba7f0e368 09/06/23 10:43:02.002
STEP: Creating a pod to test consume secrets 09/06/23 10:43:02.007
Sep  6 10:43:02.105: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396" in namespace "projected-117" to be "Succeeded or Failed"
Sep  6 10:43:02.155: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396": Phase="Pending", Reason="", readiness=false. Elapsed: 48.860326ms
Sep  6 10:43:04.166: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060416879s
Sep  6 10:43:06.171: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065148521s
Sep  6 10:43:08.174: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06875226s
STEP: Saw pod success 09/06/23 10:43:08.175
Sep  6 10:43:08.176: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396" satisfied condition "Succeeded or Failed"
Sep  6 10:43:08.188: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/06/23 10:43:08.202
Sep  6 10:43:08.222: INFO: Waiting for pod pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396 to disappear
Sep  6 10:43:08.225: INFO: Pod pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  6 10:43:08.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-117" for this suite. 09/06/23 10:43:08.23
------------------------------
• [SLOW TEST] [6.262 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:43:01.977
    Sep  6 10:43:01.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:43:01.978
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:43:01.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:43:02
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-0e47350f-c042-421e-a462-fb5ba7f0e368 09/06/23 10:43:02.002
    STEP: Creating a pod to test consume secrets 09/06/23 10:43:02.007
    Sep  6 10:43:02.105: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396" in namespace "projected-117" to be "Succeeded or Failed"
    Sep  6 10:43:02.155: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396": Phase="Pending", Reason="", readiness=false. Elapsed: 48.860326ms
    Sep  6 10:43:04.166: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060416879s
    Sep  6 10:43:06.171: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065148521s
    Sep  6 10:43:08.174: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06875226s
    STEP: Saw pod success 09/06/23 10:43:08.175
    Sep  6 10:43:08.176: INFO: Pod "pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396" satisfied condition "Succeeded or Failed"
    Sep  6 10:43:08.188: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:43:08.202
    Sep  6 10:43:08.222: INFO: Waiting for pod pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396 to disappear
    Sep  6 10:43:08.225: INFO: Pod pod-projected-secrets-cfa85afa-92c4-48b2-8b48-af1014b11396 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:43:08.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-117" for this suite. 09/06/23 10:43:08.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:43:08.24
Sep  6 10:43:08.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-preemption 09/06/23 10:43:08.242
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:43:08.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:43:08.267
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  6 10:43:08.283: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 10:44:08.353: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:44:08.362
Sep  6 10:44:08.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-preemption-path 09/06/23 10:44:08.363
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:44:08.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:44:08.4
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Sep  6 10:44:08.415: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Sep  6 10:44:08.418: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Sep  6 10:44:08.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:44:08.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6117" for this suite. 09/06/23 10:44:08.515
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1132" for this suite. 09/06/23 10:44:08.524
------------------------------
• [SLOW TEST] [60.293 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:43:08.24
    Sep  6 10:43:08.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-preemption 09/06/23 10:43:08.242
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:43:08.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:43:08.267
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  6 10:43:08.283: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  6 10:44:08.353: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:44:08.362
    Sep  6 10:44:08.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-preemption-path 09/06/23 10:44:08.363
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:44:08.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:44:08.4
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Sep  6 10:44:08.415: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Sep  6 10:44:08.418: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:44:08.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:44:08.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6117" for this suite. 09/06/23 10:44:08.515
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1132" for this suite. 09/06/23 10:44:08.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:44:08.538
Sep  6 10:44:08.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-probe 09/06/23 10:44:08.539
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:44:08.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:44:08.558
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-04c13641-1f25-40a4-a278-761aa6ee1e52 in namespace container-probe-8942 09/06/23 10:44:08.56
Sep  6 10:44:08.569: INFO: Waiting up to 5m0s for pod "busybox-04c13641-1f25-40a4-a278-761aa6ee1e52" in namespace "container-probe-8942" to be "not pending"
Sep  6 10:44:08.576: INFO: Pod "busybox-04c13641-1f25-40a4-a278-761aa6ee1e52": Phase="Pending", Reason="", readiness=false. Elapsed: 7.470139ms
Sep  6 10:44:10.593: INFO: Pod "busybox-04c13641-1f25-40a4-a278-761aa6ee1e52": Phase="Running", Reason="", readiness=true. Elapsed: 2.024468035s
Sep  6 10:44:10.594: INFO: Pod "busybox-04c13641-1f25-40a4-a278-761aa6ee1e52" satisfied condition "not pending"
Sep  6 10:44:10.594: INFO: Started pod busybox-04c13641-1f25-40a4-a278-761aa6ee1e52 in namespace container-probe-8942
STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:44:10.594
Sep  6 10:44:10.623: INFO: Initial restart count of pod busybox-04c13641-1f25-40a4-a278-761aa6ee1e52 is 0
STEP: deleting the pod 09/06/23 10:48:11.343
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  6 10:48:11.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8942" for this suite. 09/06/23 10:48:11.387
------------------------------
• [SLOW TEST] [242.862 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:44:08.538
    Sep  6 10:44:08.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-probe 09/06/23 10:44:08.539
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:44:08.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:44:08.558
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-04c13641-1f25-40a4-a278-761aa6ee1e52 in namespace container-probe-8942 09/06/23 10:44:08.56
    Sep  6 10:44:08.569: INFO: Waiting up to 5m0s for pod "busybox-04c13641-1f25-40a4-a278-761aa6ee1e52" in namespace "container-probe-8942" to be "not pending"
    Sep  6 10:44:08.576: INFO: Pod "busybox-04c13641-1f25-40a4-a278-761aa6ee1e52": Phase="Pending", Reason="", readiness=false. Elapsed: 7.470139ms
    Sep  6 10:44:10.593: INFO: Pod "busybox-04c13641-1f25-40a4-a278-761aa6ee1e52": Phase="Running", Reason="", readiness=true. Elapsed: 2.024468035s
    Sep  6 10:44:10.594: INFO: Pod "busybox-04c13641-1f25-40a4-a278-761aa6ee1e52" satisfied condition "not pending"
    Sep  6 10:44:10.594: INFO: Started pod busybox-04c13641-1f25-40a4-a278-761aa6ee1e52 in namespace container-probe-8942
    STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:44:10.594
    Sep  6 10:44:10.623: INFO: Initial restart count of pod busybox-04c13641-1f25-40a4-a278-761aa6ee1e52 is 0
    STEP: deleting the pod 09/06/23 10:48:11.343
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:48:11.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8942" for this suite. 09/06/23 10:48:11.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:48:11.4
Sep  6 10:48:11.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename svcaccounts 09/06/23 10:48:11.401
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:48:11.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:48:11.443
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-w9dcx"  09/06/23 10:48:11.446
Sep  6 10:48:11.451: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-w9dcx"  09/06/23 10:48:11.451
Sep  6 10:48:11.461: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  6 10:48:11.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9903" for this suite. 09/06/23 10:48:11.465
------------------------------
• [0.074 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:48:11.4
    Sep  6 10:48:11.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename svcaccounts 09/06/23 10:48:11.401
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:48:11.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:48:11.443
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-w9dcx"  09/06/23 10:48:11.446
    Sep  6 10:48:11.451: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-w9dcx"  09/06/23 10:48:11.451
    Sep  6 10:48:11.461: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:48:11.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9903" for this suite. 09/06/23 10:48:11.465
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:48:11.475
Sep  6 10:48:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-probe 09/06/23 10:48:11.476
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:48:11.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:48:11.496
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-21395eb9-33ef-4c39-a97e-fda00d671399 in namespace container-probe-7253 09/06/23 10:48:11.498
Sep  6 10:48:11.516: INFO: Waiting up to 5m0s for pod "liveness-21395eb9-33ef-4c39-a97e-fda00d671399" in namespace "container-probe-7253" to be "not pending"
Sep  6 10:48:11.518: INFO: Pod "liveness-21395eb9-33ef-4c39-a97e-fda00d671399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418627ms
Sep  6 10:48:13.530: INFO: Pod "liveness-21395eb9-33ef-4c39-a97e-fda00d671399": Phase="Running", Reason="", readiness=true. Elapsed: 2.01444912s
Sep  6 10:48:13.530: INFO: Pod "liveness-21395eb9-33ef-4c39-a97e-fda00d671399" satisfied condition "not pending"
Sep  6 10:48:13.530: INFO: Started pod liveness-21395eb9-33ef-4c39-a97e-fda00d671399 in namespace container-probe-7253
STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:48:13.53
Sep  6 10:48:13.541: INFO: Initial restart count of pod liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is 0
Sep  6 10:48:35.658: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 1 (22.11721728s elapsed)
Sep  6 10:48:53.719: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 2 (40.178824696s elapsed)
Sep  6 10:49:13.844: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 3 (1m0.30363812s elapsed)
Sep  6 10:49:33.953: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 4 (1m20.412301762s elapsed)
Sep  6 10:50:36.334: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 5 (2m22.793282075s elapsed)
STEP: deleting the pod 09/06/23 10:50:36.335
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  6 10:50:36.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7253" for this suite. 09/06/23 10:50:36.376
------------------------------
• [SLOW TEST] [144.908 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:48:11.475
    Sep  6 10:48:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-probe 09/06/23 10:48:11.476
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:48:11.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:48:11.496
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-21395eb9-33ef-4c39-a97e-fda00d671399 in namespace container-probe-7253 09/06/23 10:48:11.498
    Sep  6 10:48:11.516: INFO: Waiting up to 5m0s for pod "liveness-21395eb9-33ef-4c39-a97e-fda00d671399" in namespace "container-probe-7253" to be "not pending"
    Sep  6 10:48:11.518: INFO: Pod "liveness-21395eb9-33ef-4c39-a97e-fda00d671399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418627ms
    Sep  6 10:48:13.530: INFO: Pod "liveness-21395eb9-33ef-4c39-a97e-fda00d671399": Phase="Running", Reason="", readiness=true. Elapsed: 2.01444912s
    Sep  6 10:48:13.530: INFO: Pod "liveness-21395eb9-33ef-4c39-a97e-fda00d671399" satisfied condition "not pending"
    Sep  6 10:48:13.530: INFO: Started pod liveness-21395eb9-33ef-4c39-a97e-fda00d671399 in namespace container-probe-7253
    STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 10:48:13.53
    Sep  6 10:48:13.541: INFO: Initial restart count of pod liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is 0
    Sep  6 10:48:35.658: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 1 (22.11721728s elapsed)
    Sep  6 10:48:53.719: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 2 (40.178824696s elapsed)
    Sep  6 10:49:13.844: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 3 (1m0.30363812s elapsed)
    Sep  6 10:49:33.953: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 4 (1m20.412301762s elapsed)
    Sep  6 10:50:36.334: INFO: Restart count of pod container-probe-7253/liveness-21395eb9-33ef-4c39-a97e-fda00d671399 is now 5 (2m22.793282075s elapsed)
    STEP: deleting the pod 09/06/23 10:50:36.335
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:50:36.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7253" for this suite. 09/06/23 10:50:36.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:50:36.384
Sep  6 10:50:36.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:50:36.386
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:36.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:36.409
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 09/06/23 10:50:36.411
Sep  6 10:50:36.423: INFO: Waiting up to 5m0s for pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5" in namespace "emptydir-7565" to be "Succeeded or Failed"
Sep  6 10:50:36.429: INFO: Pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.348206ms
Sep  6 10:50:38.442: INFO: Pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018691802s
Sep  6 10:50:40.444: INFO: Pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020132367s
STEP: Saw pod success 09/06/23 10:50:40.444
Sep  6 10:50:40.444: INFO: Pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5" satisfied condition "Succeeded or Failed"
Sep  6 10:50:40.453: INFO: Trying to get logs from node kube-3 pod pod-9cd9abfa-818c-457b-a360-041cc07d7be5 container test-container: <nil>
STEP: delete the pod 09/06/23 10:50:40.485
Sep  6 10:50:40.504: INFO: Waiting for pod pod-9cd9abfa-818c-457b-a360-041cc07d7be5 to disappear
Sep  6 10:50:40.508: INFO: Pod pod-9cd9abfa-818c-457b-a360-041cc07d7be5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:50:40.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7565" for this suite. 09/06/23 10:50:40.511
------------------------------
• [4.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:50:36.384
    Sep  6 10:50:36.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:50:36.386
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:36.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:36.409
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/06/23 10:50:36.411
    Sep  6 10:50:36.423: INFO: Waiting up to 5m0s for pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5" in namespace "emptydir-7565" to be "Succeeded or Failed"
    Sep  6 10:50:36.429: INFO: Pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.348206ms
    Sep  6 10:50:38.442: INFO: Pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018691802s
    Sep  6 10:50:40.444: INFO: Pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020132367s
    STEP: Saw pod success 09/06/23 10:50:40.444
    Sep  6 10:50:40.444: INFO: Pod "pod-9cd9abfa-818c-457b-a360-041cc07d7be5" satisfied condition "Succeeded or Failed"
    Sep  6 10:50:40.453: INFO: Trying to get logs from node kube-3 pod pod-9cd9abfa-818c-457b-a360-041cc07d7be5 container test-container: <nil>
    STEP: delete the pod 09/06/23 10:50:40.485
    Sep  6 10:50:40.504: INFO: Waiting for pod pod-9cd9abfa-818c-457b-a360-041cc07d7be5 to disappear
    Sep  6 10:50:40.508: INFO: Pod pod-9cd9abfa-818c-457b-a360-041cc07d7be5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:50:40.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7565" for this suite. 09/06/23 10:50:40.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:50:40.521
Sep  6 10:50:40.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 10:50:40.522
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:40.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:40.547
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/06/23 10:50:40.549
Sep  6 10:50:40.558: INFO: Waiting up to 5m0s for pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279" in namespace "emptydir-5227" to be "Succeeded or Failed"
Sep  6 10:50:40.566: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Pending", Reason="", readiness=false. Elapsed: 8.275362ms
Sep  6 10:50:42.593: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035136761s
Sep  6 10:50:44.581: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Running", Reason="", readiness=true. Elapsed: 4.023180054s
Sep  6 10:50:46.573: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Running", Reason="", readiness=false. Elapsed: 6.014358573s
Sep  6 10:50:48.580: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021382545s
STEP: Saw pod success 09/06/23 10:50:48.58
Sep  6 10:50:48.580: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279" satisfied condition "Succeeded or Failed"
Sep  6 10:50:48.592: INFO: Trying to get logs from node kube-3 pod pod-24efc2a9-8c08-4959-bd3a-62be5e93f279 container test-container: <nil>
STEP: delete the pod 09/06/23 10:50:48.616
Sep  6 10:50:48.654: INFO: Waiting for pod pod-24efc2a9-8c08-4959-bd3a-62be5e93f279 to disappear
Sep  6 10:50:48.658: INFO: Pod pod-24efc2a9-8c08-4959-bd3a-62be5e93f279 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:50:48.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5227" for this suite. 09/06/23 10:50:48.661
------------------------------
• [SLOW TEST] [8.148 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:50:40.521
    Sep  6 10:50:40.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 10:50:40.522
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:40.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:40.547
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/06/23 10:50:40.549
    Sep  6 10:50:40.558: INFO: Waiting up to 5m0s for pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279" in namespace "emptydir-5227" to be "Succeeded or Failed"
    Sep  6 10:50:40.566: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Pending", Reason="", readiness=false. Elapsed: 8.275362ms
    Sep  6 10:50:42.593: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035136761s
    Sep  6 10:50:44.581: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Running", Reason="", readiness=true. Elapsed: 4.023180054s
    Sep  6 10:50:46.573: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Running", Reason="", readiness=false. Elapsed: 6.014358573s
    Sep  6 10:50:48.580: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021382545s
    STEP: Saw pod success 09/06/23 10:50:48.58
    Sep  6 10:50:48.580: INFO: Pod "pod-24efc2a9-8c08-4959-bd3a-62be5e93f279" satisfied condition "Succeeded or Failed"
    Sep  6 10:50:48.592: INFO: Trying to get logs from node kube-3 pod pod-24efc2a9-8c08-4959-bd3a-62be5e93f279 container test-container: <nil>
    STEP: delete the pod 09/06/23 10:50:48.616
    Sep  6 10:50:48.654: INFO: Waiting for pod pod-24efc2a9-8c08-4959-bd3a-62be5e93f279 to disappear
    Sep  6 10:50:48.658: INFO: Pod pod-24efc2a9-8c08-4959-bd3a-62be5e93f279 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:50:48.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5227" for this suite. 09/06/23 10:50:48.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:50:48.669
Sep  6 10:50:48.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename events 09/06/23 10:50:48.67
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:48.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:48.691
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 09/06/23 10:50:48.693
STEP: listing all events in all namespaces 09/06/23 10:50:48.702
STEP: patching the test event 09/06/23 10:50:48.71
STEP: fetching the test event 09/06/23 10:50:48.716
STEP: updating the test event 09/06/23 10:50:48.719
STEP: getting the test event 09/06/23 10:50:48.727
STEP: deleting the test event 09/06/23 10:50:48.73
STEP: listing all events in all namespaces 09/06/23 10:50:48.736
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  6 10:50:48.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3104" for this suite. 09/06/23 10:50:48.748
------------------------------
• [0.086 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:50:48.669
    Sep  6 10:50:48.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename events 09/06/23 10:50:48.67
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:48.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:48.691
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 09/06/23 10:50:48.693
    STEP: listing all events in all namespaces 09/06/23 10:50:48.702
    STEP: patching the test event 09/06/23 10:50:48.71
    STEP: fetching the test event 09/06/23 10:50:48.716
    STEP: updating the test event 09/06/23 10:50:48.719
    STEP: getting the test event 09/06/23 10:50:48.727
    STEP: deleting the test event 09/06/23 10:50:48.73
    STEP: listing all events in all namespaces 09/06/23 10:50:48.736
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:50:48.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3104" for this suite. 09/06/23 10:50:48.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:50:48.755
Sep  6 10:50:48.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename svcaccounts 09/06/23 10:50:48.756
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:48.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:48.781
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Sep  6 10:50:48.797: INFO: Waiting up to 5m0s for pod "pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273" in namespace "svcaccounts-6754" to be "running"
Sep  6 10:50:48.814: INFO: Pod "pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273": Phase="Pending", Reason="", readiness=false. Elapsed: 17.116376ms
Sep  6 10:50:50.829: INFO: Pod "pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273": Phase="Running", Reason="", readiness=true. Elapsed: 2.03285113s
Sep  6 10:50:50.829: INFO: Pod "pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273" satisfied condition "running"
STEP: reading a file in the container 09/06/23 10:50:50.83
Sep  6 10:50:50.830: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6754 pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 09/06/23 10:50:51.093
Sep  6 10:50:51.094: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6754 pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 09/06/23 10:50:51.236
Sep  6 10:50:51.236: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6754 pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Sep  6 10:50:51.369: INFO: Got root ca configmap in namespace "svcaccounts-6754"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  6 10:50:51.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6754" for this suite. 09/06/23 10:50:51.374
------------------------------
• [2.630 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:50:48.755
    Sep  6 10:50:48.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename svcaccounts 09/06/23 10:50:48.756
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:48.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:48.781
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Sep  6 10:50:48.797: INFO: Waiting up to 5m0s for pod "pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273" in namespace "svcaccounts-6754" to be "running"
    Sep  6 10:50:48.814: INFO: Pod "pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273": Phase="Pending", Reason="", readiness=false. Elapsed: 17.116376ms
    Sep  6 10:50:50.829: INFO: Pod "pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273": Phase="Running", Reason="", readiness=true. Elapsed: 2.03285113s
    Sep  6 10:50:50.829: INFO: Pod "pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273" satisfied condition "running"
    STEP: reading a file in the container 09/06/23 10:50:50.83
    Sep  6 10:50:50.830: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6754 pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 09/06/23 10:50:51.093
    Sep  6 10:50:51.094: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6754 pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 09/06/23 10:50:51.236
    Sep  6 10:50:51.236: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6754 pod-service-account-b942930d-cab3-49db-bdd8-a8edf15dc273 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Sep  6 10:50:51.369: INFO: Got root ca configmap in namespace "svcaccounts-6754"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:50:51.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6754" for this suite. 09/06/23 10:50:51.374
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:50:51.385
Sep  6 10:50:51.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename init-container 09/06/23 10:50:51.386
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:51.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:51.41
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 09/06/23 10:50:51.413
Sep  6 10:50:51.413: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:50:58.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2379" for this suite. 09/06/23 10:50:58.157
------------------------------
• [SLOW TEST] [6.780 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:50:51.385
    Sep  6 10:50:51.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename init-container 09/06/23 10:50:51.386
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:51.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:51.41
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 09/06/23 10:50:51.413
    Sep  6 10:50:51.413: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:50:58.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2379" for this suite. 09/06/23 10:50:58.157
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:50:58.165
Sep  6 10:50:58.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sysctl 09/06/23 10:50:58.167
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:58.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:58.191
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 09/06/23 10:50:58.193
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:50:58.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3450" for this suite. 09/06/23 10:50:58.2
------------------------------
• [0.042 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:50:58.165
    Sep  6 10:50:58.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sysctl 09/06/23 10:50:58.167
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:58.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:58.191
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 09/06/23 10:50:58.193
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:50:58.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3450" for this suite. 09/06/23 10:50:58.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:50:58.207
Sep  6 10:50:58.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:50:58.208
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:58.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:58.23
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 09/06/23 10:50:58.234
Sep  6 10:50:58.242: INFO: Waiting up to 5m0s for pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1" in namespace "downward-api-4521" to be "running and ready"
Sep  6 10:50:58.255: INFO: Pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.76423ms
Sep  6 10:50:58.255: INFO: The phase of Pod labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:51:00.259: INFO: Pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017110633s
Sep  6 10:51:00.259: INFO: The phase of Pod labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1 is Running (Ready = true)
Sep  6 10:51:00.259: INFO: Pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1" satisfied condition "running and ready"
Sep  6 10:51:00.810: INFO: Successfully updated pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 10:51:02.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4521" for this suite. 09/06/23 10:51:02.874
------------------------------
• [4.687 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:50:58.207
    Sep  6 10:50:58.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:50:58.208
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:50:58.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:50:58.23
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 09/06/23 10:50:58.234
    Sep  6 10:50:58.242: INFO: Waiting up to 5m0s for pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1" in namespace "downward-api-4521" to be "running and ready"
    Sep  6 10:50:58.255: INFO: Pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.76423ms
    Sep  6 10:50:58.255: INFO: The phase of Pod labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:51:00.259: INFO: Pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017110633s
    Sep  6 10:51:00.259: INFO: The phase of Pod labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1 is Running (Ready = true)
    Sep  6 10:51:00.259: INFO: Pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1" satisfied condition "running and ready"
    Sep  6 10:51:00.810: INFO: Successfully updated pod "labelsupdate9efb3b7e-ad5e-4194-9205-0c40c85792c1"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:51:02.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4521" for this suite. 09/06/23 10:51:02.874
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:51:02.897
Sep  6 10:51:02.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename podtemplate 09/06/23 10:51:02.898
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:02.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:02.929
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 09/06/23 10:51:02.931
STEP: Replace a pod template 09/06/23 10:51:02.936
Sep  6 10:51:02.944: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  6 10:51:02.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6988" for this suite. 09/06/23 10:51:02.947
------------------------------
• [0.058 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:51:02.897
    Sep  6 10:51:02.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename podtemplate 09/06/23 10:51:02.898
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:02.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:02.929
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 09/06/23 10:51:02.931
    STEP: Replace a pod template 09/06/23 10:51:02.936
    Sep  6 10:51:02.944: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:51:02.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6988" for this suite. 09/06/23 10:51:02.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:51:02.956
Sep  6 10:51:02.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename subpath 09/06/23 10:51:02.956
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:02.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:02.976
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/06/23 10:51:02.978
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-zsz2 09/06/23 10:51:02.988
STEP: Creating a pod to test atomic-volume-subpath 09/06/23 10:51:02.988
Sep  6 10:51:03.013: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zsz2" in namespace "subpath-4120" to be "Succeeded or Failed"
Sep  6 10:51:03.023: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.93525ms
Sep  6 10:51:05.040: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025787449s
Sep  6 10:51:07.038: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 4.024538547s
Sep  6 10:51:09.179: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 6.165232973s
Sep  6 10:51:11.036: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 8.022594837s
Sep  6 10:51:13.031: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 10.016945728s
Sep  6 10:51:15.039: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 12.025695771s
Sep  6 10:51:17.037: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 14.022948424s
Sep  6 10:51:19.037: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 16.022881556s
Sep  6 10:51:21.034: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 18.020216237s
Sep  6 10:51:23.043: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 20.028925717s
Sep  6 10:51:25.037: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=false. Elapsed: 22.022789265s
Sep  6 10:51:27.037: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022943899s
STEP: Saw pod success 09/06/23 10:51:27.037
Sep  6 10:51:27.038: INFO: Pod "pod-subpath-test-projected-zsz2" satisfied condition "Succeeded or Failed"
Sep  6 10:51:27.049: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-projected-zsz2 container test-container-subpath-projected-zsz2: <nil>
STEP: delete the pod 09/06/23 10:51:27.073
Sep  6 10:51:27.101: INFO: Waiting for pod pod-subpath-test-projected-zsz2 to disappear
Sep  6 10:51:27.104: INFO: Pod pod-subpath-test-projected-zsz2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-zsz2 09/06/23 10:51:27.104
Sep  6 10:51:27.104: INFO: Deleting pod "pod-subpath-test-projected-zsz2" in namespace "subpath-4120"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  6 10:51:27.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4120" for this suite. 09/06/23 10:51:27.11
------------------------------
• [SLOW TEST] [24.162 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:51:02.956
    Sep  6 10:51:02.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename subpath 09/06/23 10:51:02.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:02.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:02.976
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/06/23 10:51:02.978
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-zsz2 09/06/23 10:51:02.988
    STEP: Creating a pod to test atomic-volume-subpath 09/06/23 10:51:02.988
    Sep  6 10:51:03.013: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zsz2" in namespace "subpath-4120" to be "Succeeded or Failed"
    Sep  6 10:51:03.023: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.93525ms
    Sep  6 10:51:05.040: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025787449s
    Sep  6 10:51:07.038: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 4.024538547s
    Sep  6 10:51:09.179: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 6.165232973s
    Sep  6 10:51:11.036: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 8.022594837s
    Sep  6 10:51:13.031: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 10.016945728s
    Sep  6 10:51:15.039: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 12.025695771s
    Sep  6 10:51:17.037: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 14.022948424s
    Sep  6 10:51:19.037: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 16.022881556s
    Sep  6 10:51:21.034: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 18.020216237s
    Sep  6 10:51:23.043: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=true. Elapsed: 20.028925717s
    Sep  6 10:51:25.037: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Running", Reason="", readiness=false. Elapsed: 22.022789265s
    Sep  6 10:51:27.037: INFO: Pod "pod-subpath-test-projected-zsz2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022943899s
    STEP: Saw pod success 09/06/23 10:51:27.037
    Sep  6 10:51:27.038: INFO: Pod "pod-subpath-test-projected-zsz2" satisfied condition "Succeeded or Failed"
    Sep  6 10:51:27.049: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-projected-zsz2 container test-container-subpath-projected-zsz2: <nil>
    STEP: delete the pod 09/06/23 10:51:27.073
    Sep  6 10:51:27.101: INFO: Waiting for pod pod-subpath-test-projected-zsz2 to disappear
    Sep  6 10:51:27.104: INFO: Pod pod-subpath-test-projected-zsz2 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-zsz2 09/06/23 10:51:27.104
    Sep  6 10:51:27.104: INFO: Deleting pod "pod-subpath-test-projected-zsz2" in namespace "subpath-4120"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:51:27.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4120" for this suite. 09/06/23 10:51:27.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:51:27.118
Sep  6 10:51:27.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename deployment 09/06/23 10:51:27.119
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:27.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:27.14
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Sep  6 10:51:27.141: INFO: Creating simple deployment test-new-deployment
Sep  6 10:51:27.157: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 09/06/23 10:51:29.191
STEP: updating a scale subresource 09/06/23 10:51:29.195
STEP: verifying the deployment Spec.Replicas was modified 09/06/23 10:51:29.205
STEP: Patch a scale subresource 09/06/23 10:51:29.21
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  6 10:51:29.249: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5317  b21f1cad-61c3-4500-b2c2-524d57f73a01 18885 3 2023-09-06 10:51:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-06 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:51:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004987c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-06 10:51:28 +0000 UTC,LastTransitionTime:2023-09-06 10:51:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-06 10:51:28 +0000 UTC,LastTransitionTime:2023-09-06 10:51:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 10:51:29.258: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5317  9137c8b7-ef33-4e5a-b1f5-54f83e7417d0 18890 2 2023-09-06 10:51:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b21f1cad-61c3-4500-b2c2-524d57f73a01 0xc004eecd77 0xc004eecd78}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b21f1cad-61c3-4500-b2c2-524d57f73a01\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eece08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:51:29.264: INFO: Pod "test-new-deployment-7f5969cbc7-bg8bf" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-bg8bf test-new-deployment-7f5969cbc7- deployment-5317  3152cfdf-01d8-4153-8897-04935cadb918 18879 0 2023-09-06 10:51:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:36e529cc2fc88d78f35b50609af622b77cc8132a1624d472f5afeba8c8967d55 cni.projectcalico.org/podIP:10.233.99.120/32 cni.projectcalico.org/podIPs:10.233.99.120/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 9137c8b7-ef33-4e5a-b1f5-54f83e7417d0 0xc0050aff97 0xc0050aff98}] [] [{calico Update v1 2023-09-06 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-06 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9137c8b7-ef33-4e5a-b1f5-54f83e7417d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 10:51:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5lnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5lnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.120,StartTime:2023-09-06 10:51:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 10:51:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b77c67311f241a0aa1e53738eccff23ba9981abf139641ff6d6dfaad3b4a2e78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:51:29.264: INFO: Pod "test-new-deployment-7f5969cbc7-qzxsz" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qzxsz test-new-deployment-7f5969cbc7- deployment-5317  1e296c81-5c4b-4309-8909-beb78982fc36 18893 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 9137c8b7-ef33-4e5a-b1f5-54f83e7417d0 0xc0026d8197 0xc0026d8198}] [] [{kube-controller-manager Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9137c8b7-ef33-4e5a-b1f5-54f83e7417d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4d59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4d59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 10:51:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  6 10:51:29.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5317" for this suite. 09/06/23 10:51:29.277
------------------------------
• [2.186 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:51:27.118
    Sep  6 10:51:27.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename deployment 09/06/23 10:51:27.119
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:27.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:27.14
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Sep  6 10:51:27.141: INFO: Creating simple deployment test-new-deployment
    Sep  6 10:51:27.157: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 09/06/23 10:51:29.191
    STEP: updating a scale subresource 09/06/23 10:51:29.195
    STEP: verifying the deployment Spec.Replicas was modified 09/06/23 10:51:29.205
    STEP: Patch a scale subresource 09/06/23 10:51:29.21
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  6 10:51:29.249: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-5317  b21f1cad-61c3-4500-b2c2-524d57f73a01 18885 3 2023-09-06 10:51:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-06 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:51:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004987c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-06 10:51:28 +0000 UTC,LastTransitionTime:2023-09-06 10:51:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-06 10:51:28 +0000 UTC,LastTransitionTime:2023-09-06 10:51:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  6 10:51:29.258: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5317  9137c8b7-ef33-4e5a-b1f5-54f83e7417d0 18890 2 2023-09-06 10:51:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b21f1cad-61c3-4500-b2c2-524d57f73a01 0xc004eecd77 0xc004eecd78}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b21f1cad-61c3-4500-b2c2-524d57f73a01\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eece08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:51:29.264: INFO: Pod "test-new-deployment-7f5969cbc7-bg8bf" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-bg8bf test-new-deployment-7f5969cbc7- deployment-5317  3152cfdf-01d8-4153-8897-04935cadb918 18879 0 2023-09-06 10:51:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:36e529cc2fc88d78f35b50609af622b77cc8132a1624d472f5afeba8c8967d55 cni.projectcalico.org/podIP:10.233.99.120/32 cni.projectcalico.org/podIPs:10.233.99.120/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 9137c8b7-ef33-4e5a-b1f5-54f83e7417d0 0xc0050aff97 0xc0050aff98}] [] [{calico Update v1 2023-09-06 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-06 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9137c8b7-ef33-4e5a-b1f5-54f83e7417d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 10:51:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5lnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5lnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.120,StartTime:2023-09-06 10:51:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 10:51:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b77c67311f241a0aa1e53738eccff23ba9981abf139641ff6d6dfaad3b4a2e78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 10:51:29.264: INFO: Pod "test-new-deployment-7f5969cbc7-qzxsz" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qzxsz test-new-deployment-7f5969cbc7- deployment-5317  1e296c81-5c4b-4309-8909-beb78982fc36 18893 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 9137c8b7-ef33-4e5a-b1f5-54f83e7417d0 0xc0026d8197 0xc0026d8198}] [] [{kube-controller-manager Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9137c8b7-ef33-4e5a-b1f5-54f83e7417d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4d59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4d59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:51:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 10:51:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:51:29.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5317" for this suite. 09/06/23 10:51:29.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:51:29.306
Sep  6 10:51:29.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename watch 09/06/23 10:51:29.307
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:29.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:29.356
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 09/06/23 10:51:29.358
STEP: creating a watch on configmaps with label B 09/06/23 10:51:29.358
STEP: creating a watch on configmaps with label A or B 09/06/23 10:51:29.359
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/06/23 10:51:29.36
Sep  6 10:51:29.364: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18912 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:51:29.364: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18912 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/06/23 10:51:29.364
Sep  6 10:51:29.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18913 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:51:29.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18913 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/06/23 10:51:29.372
Sep  6 10:51:29.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18914 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:51:29.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18914 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/06/23 10:51:29.381
Sep  6 10:51:29.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18915 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:51:29.389: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18915 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/06/23 10:51:29.389
Sep  6 10:51:29.393: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7282  a6ac7ba9-2a52-4ed0-9c01-5e5b7929edca 18916 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:51:29.393: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7282  a6ac7ba9-2a52-4ed0-9c01-5e5b7929edca 18916 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/06/23 10:51:39.394
Sep  6 10:51:39.427: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7282  a6ac7ba9-2a52-4ed0-9c01-5e5b7929edca 18989 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:51:39.427: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7282  a6ac7ba9-2a52-4ed0-9c01-5e5b7929edca 18989 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  6 10:51:49.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7282" for this suite. 09/06/23 10:51:49.45
------------------------------
• [SLOW TEST] [20.160 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:51:29.306
    Sep  6 10:51:29.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename watch 09/06/23 10:51:29.307
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:29.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:29.356
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 09/06/23 10:51:29.358
    STEP: creating a watch on configmaps with label B 09/06/23 10:51:29.358
    STEP: creating a watch on configmaps with label A or B 09/06/23 10:51:29.359
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/06/23 10:51:29.36
    Sep  6 10:51:29.364: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18912 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 10:51:29.364: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18912 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/06/23 10:51:29.364
    Sep  6 10:51:29.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18913 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 10:51:29.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18913 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/06/23 10:51:29.372
    Sep  6 10:51:29.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18914 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 10:51:29.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18914 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/06/23 10:51:29.381
    Sep  6 10:51:29.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18915 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 10:51:29.389: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7282  c319ce22-b256-4b1c-8c6c-871d22feea29 18915 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/06/23 10:51:29.389
    Sep  6 10:51:29.393: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7282  a6ac7ba9-2a52-4ed0-9c01-5e5b7929edca 18916 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 10:51:29.393: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7282  a6ac7ba9-2a52-4ed0-9c01-5e5b7929edca 18916 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/06/23 10:51:39.394
    Sep  6 10:51:39.427: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7282  a6ac7ba9-2a52-4ed0-9c01-5e5b7929edca 18989 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 10:51:39.427: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7282  a6ac7ba9-2a52-4ed0-9c01-5e5b7929edca 18989 0 2023-09-06 10:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-06 10:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:51:49.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7282" for this suite. 09/06/23 10:51:49.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:51:49.467
Sep  6 10:51:49.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replication-controller 09/06/23 10:51:49.468
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:49.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:49.491
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380 09/06/23 10:51:49.493
Sep  6 10:51:49.505: INFO: Pod name my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380: Found 0 pods out of 1
Sep  6 10:51:54.514: INFO: Pod name my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380: Found 1 pods out of 1
Sep  6 10:51:54.514: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380" are running
Sep  6 10:51:54.514: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8" in namespace "replication-controller-3499" to be "running"
Sep  6 10:51:54.520: INFO: Pod "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8": Phase="Running", Reason="", readiness=true. Elapsed: 6.237788ms
Sep  6 10:51:54.520: INFO: Pod "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8" satisfied condition "running"
Sep  6 10:51:54.520: INFO: Pod "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:51:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:51:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:51:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:51:49 +0000 UTC Reason: Message:}])
Sep  6 10:51:54.520: INFO: Trying to dial the pod
Sep  6 10:51:59.535: INFO: Controller my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380: Got expected result from replica 1 [my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8]: "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  6 10:51:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3499" for this suite. 09/06/23 10:51:59.539
------------------------------
• [SLOW TEST] [10.079 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:51:49.467
    Sep  6 10:51:49.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replication-controller 09/06/23 10:51:49.468
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:49.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:49.491
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380 09/06/23 10:51:49.493
    Sep  6 10:51:49.505: INFO: Pod name my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380: Found 0 pods out of 1
    Sep  6 10:51:54.514: INFO: Pod name my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380: Found 1 pods out of 1
    Sep  6 10:51:54.514: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380" are running
    Sep  6 10:51:54.514: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8" in namespace "replication-controller-3499" to be "running"
    Sep  6 10:51:54.520: INFO: Pod "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8": Phase="Running", Reason="", readiness=true. Elapsed: 6.237788ms
    Sep  6 10:51:54.520: INFO: Pod "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8" satisfied condition "running"
    Sep  6 10:51:54.520: INFO: Pod "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:51:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:51:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:51:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-06 10:51:49 +0000 UTC Reason: Message:}])
    Sep  6 10:51:54.520: INFO: Trying to dial the pod
    Sep  6 10:51:59.535: INFO: Controller my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380: Got expected result from replica 1 [my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8]: "my-hostname-basic-cc9d449b-9f3f-4af9-a781-af59cb8bb380-4gnp8", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:51:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3499" for this suite. 09/06/23 10:51:59.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:51:59.547
Sep  6 10:51:59.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:51:59.548
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:59.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:59.568
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 09/06/23 10:51:59.57
Sep  6 10:51:59.584: INFO: Waiting up to 5m0s for pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a" in namespace "downward-api-8623" to be "running and ready"
Sep  6 10:51:59.588: INFO: Pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.516423ms
Sep  6 10:51:59.588: INFO: The phase of Pod annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:52:01.595: INFO: Pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a": Phase="Running", Reason="", readiness=true. Elapsed: 2.010755529s
Sep  6 10:52:01.595: INFO: The phase of Pod annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a is Running (Ready = true)
Sep  6 10:52:01.595: INFO: Pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a" satisfied condition "running and ready"
Sep  6 10:52:02.154: INFO: Successfully updated pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 10:52:06.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8623" for this suite. 09/06/23 10:52:06.181
------------------------------
• [SLOW TEST] [6.640 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:51:59.547
    Sep  6 10:51:59.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:51:59.548
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:51:59.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:51:59.568
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 09/06/23 10:51:59.57
    Sep  6 10:51:59.584: INFO: Waiting up to 5m0s for pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a" in namespace "downward-api-8623" to be "running and ready"
    Sep  6 10:51:59.588: INFO: Pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.516423ms
    Sep  6 10:51:59.588: INFO: The phase of Pod annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:52:01.595: INFO: Pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a": Phase="Running", Reason="", readiness=true. Elapsed: 2.010755529s
    Sep  6 10:52:01.595: INFO: The phase of Pod annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a is Running (Ready = true)
    Sep  6 10:52:01.595: INFO: Pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a" satisfied condition "running and ready"
    Sep  6 10:52:02.154: INFO: Successfully updated pod "annotationupdate1879fd73-38d6-437f-99d5-95f8a8b9db5a"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:52:06.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8623" for this suite. 09/06/23 10:52:06.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:52:06.188
Sep  6 10:52:06.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 10:52:06.189
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:52:06.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:52:06.21
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-xfhh5" 09/06/23 10:52:06.215
Sep  6 10:52:06.222: INFO: Resource quota "e2e-rq-status-xfhh5" reports spec: hard cpu limit of 500m
Sep  6 10:52:06.222: INFO: Resource quota "e2e-rq-status-xfhh5" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-xfhh5" /status 09/06/23 10:52:06.222
STEP: Confirm /status for "e2e-rq-status-xfhh5" resourceQuota via watch 09/06/23 10:52:06.235
Sep  6 10:52:06.238: INFO: observed resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList(nil)
Sep  6 10:52:06.238: INFO: Found resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  6 10:52:06.238: INFO: ResourceQuota "e2e-rq-status-xfhh5" /status was updated
STEP: Patching hard spec values for cpu & memory 09/06/23 10:52:06.241
Sep  6 10:52:06.248: INFO: Resource quota "e2e-rq-status-xfhh5" reports spec: hard cpu limit of 1
Sep  6 10:52:06.249: INFO: Resource quota "e2e-rq-status-xfhh5" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-xfhh5" /status 09/06/23 10:52:06.249
STEP: Confirm /status for "e2e-rq-status-xfhh5" resourceQuota via watch 09/06/23 10:52:06.258
Sep  6 10:52:06.260: INFO: observed resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  6 10:52:06.260: INFO: Found resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Sep  6 10:52:06.260: INFO: ResourceQuota "e2e-rq-status-xfhh5" /status was patched
STEP: Get "e2e-rq-status-xfhh5" /status 09/06/23 10:52:06.26
Sep  6 10:52:06.263: INFO: Resourcequota "e2e-rq-status-xfhh5" reports status: hard cpu of 1
Sep  6 10:52:06.264: INFO: Resourcequota "e2e-rq-status-xfhh5" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-xfhh5" /status before checking Spec is unchanged 09/06/23 10:52:06.267
Sep  6 10:52:06.275: INFO: Resourcequota "e2e-rq-status-xfhh5" reports status: hard cpu of 2
Sep  6 10:52:06.275: INFO: Resourcequota "e2e-rq-status-xfhh5" reports status: hard memory of 2Gi
Sep  6 10:52:06.277: INFO: Found resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Sep  6 10:55:36.284: INFO: ResourceQuota "e2e-rq-status-xfhh5" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 10:55:36.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5512" for this suite. 09/06/23 10:55:36.287
------------------------------
• [SLOW TEST] [210.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:52:06.188
    Sep  6 10:52:06.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 10:52:06.189
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:52:06.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:52:06.21
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-xfhh5" 09/06/23 10:52:06.215
    Sep  6 10:52:06.222: INFO: Resource quota "e2e-rq-status-xfhh5" reports spec: hard cpu limit of 500m
    Sep  6 10:52:06.222: INFO: Resource quota "e2e-rq-status-xfhh5" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-xfhh5" /status 09/06/23 10:52:06.222
    STEP: Confirm /status for "e2e-rq-status-xfhh5" resourceQuota via watch 09/06/23 10:52:06.235
    Sep  6 10:52:06.238: INFO: observed resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList(nil)
    Sep  6 10:52:06.238: INFO: Found resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  6 10:52:06.238: INFO: ResourceQuota "e2e-rq-status-xfhh5" /status was updated
    STEP: Patching hard spec values for cpu & memory 09/06/23 10:52:06.241
    Sep  6 10:52:06.248: INFO: Resource quota "e2e-rq-status-xfhh5" reports spec: hard cpu limit of 1
    Sep  6 10:52:06.249: INFO: Resource quota "e2e-rq-status-xfhh5" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-xfhh5" /status 09/06/23 10:52:06.249
    STEP: Confirm /status for "e2e-rq-status-xfhh5" resourceQuota via watch 09/06/23 10:52:06.258
    Sep  6 10:52:06.260: INFO: observed resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  6 10:52:06.260: INFO: Found resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Sep  6 10:52:06.260: INFO: ResourceQuota "e2e-rq-status-xfhh5" /status was patched
    STEP: Get "e2e-rq-status-xfhh5" /status 09/06/23 10:52:06.26
    Sep  6 10:52:06.263: INFO: Resourcequota "e2e-rq-status-xfhh5" reports status: hard cpu of 1
    Sep  6 10:52:06.264: INFO: Resourcequota "e2e-rq-status-xfhh5" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-xfhh5" /status before checking Spec is unchanged 09/06/23 10:52:06.267
    Sep  6 10:52:06.275: INFO: Resourcequota "e2e-rq-status-xfhh5" reports status: hard cpu of 2
    Sep  6 10:52:06.275: INFO: Resourcequota "e2e-rq-status-xfhh5" reports status: hard memory of 2Gi
    Sep  6 10:52:06.277: INFO: Found resourceQuota "e2e-rq-status-xfhh5" in namespace "resourcequota-5512" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Sep  6 10:55:36.284: INFO: ResourceQuota "e2e-rq-status-xfhh5" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:55:36.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5512" for this suite. 09/06/23 10:55:36.287
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:55:36.295
Sep  6 10:55:36.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 10:55:36.295
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:55:36.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:55:36.316
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 09/06/23 10:55:36.318
Sep  6 10:55:36.326: INFO: Waiting up to 5m0s for pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6" in namespace "downward-api-4613" to be "Succeeded or Failed"
Sep  6 10:55:36.329: INFO: Pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.559535ms
Sep  6 10:55:38.336: INFO: Pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010168106s
Sep  6 10:55:40.339: INFO: Pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013544688s
STEP: Saw pod success 09/06/23 10:55:40.339
Sep  6 10:55:40.340: INFO: Pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6" satisfied condition "Succeeded or Failed"
Sep  6 10:55:40.345: INFO: Trying to get logs from node kube-3 pod downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6 container dapi-container: <nil>
STEP: delete the pod 09/06/23 10:55:40.366
Sep  6 10:55:40.382: INFO: Waiting for pod downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6 to disappear
Sep  6 10:55:40.386: INFO: Pod downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  6 10:55:40.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4613" for this suite. 09/06/23 10:55:40.389
------------------------------
• [4.099 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:55:36.295
    Sep  6 10:55:36.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 10:55:36.295
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:55:36.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:55:36.316
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 09/06/23 10:55:36.318
    Sep  6 10:55:36.326: INFO: Waiting up to 5m0s for pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6" in namespace "downward-api-4613" to be "Succeeded or Failed"
    Sep  6 10:55:36.329: INFO: Pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.559535ms
    Sep  6 10:55:38.336: INFO: Pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010168106s
    Sep  6 10:55:40.339: INFO: Pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013544688s
    STEP: Saw pod success 09/06/23 10:55:40.339
    Sep  6 10:55:40.340: INFO: Pod "downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6" satisfied condition "Succeeded or Failed"
    Sep  6 10:55:40.345: INFO: Trying to get logs from node kube-3 pod downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6 container dapi-container: <nil>
    STEP: delete the pod 09/06/23 10:55:40.366
    Sep  6 10:55:40.382: INFO: Waiting for pod downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6 to disappear
    Sep  6 10:55:40.386: INFO: Pod downward-api-9e403706-e56f-4674-81ef-7f0fd61098c6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:55:40.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4613" for this suite. 09/06/23 10:55:40.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:55:40.396
Sep  6 10:55:40.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 10:55:40.397
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:55:40.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:55:40.419
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-b6dffe2e-51e9-4ffe-bb34-709e87b19020 09/06/23 10:55:40.426
STEP: Creating configMap with name cm-test-opt-upd-60360215-f01d-4581-9196-1ce7257f8648 09/06/23 10:55:40.434
STEP: Creating the pod 09/06/23 10:55:40.44
Sep  6 10:55:40.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1" in namespace "configmap-930" to be "running and ready"
Sep  6 10:55:40.455: INFO: Pod "pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.467796ms
Sep  6 10:55:40.455: INFO: The phase of Pod pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:55:42.461: INFO: Pod "pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010331619s
Sep  6 10:55:42.461: INFO: The phase of Pod pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1 is Running (Ready = true)
Sep  6 10:55:42.461: INFO: Pod "pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b6dffe2e-51e9-4ffe-bb34-709e87b19020 09/06/23 10:55:42.483
STEP: Updating configmap cm-test-opt-upd-60360215-f01d-4581-9196-1ce7257f8648 09/06/23 10:55:42.489
STEP: Creating configMap with name cm-test-opt-create-25550eed-5b6a-452a-a54c-990a593d1328 09/06/23 10:55:42.498
STEP: waiting to observe update in volume 09/06/23 10:55:42.504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 10:55:44.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-930" for this suite. 09/06/23 10:55:44.576
------------------------------
• [4.192 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:55:40.396
    Sep  6 10:55:40.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 10:55:40.397
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:55:40.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:55:40.419
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-b6dffe2e-51e9-4ffe-bb34-709e87b19020 09/06/23 10:55:40.426
    STEP: Creating configMap with name cm-test-opt-upd-60360215-f01d-4581-9196-1ce7257f8648 09/06/23 10:55:40.434
    STEP: Creating the pod 09/06/23 10:55:40.44
    Sep  6 10:55:40.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1" in namespace "configmap-930" to be "running and ready"
    Sep  6 10:55:40.455: INFO: Pod "pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.467796ms
    Sep  6 10:55:40.455: INFO: The phase of Pod pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:55:42.461: INFO: Pod "pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010331619s
    Sep  6 10:55:42.461: INFO: The phase of Pod pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1 is Running (Ready = true)
    Sep  6 10:55:42.461: INFO: Pod "pod-configmaps-904439d7-8a21-41ec-b000-c2fb8c2c7bb1" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b6dffe2e-51e9-4ffe-bb34-709e87b19020 09/06/23 10:55:42.483
    STEP: Updating configmap cm-test-opt-upd-60360215-f01d-4581-9196-1ce7257f8648 09/06/23 10:55:42.489
    STEP: Creating configMap with name cm-test-opt-create-25550eed-5b6a-452a-a54c-990a593d1328 09/06/23 10:55:42.498
    STEP: waiting to observe update in volume 09/06/23 10:55:42.504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:55:44.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-930" for this suite. 09/06/23 10:55:44.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:55:44.591
Sep  6 10:55:44.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-preemption 09/06/23 10:55:44.593
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:55:44.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:55:44.617
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  6 10:55:44.634: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 10:56:44.688: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:56:44.694
Sep  6 10:56:44.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-preemption-path 09/06/23 10:56:44.695
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:56:44.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:56:44.718
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 09/06/23 10:56:44.72
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/06/23 10:56:44.72
Sep  6 10:56:44.729: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8725" to be "running"
Sep  6 10:56:44.734: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.459771ms
Sep  6 10:56:46.749: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.020092405s
Sep  6 10:56:46.749: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/06/23 10:56:46.764
Sep  6 10:56:46.791: INFO: found a healthy node: kube-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Sep  6 10:56:55.025: INFO: pods created so far: [1 1 1]
Sep  6 10:56:55.025: INFO: length of pods created so far: 3
Sep  6 10:56:57.044: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:04.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:04.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8725" for this suite. 09/06/23 10:57:04.134
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-748" for this suite. 09/06/23 10:57:04.141
------------------------------
• [SLOW TEST] [79.560 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:55:44.591
    Sep  6 10:55:44.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-preemption 09/06/23 10:55:44.593
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:55:44.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:55:44.617
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  6 10:55:44.634: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  6 10:56:44.688: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:56:44.694
    Sep  6 10:56:44.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-preemption-path 09/06/23 10:56:44.695
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:56:44.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:56:44.718
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 09/06/23 10:56:44.72
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/06/23 10:56:44.72
    Sep  6 10:56:44.729: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8725" to be "running"
    Sep  6 10:56:44.734: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.459771ms
    Sep  6 10:56:46.749: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.020092405s
    Sep  6 10:56:46.749: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/06/23 10:56:46.764
    Sep  6 10:56:46.791: INFO: found a healthy node: kube-3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Sep  6 10:56:55.025: INFO: pods created so far: [1 1 1]
    Sep  6 10:56:55.025: INFO: length of pods created so far: 3
    Sep  6 10:56:57.044: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:04.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:04.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8725" for this suite. 09/06/23 10:57:04.134
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-748" for this suite. 09/06/23 10:57:04.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:04.152
Sep  6 10:57:04.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replicaset 09/06/23 10:57:04.153
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:04.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:04.191
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Sep  6 10:57:04.219: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 10:57:09.231: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/06/23 10:57:09.231
STEP: Scaling up "test-rs" replicaset  09/06/23 10:57:09.231
Sep  6 10:57:09.245: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 09/06/23 10:57:09.245
W0906 10:57:09.254468      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  6 10:57:09.257: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 1, AvailableReplicas 1
Sep  6 10:57:09.285: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 1, AvailableReplicas 1
Sep  6 10:57:09.302: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 1, AvailableReplicas 1
Sep  6 10:57:09.314: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 1, AvailableReplicas 1
Sep  6 10:57:10.857: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 2, AvailableReplicas 2
Sep  6 10:57:11.303: INFO: observed Replicaset test-rs in namespace replicaset-1141 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:11.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1141" for this suite. 09/06/23 10:57:11.309
------------------------------
• [SLOW TEST] [7.165 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:04.152
    Sep  6 10:57:04.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replicaset 09/06/23 10:57:04.153
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:04.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:04.191
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Sep  6 10:57:04.219: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  6 10:57:09.231: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/06/23 10:57:09.231
    STEP: Scaling up "test-rs" replicaset  09/06/23 10:57:09.231
    Sep  6 10:57:09.245: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 09/06/23 10:57:09.245
    W0906 10:57:09.254468      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  6 10:57:09.257: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 1, AvailableReplicas 1
    Sep  6 10:57:09.285: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 1, AvailableReplicas 1
    Sep  6 10:57:09.302: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 1, AvailableReplicas 1
    Sep  6 10:57:09.314: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 1, AvailableReplicas 1
    Sep  6 10:57:10.857: INFO: observed ReplicaSet test-rs in namespace replicaset-1141 with ReadyReplicas 2, AvailableReplicas 2
    Sep  6 10:57:11.303: INFO: observed Replicaset test-rs in namespace replicaset-1141 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:11.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1141" for this suite. 09/06/23 10:57:11.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:11.318
Sep  6 10:57:11.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-pred 09/06/23 10:57:11.319
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:11.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:11.343
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  6 10:57:11.345: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 10:57:11.351: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 10:57:11.354: INFO: 
Logging pods the apiserver thinks is on node kube-1 before test
Sep  6 10:57:11.360: INFO: calico-kube-controllers-6dfcdfb99-6q4ng from kube-system started at 2023-09-06 09:55:41 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 10:57:11.360: INFO: calico-node-pkqgc from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container calico-node ready: true, restart count 2
Sep  6 10:57:11.360: INFO: coredns-645b46f4b6-hq55k from kube-system started at 2023-09-06 09:55:53 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container coredns ready: true, restart count 0
Sep  6 10:57:11.360: INFO: kube-apiserver-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container kube-apiserver ready: true, restart count 2
Sep  6 10:57:11.360: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container kube-controller-manager ready: true, restart count 5
Sep  6 10:57:11.360: INFO: kube-proxy-fjqk6 from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 10:57:11.360: INFO: kube-scheduler-kube-1 from kube-system started at 2023-09-06 09:52:15 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container kube-scheduler ready: true, restart count 4
Sep  6 10:57:11.360: INFO: nodelocaldns-74qn2 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 10:57:11.360: INFO: test-rs-dnnhh from replicaset-1141 started at 2023-09-06 10:57:09 +0000 UTC (2 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container httpd ready: true, restart count 0
Sep  6 10:57:11.360: INFO: 	Container test-rs ready: true, restart count 0
Sep  6 10:57:11.360: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 10:57:11.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:57:11.360: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:57:11.360: INFO: 
Logging pods the apiserver thinks is on node kube-2 before test
Sep  6 10:57:11.366: INFO: calico-node-f57x2 from kube-system started at 2023-09-06 09:54:23 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container calico-node ready: true, restart count 2
Sep  6 10:57:11.366: INFO: coredns-645b46f4b6-9lpfv from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container coredns ready: true, restart count 0
Sep  6 10:57:11.366: INFO: dns-autoscaler-659b8c48cb-5h6w8 from kube-system started at 2023-09-06 09:55:57 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container autoscaler ready: true, restart count 0
Sep  6 10:57:11.366: INFO: kube-apiserver-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container kube-apiserver ready: true, restart count 1
Sep  6 10:57:11.366: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-09-06 09:53:08 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container kube-controller-manager ready: true, restart count 3
Sep  6 10:57:11.366: INFO: kube-proxy-7fxzk from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 10:57:11.366: INFO: kube-scheduler-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container kube-scheduler ready: true, restart count 3
Sep  6 10:57:11.366: INFO: nodelocaldns-jpj4c from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 10:57:11.366: INFO: test-rs-rw6ht from replicaset-1141 started at 2023-09-06 10:57:09 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container httpd ready: true, restart count 0
Sep  6 10:57:11.366: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 10:57:11.366: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:57:11.366: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:57:11.366: INFO: 
Logging pods the apiserver thinks is on node kube-3 before test
Sep  6 10:57:11.372: INFO: calico-node-6w7db from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.372: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 10:57:11.372: INFO: kube-proxy-sfndk from kube-system started at 2023-09-06 09:54:02 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.372: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 10:57:11.372: INFO: nginx-proxy-kube-3 from kube-system started at 2023-09-06 09:53:42 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.372: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  6 10:57:11.372: INFO: nodelocaldns-c9bb4 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.372: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 10:57:11.372: INFO: test-rs-r2ptw from replicaset-1141 started at 2023-09-06 10:57:04 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.372: INFO: 	Container httpd ready: true, restart count 0
Sep  6 10:57:11.372: INFO: sonobuoy from sonobuoy started at 2023-09-06 09:59:53 +0000 UTC (1 container statuses recorded)
Sep  6 10:57:11.372: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 10:57:11.372: INFO: sonobuoy-e2e-job-c7c8c161973b4a54 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 10:57:11.372: INFO: 	Container e2e ready: true, restart count 0
Sep  6 10:57:11.372: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:57:11.372: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 10:57:11.372: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:57:11.372: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/06/23 10:57:11.372
Sep  6 10:57:11.383: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7429" to be "running"
Sep  6 10:57:11.388: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737019ms
Sep  6 10:57:13.588: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.204746915s
Sep  6 10:57:13.588: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/06/23 10:57:14.396
STEP: Trying to apply a random label on the found node. 09/06/23 10:57:14.934
STEP: verifying the node has the label kubernetes.io/e2e-4d7e609c-2800-482c-bfd5-1bb624c4a3b4 42 09/06/23 10:57:15.288
STEP: Trying to relaunch the pod, now with labels. 09/06/23 10:57:15.325
Sep  6 10:57:15.389: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7429" to be "not pending"
Sep  6 10:57:15.412: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 22.62471ms
Sep  6 10:57:17.418: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.0288675s
Sep  6 10:57:17.418: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-4d7e609c-2800-482c-bfd5-1bb624c4a3b4 off the node kube-3 09/06/23 10:57:17.426
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4d7e609c-2800-482c-bfd5-1bb624c4a3b4 09/06/23 10:57:17.444
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:17.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7429" for this suite. 09/06/23 10:57:17.459
------------------------------
• [SLOW TEST] [6.152 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:11.318
    Sep  6 10:57:11.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-pred 09/06/23 10:57:11.319
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:11.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:11.343
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  6 10:57:11.345: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  6 10:57:11.351: INFO: Waiting for terminating namespaces to be deleted...
    Sep  6 10:57:11.354: INFO: 
    Logging pods the apiserver thinks is on node kube-1 before test
    Sep  6 10:57:11.360: INFO: calico-kube-controllers-6dfcdfb99-6q4ng from kube-system started at 2023-09-06 09:55:41 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  6 10:57:11.360: INFO: calico-node-pkqgc from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container calico-node ready: true, restart count 2
    Sep  6 10:57:11.360: INFO: coredns-645b46f4b6-hq55k from kube-system started at 2023-09-06 09:55:53 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container coredns ready: true, restart count 0
    Sep  6 10:57:11.360: INFO: kube-apiserver-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container kube-apiserver ready: true, restart count 2
    Sep  6 10:57:11.360: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Sep  6 10:57:11.360: INFO: kube-proxy-fjqk6 from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 10:57:11.360: INFO: kube-scheduler-kube-1 from kube-system started at 2023-09-06 09:52:15 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container kube-scheduler ready: true, restart count 4
    Sep  6 10:57:11.360: INFO: nodelocaldns-74qn2 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 10:57:11.360: INFO: test-rs-dnnhh from replicaset-1141 started at 2023-09-06 10:57:09 +0000 UTC (2 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container httpd ready: true, restart count 0
    Sep  6 10:57:11.360: INFO: 	Container test-rs ready: true, restart count 0
    Sep  6 10:57:11.360: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 10:57:11.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 10:57:11.360: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  6 10:57:11.360: INFO: 
    Logging pods the apiserver thinks is on node kube-2 before test
    Sep  6 10:57:11.366: INFO: calico-node-f57x2 from kube-system started at 2023-09-06 09:54:23 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container calico-node ready: true, restart count 2
    Sep  6 10:57:11.366: INFO: coredns-645b46f4b6-9lpfv from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container coredns ready: true, restart count 0
    Sep  6 10:57:11.366: INFO: dns-autoscaler-659b8c48cb-5h6w8 from kube-system started at 2023-09-06 09:55:57 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  6 10:57:11.366: INFO: kube-apiserver-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container kube-apiserver ready: true, restart count 1
    Sep  6 10:57:11.366: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-09-06 09:53:08 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container kube-controller-manager ready: true, restart count 3
    Sep  6 10:57:11.366: INFO: kube-proxy-7fxzk from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 10:57:11.366: INFO: kube-scheduler-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container kube-scheduler ready: true, restart count 3
    Sep  6 10:57:11.366: INFO: nodelocaldns-jpj4c from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 10:57:11.366: INFO: test-rs-rw6ht from replicaset-1141 started at 2023-09-06 10:57:09 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container httpd ready: true, restart count 0
    Sep  6 10:57:11.366: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 10:57:11.366: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 10:57:11.366: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  6 10:57:11.366: INFO: 
    Logging pods the apiserver thinks is on node kube-3 before test
    Sep  6 10:57:11.372: INFO: calico-node-6w7db from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.372: INFO: 	Container calico-node ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: kube-proxy-sfndk from kube-system started at 2023-09-06 09:54:02 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.372: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: nginx-proxy-kube-3 from kube-system started at 2023-09-06 09:53:42 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.372: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: nodelocaldns-c9bb4 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.372: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: test-rs-r2ptw from replicaset-1141 started at 2023-09-06 10:57:04 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.372: INFO: 	Container httpd ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: sonobuoy from sonobuoy started at 2023-09-06 09:59:53 +0000 UTC (1 container statuses recorded)
    Sep  6 10:57:11.372: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: sonobuoy-e2e-job-c7c8c161973b4a54 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 10:57:11.372: INFO: 	Container e2e ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 10:57:11.372: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 10:57:11.372: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/06/23 10:57:11.372
    Sep  6 10:57:11.383: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7429" to be "running"
    Sep  6 10:57:11.388: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737019ms
    Sep  6 10:57:13.588: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.204746915s
    Sep  6 10:57:13.588: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/06/23 10:57:14.396
    STEP: Trying to apply a random label on the found node. 09/06/23 10:57:14.934
    STEP: verifying the node has the label kubernetes.io/e2e-4d7e609c-2800-482c-bfd5-1bb624c4a3b4 42 09/06/23 10:57:15.288
    STEP: Trying to relaunch the pod, now with labels. 09/06/23 10:57:15.325
    Sep  6 10:57:15.389: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7429" to be "not pending"
    Sep  6 10:57:15.412: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 22.62471ms
    Sep  6 10:57:17.418: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.0288675s
    Sep  6 10:57:17.418: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-4d7e609c-2800-482c-bfd5-1bb624c4a3b4 off the node kube-3 09/06/23 10:57:17.426
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-4d7e609c-2800-482c-bfd5-1bb624c4a3b4 09/06/23 10:57:17.444
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:17.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7429" for this suite. 09/06/23 10:57:17.459
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:17.471
Sep  6 10:57:17.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename deployment 09/06/23 10:57:17.473
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:17.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:17.503
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Sep  6 10:57:17.507: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  6 10:57:17.523: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 10:57:22.531: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/06/23 10:57:22.531
Sep  6 10:57:22.531: INFO: Creating deployment "test-rolling-update-deployment"
Sep  6 10:57:22.543: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  6 10:57:22.550: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  6 10:57:24.569: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  6 10:57:24.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 57, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 57, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 57, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 57, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:57:26.584: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  6 10:57:26.595: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-486  3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39 20299 1 2023-09-06 10:57:22 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-06 10:57:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00192eeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-06 10:57:22 +0000 UTC,LastTransitionTime:2023-09-06 10:57:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-06 10:57:25 +0000 UTC,LastTransitionTime:2023-09-06 10:57:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 10:57:26.599: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-486  34e0ecef-09ac-4954-9169-97cdf301ab8c 20289 1 2023-09-06 10:57:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39 0xc00192f3c7 0xc00192f3c8}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:57:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00192f478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:57:26.599: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  6 10:57:26.599: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-486  9ecb05e5-ba3f-4b98-a178-a8578c02ed8c 20298 2 2023-09-06 10:57:17 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39 0xc00192f297 0xc00192f298}] [] [{e2e.test Update apps/v1 2023-09-06 10:57:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00192f358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:57:26.603: INFO: Pod "test-rolling-update-deployment-7549d9f46d-64lnn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-64lnn test-rolling-update-deployment-7549d9f46d- deployment-486  c6264282-5884-4e86-91af-ccbcb14916c9 20288 0 2023-09-06 10:57:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:d4c289c594c1239f9d8666df829b9a4ccf7a3758ef7f6bd62d7672f5831531b5 cni.projectcalico.org/podIP:10.233.99.73/32 cni.projectcalico.org/podIPs:10.233.99.73/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 34e0ecef-09ac-4954-9169-97cdf301ab8c 0xc00192f917 0xc00192f918}] [] [{kube-controller-manager Update v1 2023-09-06 10:57:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34e0ecef-09ac-4954-9169-97cdf301ab8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 10:57:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjx4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjx4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:57:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:57:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:57:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:57:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.73,StartTime:2023-09-06 10:57:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 10:57:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a10f05c547f7231e33f4950c11d917dd863fc5e3d68d19ff2337090fb8d423ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:26.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-486" for this suite. 09/06/23 10:57:26.607
------------------------------
• [SLOW TEST] [9.146 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:17.471
    Sep  6 10:57:17.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename deployment 09/06/23 10:57:17.473
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:17.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:17.503
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Sep  6 10:57:17.507: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Sep  6 10:57:17.523: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  6 10:57:22.531: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/06/23 10:57:22.531
    Sep  6 10:57:22.531: INFO: Creating deployment "test-rolling-update-deployment"
    Sep  6 10:57:22.543: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Sep  6 10:57:22.550: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Sep  6 10:57:24.569: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Sep  6 10:57:24.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 57, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 57, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 10, 57, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 10, 57, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 10:57:26.584: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  6 10:57:26.595: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-486  3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39 20299 1 2023-09-06 10:57:22 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-06 10:57:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00192eeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-06 10:57:22 +0000 UTC,LastTransitionTime:2023-09-06 10:57:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-06 10:57:25 +0000 UTC,LastTransitionTime:2023-09-06 10:57:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  6 10:57:26.599: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-486  34e0ecef-09ac-4954-9169-97cdf301ab8c 20289 1 2023-09-06 10:57:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39 0xc00192f3c7 0xc00192f3c8}] [] [{kube-controller-manager Update apps/v1 2023-09-06 10:57:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00192f478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:57:26.599: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Sep  6 10:57:26.599: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-486  9ecb05e5-ba3f-4b98-a178-a8578c02ed8c 20298 2 2023-09-06 10:57:17 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39 0xc00192f297 0xc00192f298}] [] [{e2e.test Update apps/v1 2023-09-06 10:57:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c4aa986-bc9b-4ce5-bdbb-7980e3ad2d39\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00192f358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 10:57:26.603: INFO: Pod "test-rolling-update-deployment-7549d9f46d-64lnn" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-64lnn test-rolling-update-deployment-7549d9f46d- deployment-486  c6264282-5884-4e86-91af-ccbcb14916c9 20288 0 2023-09-06 10:57:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:d4c289c594c1239f9d8666df829b9a4ccf7a3758ef7f6bd62d7672f5831531b5 cni.projectcalico.org/podIP:10.233.99.73/32 cni.projectcalico.org/podIPs:10.233.99.73/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 34e0ecef-09ac-4954-9169-97cdf301ab8c 0xc00192f917 0xc00192f918}] [] [{kube-controller-manager Update v1 2023-09-06 10:57:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34e0ecef-09ac-4954-9169-97cdf301ab8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 10:57:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 10:57:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjx4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjx4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:57:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:57:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:57:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 10:57:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.73,StartTime:2023-09-06 10:57:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 10:57:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a10f05c547f7231e33f4950c11d917dd863fc5e3d68d19ff2337090fb8d423ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:26.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-486" for this suite. 09/06/23 10:57:26.607
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:26.619
Sep  6 10:57:26.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 10:57:26.621
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:26.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:26.649
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 09/06/23 10:57:26.652
STEP: setting up watch 09/06/23 10:57:26.652
STEP: submitting the pod to kubernetes 09/06/23 10:57:26.755
STEP: verifying the pod is in kubernetes 09/06/23 10:57:26.774
STEP: verifying pod creation was observed 09/06/23 10:57:26.782
Sep  6 10:57:26.782: INFO: Waiting up to 5m0s for pod "pod-submit-remove-1ff08924-5c93-47dd-9209-b006c2ec21ff" in namespace "pods-4808" to be "running"
Sep  6 10:57:26.793: INFO: Pod "pod-submit-remove-1ff08924-5c93-47dd-9209-b006c2ec21ff": Phase="Pending", Reason="", readiness=false. Elapsed: 11.27009ms
Sep  6 10:57:28.806: INFO: Pod "pod-submit-remove-1ff08924-5c93-47dd-9209-b006c2ec21ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.023985863s
Sep  6 10:57:28.806: INFO: Pod "pod-submit-remove-1ff08924-5c93-47dd-9209-b006c2ec21ff" satisfied condition "running"
STEP: deleting the pod gracefully 09/06/23 10:57:28.816
STEP: verifying pod deletion was observed 09/06/23 10:57:28.833
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:31.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4808" for this suite. 09/06/23 10:57:31.083
------------------------------
• [4.470 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:26.619
    Sep  6 10:57:26.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 10:57:26.621
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:26.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:26.649
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 09/06/23 10:57:26.652
    STEP: setting up watch 09/06/23 10:57:26.652
    STEP: submitting the pod to kubernetes 09/06/23 10:57:26.755
    STEP: verifying the pod is in kubernetes 09/06/23 10:57:26.774
    STEP: verifying pod creation was observed 09/06/23 10:57:26.782
    Sep  6 10:57:26.782: INFO: Waiting up to 5m0s for pod "pod-submit-remove-1ff08924-5c93-47dd-9209-b006c2ec21ff" in namespace "pods-4808" to be "running"
    Sep  6 10:57:26.793: INFO: Pod "pod-submit-remove-1ff08924-5c93-47dd-9209-b006c2ec21ff": Phase="Pending", Reason="", readiness=false. Elapsed: 11.27009ms
    Sep  6 10:57:28.806: INFO: Pod "pod-submit-remove-1ff08924-5c93-47dd-9209-b006c2ec21ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.023985863s
    Sep  6 10:57:28.806: INFO: Pod "pod-submit-remove-1ff08924-5c93-47dd-9209-b006c2ec21ff" satisfied condition "running"
    STEP: deleting the pod gracefully 09/06/23 10:57:28.816
    STEP: verifying pod deletion was observed 09/06/23 10:57:28.833
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:31.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4808" for this suite. 09/06/23 10:57:31.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:31.091
Sep  6 10:57:31.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 10:57:31.092
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:31.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:31.115
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 09/06/23 10:57:31.117
Sep  6 10:57:31.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b" in namespace "projected-2594" to be "Succeeded or Failed"
Sep  6 10:57:31.129: INFO: Pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.186737ms
Sep  6 10:57:33.135: INFO: Pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010126486s
Sep  6 10:57:35.142: INFO: Pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01706809s
STEP: Saw pod success 09/06/23 10:57:35.142
Sep  6 10:57:35.142: INFO: Pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b" satisfied condition "Succeeded or Failed"
Sep  6 10:57:35.148: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b container client-container: <nil>
STEP: delete the pod 09/06/23 10:57:35.163
Sep  6 10:57:35.178: INFO: Waiting for pod downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b to disappear
Sep  6 10:57:35.181: INFO: Pod downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:35.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2594" for this suite. 09/06/23 10:57:35.185
------------------------------
• [4.101 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:31.091
    Sep  6 10:57:31.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 10:57:31.092
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:31.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:31.115
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 09/06/23 10:57:31.117
    Sep  6 10:57:31.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b" in namespace "projected-2594" to be "Succeeded or Failed"
    Sep  6 10:57:31.129: INFO: Pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.186737ms
    Sep  6 10:57:33.135: INFO: Pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010126486s
    Sep  6 10:57:35.142: INFO: Pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01706809s
    STEP: Saw pod success 09/06/23 10:57:35.142
    Sep  6 10:57:35.142: INFO: Pod "downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b" satisfied condition "Succeeded or Failed"
    Sep  6 10:57:35.148: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b container client-container: <nil>
    STEP: delete the pod 09/06/23 10:57:35.163
    Sep  6 10:57:35.178: INFO: Waiting for pod downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b to disappear
    Sep  6 10:57:35.181: INFO: Pod downwardapi-volume-4d3539ff-7bbe-448e-b60c-4160ce329c5b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:35.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2594" for this suite. 09/06/23 10:57:35.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:35.193
Sep  6 10:57:35.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename watch 09/06/23 10:57:35.193
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:35.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:35.218
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 09/06/23 10:57:35.222
STEP: starting a background goroutine to produce watch events 09/06/23 10:57:35.224
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/06/23 10:57:35.224
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:38.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2768" for this suite. 09/06/23 10:57:38.048
------------------------------
• [2.923 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:35.193
    Sep  6 10:57:35.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename watch 09/06/23 10:57:35.193
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:35.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:35.218
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 09/06/23 10:57:35.222
    STEP: starting a background goroutine to produce watch events 09/06/23 10:57:35.224
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/06/23 10:57:35.224
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:38.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2768" for this suite. 09/06/23 10:57:38.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:38.118
Sep  6 10:57:38.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename gc 09/06/23 10:57:38.118
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:38.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:38.142
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Sep  6 10:57:38.182: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c357c78a-4070-44e3-acc5-8a4f4b0a4335", Controller:(*bool)(0xc0046328fe), BlockOwnerDeletion:(*bool)(0xc0046328ff)}}
Sep  6 10:57:38.197: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"310cb8bf-397c-4283-ad3b-e4eb9d3321b5", Controller:(*bool)(0xc004632b26), BlockOwnerDeletion:(*bool)(0xc004632b27)}}
Sep  6 10:57:38.205: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"66e6a951-a61b-436e-a396-33423161accc", Controller:(*bool)(0xc004632d3a), BlockOwnerDeletion:(*bool)(0xc004632d3b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7955" for this suite. 09/06/23 10:57:43.227
------------------------------
• [SLOW TEST] [5.115 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:38.118
    Sep  6 10:57:38.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename gc 09/06/23 10:57:38.118
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:38.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:38.142
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Sep  6 10:57:38.182: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c357c78a-4070-44e3-acc5-8a4f4b0a4335", Controller:(*bool)(0xc0046328fe), BlockOwnerDeletion:(*bool)(0xc0046328ff)}}
    Sep  6 10:57:38.197: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"310cb8bf-397c-4283-ad3b-e4eb9d3321b5", Controller:(*bool)(0xc004632b26), BlockOwnerDeletion:(*bool)(0xc004632b27)}}
    Sep  6 10:57:38.205: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"66e6a951-a61b-436e-a396-33423161accc", Controller:(*bool)(0xc004632d3a), BlockOwnerDeletion:(*bool)(0xc004632d3b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7955" for this suite. 09/06/23 10:57:43.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:43.233
Sep  6 10:57:43.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename runtimeclass 09/06/23 10:57:43.234
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:43.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:43.262
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-7755-delete-me 09/06/23 10:57:43.272
STEP: Waiting for the RuntimeClass to disappear 09/06/23 10:57:43.28
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:43.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7755" for this suite. 09/06/23 10:57:43.294
------------------------------
• [0.068 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:43.233
    Sep  6 10:57:43.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename runtimeclass 09/06/23 10:57:43.234
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:43.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:43.262
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-7755-delete-me 09/06/23 10:57:43.272
    STEP: Waiting for the RuntimeClass to disappear 09/06/23 10:57:43.28
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:43.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7755" for this suite. 09/06/23 10:57:43.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:43.304
Sep  6 10:57:43.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/06/23 10:57:43.305
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:43.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:43.329
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 09/06/23 10:57:43.332
STEP: Creating hostNetwork=false pod 09/06/23 10:57:43.332
Sep  6 10:57:43.341: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4959" to be "running and ready"
Sep  6 10:57:43.349: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.984083ms
Sep  6 10:57:43.349: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:57:45.366: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025461431s
Sep  6 10:57:45.366: INFO: The phase of Pod test-pod is Running (Ready = true)
Sep  6 10:57:45.366: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 09/06/23 10:57:45.376
Sep  6 10:57:45.396: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4959" to be "running and ready"
Sep  6 10:57:45.404: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.65025ms
Sep  6 10:57:45.404: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:57:47.420: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023903362s
Sep  6 10:57:47.420: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Sep  6 10:57:47.420: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 09/06/23 10:57:47.432
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/06/23 10:57:47.433
Sep  6 10:57:47.434: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:47.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:47.437: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:47.438: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  6 10:57:47.575: INFO: Exec stderr: ""
Sep  6 10:57:47.575: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:47.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:47.576: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:47.576: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  6 10:57:47.678: INFO: Exec stderr: ""
Sep  6 10:57:47.678: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:47.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:47.679: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:47.679: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  6 10:57:47.739: INFO: Exec stderr: ""
Sep  6 10:57:47.739: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:47.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:47.740: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:47.740: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  6 10:57:47.796: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/06/23 10:57:47.796
Sep  6 10:57:47.796: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:47.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:47.797: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:47.797: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  6 10:57:47.863: INFO: Exec stderr: ""
Sep  6 10:57:47.863: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:47.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:47.863: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:47.863: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  6 10:57:47.900: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/06/23 10:57:47.9
Sep  6 10:57:47.900: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:47.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:47.900: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:47.900: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  6 10:57:47.961: INFO: Exec stderr: ""
Sep  6 10:57:47.961: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:47.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:47.961: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:47.961: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  6 10:57:48.022: INFO: Exec stderr: ""
Sep  6 10:57:48.022: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:48.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:48.023: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:48.023: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  6 10:57:48.098: INFO: Exec stderr: ""
Sep  6 10:57:48.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 10:57:48.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 10:57:48.098: INFO: ExecWithOptions: Clientset creation
Sep  6 10:57:48.098: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  6 10:57:48.133: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:48.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4959" for this suite. 09/06/23 10:57:48.137
------------------------------
• [4.840 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:43.304
    Sep  6 10:57:43.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/06/23 10:57:43.305
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:43.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:43.329
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 09/06/23 10:57:43.332
    STEP: Creating hostNetwork=false pod 09/06/23 10:57:43.332
    Sep  6 10:57:43.341: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4959" to be "running and ready"
    Sep  6 10:57:43.349: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.984083ms
    Sep  6 10:57:43.349: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:57:45.366: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025461431s
    Sep  6 10:57:45.366: INFO: The phase of Pod test-pod is Running (Ready = true)
    Sep  6 10:57:45.366: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 09/06/23 10:57:45.376
    Sep  6 10:57:45.396: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4959" to be "running and ready"
    Sep  6 10:57:45.404: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.65025ms
    Sep  6 10:57:45.404: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 10:57:47.420: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023903362s
    Sep  6 10:57:47.420: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Sep  6 10:57:47.420: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 09/06/23 10:57:47.432
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/06/23 10:57:47.433
    Sep  6 10:57:47.434: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:47.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:47.437: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:47.438: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  6 10:57:47.575: INFO: Exec stderr: ""
    Sep  6 10:57:47.575: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:47.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:47.576: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:47.576: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  6 10:57:47.678: INFO: Exec stderr: ""
    Sep  6 10:57:47.678: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:47.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:47.679: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:47.679: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  6 10:57:47.739: INFO: Exec stderr: ""
    Sep  6 10:57:47.739: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:47.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:47.740: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:47.740: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  6 10:57:47.796: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/06/23 10:57:47.796
    Sep  6 10:57:47.796: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:47.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:47.797: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:47.797: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  6 10:57:47.863: INFO: Exec stderr: ""
    Sep  6 10:57:47.863: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:47.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:47.863: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:47.863: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  6 10:57:47.900: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/06/23 10:57:47.9
    Sep  6 10:57:47.900: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:47.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:47.900: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:47.900: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  6 10:57:47.961: INFO: Exec stderr: ""
    Sep  6 10:57:47.961: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:47.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:47.961: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:47.961: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  6 10:57:48.022: INFO: Exec stderr: ""
    Sep  6 10:57:48.022: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:48.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:48.023: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:48.023: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  6 10:57:48.098: INFO: Exec stderr: ""
    Sep  6 10:57:48.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4959 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 10:57:48.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 10:57:48.098: INFO: ExecWithOptions: Clientset creation
    Sep  6 10:57:48.098: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4959/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  6 10:57:48.133: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:48.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4959" for this suite. 09/06/23 10:57:48.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:48.144
Sep  6 10:57:48.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename endpointslicemirroring 09/06/23 10:57:48.145
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:48.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:48.169
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 09/06/23 10:57:48.181
Sep  6 10:57:48.194: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 09/06/23 10:57:50.198
Sep  6 10:57:50.212: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 09/06/23 10:57:52.226
Sep  6 10:57:52.254: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:54.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-2662" for this suite. 09/06/23 10:57:54.556
------------------------------
• [SLOW TEST] [6.509 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:48.144
    Sep  6 10:57:48.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename endpointslicemirroring 09/06/23 10:57:48.145
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:48.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:48.169
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 09/06/23 10:57:48.181
    Sep  6 10:57:48.194: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 09/06/23 10:57:50.198
    Sep  6 10:57:50.212: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 09/06/23 10:57:52.226
    Sep  6 10:57:52.254: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:54.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-2662" for this suite. 09/06/23 10:57:54.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:54.654
Sep  6 10:57:54.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 10:57:54.654
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:54.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:54.751
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-d5707e84-2f65-4a4d-884c-5cd4ab59bd43 09/06/23 10:57:54.755
STEP: Creating a pod to test consume secrets 09/06/23 10:57:54.773
Sep  6 10:57:54.826: INFO: Waiting up to 5m0s for pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76" in namespace "secrets-7755" to be "Succeeded or Failed"
Sep  6 10:57:54.862: INFO: Pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76": Phase="Pending", Reason="", readiness=false. Elapsed: 36.064539ms
Sep  6 10:57:56.879: INFO: Pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052620227s
Sep  6 10:57:58.876: INFO: Pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049572596s
STEP: Saw pod success 09/06/23 10:57:58.876
Sep  6 10:57:58.876: INFO: Pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76" satisfied condition "Succeeded or Failed"
Sep  6 10:57:58.889: INFO: Trying to get logs from node kube-2 pod pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76 container secret-volume-test: <nil>
STEP: delete the pod 09/06/23 10:57:58.933
Sep  6 10:57:58.949: INFO: Waiting for pod pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76 to disappear
Sep  6 10:57:58.953: INFO: Pod pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 10:57:58.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7755" for this suite. 09/06/23 10:57:58.957
------------------------------
• [4.311 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:54.654
    Sep  6 10:57:54.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 10:57:54.654
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:54.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:54.751
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-d5707e84-2f65-4a4d-884c-5cd4ab59bd43 09/06/23 10:57:54.755
    STEP: Creating a pod to test consume secrets 09/06/23 10:57:54.773
    Sep  6 10:57:54.826: INFO: Waiting up to 5m0s for pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76" in namespace "secrets-7755" to be "Succeeded or Failed"
    Sep  6 10:57:54.862: INFO: Pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76": Phase="Pending", Reason="", readiness=false. Elapsed: 36.064539ms
    Sep  6 10:57:56.879: INFO: Pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052620227s
    Sep  6 10:57:58.876: INFO: Pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049572596s
    STEP: Saw pod success 09/06/23 10:57:58.876
    Sep  6 10:57:58.876: INFO: Pod "pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76" satisfied condition "Succeeded or Failed"
    Sep  6 10:57:58.889: INFO: Trying to get logs from node kube-2 pod pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76 container secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 10:57:58.933
    Sep  6 10:57:58.949: INFO: Waiting for pod pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76 to disappear
    Sep  6 10:57:58.953: INFO: Pod pod-secrets-d00101f4-6824-4c7c-8295-e430ace7ce76 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:57:58.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7755" for this suite. 09/06/23 10:57:58.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:57:58.969
Sep  6 10:57:58.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 10:57:58.969
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:58.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:58.993
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 09/06/23 10:58:16.009
STEP: Creating a ResourceQuota 09/06/23 10:58:21.022
STEP: Ensuring resource quota status is calculated 09/06/23 10:58:21.04
STEP: Creating a ConfigMap 09/06/23 10:58:23.053
STEP: Ensuring resource quota status captures configMap creation 09/06/23 10:58:23.147
STEP: Deleting a ConfigMap 09/06/23 10:58:25.161
STEP: Ensuring resource quota status released usage 09/06/23 10:58:25.204
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 10:58:27.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3334" for this suite. 09/06/23 10:58:27.229
------------------------------
• [SLOW TEST] [28.288 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:57:58.969
    Sep  6 10:57:58.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 10:57:58.969
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:57:58.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:57:58.993
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 09/06/23 10:58:16.009
    STEP: Creating a ResourceQuota 09/06/23 10:58:21.022
    STEP: Ensuring resource quota status is calculated 09/06/23 10:58:21.04
    STEP: Creating a ConfigMap 09/06/23 10:58:23.053
    STEP: Ensuring resource quota status captures configMap creation 09/06/23 10:58:23.147
    STEP: Deleting a ConfigMap 09/06/23 10:58:25.161
    STEP: Ensuring resource quota status released usage 09/06/23 10:58:25.204
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:58:27.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3334" for this suite. 09/06/23 10:58:27.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:58:27.26
Sep  6 10:58:27.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename watch 09/06/23 10:58:27.263
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:58:27.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:58:27.384
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 09/06/23 10:58:27.392
STEP: modifying the configmap once 09/06/23 10:58:27.455
STEP: modifying the configmap a second time 09/06/23 10:58:27.467
STEP: deleting the configmap 09/06/23 10:58:27.476
STEP: creating a watch on configmaps from the resource version returned by the first update 09/06/23 10:58:27.482
STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/06/23 10:58:27.483
Sep  6 10:58:27.483: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3260  44e4b45f-35ed-45f7-ab7e-436cd4f77f8e 20827 0 2023-09-06 10:58:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-06 10:58:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:58:27.483: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3260  44e4b45f-35ed-45f7-ab7e-436cd4f77f8e 20828 0 2023-09-06 10:58:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-06 10:58:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  6 10:58:27.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3260" for this suite. 09/06/23 10:58:27.487
------------------------------
• [0.235 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:58:27.26
    Sep  6 10:58:27.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename watch 09/06/23 10:58:27.263
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:58:27.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:58:27.384
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 09/06/23 10:58:27.392
    STEP: modifying the configmap once 09/06/23 10:58:27.455
    STEP: modifying the configmap a second time 09/06/23 10:58:27.467
    STEP: deleting the configmap 09/06/23 10:58:27.476
    STEP: creating a watch on configmaps from the resource version returned by the first update 09/06/23 10:58:27.482
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/06/23 10:58:27.483
    Sep  6 10:58:27.483: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3260  44e4b45f-35ed-45f7-ab7e-436cd4f77f8e 20827 0 2023-09-06 10:58:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-06 10:58:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 10:58:27.483: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3260  44e4b45f-35ed-45f7-ab7e-436cd4f77f8e 20828 0 2023-09-06 10:58:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-06 10:58:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:58:27.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3260" for this suite. 09/06/23 10:58:27.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:58:27.495
Sep  6 10:58:27.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir-wrapper 09/06/23 10:58:27.496
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:58:27.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:58:27.522
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 09/06/23 10:58:27.524
STEP: Creating RC which spawns configmap-volume pods 09/06/23 10:58:27.778
Sep  6 10:58:27.868: INFO: Pod name wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028: Found 3 pods out of 5
Sep  6 10:58:32.887: INFO: Pod name wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/06/23 10:58:32.887
Sep  6 10:58:32.887: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:58:32.896: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.350846ms
Sep  6 10:58:34.906: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018930414s
Sep  6 10:58:36.900: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012508347s
Sep  6 10:58:38.911: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023540122s
Sep  6 10:58:40.900: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012455238s
Sep  6 10:58:42.907: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Running", Reason="", readiness=true. Elapsed: 10.019341572s
Sep  6 10:58:42.907: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w" satisfied condition "running"
Sep  6 10:58:42.907: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jpd56" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:58:42.915: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jpd56": Phase="Running", Reason="", readiness=true. Elapsed: 7.721781ms
Sep  6 10:58:42.915: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jpd56" satisfied condition "running"
Sep  6 10:58:42.915: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jqq8w" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:58:42.923: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jqq8w": Phase="Running", Reason="", readiness=true. Elapsed: 7.790043ms
Sep  6 10:58:42.923: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jqq8w" satisfied condition "running"
Sep  6 10:58:42.923: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-llrl7" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:58:42.929: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-llrl7": Phase="Running", Reason="", readiness=true. Elapsed: 5.503926ms
Sep  6 10:58:42.929: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-llrl7" satisfied condition "running"
Sep  6 10:58:42.929: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-tdv99" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:58:42.940: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-tdv99": Phase="Running", Reason="", readiness=true. Elapsed: 10.89451ms
Sep  6 10:58:42.940: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-tdv99" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028 in namespace emptydir-wrapper-5149, will wait for the garbage collector to delete the pods 09/06/23 10:58:42.94
Sep  6 10:58:43.005: INFO: Deleting ReplicationController wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028 took: 11.35157ms
Sep  6 10:58:43.106: INFO: Terminating ReplicationController wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028 pods took: 100.767853ms
STEP: Creating RC which spawns configmap-volume pods 09/06/23 10:58:47.413
Sep  6 10:58:47.431: INFO: Pod name wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7: Found 0 pods out of 5
Sep  6 10:58:52.449: INFO: Pod name wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/06/23 10:58:52.449
Sep  6 10:58:52.449: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:58:52.460: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.406894ms
Sep  6 10:58:54.475: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025930551s
Sep  6 10:58:56.467: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017451136s
Sep  6 10:58:58.471: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021606813s
Sep  6 10:59:00.470: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Running", Reason="", readiness=true. Elapsed: 8.020635499s
Sep  6 10:59:00.470: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2" satisfied condition "running"
Sep  6 10:59:00.470: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-jcnhf" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:00.477: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-jcnhf": Phase="Running", Reason="", readiness=true. Elapsed: 6.774873ms
Sep  6 10:59:00.477: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-jcnhf" satisfied condition "running"
Sep  6 10:59:00.477: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-mhvw5" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:00.483: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-mhvw5": Phase="Running", Reason="", readiness=true. Elapsed: 5.667466ms
Sep  6 10:59:00.483: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-mhvw5" satisfied condition "running"
Sep  6 10:59:00.483: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-wmbvv" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:00.486: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-wmbvv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.501888ms
Sep  6 10:59:02.491: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-wmbvv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008658746s
Sep  6 10:59:02.491: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-wmbvv" satisfied condition "running"
Sep  6 10:59:02.492: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-z5gpj" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:02.496: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-z5gpj": Phase="Running", Reason="", readiness=true. Elapsed: 4.202304ms
Sep  6 10:59:02.496: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-z5gpj" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7 in namespace emptydir-wrapper-5149, will wait for the garbage collector to delete the pods 09/06/23 10:59:02.496
Sep  6 10:59:02.569: INFO: Deleting ReplicationController wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7 took: 16.532223ms
Sep  6 10:59:02.771: INFO: Terminating ReplicationController wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7 pods took: 202.490107ms
STEP: Creating RC which spawns configmap-volume pods 09/06/23 10:59:09.077
Sep  6 10:59:09.099: INFO: Pod name wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f: Found 0 pods out of 5
Sep  6 10:59:15.205: INFO: Pod name wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/06/23 10:59:15.205
Sep  6 10:59:15.205: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:15.369: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Pending", Reason="", readiness=false. Elapsed: 164.2098ms
Sep  6 10:59:17.374: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168564582s
Sep  6 10:59:19.381: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175592637s
Sep  6 10:59:21.375: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.170471227s
Sep  6 10:59:23.378: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Running", Reason="", readiness=true. Elapsed: 8.172829316s
Sep  6 10:59:23.378: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq" satisfied condition "running"
Sep  6 10:59:23.378: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-gbthr" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:23.383: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-gbthr": Phase="Running", Reason="", readiness=true. Elapsed: 5.33778ms
Sep  6 10:59:23.383: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-gbthr" satisfied condition "running"
Sep  6 10:59:23.383: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-hcggq" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:23.388: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-hcggq": Phase="Running", Reason="", readiness=true. Elapsed: 4.574022ms
Sep  6 10:59:23.388: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-hcggq" satisfied condition "running"
Sep  6 10:59:23.388: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-nk7g6" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:23.392: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-nk7g6": Phase="Running", Reason="", readiness=true. Elapsed: 4.069732ms
Sep  6 10:59:23.392: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-nk7g6" satisfied condition "running"
Sep  6 10:59:23.392: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-x6wdc" in namespace "emptydir-wrapper-5149" to be "running"
Sep  6 10:59:23.399: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-x6wdc": Phase="Running", Reason="", readiness=true. Elapsed: 6.410994ms
Sep  6 10:59:23.399: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-x6wdc" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f in namespace emptydir-wrapper-5149, will wait for the garbage collector to delete the pods 09/06/23 10:59:23.399
Sep  6 10:59:23.465: INFO: Deleting ReplicationController wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f took: 10.602976ms
Sep  6 10:59:23.566: INFO: Terminating ReplicationController wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f pods took: 100.984705ms
STEP: Cleaning up the configMaps 09/06/23 10:59:29.966
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 10:59:31.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5149" for this suite. 09/06/23 10:59:31.647
------------------------------
• [SLOW TEST] [64.157 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:58:27.495
    Sep  6 10:58:27.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir-wrapper 09/06/23 10:58:27.496
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:58:27.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:58:27.522
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 09/06/23 10:58:27.524
    STEP: Creating RC which spawns configmap-volume pods 09/06/23 10:58:27.778
    Sep  6 10:58:27.868: INFO: Pod name wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028: Found 3 pods out of 5
    Sep  6 10:58:32.887: INFO: Pod name wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/06/23 10:58:32.887
    Sep  6 10:58:32.887: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:58:32.896: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.350846ms
    Sep  6 10:58:34.906: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018930414s
    Sep  6 10:58:36.900: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012508347s
    Sep  6 10:58:38.911: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023540122s
    Sep  6 10:58:40.900: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012455238s
    Sep  6 10:58:42.907: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w": Phase="Running", Reason="", readiness=true. Elapsed: 10.019341572s
    Sep  6 10:58:42.907: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-7w47w" satisfied condition "running"
    Sep  6 10:58:42.907: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jpd56" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:58:42.915: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jpd56": Phase="Running", Reason="", readiness=true. Elapsed: 7.721781ms
    Sep  6 10:58:42.915: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jpd56" satisfied condition "running"
    Sep  6 10:58:42.915: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jqq8w" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:58:42.923: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jqq8w": Phase="Running", Reason="", readiness=true. Elapsed: 7.790043ms
    Sep  6 10:58:42.923: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-jqq8w" satisfied condition "running"
    Sep  6 10:58:42.923: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-llrl7" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:58:42.929: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-llrl7": Phase="Running", Reason="", readiness=true. Elapsed: 5.503926ms
    Sep  6 10:58:42.929: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-llrl7" satisfied condition "running"
    Sep  6 10:58:42.929: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-tdv99" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:58:42.940: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-tdv99": Phase="Running", Reason="", readiness=true. Elapsed: 10.89451ms
    Sep  6 10:58:42.940: INFO: Pod "wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028-tdv99" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028 in namespace emptydir-wrapper-5149, will wait for the garbage collector to delete the pods 09/06/23 10:58:42.94
    Sep  6 10:58:43.005: INFO: Deleting ReplicationController wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028 took: 11.35157ms
    Sep  6 10:58:43.106: INFO: Terminating ReplicationController wrapped-volume-race-a323776c-f1fa-4bc7-8660-437cd6fe7028 pods took: 100.767853ms
    STEP: Creating RC which spawns configmap-volume pods 09/06/23 10:58:47.413
    Sep  6 10:58:47.431: INFO: Pod name wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7: Found 0 pods out of 5
    Sep  6 10:58:52.449: INFO: Pod name wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/06/23 10:58:52.449
    Sep  6 10:58:52.449: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:58:52.460: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.406894ms
    Sep  6 10:58:54.475: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025930551s
    Sep  6 10:58:56.467: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017451136s
    Sep  6 10:58:58.471: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021606813s
    Sep  6 10:59:00.470: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2": Phase="Running", Reason="", readiness=true. Elapsed: 8.020635499s
    Sep  6 10:59:00.470: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-c74g2" satisfied condition "running"
    Sep  6 10:59:00.470: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-jcnhf" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:00.477: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-jcnhf": Phase="Running", Reason="", readiness=true. Elapsed: 6.774873ms
    Sep  6 10:59:00.477: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-jcnhf" satisfied condition "running"
    Sep  6 10:59:00.477: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-mhvw5" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:00.483: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-mhvw5": Phase="Running", Reason="", readiness=true. Elapsed: 5.667466ms
    Sep  6 10:59:00.483: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-mhvw5" satisfied condition "running"
    Sep  6 10:59:00.483: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-wmbvv" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:00.486: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-wmbvv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.501888ms
    Sep  6 10:59:02.491: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-wmbvv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008658746s
    Sep  6 10:59:02.491: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-wmbvv" satisfied condition "running"
    Sep  6 10:59:02.492: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-z5gpj" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:02.496: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-z5gpj": Phase="Running", Reason="", readiness=true. Elapsed: 4.202304ms
    Sep  6 10:59:02.496: INFO: Pod "wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7-z5gpj" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7 in namespace emptydir-wrapper-5149, will wait for the garbage collector to delete the pods 09/06/23 10:59:02.496
    Sep  6 10:59:02.569: INFO: Deleting ReplicationController wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7 took: 16.532223ms
    Sep  6 10:59:02.771: INFO: Terminating ReplicationController wrapped-volume-race-5043eee8-ddd6-4ac5-bb59-03de73dfb7e7 pods took: 202.490107ms
    STEP: Creating RC which spawns configmap-volume pods 09/06/23 10:59:09.077
    Sep  6 10:59:09.099: INFO: Pod name wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f: Found 0 pods out of 5
    Sep  6 10:59:15.205: INFO: Pod name wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/06/23 10:59:15.205
    Sep  6 10:59:15.205: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:15.369: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Pending", Reason="", readiness=false. Elapsed: 164.2098ms
    Sep  6 10:59:17.374: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168564582s
    Sep  6 10:59:19.381: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175592637s
    Sep  6 10:59:21.375: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.170471227s
    Sep  6 10:59:23.378: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq": Phase="Running", Reason="", readiness=true. Elapsed: 8.172829316s
    Sep  6 10:59:23.378: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-4xjvq" satisfied condition "running"
    Sep  6 10:59:23.378: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-gbthr" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:23.383: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-gbthr": Phase="Running", Reason="", readiness=true. Elapsed: 5.33778ms
    Sep  6 10:59:23.383: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-gbthr" satisfied condition "running"
    Sep  6 10:59:23.383: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-hcggq" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:23.388: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-hcggq": Phase="Running", Reason="", readiness=true. Elapsed: 4.574022ms
    Sep  6 10:59:23.388: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-hcggq" satisfied condition "running"
    Sep  6 10:59:23.388: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-nk7g6" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:23.392: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-nk7g6": Phase="Running", Reason="", readiness=true. Elapsed: 4.069732ms
    Sep  6 10:59:23.392: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-nk7g6" satisfied condition "running"
    Sep  6 10:59:23.392: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-x6wdc" in namespace "emptydir-wrapper-5149" to be "running"
    Sep  6 10:59:23.399: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-x6wdc": Phase="Running", Reason="", readiness=true. Elapsed: 6.410994ms
    Sep  6 10:59:23.399: INFO: Pod "wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f-x6wdc" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f in namespace emptydir-wrapper-5149, will wait for the garbage collector to delete the pods 09/06/23 10:59:23.399
    Sep  6 10:59:23.465: INFO: Deleting ReplicationController wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f took: 10.602976ms
    Sep  6 10:59:23.566: INFO: Terminating ReplicationController wrapped-volume-race-fbe70728-db3f-46b6-a3c2-d8cb9ce7212f pods took: 100.984705ms
    STEP: Cleaning up the configMaps 09/06/23 10:59:29.966
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:59:31.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5149" for this suite. 09/06/23 10:59:31.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:59:31.656
Sep  6 10:59:31.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 10:59:31.657
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:59:31.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:59:31.687
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-62a02904-de67-402b-abec-512d2c1a06ea 09/06/23 10:59:31.689
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 10:59:31.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4230" for this suite. 09/06/23 10:59:31.693
------------------------------
• [0.043 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:59:31.656
    Sep  6 10:59:31.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 10:59:31.657
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:59:31.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:59:31.687
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-62a02904-de67-402b-abec-512d2c1a06ea 09/06/23 10:59:31.689
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 10:59:31.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4230" for this suite. 09/06/23 10:59:31.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 10:59:31.702
Sep  6 10:59:31.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename statefulset 09/06/23 10:59:31.703
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:59:31.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:59:31.723
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6232 09/06/23 10:59:31.725
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 09/06/23 10:59:31.729
Sep  6 10:59:31.741: INFO: Found 0 stateful pods, waiting for 3
Sep  6 10:59:41.755: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:59:41.755: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:59:41.755: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:59:41.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-6232 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:59:41.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:59:41.938: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:59:41.938: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/06/23 10:59:51.97
Sep  6 10:59:52.011: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/06/23 10:59:52.011
STEP: Updating Pods in reverse ordinal order 09/06/23 11:00:02.052
Sep  6 11:00:02.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-6232 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 11:00:02.205: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 11:00:02.205: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 11:00:02.205: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 11:00:24.551: INFO: Waiting for StatefulSet statefulset-6232/ss2 to complete update
STEP: Rolling back to a previous revision 09/06/23 11:00:34.561
Sep  6 11:00:34.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-6232 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 11:00:34.681: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 11:00:34.681: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 11:00:34.681: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 11:00:44.716: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 09/06/23 11:00:54.735
Sep  6 11:00:54.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-6232 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 11:00:54.843: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 11:00:54.843: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 11:00:54.843: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 11:01:04.915: INFO: Waiting for StatefulSet statefulset-6232/ss2 to complete update
Sep  6 11:01:04.915: INFO: Waiting for Pod statefulset-6232/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  6 11:01:14.948: INFO: Deleting all statefulset in ns statefulset-6232
Sep  6 11:01:14.960: INFO: Scaling statefulset ss2 to 0
Sep  6 11:01:25.008: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:01:25.018: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:01:25.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6232" for this suite. 09/06/23 11:01:25.061
------------------------------
• [SLOW TEST] [113.368 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 10:59:31.702
    Sep  6 10:59:31.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename statefulset 09/06/23 10:59:31.703
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 10:59:31.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 10:59:31.723
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6232 09/06/23 10:59:31.725
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 09/06/23 10:59:31.729
    Sep  6 10:59:31.741: INFO: Found 0 stateful pods, waiting for 3
    Sep  6 10:59:41.755: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 10:59:41.755: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 10:59:41.755: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 10:59:41.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-6232 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 10:59:41.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 10:59:41.938: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 10:59:41.938: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/06/23 10:59:51.97
    Sep  6 10:59:52.011: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/06/23 10:59:52.011
    STEP: Updating Pods in reverse ordinal order 09/06/23 11:00:02.052
    Sep  6 11:00:02.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-6232 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 11:00:02.205: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  6 11:00:02.205: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 11:00:02.205: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 11:00:24.551: INFO: Waiting for StatefulSet statefulset-6232/ss2 to complete update
    STEP: Rolling back to a previous revision 09/06/23 11:00:34.561
    Sep  6 11:00:34.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-6232 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 11:00:34.681: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 11:00:34.681: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 11:00:34.681: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 11:00:44.716: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 09/06/23 11:00:54.735
    Sep  6 11:00:54.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-6232 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 11:00:54.843: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  6 11:00:54.843: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 11:00:54.843: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 11:01:04.915: INFO: Waiting for StatefulSet statefulset-6232/ss2 to complete update
    Sep  6 11:01:04.915: INFO: Waiting for Pod statefulset-6232/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  6 11:01:14.948: INFO: Deleting all statefulset in ns statefulset-6232
    Sep  6 11:01:14.960: INFO: Scaling statefulset ss2 to 0
    Sep  6 11:01:25.008: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 11:01:25.018: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:01:25.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6232" for this suite. 09/06/23 11:01:25.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:01:25.072
Sep  6 11:01:25.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename statefulset 09/06/23 11:01:25.073
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:01:25.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:01:25.098
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-149 09/06/23 11:01:25.1
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 09/06/23 11:01:25.106
Sep  6 11:01:25.118: INFO: Found 0 stateful pods, waiting for 3
Sep  6 11:01:35.134: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:01:35.134: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:01:35.134: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/06/23 11:01:35.172
Sep  6 11:01:35.211: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/06/23 11:01:35.211
STEP: Not applying an update when the partition is greater than the number of replicas 09/06/23 11:01:45.268
STEP: Performing a canary update 09/06/23 11:01:45.268
Sep  6 11:01:45.306: INFO: Updating stateful set ss2
Sep  6 11:01:45.314: INFO: Waiting for Pod statefulset-149/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 09/06/23 11:01:55.337
Sep  6 11:01:55.418: INFO: Found 2 stateful pods, waiting for 3
Sep  6 11:02:05.441: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:02:05.441: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:02:05.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 09/06/23 11:02:05.465
Sep  6 11:02:05.556: INFO: Updating stateful set ss2
Sep  6 11:02:05.656: INFO: Waiting for Pod statefulset-149/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Sep  6 11:02:15.724: INFO: Updating stateful set ss2
Sep  6 11:02:15.733: INFO: Waiting for StatefulSet statefulset-149/ss2 to complete update
Sep  6 11:02:15.734: INFO: Waiting for Pod statefulset-149/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  6 11:02:25.760: INFO: Deleting all statefulset in ns statefulset-149
Sep  6 11:02:25.771: INFO: Scaling statefulset ss2 to 0
Sep  6 11:02:35.829: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:02:35.844: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:02:35.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-149" for this suite. 09/06/23 11:02:35.89
------------------------------
• [SLOW TEST] [70.827 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:01:25.072
    Sep  6 11:01:25.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename statefulset 09/06/23 11:01:25.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:01:25.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:01:25.098
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-149 09/06/23 11:01:25.1
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 09/06/23 11:01:25.106
    Sep  6 11:01:25.118: INFO: Found 0 stateful pods, waiting for 3
    Sep  6 11:01:35.134: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 11:01:35.134: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 11:01:35.134: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/06/23 11:01:35.172
    Sep  6 11:01:35.211: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/06/23 11:01:35.211
    STEP: Not applying an update when the partition is greater than the number of replicas 09/06/23 11:01:45.268
    STEP: Performing a canary update 09/06/23 11:01:45.268
    Sep  6 11:01:45.306: INFO: Updating stateful set ss2
    Sep  6 11:01:45.314: INFO: Waiting for Pod statefulset-149/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 09/06/23 11:01:55.337
    Sep  6 11:01:55.418: INFO: Found 2 stateful pods, waiting for 3
    Sep  6 11:02:05.441: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 11:02:05.441: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 11:02:05.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 09/06/23 11:02:05.465
    Sep  6 11:02:05.556: INFO: Updating stateful set ss2
    Sep  6 11:02:05.656: INFO: Waiting for Pod statefulset-149/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Sep  6 11:02:15.724: INFO: Updating stateful set ss2
    Sep  6 11:02:15.733: INFO: Waiting for StatefulSet statefulset-149/ss2 to complete update
    Sep  6 11:02:15.734: INFO: Waiting for Pod statefulset-149/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  6 11:02:25.760: INFO: Deleting all statefulset in ns statefulset-149
    Sep  6 11:02:25.771: INFO: Scaling statefulset ss2 to 0
    Sep  6 11:02:35.829: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 11:02:35.844: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:02:35.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-149" for this suite. 09/06/23 11:02:35.89
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:02:35.9
Sep  6 11:02:35.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:02:35.9
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:02:35.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:02:35.923
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3034 09/06/23 11:02:35.925
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/06/23 11:02:35.943
STEP: creating service externalsvc in namespace services-3034 09/06/23 11:02:35.943
STEP: creating replication controller externalsvc in namespace services-3034 09/06/23 11:02:35.967
I0906 11:02:35.981068      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3034, replica count: 2
I0906 11:02:39.031815      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 09/06/23 11:02:39.043
Sep  6 11:02:39.092: INFO: Creating new exec pod
Sep  6 11:02:39.105: INFO: Waiting up to 5m0s for pod "execpodq7fwn" in namespace "services-3034" to be "running"
Sep  6 11:02:39.110: INFO: Pod "execpodq7fwn": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010328ms
Sep  6 11:02:41.115: INFO: Pod "execpodq7fwn": Phase="Running", Reason="", readiness=true. Elapsed: 2.00908151s
Sep  6 11:02:41.115: INFO: Pod "execpodq7fwn" satisfied condition "running"
Sep  6 11:02:41.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-3034 exec execpodq7fwn -- /bin/sh -x -c nslookup nodeport-service.services-3034.svc.cluster.local'
Sep  6 11:02:41.298: INFO: stderr: "+ nslookup nodeport-service.services-3034.svc.cluster.local\n"
Sep  6 11:02:41.298: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-3034.svc.cluster.local\tcanonical name = externalsvc.services-3034.svc.cluster.local.\nName:\texternalsvc.services-3034.svc.cluster.local\nAddress: 10.233.21.145\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3034, will wait for the garbage collector to delete the pods 09/06/23 11:02:41.298
Sep  6 11:02:41.440: INFO: Deleting ReplicationController externalsvc took: 89.266349ms
Sep  6 11:02:41.741: INFO: Terminating ReplicationController externalsvc pods took: 300.375913ms
Sep  6 11:02:43.674: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:02:43.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3034" for this suite. 09/06/23 11:02:43.711
------------------------------
• [SLOW TEST] [7.823 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:02:35.9
    Sep  6 11:02:35.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:02:35.9
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:02:35.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:02:35.923
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3034 09/06/23 11:02:35.925
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/06/23 11:02:35.943
    STEP: creating service externalsvc in namespace services-3034 09/06/23 11:02:35.943
    STEP: creating replication controller externalsvc in namespace services-3034 09/06/23 11:02:35.967
    I0906 11:02:35.981068      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3034, replica count: 2
    I0906 11:02:39.031815      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 09/06/23 11:02:39.043
    Sep  6 11:02:39.092: INFO: Creating new exec pod
    Sep  6 11:02:39.105: INFO: Waiting up to 5m0s for pod "execpodq7fwn" in namespace "services-3034" to be "running"
    Sep  6 11:02:39.110: INFO: Pod "execpodq7fwn": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010328ms
    Sep  6 11:02:41.115: INFO: Pod "execpodq7fwn": Phase="Running", Reason="", readiness=true. Elapsed: 2.00908151s
    Sep  6 11:02:41.115: INFO: Pod "execpodq7fwn" satisfied condition "running"
    Sep  6 11:02:41.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-3034 exec execpodq7fwn -- /bin/sh -x -c nslookup nodeport-service.services-3034.svc.cluster.local'
    Sep  6 11:02:41.298: INFO: stderr: "+ nslookup nodeport-service.services-3034.svc.cluster.local\n"
    Sep  6 11:02:41.298: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-3034.svc.cluster.local\tcanonical name = externalsvc.services-3034.svc.cluster.local.\nName:\texternalsvc.services-3034.svc.cluster.local\nAddress: 10.233.21.145\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3034, will wait for the garbage collector to delete the pods 09/06/23 11:02:41.298
    Sep  6 11:02:41.440: INFO: Deleting ReplicationController externalsvc took: 89.266349ms
    Sep  6 11:02:41.741: INFO: Terminating ReplicationController externalsvc pods took: 300.375913ms
    Sep  6 11:02:43.674: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:02:43.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3034" for this suite. 09/06/23 11:02:43.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:02:43.725
Sep  6 11:02:43.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:02:43.725
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:02:43.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:02:43.755
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:02:43.76
Sep  6 11:02:43.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d" in namespace "projected-1457" to be "Succeeded or Failed"
Sep  6 11:02:43.780: INFO: Pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438105ms
Sep  6 11:02:45.796: INFO: Pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d": Phase="Running", Reason="", readiness=false. Elapsed: 2.021348454s
Sep  6 11:02:47.786: INFO: Pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011102424s
STEP: Saw pod success 09/06/23 11:02:47.786
Sep  6 11:02:47.786: INFO: Pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d" satisfied condition "Succeeded or Failed"
Sep  6 11:02:47.791: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d container client-container: <nil>
STEP: delete the pod 09/06/23 11:02:47.809
Sep  6 11:02:47.823: INFO: Waiting for pod downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d to disappear
Sep  6 11:02:47.827: INFO: Pod downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 11:02:47.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1457" for this suite. 09/06/23 11:02:47.831
------------------------------
• [4.112 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:02:43.725
    Sep  6 11:02:43.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:02:43.725
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:02:43.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:02:43.755
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:02:43.76
    Sep  6 11:02:43.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d" in namespace "projected-1457" to be "Succeeded or Failed"
    Sep  6 11:02:43.780: INFO: Pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438105ms
    Sep  6 11:02:45.796: INFO: Pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d": Phase="Running", Reason="", readiness=false. Elapsed: 2.021348454s
    Sep  6 11:02:47.786: INFO: Pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011102424s
    STEP: Saw pod success 09/06/23 11:02:47.786
    Sep  6 11:02:47.786: INFO: Pod "downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d" satisfied condition "Succeeded or Failed"
    Sep  6 11:02:47.791: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d container client-container: <nil>
    STEP: delete the pod 09/06/23 11:02:47.809
    Sep  6 11:02:47.823: INFO: Waiting for pod downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d to disappear
    Sep  6 11:02:47.827: INFO: Pod downwardapi-volume-d83dca9b-2358-4d81-8d47-f7faa673743d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:02:47.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1457" for this suite. 09/06/23 11:02:47.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:02:47.837
Sep  6 11:02:47.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:02:47.838
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:02:47.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:02:47.861
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-8386 09/06/23 11:02:47.863
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[] 09/06/23 11:02:47.872
Sep  6 11:02:47.883: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep  6 11:02:48.896: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8386 09/06/23 11:02:48.896
Sep  6 11:02:48.904: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8386" to be "running and ready"
Sep  6 11:02:48.921: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.125134ms
Sep  6 11:02:48.921: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:02:50.934: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.029886558s
Sep  6 11:02:50.934: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  6 11:02:50.934: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[pod1:[100]] 09/06/23 11:02:50.948
Sep  6 11:02:50.980: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8386 09/06/23 11:02:50.98
Sep  6 11:02:50.991: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8386" to be "running and ready"
Sep  6 11:02:51.001: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.535076ms
Sep  6 11:02:51.001: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:02:53.006: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014984631s
Sep  6 11:02:53.007: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:02:55.009: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.018038882s
Sep  6 11:02:55.010: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  6 11:02:55.010: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[pod1:[100] pod2:[101]] 09/06/23 11:02:55.016
Sep  6 11:02:55.039: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 09/06/23 11:02:55.039
Sep  6 11:02:55.039: INFO: Creating new exec pod
Sep  6 11:02:55.051: INFO: Waiting up to 5m0s for pod "execpodncth7" in namespace "services-8386" to be "running"
Sep  6 11:02:55.055: INFO: Pod "execpodncth7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319597ms
Sep  6 11:02:57.068: INFO: Pod "execpodncth7": Phase="Running", Reason="", readiness=true. Elapsed: 2.016816414s
Sep  6 11:02:57.068: INFO: Pod "execpodncth7" satisfied condition "running"
Sep  6 11:02:58.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8386 exec execpodncth7 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Sep  6 11:02:58.281: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep  6 11:02:58.281: INFO: stdout: ""
Sep  6 11:02:58.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8386 exec execpodncth7 -- /bin/sh -x -c nc -v -z -w 2 10.233.30.110 80'
Sep  6 11:02:58.387: INFO: stderr: "+ nc -v -z -w 2 10.233.30.110 80\nConnection to 10.233.30.110 80 port [tcp/http] succeeded!\n"
Sep  6 11:02:58.387: INFO: stdout: ""
Sep  6 11:02:58.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8386 exec execpodncth7 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Sep  6 11:02:58.508: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep  6 11:02:58.508: INFO: stdout: ""
Sep  6 11:02:58.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8386 exec execpodncth7 -- /bin/sh -x -c nc -v -z -w 2 10.233.30.110 81'
Sep  6 11:02:58.636: INFO: stderr: "+ nc -v -z -w 2 10.233.30.110 81\nConnection to 10.233.30.110 81 port [tcp/*] succeeded!\n"
Sep  6 11:02:58.636: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8386 09/06/23 11:02:58.636
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[pod2:[101]] 09/06/23 11:02:58.672
Sep  6 11:02:58.694: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8386 09/06/23 11:02:58.694
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[] 09/06/23 11:02:58.733
Sep  6 11:02:59.763: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:03:00.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8386" for this suite. 09/06/23 11:03:00.232
------------------------------
• [SLOW TEST] [12.410 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:02:47.837
    Sep  6 11:02:47.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:02:47.838
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:02:47.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:02:47.861
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-8386 09/06/23 11:02:47.863
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[] 09/06/23 11:02:47.872
    Sep  6 11:02:47.883: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Sep  6 11:02:48.896: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8386 09/06/23 11:02:48.896
    Sep  6 11:02:48.904: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8386" to be "running and ready"
    Sep  6 11:02:48.921: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.125134ms
    Sep  6 11:02:48.921: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:02:50.934: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.029886558s
    Sep  6 11:02:50.934: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  6 11:02:50.934: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[pod1:[100]] 09/06/23 11:02:50.948
    Sep  6 11:02:50.980: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-8386 09/06/23 11:02:50.98
    Sep  6 11:02:50.991: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8386" to be "running and ready"
    Sep  6 11:02:51.001: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.535076ms
    Sep  6 11:02:51.001: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:02:53.006: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014984631s
    Sep  6 11:02:53.007: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:02:55.009: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.018038882s
    Sep  6 11:02:55.010: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  6 11:02:55.010: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[pod1:[100] pod2:[101]] 09/06/23 11:02:55.016
    Sep  6 11:02:55.039: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 09/06/23 11:02:55.039
    Sep  6 11:02:55.039: INFO: Creating new exec pod
    Sep  6 11:02:55.051: INFO: Waiting up to 5m0s for pod "execpodncth7" in namespace "services-8386" to be "running"
    Sep  6 11:02:55.055: INFO: Pod "execpodncth7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319597ms
    Sep  6 11:02:57.068: INFO: Pod "execpodncth7": Phase="Running", Reason="", readiness=true. Elapsed: 2.016816414s
    Sep  6 11:02:57.068: INFO: Pod "execpodncth7" satisfied condition "running"
    Sep  6 11:02:58.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8386 exec execpodncth7 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Sep  6 11:02:58.281: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Sep  6 11:02:58.281: INFO: stdout: ""
    Sep  6 11:02:58.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8386 exec execpodncth7 -- /bin/sh -x -c nc -v -z -w 2 10.233.30.110 80'
    Sep  6 11:02:58.387: INFO: stderr: "+ nc -v -z -w 2 10.233.30.110 80\nConnection to 10.233.30.110 80 port [tcp/http] succeeded!\n"
    Sep  6 11:02:58.387: INFO: stdout: ""
    Sep  6 11:02:58.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8386 exec execpodncth7 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Sep  6 11:02:58.508: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Sep  6 11:02:58.508: INFO: stdout: ""
    Sep  6 11:02:58.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8386 exec execpodncth7 -- /bin/sh -x -c nc -v -z -w 2 10.233.30.110 81'
    Sep  6 11:02:58.636: INFO: stderr: "+ nc -v -z -w 2 10.233.30.110 81\nConnection to 10.233.30.110 81 port [tcp/*] succeeded!\n"
    Sep  6 11:02:58.636: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8386 09/06/23 11:02:58.636
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[pod2:[101]] 09/06/23 11:02:58.672
    Sep  6 11:02:58.694: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-8386 09/06/23 11:02:58.694
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8386 to expose endpoints map[] 09/06/23 11:02:58.733
    Sep  6 11:02:59.763: INFO: successfully validated that service multi-endpoint-test in namespace services-8386 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:03:00.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8386" for this suite. 09/06/23 11:03:00.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:03:00.248
Sep  6 11:03:00.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 11:03:00.252
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:00.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:00.305
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:03:00.309
Sep  6 11:03:00.328: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e" in namespace "downward-api-5241" to be "Succeeded or Failed"
Sep  6 11:03:00.334: INFO: Pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.398513ms
Sep  6 11:03:02.345: INFO: Pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017033316s
Sep  6 11:03:04.339: INFO: Pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011239607s
STEP: Saw pod success 09/06/23 11:03:04.339
Sep  6 11:03:04.339: INFO: Pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e" satisfied condition "Succeeded or Failed"
Sep  6 11:03:04.343: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e container client-container: <nil>
STEP: delete the pod 09/06/23 11:03:04.348
Sep  6 11:03:04.367: INFO: Waiting for pod downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e to disappear
Sep  6 11:03:04.370: INFO: Pod downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 11:03:04.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5241" for this suite. 09/06/23 11:03:04.374
------------------------------
• [4.133 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:03:00.248
    Sep  6 11:03:00.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 11:03:00.252
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:00.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:00.305
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:03:00.309
    Sep  6 11:03:00.328: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e" in namespace "downward-api-5241" to be "Succeeded or Failed"
    Sep  6 11:03:00.334: INFO: Pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.398513ms
    Sep  6 11:03:02.345: INFO: Pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017033316s
    Sep  6 11:03:04.339: INFO: Pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011239607s
    STEP: Saw pod success 09/06/23 11:03:04.339
    Sep  6 11:03:04.339: INFO: Pod "downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e" satisfied condition "Succeeded or Failed"
    Sep  6 11:03:04.343: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e container client-container: <nil>
    STEP: delete the pod 09/06/23 11:03:04.348
    Sep  6 11:03:04.367: INFO: Waiting for pod downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e to disappear
    Sep  6 11:03:04.370: INFO: Pod downwardapi-volume-6d6cc7df-57d1-4375-95a4-d6c84b2d941e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:03:04.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5241" for this suite. 09/06/23 11:03:04.374
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:03:04.384
Sep  6 11:03:04.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename certificates 09/06/23 11:03:04.385
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:04.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:04.405
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 09/06/23 11:03:04.777
STEP: getting /apis/certificates.k8s.io 09/06/23 11:03:04.779
STEP: getting /apis/certificates.k8s.io/v1 09/06/23 11:03:04.78
STEP: creating 09/06/23 11:03:04.781
STEP: getting 09/06/23 11:03:04.8
STEP: listing 09/06/23 11:03:04.802
STEP: watching 09/06/23 11:03:04.805
Sep  6 11:03:04.805: INFO: starting watch
STEP: patching 09/06/23 11:03:04.806
STEP: updating 09/06/23 11:03:04.812
Sep  6 11:03:04.818: INFO: waiting for watch events with expected annotations
Sep  6 11:03:04.818: INFO: saw patched and updated annotations
STEP: getting /approval 09/06/23 11:03:04.818
STEP: patching /approval 09/06/23 11:03:04.821
STEP: updating /approval 09/06/23 11:03:04.827
STEP: getting /status 09/06/23 11:03:04.833
STEP: patching /status 09/06/23 11:03:04.837
STEP: updating /status 09/06/23 11:03:04.844
STEP: deleting 09/06/23 11:03:04.854
STEP: deleting a collection 09/06/23 11:03:04.865
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:03:04.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-3292" for this suite. 09/06/23 11:03:04.883
------------------------------
• [0.507 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:03:04.384
    Sep  6 11:03:04.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename certificates 09/06/23 11:03:04.385
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:04.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:04.405
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 09/06/23 11:03:04.777
    STEP: getting /apis/certificates.k8s.io 09/06/23 11:03:04.779
    STEP: getting /apis/certificates.k8s.io/v1 09/06/23 11:03:04.78
    STEP: creating 09/06/23 11:03:04.781
    STEP: getting 09/06/23 11:03:04.8
    STEP: listing 09/06/23 11:03:04.802
    STEP: watching 09/06/23 11:03:04.805
    Sep  6 11:03:04.805: INFO: starting watch
    STEP: patching 09/06/23 11:03:04.806
    STEP: updating 09/06/23 11:03:04.812
    Sep  6 11:03:04.818: INFO: waiting for watch events with expected annotations
    Sep  6 11:03:04.818: INFO: saw patched and updated annotations
    STEP: getting /approval 09/06/23 11:03:04.818
    STEP: patching /approval 09/06/23 11:03:04.821
    STEP: updating /approval 09/06/23 11:03:04.827
    STEP: getting /status 09/06/23 11:03:04.833
    STEP: patching /status 09/06/23 11:03:04.837
    STEP: updating /status 09/06/23 11:03:04.844
    STEP: deleting 09/06/23 11:03:04.854
    STEP: deleting a collection 09/06/23 11:03:04.865
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:03:04.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-3292" for this suite. 09/06/23 11:03:04.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:03:04.894
Sep  6 11:03:04.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-lifecycle-hook 09/06/23 11:03:04.894
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:04.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:04.915
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/06/23 11:03:04.92
Sep  6 11:03:04.927: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3465" to be "running and ready"
Sep  6 11:03:04.931: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.71243ms
Sep  6 11:03:04.931: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:03:06.946: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018395933s
Sep  6 11:03:06.946: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  6 11:03:06.946: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 09/06/23 11:03:06.961
Sep  6 11:03:06.976: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3465" to be "running and ready"
Sep  6 11:03:06.987: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.390786ms
Sep  6 11:03:06.987: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:03:08.991: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015254505s
Sep  6 11:03:08.991: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Sep  6 11:03:08.991: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/06/23 11:03:08.995
Sep  6 11:03:09.006: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:03:09.010: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 11:03:11.011: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:03:11.015: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 11:03:13.012: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:03:13.027: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 09/06/23 11:03:13.028
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  6 11:03:13.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3465" for this suite. 09/06/23 11:03:13.058
------------------------------
• [SLOW TEST] [8.178 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:03:04.894
    Sep  6 11:03:04.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/06/23 11:03:04.894
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:04.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:04.915
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/06/23 11:03:04.92
    Sep  6 11:03:04.927: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3465" to be "running and ready"
    Sep  6 11:03:04.931: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.71243ms
    Sep  6 11:03:04.931: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:03:06.946: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018395933s
    Sep  6 11:03:06.946: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  6 11:03:06.946: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 09/06/23 11:03:06.961
    Sep  6 11:03:06.976: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3465" to be "running and ready"
    Sep  6 11:03:06.987: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.390786ms
    Sep  6 11:03:06.987: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:03:08.991: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015254505s
    Sep  6 11:03:08.991: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Sep  6 11:03:08.991: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/06/23 11:03:08.995
    Sep  6 11:03:09.006: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  6 11:03:09.010: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  6 11:03:11.011: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  6 11:03:11.015: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  6 11:03:13.012: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  6 11:03:13.027: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 09/06/23 11:03:13.028
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:03:13.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3465" for this suite. 09/06/23 11:03:13.058
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:03:13.076
Sep  6 11:03:13.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename gc 09/06/23 11:03:13.076
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:13.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:13.098
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 09/06/23 11:03:13.104
STEP: create the rc2 09/06/23 11:03:13.111
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/06/23 11:03:18.137
STEP: delete the rc simpletest-rc-to-be-deleted 09/06/23 11:03:26.713
STEP: wait for the rc to be deleted 09/06/23 11:03:26.828
Sep  6 11:03:32.583: INFO: 87 pods remaining
Sep  6 11:03:32.583: INFO: 83 pods has nil DeletionTimestamp
Sep  6 11:03:32.583: INFO: 
Sep  6 11:03:36.920: INFO: 57 pods remaining
Sep  6 11:03:36.920: INFO: 57 pods has nil DeletionTimestamp
Sep  6 11:03:36.920: INFO: 
STEP: Gathering metrics 09/06/23 11:03:42.38
Sep  6 11:03:43.047: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Sep  6 11:03:43.382: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 334.190242ms
Sep  6 11:03:43.382: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Sep  6 11:03:43.382: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Sep  6 11:03:43.520: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  6 11:03:43.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-25tfr" in namespace "gc-7932"
Sep  6 11:03:43.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f45c" in namespace "gc-7932"
Sep  6 11:03:43.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g672" in namespace "gc-7932"
Sep  6 11:03:43.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nhgx" in namespace "gc-7932"
Sep  6 11:03:43.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zj2t" in namespace "gc-7932"
Sep  6 11:03:44.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-442xv" in namespace "gc-7932"
Sep  6 11:03:44.141: INFO: Deleting pod "simpletest-rc-to-be-deleted-456lp" in namespace "gc-7932"
Sep  6 11:03:44.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-458k5" in namespace "gc-7932"
Sep  6 11:03:44.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hll8" in namespace "gc-7932"
Sep  6 11:03:44.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lgsh" in namespace "gc-7932"
Sep  6 11:03:44.872: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rcbz" in namespace "gc-7932"
Sep  6 11:03:45.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hlnb" in namespace "gc-7932"
Sep  6 11:03:45.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lwjc" in namespace "gc-7932"
Sep  6 11:03:45.315: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pdqp" in namespace "gc-7932"
Sep  6 11:03:45.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tm4t" in namespace "gc-7932"
Sep  6 11:03:45.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ts4c" in namespace "gc-7932"
Sep  6 11:03:45.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pjbf" in namespace "gc-7932"
Sep  6 11:03:45.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tzhj" in namespace "gc-7932"
Sep  6 11:03:45.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-7btrb" in namespace "gc-7932"
Sep  6 11:03:46.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ff62" in namespace "gc-7932"
Sep  6 11:03:46.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ktm8" in namespace "gc-7932"
Sep  6 11:03:47.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-7l9vh" in namespace "gc-7932"
Sep  6 11:03:47.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xhzl" in namespace "gc-7932"
Sep  6 11:03:47.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-87xjd" in namespace "gc-7932"
Sep  6 11:03:47.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-8crpg" in namespace "gc-7932"
Sep  6 11:03:47.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-8j6rv" in namespace "gc-7932"
Sep  6 11:03:47.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ncf8" in namespace "gc-7932"
Sep  6 11:03:48.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-8q2mx" in namespace "gc-7932"
Sep  6 11:03:49.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-8r4xv" in namespace "gc-7932"
Sep  6 11:03:50.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wgt4" in namespace "gc-7932"
Sep  6 11:03:50.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cvjj" in namespace "gc-7932"
Sep  6 11:03:51.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pz2w" in namespace "gc-7932"
Sep  6 11:03:51.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjqk5" in namespace "gc-7932"
Sep  6 11:03:51.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-bksb4" in namespace "gc-7932"
Sep  6 11:03:51.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqsk9" in namespace "gc-7932"
Sep  6 11:03:51.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6pzr" in namespace "gc-7932"
Sep  6 11:03:51.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-clxpd" in namespace "gc-7932"
Sep  6 11:03:51.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmpqf" in namespace "gc-7932"
Sep  6 11:03:51.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-cndsg" in namespace "gc-7932"
Sep  6 11:03:51.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctb5f" in namespace "gc-7932"
Sep  6 11:03:51.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5hg6" in namespace "gc-7932"
Sep  6 11:03:52.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc5j8" in namespace "gc-7932"
Sep  6 11:03:52.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl56d" in namespace "gc-7932"
Sep  6 11:03:52.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpwr7" in namespace "gc-7932"
Sep  6 11:03:52.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtfcw" in namespace "gc-7932"
Sep  6 11:03:52.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzjks" in namespace "gc-7932"
Sep  6 11:03:52.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-gcz2m" in namespace "gc-7932"
Sep  6 11:03:52.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-gddfg" in namespace "gc-7932"
Sep  6 11:03:52.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-gf2x4" in namespace "gc-7932"
Sep  6 11:03:53.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjnmm" in namespace "gc-7932"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  6 11:03:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7932" for this suite. 09/06/23 11:03:53.182
------------------------------
• [SLOW TEST] [40.176 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:03:13.076
    Sep  6 11:03:13.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename gc 09/06/23 11:03:13.076
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:13.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:13.098
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 09/06/23 11:03:13.104
    STEP: create the rc2 09/06/23 11:03:13.111
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/06/23 11:03:18.137
    STEP: delete the rc simpletest-rc-to-be-deleted 09/06/23 11:03:26.713
    STEP: wait for the rc to be deleted 09/06/23 11:03:26.828
    Sep  6 11:03:32.583: INFO: 87 pods remaining
    Sep  6 11:03:32.583: INFO: 83 pods has nil DeletionTimestamp
    Sep  6 11:03:32.583: INFO: 
    Sep  6 11:03:36.920: INFO: 57 pods remaining
    Sep  6 11:03:36.920: INFO: 57 pods has nil DeletionTimestamp
    Sep  6 11:03:36.920: INFO: 
    STEP: Gathering metrics 09/06/23 11:03:42.38
    Sep  6 11:03:43.047: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Sep  6 11:03:43.382: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 334.190242ms
    Sep  6 11:03:43.382: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Sep  6 11:03:43.382: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Sep  6 11:03:43.520: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  6 11:03:43.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-25tfr" in namespace "gc-7932"
    Sep  6 11:03:43.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f45c" in namespace "gc-7932"
    Sep  6 11:03:43.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g672" in namespace "gc-7932"
    Sep  6 11:03:43.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nhgx" in namespace "gc-7932"
    Sep  6 11:03:43.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zj2t" in namespace "gc-7932"
    Sep  6 11:03:44.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-442xv" in namespace "gc-7932"
    Sep  6 11:03:44.141: INFO: Deleting pod "simpletest-rc-to-be-deleted-456lp" in namespace "gc-7932"
    Sep  6 11:03:44.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-458k5" in namespace "gc-7932"
    Sep  6 11:03:44.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hll8" in namespace "gc-7932"
    Sep  6 11:03:44.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lgsh" in namespace "gc-7932"
    Sep  6 11:03:44.872: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rcbz" in namespace "gc-7932"
    Sep  6 11:03:45.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hlnb" in namespace "gc-7932"
    Sep  6 11:03:45.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lwjc" in namespace "gc-7932"
    Sep  6 11:03:45.315: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pdqp" in namespace "gc-7932"
    Sep  6 11:03:45.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tm4t" in namespace "gc-7932"
    Sep  6 11:03:45.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ts4c" in namespace "gc-7932"
    Sep  6 11:03:45.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pjbf" in namespace "gc-7932"
    Sep  6 11:03:45.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tzhj" in namespace "gc-7932"
    Sep  6 11:03:45.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-7btrb" in namespace "gc-7932"
    Sep  6 11:03:46.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ff62" in namespace "gc-7932"
    Sep  6 11:03:46.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ktm8" in namespace "gc-7932"
    Sep  6 11:03:47.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-7l9vh" in namespace "gc-7932"
    Sep  6 11:03:47.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xhzl" in namespace "gc-7932"
    Sep  6 11:03:47.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-87xjd" in namespace "gc-7932"
    Sep  6 11:03:47.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-8crpg" in namespace "gc-7932"
    Sep  6 11:03:47.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-8j6rv" in namespace "gc-7932"
    Sep  6 11:03:47.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ncf8" in namespace "gc-7932"
    Sep  6 11:03:48.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-8q2mx" in namespace "gc-7932"
    Sep  6 11:03:49.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-8r4xv" in namespace "gc-7932"
    Sep  6 11:03:50.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wgt4" in namespace "gc-7932"
    Sep  6 11:03:50.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cvjj" in namespace "gc-7932"
    Sep  6 11:03:51.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pz2w" in namespace "gc-7932"
    Sep  6 11:03:51.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjqk5" in namespace "gc-7932"
    Sep  6 11:03:51.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-bksb4" in namespace "gc-7932"
    Sep  6 11:03:51.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqsk9" in namespace "gc-7932"
    Sep  6 11:03:51.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6pzr" in namespace "gc-7932"
    Sep  6 11:03:51.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-clxpd" in namespace "gc-7932"
    Sep  6 11:03:51.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmpqf" in namespace "gc-7932"
    Sep  6 11:03:51.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-cndsg" in namespace "gc-7932"
    Sep  6 11:03:51.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctb5f" in namespace "gc-7932"
    Sep  6 11:03:51.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5hg6" in namespace "gc-7932"
    Sep  6 11:03:52.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc5j8" in namespace "gc-7932"
    Sep  6 11:03:52.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl56d" in namespace "gc-7932"
    Sep  6 11:03:52.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpwr7" in namespace "gc-7932"
    Sep  6 11:03:52.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtfcw" in namespace "gc-7932"
    Sep  6 11:03:52.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzjks" in namespace "gc-7932"
    Sep  6 11:03:52.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-gcz2m" in namespace "gc-7932"
    Sep  6 11:03:52.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-gddfg" in namespace "gc-7932"
    Sep  6 11:03:52.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-gf2x4" in namespace "gc-7932"
    Sep  6 11:03:53.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjnmm" in namespace "gc-7932"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:03:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7932" for this suite. 09/06/23 11:03:53.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:03:53.256
Sep  6 11:03:53.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 11:03:53.257
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:54.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:54.159
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 11:03:54.862
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:03:55.398
STEP: Deploying the webhook pod 09/06/23 11:03:59.915
STEP: Wait for the deployment to be ready 09/06/23 11:03:59.994
Sep  6 11:04:00.027: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  6 11:04:02.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:04:04.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:04:06.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:04:08.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:04:10.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/06/23 11:04:12.133
STEP: Verifying the service has paired with the endpoint 09/06/23 11:04:12.152
Sep  6 11:04:13.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Sep  6 11:04:13.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/06/23 11:04:18.673
STEP: Creating a custom resource that should be denied by the webhook 09/06/23 11:04:18.701
STEP: Creating a custom resource whose deletion would be denied by the webhook 09/06/23 11:04:20.749
STEP: Updating the custom resource with disallowed data should be denied 09/06/23 11:04:20.761
STEP: Deleting the custom resource should be denied 09/06/23 11:04:20.774
STEP: Remove the offending key and value from the custom resource data 09/06/23 11:04:20.781
STEP: Deleting the updated custom resource should be successful 09/06/23 11:04:20.791
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:21.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1583" for this suite. 09/06/23 11:04:21.393
STEP: Destroying namespace "webhook-1583-markers" for this suite. 09/06/23 11:04:21.423
------------------------------
• [SLOW TEST] [28.191 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:03:53.256
    Sep  6 11:03:53.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 11:03:53.257
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:03:54.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:03:54.159
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 11:03:54.862
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:03:55.398
    STEP: Deploying the webhook pod 09/06/23 11:03:59.915
    STEP: Wait for the deployment to be ready 09/06/23 11:03:59.994
    Sep  6 11:04:00.027: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep  6 11:04:02.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 11:04:04.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 11:04:06.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 11:04:08.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  6 11:04:10.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/06/23 11:04:12.133
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:04:12.152
    Sep  6 11:04:13.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Sep  6 11:04:13.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/06/23 11:04:18.673
    STEP: Creating a custom resource that should be denied by the webhook 09/06/23 11:04:18.701
    STEP: Creating a custom resource whose deletion would be denied by the webhook 09/06/23 11:04:20.749
    STEP: Updating the custom resource with disallowed data should be denied 09/06/23 11:04:20.761
    STEP: Deleting the custom resource should be denied 09/06/23 11:04:20.774
    STEP: Remove the offending key and value from the custom resource data 09/06/23 11:04:20.781
    STEP: Deleting the updated custom resource should be successful 09/06/23 11:04:20.791
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:21.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1583" for this suite. 09/06/23 11:04:21.393
    STEP: Destroying namespace "webhook-1583-markers" for this suite. 09/06/23 11:04:21.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:21.447
Sep  6 11:04:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:04:21.448
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:21.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:21.494
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-bd91b8eb-239d-47fb-bbdf-aa3e79d7b3f2 09/06/23 11:04:23.145
STEP: Creating a pod to test consume secrets 09/06/23 11:04:23.182
Sep  6 11:04:23.432: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3" in namespace "projected-3703" to be "Succeeded or Failed"
Sep  6 11:04:23.457: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.498488ms
Sep  6 11:04:27.413: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981669342s
Sep  6 11:04:27.462: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030579178s
Sep  6 11:04:29.467: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034962636s
Sep  6 11:04:31.468: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03618732s
STEP: Saw pod success 09/06/23 11:04:31.468
Sep  6 11:04:31.469: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3" satisfied condition "Succeeded or Failed"
Sep  6 11:04:31.474: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3 container secret-volume-test: <nil>
STEP: delete the pod 09/06/23 11:04:31.483
Sep  6 11:04:31.504: INFO: Waiting for pod pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3 to disappear
Sep  6 11:04:31.509: INFO: Pod pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:31.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3703" for this suite. 09/06/23 11:04:31.516
------------------------------
• [SLOW TEST] [10.077 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:21.447
    Sep  6 11:04:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:04:21.448
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:21.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:21.494
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-bd91b8eb-239d-47fb-bbdf-aa3e79d7b3f2 09/06/23 11:04:23.145
    STEP: Creating a pod to test consume secrets 09/06/23 11:04:23.182
    Sep  6 11:04:23.432: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3" in namespace "projected-3703" to be "Succeeded or Failed"
    Sep  6 11:04:23.457: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.498488ms
    Sep  6 11:04:27.413: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981669342s
    Sep  6 11:04:27.462: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030579178s
    Sep  6 11:04:29.467: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034962636s
    Sep  6 11:04:31.468: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03618732s
    STEP: Saw pod success 09/06/23 11:04:31.468
    Sep  6 11:04:31.469: INFO: Pod "pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3" satisfied condition "Succeeded or Failed"
    Sep  6 11:04:31.474: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3 container secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 11:04:31.483
    Sep  6 11:04:31.504: INFO: Waiting for pod pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3 to disappear
    Sep  6 11:04:31.509: INFO: Pod pod-projected-secrets-1eccbe6a-ef4c-44f9-a1b3-03e3d74d36a3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:31.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3703" for this suite. 09/06/23 11:04:31.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:31.525
Sep  6 11:04:31.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 11:04:31.526
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:31.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:31.553
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:04:31.555
Sep  6 11:04:31.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f" in namespace "downward-api-6116" to be "Succeeded or Failed"
Sep  6 11:04:31.573: INFO: Pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.596079ms
Sep  6 11:04:33.577: INFO: Pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012946936s
Sep  6 11:04:35.581: INFO: Pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016715742s
STEP: Saw pod success 09/06/23 11:04:35.581
Sep  6 11:04:35.582: INFO: Pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f" satisfied condition "Succeeded or Failed"
Sep  6 11:04:35.588: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f container client-container: <nil>
STEP: delete the pod 09/06/23 11:04:35.6
Sep  6 11:04:35.623: INFO: Waiting for pod downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f to disappear
Sep  6 11:04:35.626: INFO: Pod downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:35.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6116" for this suite. 09/06/23 11:04:35.629
------------------------------
• [4.113 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:31.525
    Sep  6 11:04:31.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 11:04:31.526
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:31.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:31.553
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:04:31.555
    Sep  6 11:04:31.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f" in namespace "downward-api-6116" to be "Succeeded or Failed"
    Sep  6 11:04:31.573: INFO: Pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.596079ms
    Sep  6 11:04:33.577: INFO: Pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012946936s
    Sep  6 11:04:35.581: INFO: Pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016715742s
    STEP: Saw pod success 09/06/23 11:04:35.581
    Sep  6 11:04:35.582: INFO: Pod "downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f" satisfied condition "Succeeded or Failed"
    Sep  6 11:04:35.588: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f container client-container: <nil>
    STEP: delete the pod 09/06/23 11:04:35.6
    Sep  6 11:04:35.623: INFO: Waiting for pod downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f to disappear
    Sep  6 11:04:35.626: INFO: Pod downwardapi-volume-58f1488a-c8ae-4e79-a736-caf31397367f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:35.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6116" for this suite. 09/06/23 11:04:35.629
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:35.638
Sep  6 11:04:35.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 11:04:35.639
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:35.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:35.667
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/06/23 11:04:35.669
Sep  6 11:04:35.679: INFO: Waiting up to 5m0s for pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af" in namespace "emptydir-4561" to be "Succeeded or Failed"
Sep  6 11:04:35.685: INFO: Pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.437017ms
Sep  6 11:04:37.690: INFO: Pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011048097s
Sep  6 11:04:39.699: INFO: Pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019935663s
STEP: Saw pod success 09/06/23 11:04:39.699
Sep  6 11:04:39.699: INFO: Pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af" satisfied condition "Succeeded or Failed"
Sep  6 11:04:39.708: INFO: Trying to get logs from node kube-3 pod pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af container test-container: <nil>
STEP: delete the pod 09/06/23 11:04:39.721
Sep  6 11:04:39.743: INFO: Waiting for pod pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af to disappear
Sep  6 11:04:39.745: INFO: Pod pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:39.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4561" for this suite. 09/06/23 11:04:39.749
------------------------------
• [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:35.638
    Sep  6 11:04:35.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 11:04:35.639
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:35.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:35.667
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/06/23 11:04:35.669
    Sep  6 11:04:35.679: INFO: Waiting up to 5m0s for pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af" in namespace "emptydir-4561" to be "Succeeded or Failed"
    Sep  6 11:04:35.685: INFO: Pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.437017ms
    Sep  6 11:04:37.690: INFO: Pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011048097s
    Sep  6 11:04:39.699: INFO: Pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019935663s
    STEP: Saw pod success 09/06/23 11:04:39.699
    Sep  6 11:04:39.699: INFO: Pod "pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af" satisfied condition "Succeeded or Failed"
    Sep  6 11:04:39.708: INFO: Trying to get logs from node kube-3 pod pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af container test-container: <nil>
    STEP: delete the pod 09/06/23 11:04:39.721
    Sep  6 11:04:39.743: INFO: Waiting for pod pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af to disappear
    Sep  6 11:04:39.745: INFO: Pod pod-be91c065-751a-433b-a8a0-dcdfa1d6c1af no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:39.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4561" for this suite. 09/06/23 11:04:39.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:39.759
Sep  6 11:04:39.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename csiinlinevolumes 09/06/23 11:04:39.76
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:39.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:39.779
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 09/06/23 11:04:39.78
STEP: getting 09/06/23 11:04:39.804
STEP: listing in namespace 09/06/23 11:04:39.812
STEP: patching 09/06/23 11:04:39.818
STEP: deleting 09/06/23 11:04:39.829
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:39.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-214" for this suite. 09/06/23 11:04:39.843
------------------------------
• [0.090 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:39.759
    Sep  6 11:04:39.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename csiinlinevolumes 09/06/23 11:04:39.76
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:39.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:39.779
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 09/06/23 11:04:39.78
    STEP: getting 09/06/23 11:04:39.804
    STEP: listing in namespace 09/06/23 11:04:39.812
    STEP: patching 09/06/23 11:04:39.818
    STEP: deleting 09/06/23 11:04:39.829
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:39.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-214" for this suite. 09/06/23 11:04:39.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:39.85
Sep  6 11:04:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:04:39.852
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:39.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:39.88
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 09/06/23 11:04:39.886
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:39.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5365" for this suite. 09/06/23 11:04:39.899
------------------------------
• [0.059 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:39.85
    Sep  6 11:04:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:04:39.852
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:39.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:39.88
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 09/06/23 11:04:39.886
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:39.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5365" for this suite. 09/06/23 11:04:39.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:39.91
Sep  6 11:04:39.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 11:04:39.911
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:39.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:39.945
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Sep  6 11:04:39.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: creating the pod 09/06/23 11:04:39.952
STEP: submitting the pod to kubernetes 09/06/23 11:04:39.952
Sep  6 11:04:39.962: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a" in namespace "pods-4717" to be "running and ready"
Sep  6 11:04:39.967: INFO: Pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926757ms
Sep  6 11:04:39.967: INFO: The phase of Pod pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:04:41.981: INFO: Pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018625214s
Sep  6 11:04:41.981: INFO: The phase of Pod pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:04:44.526: INFO: Pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a": Phase="Running", Reason="", readiness=true. Elapsed: 4.563580271s
Sep  6 11:04:44.526: INFO: The phase of Pod pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a is Running (Ready = true)
Sep  6 11:04:44.526: INFO: Pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:45.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4717" for this suite. 09/06/23 11:04:45.248
------------------------------
• [SLOW TEST] [5.418 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:39.91
    Sep  6 11:04:39.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 11:04:39.911
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:39.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:39.945
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Sep  6 11:04:39.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: creating the pod 09/06/23 11:04:39.952
    STEP: submitting the pod to kubernetes 09/06/23 11:04:39.952
    Sep  6 11:04:39.962: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a" in namespace "pods-4717" to be "running and ready"
    Sep  6 11:04:39.967: INFO: Pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926757ms
    Sep  6 11:04:39.967: INFO: The phase of Pod pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:04:41.981: INFO: Pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018625214s
    Sep  6 11:04:41.981: INFO: The phase of Pod pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:04:44.526: INFO: Pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a": Phase="Running", Reason="", readiness=true. Elapsed: 4.563580271s
    Sep  6 11:04:44.526: INFO: The phase of Pod pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a is Running (Ready = true)
    Sep  6 11:04:44.526: INFO: Pod "pod-logs-websocket-226a319c-f01a-4498-a3e1-c396d820e44a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:45.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4717" for this suite. 09/06/23 11:04:45.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:45.331
Sep  6 11:04:45.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename job 09/06/23 11:04:45.332
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:45.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:45.592
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 09/06/23 11:04:45.594
STEP: Ensuring active pods == parallelism 09/06/23 11:04:45.665
STEP: Orphaning one of the Job's Pods 09/06/23 11:04:47.67
Sep  6 11:04:48.195: INFO: Successfully updated pod "adopt-release-76bdv"
STEP: Checking that the Job readopts the Pod 09/06/23 11:04:48.195
Sep  6 11:04:48.196: INFO: Waiting up to 15m0s for pod "adopt-release-76bdv" in namespace "job-3486" to be "adopted"
Sep  6 11:04:48.205: INFO: Pod "adopt-release-76bdv": Phase="Running", Reason="", readiness=true. Elapsed: 9.001519ms
Sep  6 11:04:50.214: INFO: Pod "adopt-release-76bdv": Phase="Running", Reason="", readiness=true. Elapsed: 2.018421871s
Sep  6 11:04:50.214: INFO: Pod "adopt-release-76bdv" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 09/06/23 11:04:50.214
Sep  6 11:04:50.733: INFO: Successfully updated pod "adopt-release-76bdv"
STEP: Checking that the Job releases the Pod 09/06/23 11:04:50.733
Sep  6 11:04:50.733: INFO: Waiting up to 15m0s for pod "adopt-release-76bdv" in namespace "job-3486" to be "released"
Sep  6 11:04:50.738: INFO: Pod "adopt-release-76bdv": Phase="Running", Reason="", readiness=true. Elapsed: 5.075136ms
Sep  6 11:04:52.751: INFO: Pod "adopt-release-76bdv": Phase="Running", Reason="", readiness=true. Elapsed: 2.018638625s
Sep  6 11:04:52.752: INFO: Pod "adopt-release-76bdv" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:52.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3486" for this suite. 09/06/23 11:04:52.767
------------------------------
• [SLOW TEST] [7.444 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:45.331
    Sep  6 11:04:45.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename job 09/06/23 11:04:45.332
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:45.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:45.592
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 09/06/23 11:04:45.594
    STEP: Ensuring active pods == parallelism 09/06/23 11:04:45.665
    STEP: Orphaning one of the Job's Pods 09/06/23 11:04:47.67
    Sep  6 11:04:48.195: INFO: Successfully updated pod "adopt-release-76bdv"
    STEP: Checking that the Job readopts the Pod 09/06/23 11:04:48.195
    Sep  6 11:04:48.196: INFO: Waiting up to 15m0s for pod "adopt-release-76bdv" in namespace "job-3486" to be "adopted"
    Sep  6 11:04:48.205: INFO: Pod "adopt-release-76bdv": Phase="Running", Reason="", readiness=true. Elapsed: 9.001519ms
    Sep  6 11:04:50.214: INFO: Pod "adopt-release-76bdv": Phase="Running", Reason="", readiness=true. Elapsed: 2.018421871s
    Sep  6 11:04:50.214: INFO: Pod "adopt-release-76bdv" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 09/06/23 11:04:50.214
    Sep  6 11:04:50.733: INFO: Successfully updated pod "adopt-release-76bdv"
    STEP: Checking that the Job releases the Pod 09/06/23 11:04:50.733
    Sep  6 11:04:50.733: INFO: Waiting up to 15m0s for pod "adopt-release-76bdv" in namespace "job-3486" to be "released"
    Sep  6 11:04:50.738: INFO: Pod "adopt-release-76bdv": Phase="Running", Reason="", readiness=true. Elapsed: 5.075136ms
    Sep  6 11:04:52.751: INFO: Pod "adopt-release-76bdv": Phase="Running", Reason="", readiness=true. Elapsed: 2.018638625s
    Sep  6 11:04:52.752: INFO: Pod "adopt-release-76bdv" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:52.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3486" for this suite. 09/06/23 11:04:52.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:52.776
Sep  6 11:04:52.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-runtime 09/06/23 11:04:52.777
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:52.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:52.798
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 09/06/23 11:04:52.8
STEP: wait for the container to reach Succeeded 09/06/23 11:04:52.807
STEP: get the container status 09/06/23 11:04:58.896
STEP: the container should be terminated 09/06/23 11:04:58.91
STEP: the termination message should be set 09/06/23 11:04:58.91
Sep  6 11:04:58.910: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/06/23 11:04:58.911
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:58.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1051" for this suite. 09/06/23 11:04:58.946
------------------------------
• [SLOW TEST] [6.175 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:52.776
    Sep  6 11:04:52.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-runtime 09/06/23 11:04:52.777
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:52.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:52.798
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 09/06/23 11:04:52.8
    STEP: wait for the container to reach Succeeded 09/06/23 11:04:52.807
    STEP: get the container status 09/06/23 11:04:58.896
    STEP: the container should be terminated 09/06/23 11:04:58.91
    STEP: the termination message should be set 09/06/23 11:04:58.91
    Sep  6 11:04:58.910: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/06/23 11:04:58.911
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:58.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1051" for this suite. 09/06/23 11:04:58.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:58.952
Sep  6 11:04:58.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename watch 09/06/23 11:04:58.954
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:58.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:58.978
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 09/06/23 11:04:58.98
STEP: creating a new configmap 09/06/23 11:04:58.981
STEP: modifying the configmap once 09/06/23 11:04:58.986
STEP: closing the watch once it receives two notifications 09/06/23 11:04:58.994
Sep  6 11:04:58.994: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5633  1b160e0e-095e-43ff-bdcd-3995d34289e6 26047 0 2023-09-06 11:04:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-06 11:04:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:04:58.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5633  1b160e0e-095e-43ff-bdcd-3995d34289e6 26048 0 2023-09-06 11:04:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-06 11:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 09/06/23 11:04:58.994
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/06/23 11:04:59.004
STEP: deleting the configmap 09/06/23 11:04:59.005
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/06/23 11:04:59.015
Sep  6 11:04:59.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5633  1b160e0e-095e-43ff-bdcd-3995d34289e6 26049 0 2023-09-06 11:04:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-06 11:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:04:59.016: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5633  1b160e0e-095e-43ff-bdcd-3995d34289e6 26050 0 2023-09-06 11:04:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-06 11:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  6 11:04:59.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5633" for this suite. 09/06/23 11:04:59.019
------------------------------
• [0.074 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:58.952
    Sep  6 11:04:58.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename watch 09/06/23 11:04:58.954
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:58.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:58.978
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 09/06/23 11:04:58.98
    STEP: creating a new configmap 09/06/23 11:04:58.981
    STEP: modifying the configmap once 09/06/23 11:04:58.986
    STEP: closing the watch once it receives two notifications 09/06/23 11:04:58.994
    Sep  6 11:04:58.994: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5633  1b160e0e-095e-43ff-bdcd-3995d34289e6 26047 0 2023-09-06 11:04:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-06 11:04:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 11:04:58.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5633  1b160e0e-095e-43ff-bdcd-3995d34289e6 26048 0 2023-09-06 11:04:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-06 11:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 09/06/23 11:04:58.994
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/06/23 11:04:59.004
    STEP: deleting the configmap 09/06/23 11:04:59.005
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/06/23 11:04:59.015
    Sep  6 11:04:59.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5633  1b160e0e-095e-43ff-bdcd-3995d34289e6 26049 0 2023-09-06 11:04:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-06 11:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 11:04:59.016: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5633  1b160e0e-095e-43ff-bdcd-3995d34289e6 26050 0 2023-09-06 11:04:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-06 11:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:04:59.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5633" for this suite. 09/06/23 11:04:59.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:04:59.03
Sep  6 11:04:59.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:04:59.031
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:59.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:59.053
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Sep  6 11:04:59.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/06/23 11:05:05.623
Sep  6 11:05:05.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 create -f -'
Sep  6 11:05:06.138: INFO: stderr: ""
Sep  6 11:05:06.138: INFO: stdout: "e2e-test-crd-publish-openapi-247-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  6 11:05:06.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 delete e2e-test-crd-publish-openapi-247-crds test-foo'
Sep  6 11:05:06.219: INFO: stderr: ""
Sep  6 11:05:06.219: INFO: stdout: "e2e-test-crd-publish-openapi-247-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  6 11:05:06.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 apply -f -'
Sep  6 11:05:06.384: INFO: stderr: ""
Sep  6 11:05:06.384: INFO: stdout: "e2e-test-crd-publish-openapi-247-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  6 11:05:06.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 delete e2e-test-crd-publish-openapi-247-crds test-foo'
Sep  6 11:05:06.443: INFO: stderr: ""
Sep  6 11:05:06.443: INFO: stdout: "e2e-test-crd-publish-openapi-247-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/06/23 11:05:06.443
Sep  6 11:05:06.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 create -f -'
Sep  6 11:05:06.598: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/06/23 11:05:06.598
Sep  6 11:05:06.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 create -f -'
Sep  6 11:05:07.103: INFO: rc: 1
Sep  6 11:05:07.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 apply -f -'
Sep  6 11:05:07.272: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/06/23 11:05:07.272
Sep  6 11:05:07.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 create -f -'
Sep  6 11:05:07.507: INFO: rc: 1
Sep  6 11:05:07.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 apply -f -'
Sep  6 11:05:07.664: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 09/06/23 11:05:07.664
Sep  6 11:05:07.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds'
Sep  6 11:05:07.815: INFO: stderr: ""
Sep  6 11:05:07.815: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-247-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 09/06/23 11:05:07.815
Sep  6 11:05:07.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds.metadata'
Sep  6 11:05:08.570: INFO: stderr: ""
Sep  6 11:05:08.570: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-247-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  6 11:05:08.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds.spec'
Sep  6 11:05:09.804: INFO: stderr: ""
Sep  6 11:05:09.804: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-247-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  6 11:05:09.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds.spec.bars'
Sep  6 11:05:10.002: INFO: stderr: ""
Sep  6 11:05:10.002: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-247-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/06/23 11:05:10.002
Sep  6 11:05:10.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds.spec.bars2'
Sep  6 11:05:10.298: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:05:12.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7186" for this suite. 09/06/23 11:05:12.283
------------------------------
• [SLOW TEST] [13.263 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:04:59.03
    Sep  6 11:04:59.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:04:59.031
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:04:59.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:04:59.053
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Sep  6 11:04:59.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/06/23 11:05:05.623
    Sep  6 11:05:05.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 create -f -'
    Sep  6 11:05:06.138: INFO: stderr: ""
    Sep  6 11:05:06.138: INFO: stdout: "e2e-test-crd-publish-openapi-247-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  6 11:05:06.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 delete e2e-test-crd-publish-openapi-247-crds test-foo'
    Sep  6 11:05:06.219: INFO: stderr: ""
    Sep  6 11:05:06.219: INFO: stdout: "e2e-test-crd-publish-openapi-247-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Sep  6 11:05:06.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 apply -f -'
    Sep  6 11:05:06.384: INFO: stderr: ""
    Sep  6 11:05:06.384: INFO: stdout: "e2e-test-crd-publish-openapi-247-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  6 11:05:06.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 delete e2e-test-crd-publish-openapi-247-crds test-foo'
    Sep  6 11:05:06.443: INFO: stderr: ""
    Sep  6 11:05:06.443: INFO: stdout: "e2e-test-crd-publish-openapi-247-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/06/23 11:05:06.443
    Sep  6 11:05:06.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 create -f -'
    Sep  6 11:05:06.598: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/06/23 11:05:06.598
    Sep  6 11:05:06.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 create -f -'
    Sep  6 11:05:07.103: INFO: rc: 1
    Sep  6 11:05:07.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 apply -f -'
    Sep  6 11:05:07.272: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/06/23 11:05:07.272
    Sep  6 11:05:07.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 create -f -'
    Sep  6 11:05:07.507: INFO: rc: 1
    Sep  6 11:05:07.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 --namespace=crd-publish-openapi-7186 apply -f -'
    Sep  6 11:05:07.664: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 09/06/23 11:05:07.664
    Sep  6 11:05:07.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds'
    Sep  6 11:05:07.815: INFO: stderr: ""
    Sep  6 11:05:07.815: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-247-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 09/06/23 11:05:07.815
    Sep  6 11:05:07.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds.metadata'
    Sep  6 11:05:08.570: INFO: stderr: ""
    Sep  6 11:05:08.570: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-247-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Sep  6 11:05:08.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds.spec'
    Sep  6 11:05:09.804: INFO: stderr: ""
    Sep  6 11:05:09.804: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-247-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Sep  6 11:05:09.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds.spec.bars'
    Sep  6 11:05:10.002: INFO: stderr: ""
    Sep  6 11:05:10.002: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-247-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/06/23 11:05:10.002
    Sep  6 11:05:10.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-7186 explain e2e-test-crd-publish-openapi-247-crds.spec.bars2'
    Sep  6 11:05:10.298: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:05:12.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7186" for this suite. 09/06/23 11:05:12.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:05:12.293
Sep  6 11:05:12.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 11:05:12.295
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:05:12.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:05:12.325
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 09/06/23 11:05:12.329
Sep  6 11:05:12.416: INFO: Waiting up to 5m0s for pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7" in namespace "emptydir-4985" to be "Succeeded or Failed"
Sep  6 11:05:14.089: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.673293871s
Sep  6 11:05:16.093: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.677215497s
Sep  6 11:05:18.093: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.677006219s
Sep  6 11:05:20.094: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.67843357s
STEP: Saw pod success 09/06/23 11:05:20.094
Sep  6 11:05:20.094: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7" satisfied condition "Succeeded or Failed"
Sep  6 11:05:20.100: INFO: Trying to get logs from node kube-2 pod pod-e0cac6ff-da03-4640-8446-3f99498231a7 container test-container: <nil>
STEP: delete the pod 09/06/23 11:05:20.126
Sep  6 11:05:20.151: INFO: Waiting for pod pod-e0cac6ff-da03-4640-8446-3f99498231a7 to disappear
Sep  6 11:05:20.155: INFO: Pod pod-e0cac6ff-da03-4640-8446-3f99498231a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 11:05:20.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4985" for this suite. 09/06/23 11:05:20.162
------------------------------
• [SLOW TEST] [7.878 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:05:12.293
    Sep  6 11:05:12.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 11:05:12.295
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:05:12.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:05:12.325
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/06/23 11:05:12.329
    Sep  6 11:05:12.416: INFO: Waiting up to 5m0s for pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7" in namespace "emptydir-4985" to be "Succeeded or Failed"
    Sep  6 11:05:14.089: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.673293871s
    Sep  6 11:05:16.093: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.677215497s
    Sep  6 11:05:18.093: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.677006219s
    Sep  6 11:05:20.094: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.67843357s
    STEP: Saw pod success 09/06/23 11:05:20.094
    Sep  6 11:05:20.094: INFO: Pod "pod-e0cac6ff-da03-4640-8446-3f99498231a7" satisfied condition "Succeeded or Failed"
    Sep  6 11:05:20.100: INFO: Trying to get logs from node kube-2 pod pod-e0cac6ff-da03-4640-8446-3f99498231a7 container test-container: <nil>
    STEP: delete the pod 09/06/23 11:05:20.126
    Sep  6 11:05:20.151: INFO: Waiting for pod pod-e0cac6ff-da03-4640-8446-3f99498231a7 to disappear
    Sep  6 11:05:20.155: INFO: Pod pod-e0cac6ff-da03-4640-8446-3f99498231a7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:05:20.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4985" for this suite. 09/06/23 11:05:20.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:05:20.173
Sep  6 11:05:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename namespaces 09/06/23 11:05:20.174
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:05:20.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:05:20.198
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 09/06/23 11:05:20.205
STEP: patching the Namespace 09/06/23 11:05:20.222
STEP: get the Namespace and ensuring it has the label 09/06/23 11:05:20.229
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:05:20.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6500" for this suite. 09/06/23 11:05:20.25
STEP: Destroying namespace "nspatchtest-2c615986-fabc-4c51-b60b-88c0a8e53aa8-2209" for this suite. 09/06/23 11:05:20.257
------------------------------
• [0.093 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:05:20.173
    Sep  6 11:05:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename namespaces 09/06/23 11:05:20.174
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:05:20.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:05:20.198
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 09/06/23 11:05:20.205
    STEP: patching the Namespace 09/06/23 11:05:20.222
    STEP: get the Namespace and ensuring it has the label 09/06/23 11:05:20.229
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:05:20.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6500" for this suite. 09/06/23 11:05:20.25
    STEP: Destroying namespace "nspatchtest-2c615986-fabc-4c51-b60b-88c0a8e53aa8-2209" for this suite. 09/06/23 11:05:20.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:05:20.269
Sep  6 11:05:20.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 11:05:20.27
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:05:20.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:05:20.29
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Sep  6 11:05:20.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:05:26.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4529" for this suite. 09/06/23 11:05:26.325
------------------------------
• [SLOW TEST] [6.063 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:05:20.269
    Sep  6 11:05:20.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 11:05:20.27
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:05:20.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:05:20.29
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Sep  6 11:05:20.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:05:26.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4529" for this suite. 09/06/23 11:05:26.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:05:26.334
Sep  6 11:05:26.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename init-container 09/06/23 11:05:26.338
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:05:26.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:05:26.37
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 09/06/23 11:05:26.371
Sep  6 11:05:26.372: INFO: PodSpec: initContainers in spec.initContainers
Sep  6 11:06:11.013: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-817e4178-af99-4d8d-80d6-802bbf283996", GenerateName:"", Namespace:"init-container-8941", SelfLink:"", UID:"c5170912-98f9-4d11-8b25-d3bd8e54e453", ResourceVersion:"26367", Generation:0, CreationTimestamp:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"372065693"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"052c86d7100bbfde6209864de7c038be87ad66aa3a3b173865674ffc3daef145", "cni.projectcalico.org/podIP":"10.233.99.83/32", "cni.projectcalico.org/podIPs":"10.233.99.83/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a5c750), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a5c780), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 6, 11, 6, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a5c7b0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-6lzfg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001427d60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6lzfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6lzfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6lzfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0013106b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00035e690), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0013107c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0013107e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0013107e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0013107ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012d6af0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.2.20.103", PodIP:"10.233.99.83", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.99.83"}}, StartTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00035e850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00035e930)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://547d5e2e4a10f181c4ebf04d431f65fa259ca459f0353e32d02c612ad5466922", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001427de0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001427dc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00131093f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:06:11.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8941" for this suite. 09/06/23 11:06:11.021
------------------------------
• [SLOW TEST] [44.694 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:05:26.334
    Sep  6 11:05:26.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename init-container 09/06/23 11:05:26.338
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:05:26.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:05:26.37
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 09/06/23 11:05:26.371
    Sep  6 11:05:26.372: INFO: PodSpec: initContainers in spec.initContainers
    Sep  6 11:06:11.013: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-817e4178-af99-4d8d-80d6-802bbf283996", GenerateName:"", Namespace:"init-container-8941", SelfLink:"", UID:"c5170912-98f9-4d11-8b25-d3bd8e54e453", ResourceVersion:"26367", Generation:0, CreationTimestamp:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"372065693"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"052c86d7100bbfde6209864de7c038be87ad66aa3a3b173865674ffc3daef145", "cni.projectcalico.org/podIP":"10.233.99.83/32", "cni.projectcalico.org/podIPs":"10.233.99.83/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a5c750), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a5c780), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 6, 11, 6, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a5c7b0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-6lzfg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001427d60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6lzfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6lzfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6lzfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0013106b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00035e690), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0013107c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0013107e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0013107e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0013107ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012d6af0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.2.20.103", PodIP:"10.233.99.83", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.99.83"}}, StartTime:time.Date(2023, time.September, 6, 11, 5, 26, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00035e850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00035e930)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://547d5e2e4a10f181c4ebf04d431f65fa259ca459f0353e32d02c612ad5466922", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001427de0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001427dc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00131093f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:06:11.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8941" for this suite. 09/06/23 11:06:11.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:06:11.029
Sep  6 11:06:11.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-runtime 09/06/23 11:06:11.029
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:11.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:06:11.05
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 09/06/23 11:06:11.051
STEP: wait for the container to reach Succeeded 09/06/23 11:06:11.058
STEP: get the container status 09/06/23 11:06:14.117
STEP: the container should be terminated 09/06/23 11:06:14.121
STEP: the termination message should be set 09/06/23 11:06:14.122
Sep  6 11:06:14.123: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 09/06/23 11:06:14.123
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  6 11:06:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7215" for this suite. 09/06/23 11:06:14.249
------------------------------
• [3.289 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:06:11.029
    Sep  6 11:06:11.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-runtime 09/06/23 11:06:11.029
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:11.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:06:11.05
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 09/06/23 11:06:11.051
    STEP: wait for the container to reach Succeeded 09/06/23 11:06:11.058
    STEP: get the container status 09/06/23 11:06:14.117
    STEP: the container should be terminated 09/06/23 11:06:14.121
    STEP: the termination message should be set 09/06/23 11:06:14.122
    Sep  6 11:06:14.123: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 09/06/23 11:06:14.123
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:06:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7215" for this suite. 09/06/23 11:06:14.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:06:14.319
Sep  6 11:06:14.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pod-network-test 09/06/23 11:06:14.32
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:14.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:06:14.442
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5725 09/06/23 11:06:14.517
STEP: creating a selector 09/06/23 11:06:14.517
STEP: Creating the service pods in kubernetes 09/06/23 11:06:14.517
Sep  6 11:06:14.518: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  6 11:06:14.809: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5725" to be "running and ready"
Sep  6 11:06:14.907: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 98.121492ms
Sep  6 11:06:14.908: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:06:16.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.103730168s
Sep  6 11:06:16.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:18.923: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.11373152s
Sep  6 11:06:18.923: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:20.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.11107031s
Sep  6 11:06:20.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:22.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.110910336s
Sep  6 11:06:22.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:24.916: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.106781484s
Sep  6 11:06:24.916: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:26.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.111600213s
Sep  6 11:06:26.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:28.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.112804486s
Sep  6 11:06:28.922: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:30.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.110608945s
Sep  6 11:06:30.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:32.923: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.114104854s
Sep  6 11:06:32.924: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:34.919: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.109635389s
Sep  6 11:06:34.919: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:06:36.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.102882843s
Sep  6 11:06:36.912: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  6 11:06:36.912: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  6 11:06:36.917: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5725" to be "running and ready"
Sep  6 11:06:36.921: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.642388ms
Sep  6 11:06:36.921: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  6 11:06:36.921: INFO: Pod "netserver-1" satisfied condition "running and ready"
Sep  6 11:06:36.925: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5725" to be "running and ready"
Sep  6 11:06:36.929: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.811448ms
Sep  6 11:06:36.929: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Sep  6 11:06:36.929: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 09/06/23 11:06:36.932
Sep  6 11:06:36.948: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5725" to be "running"
Sep  6 11:06:36.962: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119662ms
Sep  6 11:06:38.977: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029136898s
Sep  6 11:06:38.977: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  6 11:06:38.989: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5725" to be "running"
Sep  6 11:06:39.000: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.336204ms
Sep  6 11:06:39.001: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  6 11:06:39.012: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep  6 11:06:39.012: INFO: Going to poll 10.233.120.84 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep  6 11:06:39.020: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.120.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5725 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:06:39.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:06:39.023: INFO: ExecWithOptions: Clientset creation
Sep  6 11:06:39.023: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5725/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.120.84+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  6 11:06:40.162: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  6 11:06:40.162: INFO: Going to poll 10.233.120.228 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep  6 11:06:40.175: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.120.228 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5725 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:06:40.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:06:40.176: INFO: ExecWithOptions: Clientset creation
Sep  6 11:06:40.176: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5725/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.120.228+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  6 11:06:41.253: INFO: Found all 1 expected endpoints: [netserver-1]
Sep  6 11:06:41.253: INFO: Going to poll 10.233.99.81 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep  6 11:06:41.264: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.99.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5725 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:06:41.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:06:41.266: INFO: ExecWithOptions: Clientset creation
Sep  6 11:06:41.266: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5725/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.99.81+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  6 11:06:42.407: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  6 11:06:42.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5725" for this suite. 09/06/23 11:06:42.422
------------------------------
• [SLOW TEST] [28.112 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:06:14.319
    Sep  6 11:06:14.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pod-network-test 09/06/23 11:06:14.32
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:14.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:06:14.442
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5725 09/06/23 11:06:14.517
    STEP: creating a selector 09/06/23 11:06:14.517
    STEP: Creating the service pods in kubernetes 09/06/23 11:06:14.517
    Sep  6 11:06:14.518: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  6 11:06:14.809: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5725" to be "running and ready"
    Sep  6 11:06:14.907: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 98.121492ms
    Sep  6 11:06:14.908: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:06:16.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.103730168s
    Sep  6 11:06:16.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:18.923: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.11373152s
    Sep  6 11:06:18.923: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:20.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.11107031s
    Sep  6 11:06:20.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:22.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.110910336s
    Sep  6 11:06:22.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:24.916: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.106781484s
    Sep  6 11:06:24.916: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:26.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.111600213s
    Sep  6 11:06:26.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:28.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.112804486s
    Sep  6 11:06:28.922: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:30.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.110608945s
    Sep  6 11:06:30.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:32.923: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.114104854s
    Sep  6 11:06:32.924: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:34.919: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.109635389s
    Sep  6 11:06:34.919: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:06:36.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.102882843s
    Sep  6 11:06:36.912: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  6 11:06:36.912: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  6 11:06:36.917: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5725" to be "running and ready"
    Sep  6 11:06:36.921: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.642388ms
    Sep  6 11:06:36.921: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  6 11:06:36.921: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Sep  6 11:06:36.925: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5725" to be "running and ready"
    Sep  6 11:06:36.929: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.811448ms
    Sep  6 11:06:36.929: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Sep  6 11:06:36.929: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 09/06/23 11:06:36.932
    Sep  6 11:06:36.948: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5725" to be "running"
    Sep  6 11:06:36.962: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119662ms
    Sep  6 11:06:38.977: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029136898s
    Sep  6 11:06:38.977: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  6 11:06:38.989: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5725" to be "running"
    Sep  6 11:06:39.000: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.336204ms
    Sep  6 11:06:39.001: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  6 11:06:39.012: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Sep  6 11:06:39.012: INFO: Going to poll 10.233.120.84 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Sep  6 11:06:39.020: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.120.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5725 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:06:39.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:06:39.023: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:06:39.023: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5725/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.120.84+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  6 11:06:40.162: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  6 11:06:40.162: INFO: Going to poll 10.233.120.228 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Sep  6 11:06:40.175: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.120.228 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5725 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:06:40.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:06:40.176: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:06:40.176: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5725/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.120.228+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  6 11:06:41.253: INFO: Found all 1 expected endpoints: [netserver-1]
    Sep  6 11:06:41.253: INFO: Going to poll 10.233.99.81 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Sep  6 11:06:41.264: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.99.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5725 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:06:41.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:06:41.266: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:06:41.266: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5725/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.99.81+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  6 11:06:42.407: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:06:42.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5725" for this suite. 09/06/23 11:06:42.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:06:42.432
Sep  6 11:06:42.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 11:06:42.433
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:42.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:06:42.461
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 09/06/23 11:06:42.496
STEP: Ensuring ResourceQuota status is calculated 09/06/23 11:06:42.505
STEP: Creating a ResourceQuota with not terminating scope 09/06/23 11:06:44.52
STEP: Ensuring ResourceQuota status is calculated 09/06/23 11:06:44.533
STEP: Creating a long running pod 09/06/23 11:06:46.551
STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/06/23 11:06:46.588
STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/06/23 11:06:48.605
STEP: Deleting the pod 09/06/23 11:06:50.62
STEP: Ensuring resource quota status released the pod usage 09/06/23 11:06:50.65
STEP: Creating a terminating pod 09/06/23 11:06:52.662
STEP: Ensuring resource quota with terminating scope captures the pod usage 09/06/23 11:06:52.692
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/06/23 11:06:54.697
STEP: Deleting the pod 09/06/23 11:06:56.709
STEP: Ensuring resource quota status released the pod usage 09/06/23 11:06:56.743
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 11:06:58.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7533" for this suite. 09/06/23 11:06:58.761
------------------------------
• [SLOW TEST] [16.346 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:06:42.432
    Sep  6 11:06:42.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 11:06:42.433
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:42.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:06:42.461
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 09/06/23 11:06:42.496
    STEP: Ensuring ResourceQuota status is calculated 09/06/23 11:06:42.505
    STEP: Creating a ResourceQuota with not terminating scope 09/06/23 11:06:44.52
    STEP: Ensuring ResourceQuota status is calculated 09/06/23 11:06:44.533
    STEP: Creating a long running pod 09/06/23 11:06:46.551
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/06/23 11:06:46.588
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/06/23 11:06:48.605
    STEP: Deleting the pod 09/06/23 11:06:50.62
    STEP: Ensuring resource quota status released the pod usage 09/06/23 11:06:50.65
    STEP: Creating a terminating pod 09/06/23 11:06:52.662
    STEP: Ensuring resource quota with terminating scope captures the pod usage 09/06/23 11:06:52.692
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/06/23 11:06:54.697
    STEP: Deleting the pod 09/06/23 11:06:56.709
    STEP: Ensuring resource quota status released the pod usage 09/06/23 11:06:56.743
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:06:58.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7533" for this suite. 09/06/23 11:06:58.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:06:58.783
Sep  6 11:06:58.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename namespaces 09/06/23 11:06:58.784
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:58.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:06:58.814
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 09/06/23 11:06:58.816
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:58.833
STEP: Creating a pod in the namespace 09/06/23 11:06:58.838
STEP: Waiting for the pod to have running status 09/06/23 11:06:58.845
Sep  6 11:06:58.845: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-411" to be "running"
Sep  6 11:06:58.848: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015762ms
Sep  6 11:07:00.856: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011080879s
Sep  6 11:07:00.856: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 09/06/23 11:07:00.856
STEP: Waiting for the namespace to be removed. 09/06/23 11:07:00.867
STEP: Recreating the namespace 09/06/23 11:07:11.88
STEP: Verifying there are no pods in the namespace 09/06/23 11:07:11.914
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:07:11.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2690" for this suite. 09/06/23 11:07:11.921
STEP: Destroying namespace "nsdeletetest-411" for this suite. 09/06/23 11:07:11.926
Sep  6 11:07:11.931: INFO: Namespace nsdeletetest-411 was already deleted
STEP: Destroying namespace "nsdeletetest-9308" for this suite. 09/06/23 11:07:11.931
------------------------------
• [SLOW TEST] [13.156 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:06:58.783
    Sep  6 11:06:58.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename namespaces 09/06/23 11:06:58.784
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:58.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:06:58.814
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 09/06/23 11:06:58.816
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:06:58.833
    STEP: Creating a pod in the namespace 09/06/23 11:06:58.838
    STEP: Waiting for the pod to have running status 09/06/23 11:06:58.845
    Sep  6 11:06:58.845: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-411" to be "running"
    Sep  6 11:06:58.848: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015762ms
    Sep  6 11:07:00.856: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011080879s
    Sep  6 11:07:00.856: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 09/06/23 11:07:00.856
    STEP: Waiting for the namespace to be removed. 09/06/23 11:07:00.867
    STEP: Recreating the namespace 09/06/23 11:07:11.88
    STEP: Verifying there are no pods in the namespace 09/06/23 11:07:11.914
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:07:11.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2690" for this suite. 09/06/23 11:07:11.921
    STEP: Destroying namespace "nsdeletetest-411" for this suite. 09/06/23 11:07:11.926
    Sep  6 11:07:11.931: INFO: Namespace nsdeletetest-411 was already deleted
    STEP: Destroying namespace "nsdeletetest-9308" for this suite. 09/06/23 11:07:11.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:07:11.94
Sep  6 11:07:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-runtime 09/06/23 11:07:11.941
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:07:11.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:07:11.961
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/06/23 11:07:11.972
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/06/23 11:07:30.417
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/06/23 11:07:30.42
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/06/23 11:07:30.428
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/06/23 11:07:30.428
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/06/23 11:07:30.462
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/06/23 11:07:32.49
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/06/23 11:07:34.505
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/06/23 11:07:34.515
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/06/23 11:07:34.515
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/06/23 11:07:34.542
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/06/23 11:07:35.554
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/06/23 11:07:55.441
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/06/23 11:07:55.473
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/06/23 11:07:55.473
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  6 11:07:55.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8439" for this suite. 09/06/23 11:07:55.527
------------------------------
• [SLOW TEST] [43.594 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:07:11.94
    Sep  6 11:07:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-runtime 09/06/23 11:07:11.941
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:07:11.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:07:11.961
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/06/23 11:07:11.972
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/06/23 11:07:30.417
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/06/23 11:07:30.42
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/06/23 11:07:30.428
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/06/23 11:07:30.428
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/06/23 11:07:30.462
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/06/23 11:07:32.49
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/06/23 11:07:34.505
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/06/23 11:07:34.515
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/06/23 11:07:34.515
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/06/23 11:07:34.542
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/06/23 11:07:35.554
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/06/23 11:07:55.441
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/06/23 11:07:55.473
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/06/23 11:07:55.473
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:07:55.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8439" for this suite. 09/06/23 11:07:55.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:07:55.535
Sep  6 11:07:55.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename proxy 09/06/23 11:07:55.536
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:21.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:21.027
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 09/06/23 11:08:21.093
STEP: creating replication controller proxy-service-7kskg in namespace proxy-2373 09/06/23 11:08:21.093
I0906 11:08:21.102772      22 runners.go:193] Created replication controller with name: proxy-service-7kskg, namespace: proxy-2373, replica count: 1
I0906 11:08:22.154150      22 runners.go:193] proxy-service-7kskg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 11:08:23.154412      22 runners.go:193] proxy-service-7kskg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 11:08:24.155020      22 runners.go:193] proxy-service-7kskg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:08:24.173: INFO: setup took 3.098154167s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/06/23 11:08:24.173
Sep  6 11:08:24.201: INFO: (0) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 27.661659ms)
Sep  6 11:08:24.201: INFO: (0) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 27.477519ms)
Sep  6 11:08:24.201: INFO: (0) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 27.808413ms)
Sep  6 11:08:24.201: INFO: (0) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 27.587901ms)
Sep  6 11:08:24.202: INFO: (0) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 27.963279ms)
Sep  6 11:08:24.202: INFO: (0) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 27.648366ms)
Sep  6 11:08:24.202: INFO: (0) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 27.80479ms)
Sep  6 11:08:24.205: INFO: (0) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 30.902561ms)
Sep  6 11:08:24.207: INFO: (0) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 33.669039ms)
Sep  6 11:08:24.208: INFO: (0) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 34.839482ms)
Sep  6 11:08:24.209: INFO: (0) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 35.144381ms)
Sep  6 11:08:24.210: INFO: (0) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 35.931383ms)
Sep  6 11:08:24.210: INFO: (0) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 37.237757ms)
Sep  6 11:08:24.211: INFO: (0) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 37.34915ms)
Sep  6 11:08:24.211: INFO: (0) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 37.415342ms)
Sep  6 11:08:24.211: INFO: (0) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 37.516081ms)
Sep  6 11:08:24.217: INFO: (1) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 5.083056ms)
Sep  6 11:08:24.222: INFO: (1) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 9.465791ms)
Sep  6 11:08:24.222: INFO: (1) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 9.436944ms)
Sep  6 11:08:24.222: INFO: (1) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.464743ms)
Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 10.862753ms)
Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 10.723353ms)
Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 10.528347ms)
Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 10.841021ms)
Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 11.610771ms)
Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 10.751959ms)
Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 11.808687ms)
Sep  6 11:08:24.224: INFO: (1) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.339634ms)
Sep  6 11:08:24.228: INFO: (1) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 15.513023ms)
Sep  6 11:08:24.229: INFO: (1) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 17.01759ms)
Sep  6 11:08:24.229: INFO: (1) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 16.558307ms)
Sep  6 11:08:24.229: INFO: (1) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 17.087576ms)
Sep  6 11:08:24.240: INFO: (2) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 9.753427ms)
Sep  6 11:08:24.241: INFO: (2) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 9.901998ms)
Sep  6 11:08:24.242: INFO: (2) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 11.010823ms)
Sep  6 11:08:24.242: INFO: (2) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 11.075168ms)
Sep  6 11:08:24.242: INFO: (2) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 11.164946ms)
Sep  6 11:08:24.242: INFO: (2) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 11.468213ms)
Sep  6 11:08:24.243: INFO: (2) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.667038ms)
Sep  6 11:08:24.244: INFO: (2) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 13.99119ms)
Sep  6 11:08:24.246: INFO: (2) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 15.218475ms)
Sep  6 11:08:24.247: INFO: (2) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 16.360884ms)
Sep  6 11:08:24.247: INFO: (2) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 16.808213ms)
Sep  6 11:08:24.247: INFO: (2) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 16.861911ms)
Sep  6 11:08:24.248: INFO: (2) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 17.098528ms)
Sep  6 11:08:24.248: INFO: (2) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 17.017417ms)
Sep  6 11:08:24.249: INFO: (2) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 18.435816ms)
Sep  6 11:08:24.250: INFO: (2) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 19.54025ms)
Sep  6 11:08:24.258: INFO: (3) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.872133ms)
Sep  6 11:08:24.258: INFO: (3) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.083945ms)
Sep  6 11:08:24.259: INFO: (3) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.278961ms)
Sep  6 11:08:24.259: INFO: (3) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 8.251848ms)
Sep  6 11:08:24.261: INFO: (3) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 10.706906ms)
Sep  6 11:08:24.261: INFO: (3) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 10.636519ms)
Sep  6 11:08:24.263: INFO: (3) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 12.480287ms)
Sep  6 11:08:24.263: INFO: (3) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 12.355385ms)
Sep  6 11:08:24.263: INFO: (3) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 12.450089ms)
Sep  6 11:08:24.265: INFO: (3) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 14.630335ms)
Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 15.084305ms)
Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 14.929286ms)
Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 15.1885ms)
Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 15.503802ms)
Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 15.64739ms)
Sep  6 11:08:24.267: INFO: (3) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 16.155144ms)
Sep  6 11:08:24.276: INFO: (4) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.129503ms)
Sep  6 11:08:24.277: INFO: (4) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.500679ms)
Sep  6 11:08:24.279: INFO: (4) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 12.112058ms)
Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 12.630124ms)
Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 12.227044ms)
Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 12.24763ms)
Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 12.544545ms)
Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.416559ms)
Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 12.897787ms)
Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 12.863775ms)
Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 13.44319ms)
Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 13.353892ms)
Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.599498ms)
Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 13.7163ms)
Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 13.653435ms)
Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 13.280668ms)
Sep  6 11:08:24.285: INFO: (5) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 4.46398ms)
Sep  6 11:08:24.288: INFO: (5) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.700541ms)
Sep  6 11:08:24.289: INFO: (5) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.755966ms)
Sep  6 11:08:24.289: INFO: (5) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 7.865772ms)
Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 9.699696ms)
Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 9.757203ms)
Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 9.818896ms)
Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.919423ms)
Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 9.915249ms)
Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 10.426056ms)
Sep  6 11:08:24.292: INFO: (5) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 10.715999ms)
Sep  6 11:08:24.292: INFO: (5) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 11.387765ms)
Sep  6 11:08:24.293: INFO: (5) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.200016ms)
Sep  6 11:08:24.293: INFO: (5) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 12.148057ms)
Sep  6 11:08:24.293: INFO: (5) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 12.601451ms)
Sep  6 11:08:24.294: INFO: (5) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.305333ms)
Sep  6 11:08:24.300: INFO: (6) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 5.203412ms)
Sep  6 11:08:24.300: INFO: (6) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 5.873975ms)
Sep  6 11:08:24.302: INFO: (6) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 7.411886ms)
Sep  6 11:08:24.302: INFO: (6) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 7.37774ms)
Sep  6 11:08:24.302: INFO: (6) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 7.409482ms)
Sep  6 11:08:24.303: INFO: (6) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.48768ms)
Sep  6 11:08:24.303: INFO: (6) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 8.970436ms)
Sep  6 11:08:24.303: INFO: (6) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 8.991055ms)
Sep  6 11:08:24.303: INFO: (6) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 8.985438ms)
Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 9.597538ms)
Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.589344ms)
Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 9.811673ms)
Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.791461ms)
Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.069497ms)
Sep  6 11:08:24.306: INFO: (6) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 12.028961ms)
Sep  6 11:08:24.307: INFO: (6) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 12.171818ms)
Sep  6 11:08:24.313: INFO: (7) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.616285ms)
Sep  6 11:08:24.313: INFO: (7) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.839262ms)
Sep  6 11:08:24.316: INFO: (7) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 8.773447ms)
Sep  6 11:08:24.316: INFO: (7) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 9.084807ms)
Sep  6 11:08:24.317: INFO: (7) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.51504ms)
Sep  6 11:08:24.317: INFO: (7) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 10.569172ms)
Sep  6 11:08:24.319: INFO: (7) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 11.858146ms)
Sep  6 11:08:24.319: INFO: (7) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 12.501236ms)
Sep  6 11:08:24.319: INFO: (7) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 12.553544ms)
Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 12.941286ms)
Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 13.055726ms)
Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 13.103984ms)
Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 13.16752ms)
Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.443413ms)
Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 13.565328ms)
Sep  6 11:08:24.321: INFO: (7) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 13.948975ms)
Sep  6 11:08:24.325: INFO: (8) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 4.67447ms)
Sep  6 11:08:24.327: INFO: (8) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.207571ms)
Sep  6 11:08:24.328: INFO: (8) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 6.335889ms)
Sep  6 11:08:24.328: INFO: (8) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.475697ms)
Sep  6 11:08:24.328: INFO: (8) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.483091ms)
Sep  6 11:08:24.329: INFO: (8) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.560767ms)
Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 9.801782ms)
Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 9.701575ms)
Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 9.870745ms)
Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.83625ms)
Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 9.626726ms)
Sep  6 11:08:24.332: INFO: (8) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 10.798414ms)
Sep  6 11:08:24.332: INFO: (8) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.040125ms)
Sep  6 11:08:24.333: INFO: (8) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 11.825852ms)
Sep  6 11:08:24.334: INFO: (8) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.64658ms)
Sep  6 11:08:24.334: INFO: (8) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 12.998629ms)
Sep  6 11:08:24.341: INFO: (9) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 6.740869ms)
Sep  6 11:08:24.341: INFO: (9) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.85742ms)
Sep  6 11:08:24.341: INFO: (9) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 6.94969ms)
Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 9.847682ms)
Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 9.843675ms)
Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.912875ms)
Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.98695ms)
Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 10.062203ms)
Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 9.959512ms)
Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 9.965241ms)
Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.970896ms)
Sep  6 11:08:24.345: INFO: (9) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 10.310577ms)
Sep  6 11:08:24.346: INFO: (9) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.709814ms)
Sep  6 11:08:24.347: INFO: (9) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.350133ms)
Sep  6 11:08:24.347: INFO: (9) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 13.183069ms)
Sep  6 11:08:24.348: INFO: (9) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.845793ms)
Sep  6 11:08:24.354: INFO: (10) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 6.04612ms)
Sep  6 11:08:24.357: INFO: (10) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.27668ms)
Sep  6 11:08:24.358: INFO: (10) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 8.942147ms)
Sep  6 11:08:24.358: INFO: (10) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.616072ms)
Sep  6 11:08:24.358: INFO: (10) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 8.233604ms)
Sep  6 11:08:24.358: INFO: (10) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 8.716129ms)
Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 9.809232ms)
Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 10.086803ms)
Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.506973ms)
Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 10.692197ms)
Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.806736ms)
Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 9.952093ms)
Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 10.61605ms)
Sep  6 11:08:24.360: INFO: (10) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 11.019624ms)
Sep  6 11:08:24.360: INFO: (10) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.250578ms)
Sep  6 11:08:24.360: INFO: (10) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.195821ms)
Sep  6 11:08:24.366: INFO: (11) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 5.730931ms)
Sep  6 11:08:24.367: INFO: (11) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.000464ms)
Sep  6 11:08:24.367: INFO: (11) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 6.191795ms)
Sep  6 11:08:24.367: INFO: (11) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 5.923954ms)
Sep  6 11:08:24.368: INFO: (11) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.521166ms)
Sep  6 11:08:24.369: INFO: (11) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.104411ms)
Sep  6 11:08:24.369: INFO: (11) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 8.442174ms)
Sep  6 11:08:24.369: INFO: (11) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 8.769385ms)
Sep  6 11:08:24.369: INFO: (11) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.524908ms)
Sep  6 11:08:24.372: INFO: (11) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 10.516729ms)
Sep  6 11:08:24.372: INFO: (11) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.361438ms)
Sep  6 11:08:24.372: INFO: (11) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 11.112576ms)
Sep  6 11:08:24.372: INFO: (11) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 11.26692ms)
Sep  6 11:08:24.373: INFO: (11) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 11.786901ms)
Sep  6 11:08:24.374: INFO: (11) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.993782ms)
Sep  6 11:08:24.376: INFO: (11) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 14.623937ms)
Sep  6 11:08:24.382: INFO: (12) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 6.101804ms)
Sep  6 11:08:24.382: INFO: (12) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 5.774113ms)
Sep  6 11:08:24.383: INFO: (12) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 5.699359ms)
Sep  6 11:08:24.383: INFO: (12) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 7.727183ms)
Sep  6 11:08:24.383: INFO: (12) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.798953ms)
Sep  6 11:08:24.384: INFO: (12) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 7.381111ms)
Sep  6 11:08:24.384: INFO: (12) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.04894ms)
Sep  6 11:08:24.386: INFO: (12) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 10.17539ms)
Sep  6 11:08:24.389: INFO: (12) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 12.516616ms)
Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 14.134929ms)
Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 14.406973ms)
Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 12.857973ms)
Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 13.103215ms)
Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 13.415794ms)
Sep  6 11:08:24.391: INFO: (12) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 14.056548ms)
Sep  6 11:08:24.391: INFO: (12) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 15.745247ms)
Sep  6 11:08:24.398: INFO: (13) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 6.962987ms)
Sep  6 11:08:24.398: INFO: (13) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.022351ms)
Sep  6 11:08:24.400: INFO: (13) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.258717ms)
Sep  6 11:08:24.400: INFO: (13) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.138493ms)
Sep  6 11:08:24.400: INFO: (13) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 7.97518ms)
Sep  6 11:08:24.400: INFO: (13) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 8.377223ms)
Sep  6 11:08:24.402: INFO: (13) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 10.535091ms)
Sep  6 11:08:24.402: INFO: (13) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.501571ms)
Sep  6 11:08:24.402: INFO: (13) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 10.701869ms)
Sep  6 11:08:24.403: INFO: (13) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.409494ms)
Sep  6 11:08:24.404: INFO: (13) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 12.886394ms)
Sep  6 11:08:24.404: INFO: (13) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 12.964558ms)
Sep  6 11:08:24.405: INFO: (13) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 12.789109ms)
Sep  6 11:08:24.405: INFO: (13) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 13.326462ms)
Sep  6 11:08:24.406: INFO: (13) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 14.605193ms)
Sep  6 11:08:24.407: INFO: (13) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 14.954234ms)
Sep  6 11:08:24.413: INFO: (14) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 6.021757ms)
Sep  6 11:08:24.413: INFO: (14) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.041672ms)
Sep  6 11:08:24.413: INFO: (14) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 6.64447ms)
Sep  6 11:08:24.415: INFO: (14) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.184866ms)
Sep  6 11:08:24.415: INFO: (14) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.215791ms)
Sep  6 11:08:24.415: INFO: (14) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.274483ms)
Sep  6 11:08:24.416: INFO: (14) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 9.377255ms)
Sep  6 11:08:24.417: INFO: (14) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 10.524674ms)
Sep  6 11:08:24.417: INFO: (14) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.66461ms)
Sep  6 11:08:24.418: INFO: (14) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 11.412209ms)
Sep  6 11:08:24.418: INFO: (14) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.630284ms)
Sep  6 11:08:24.419: INFO: (14) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.941613ms)
Sep  6 11:08:24.419: INFO: (14) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 11.913104ms)
Sep  6 11:08:24.420: INFO: (14) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.852243ms)
Sep  6 11:08:24.420: INFO: (14) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.093174ms)
Sep  6 11:08:24.420: INFO: (14) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 13.74385ms)
Sep  6 11:08:24.426: INFO: (15) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 4.75544ms)
Sep  6 11:08:24.426: INFO: (15) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 5.407921ms)
Sep  6 11:08:24.428: INFO: (15) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.916666ms)
Sep  6 11:08:24.428: INFO: (15) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 7.128205ms)
Sep  6 11:08:24.428: INFO: (15) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 7.063857ms)
Sep  6 11:08:24.428: INFO: (15) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 7.034224ms)
Sep  6 11:08:24.429: INFO: (15) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 8.128984ms)
Sep  6 11:08:24.429: INFO: (15) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 8.202525ms)
Sep  6 11:08:24.429: INFO: (15) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 8.451978ms)
Sep  6 11:08:24.430: INFO: (15) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 9.092037ms)
Sep  6 11:08:24.431: INFO: (15) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 10.328885ms)
Sep  6 11:08:24.431: INFO: (15) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.655166ms)
Sep  6 11:08:24.431: INFO: (15) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.825875ms)
Sep  6 11:08:24.432: INFO: (15) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 10.879623ms)
Sep  6 11:08:24.433: INFO: (15) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 12.014994ms)
Sep  6 11:08:24.433: INFO: (15) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 12.314371ms)
Sep  6 11:08:24.439: INFO: (16) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 5.664985ms)
Sep  6 11:08:24.439: INFO: (16) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 5.878022ms)
Sep  6 11:08:24.440: INFO: (16) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 6.936175ms)
Sep  6 11:08:24.440: INFO: (16) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 6.910944ms)
Sep  6 11:08:24.441: INFO: (16) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 7.528606ms)
Sep  6 11:08:24.441: INFO: (16) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 8.011539ms)
Sep  6 11:08:24.442: INFO: (16) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 8.394515ms)
Sep  6 11:08:24.443: INFO: (16) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 9.360303ms)
Sep  6 11:08:24.443: INFO: (16) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 9.698192ms)
Sep  6 11:08:24.444: INFO: (16) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 10.595563ms)
Sep  6 11:08:24.444: INFO: (16) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 10.968164ms)
Sep  6 11:08:24.446: INFO: (16) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 12.580573ms)
Sep  6 11:08:24.446: INFO: (16) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 12.541722ms)
Sep  6 11:08:24.446: INFO: (16) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 12.599379ms)
Sep  6 11:08:24.447: INFO: (16) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 13.602446ms)
Sep  6 11:08:24.448: INFO: (16) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 14.488494ms)
Sep  6 11:08:24.452: INFO: (17) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 4.638782ms)
Sep  6 11:08:24.455: INFO: (17) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.293458ms)
Sep  6 11:08:24.455: INFO: (17) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.39522ms)
Sep  6 11:08:24.457: INFO: (17) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 8.912565ms)
Sep  6 11:08:24.457: INFO: (17) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 9.011552ms)
Sep  6 11:08:24.457: INFO: (17) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.189627ms)
Sep  6 11:08:24.457: INFO: (17) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.278892ms)
Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 9.920688ms)
Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 9.988816ms)
Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 10.663769ms)
Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 10.667138ms)
Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 10.825662ms)
Sep  6 11:08:24.459: INFO: (17) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.608501ms)
Sep  6 11:08:24.459: INFO: (17) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.546624ms)
Sep  6 11:08:24.459: INFO: (17) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 11.611118ms)
Sep  6 11:08:24.460: INFO: (17) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.427023ms)
Sep  6 11:08:24.465: INFO: (18) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 5.068073ms)
Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 7.641968ms)
Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 7.594096ms)
Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 7.682141ms)
Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 7.731089ms)
Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.730033ms)
Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.763852ms)
Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 7.758128ms)
Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.765931ms)
Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 9.846147ms)
Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.031714ms)
Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 9.866788ms)
Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 10.029693ms)
Sep  6 11:08:24.472: INFO: (18) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.517236ms)
Sep  6 11:08:24.473: INFO: (18) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.749041ms)
Sep  6 11:08:24.473: INFO: (18) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 12.866805ms)
Sep  6 11:08:24.478: INFO: (19) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 5.416291ms)
Sep  6 11:08:24.479: INFO: (19) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 5.271755ms)
Sep  6 11:08:24.479: INFO: (19) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 5.752734ms)
Sep  6 11:08:24.480: INFO: (19) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 5.656578ms)
Sep  6 11:08:24.481: INFO: (19) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.237677ms)
Sep  6 11:08:24.481: INFO: (19) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 7.72738ms)
Sep  6 11:08:24.481: INFO: (19) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.820633ms)
Sep  6 11:08:24.482: INFO: (19) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 9.340765ms)
Sep  6 11:08:24.483: INFO: (19) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.894535ms)
Sep  6 11:08:24.483: INFO: (19) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.816788ms)
Sep  6 11:08:24.485: INFO: (19) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 11.406218ms)
Sep  6 11:08:24.485: INFO: (19) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 11.649064ms)
Sep  6 11:08:24.485: INFO: (19) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 11.67175ms)
Sep  6 11:08:24.486: INFO: (19) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 11.490887ms)
Sep  6 11:08:24.486: INFO: (19) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.610585ms)
Sep  6 11:08:24.486: INFO: (19) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 12.276011ms)
STEP: deleting ReplicationController proxy-service-7kskg in namespace proxy-2373, will wait for the garbage collector to delete the pods 09/06/23 11:08:24.486
Sep  6 11:08:24.547: INFO: Deleting ReplicationController proxy-service-7kskg took: 7.400629ms
Sep  6 11:08:24.648: INFO: Terminating ReplicationController proxy-service-7kskg pods took: 100.957989ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  6 11:08:26.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2373" for this suite. 09/06/23 11:08:26.76
------------------------------
• [SLOW TEST] [31.234 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:07:55.535
    Sep  6 11:07:55.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename proxy 09/06/23 11:07:55.536
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:21.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:21.027
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 09/06/23 11:08:21.093
    STEP: creating replication controller proxy-service-7kskg in namespace proxy-2373 09/06/23 11:08:21.093
    I0906 11:08:21.102772      22 runners.go:193] Created replication controller with name: proxy-service-7kskg, namespace: proxy-2373, replica count: 1
    I0906 11:08:22.154150      22 runners.go:193] proxy-service-7kskg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0906 11:08:23.154412      22 runners.go:193] proxy-service-7kskg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0906 11:08:24.155020      22 runners.go:193] proxy-service-7kskg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 11:08:24.173: INFO: setup took 3.098154167s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/06/23 11:08:24.173
    Sep  6 11:08:24.201: INFO: (0) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 27.661659ms)
    Sep  6 11:08:24.201: INFO: (0) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 27.477519ms)
    Sep  6 11:08:24.201: INFO: (0) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 27.808413ms)
    Sep  6 11:08:24.201: INFO: (0) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 27.587901ms)
    Sep  6 11:08:24.202: INFO: (0) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 27.963279ms)
    Sep  6 11:08:24.202: INFO: (0) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 27.648366ms)
    Sep  6 11:08:24.202: INFO: (0) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 27.80479ms)
    Sep  6 11:08:24.205: INFO: (0) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 30.902561ms)
    Sep  6 11:08:24.207: INFO: (0) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 33.669039ms)
    Sep  6 11:08:24.208: INFO: (0) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 34.839482ms)
    Sep  6 11:08:24.209: INFO: (0) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 35.144381ms)
    Sep  6 11:08:24.210: INFO: (0) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 35.931383ms)
    Sep  6 11:08:24.210: INFO: (0) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 37.237757ms)
    Sep  6 11:08:24.211: INFO: (0) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 37.34915ms)
    Sep  6 11:08:24.211: INFO: (0) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 37.415342ms)
    Sep  6 11:08:24.211: INFO: (0) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 37.516081ms)
    Sep  6 11:08:24.217: INFO: (1) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 5.083056ms)
    Sep  6 11:08:24.222: INFO: (1) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 9.465791ms)
    Sep  6 11:08:24.222: INFO: (1) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 9.436944ms)
    Sep  6 11:08:24.222: INFO: (1) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.464743ms)
    Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 10.862753ms)
    Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 10.723353ms)
    Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 10.528347ms)
    Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 10.841021ms)
    Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 11.610771ms)
    Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 10.751959ms)
    Sep  6 11:08:24.223: INFO: (1) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 11.808687ms)
    Sep  6 11:08:24.224: INFO: (1) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.339634ms)
    Sep  6 11:08:24.228: INFO: (1) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 15.513023ms)
    Sep  6 11:08:24.229: INFO: (1) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 17.01759ms)
    Sep  6 11:08:24.229: INFO: (1) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 16.558307ms)
    Sep  6 11:08:24.229: INFO: (1) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 17.087576ms)
    Sep  6 11:08:24.240: INFO: (2) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 9.753427ms)
    Sep  6 11:08:24.241: INFO: (2) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 9.901998ms)
    Sep  6 11:08:24.242: INFO: (2) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 11.010823ms)
    Sep  6 11:08:24.242: INFO: (2) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 11.075168ms)
    Sep  6 11:08:24.242: INFO: (2) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 11.164946ms)
    Sep  6 11:08:24.242: INFO: (2) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 11.468213ms)
    Sep  6 11:08:24.243: INFO: (2) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.667038ms)
    Sep  6 11:08:24.244: INFO: (2) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 13.99119ms)
    Sep  6 11:08:24.246: INFO: (2) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 15.218475ms)
    Sep  6 11:08:24.247: INFO: (2) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 16.360884ms)
    Sep  6 11:08:24.247: INFO: (2) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 16.808213ms)
    Sep  6 11:08:24.247: INFO: (2) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 16.861911ms)
    Sep  6 11:08:24.248: INFO: (2) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 17.098528ms)
    Sep  6 11:08:24.248: INFO: (2) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 17.017417ms)
    Sep  6 11:08:24.249: INFO: (2) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 18.435816ms)
    Sep  6 11:08:24.250: INFO: (2) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 19.54025ms)
    Sep  6 11:08:24.258: INFO: (3) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.872133ms)
    Sep  6 11:08:24.258: INFO: (3) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.083945ms)
    Sep  6 11:08:24.259: INFO: (3) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.278961ms)
    Sep  6 11:08:24.259: INFO: (3) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 8.251848ms)
    Sep  6 11:08:24.261: INFO: (3) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 10.706906ms)
    Sep  6 11:08:24.261: INFO: (3) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 10.636519ms)
    Sep  6 11:08:24.263: INFO: (3) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 12.480287ms)
    Sep  6 11:08:24.263: INFO: (3) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 12.355385ms)
    Sep  6 11:08:24.263: INFO: (3) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 12.450089ms)
    Sep  6 11:08:24.265: INFO: (3) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 14.630335ms)
    Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 15.084305ms)
    Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 14.929286ms)
    Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 15.1885ms)
    Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 15.503802ms)
    Sep  6 11:08:24.266: INFO: (3) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 15.64739ms)
    Sep  6 11:08:24.267: INFO: (3) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 16.155144ms)
    Sep  6 11:08:24.276: INFO: (4) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.129503ms)
    Sep  6 11:08:24.277: INFO: (4) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.500679ms)
    Sep  6 11:08:24.279: INFO: (4) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 12.112058ms)
    Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 12.630124ms)
    Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 12.227044ms)
    Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 12.24763ms)
    Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 12.544545ms)
    Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.416559ms)
    Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 12.897787ms)
    Sep  6 11:08:24.280: INFO: (4) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 12.863775ms)
    Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 13.44319ms)
    Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 13.353892ms)
    Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.599498ms)
    Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 13.7163ms)
    Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 13.653435ms)
    Sep  6 11:08:24.281: INFO: (4) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 13.280668ms)
    Sep  6 11:08:24.285: INFO: (5) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 4.46398ms)
    Sep  6 11:08:24.288: INFO: (5) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.700541ms)
    Sep  6 11:08:24.289: INFO: (5) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.755966ms)
    Sep  6 11:08:24.289: INFO: (5) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 7.865772ms)
    Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 9.699696ms)
    Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 9.757203ms)
    Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 9.818896ms)
    Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.919423ms)
    Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 9.915249ms)
    Sep  6 11:08:24.291: INFO: (5) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 10.426056ms)
    Sep  6 11:08:24.292: INFO: (5) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 10.715999ms)
    Sep  6 11:08:24.292: INFO: (5) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 11.387765ms)
    Sep  6 11:08:24.293: INFO: (5) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.200016ms)
    Sep  6 11:08:24.293: INFO: (5) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 12.148057ms)
    Sep  6 11:08:24.293: INFO: (5) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 12.601451ms)
    Sep  6 11:08:24.294: INFO: (5) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.305333ms)
    Sep  6 11:08:24.300: INFO: (6) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 5.203412ms)
    Sep  6 11:08:24.300: INFO: (6) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 5.873975ms)
    Sep  6 11:08:24.302: INFO: (6) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 7.411886ms)
    Sep  6 11:08:24.302: INFO: (6) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 7.37774ms)
    Sep  6 11:08:24.302: INFO: (6) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 7.409482ms)
    Sep  6 11:08:24.303: INFO: (6) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.48768ms)
    Sep  6 11:08:24.303: INFO: (6) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 8.970436ms)
    Sep  6 11:08:24.303: INFO: (6) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 8.991055ms)
    Sep  6 11:08:24.303: INFO: (6) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 8.985438ms)
    Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 9.597538ms)
    Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.589344ms)
    Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 9.811673ms)
    Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.791461ms)
    Sep  6 11:08:24.304: INFO: (6) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.069497ms)
    Sep  6 11:08:24.306: INFO: (6) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 12.028961ms)
    Sep  6 11:08:24.307: INFO: (6) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 12.171818ms)
    Sep  6 11:08:24.313: INFO: (7) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.616285ms)
    Sep  6 11:08:24.313: INFO: (7) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.839262ms)
    Sep  6 11:08:24.316: INFO: (7) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 8.773447ms)
    Sep  6 11:08:24.316: INFO: (7) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 9.084807ms)
    Sep  6 11:08:24.317: INFO: (7) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.51504ms)
    Sep  6 11:08:24.317: INFO: (7) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 10.569172ms)
    Sep  6 11:08:24.319: INFO: (7) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 11.858146ms)
    Sep  6 11:08:24.319: INFO: (7) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 12.501236ms)
    Sep  6 11:08:24.319: INFO: (7) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 12.553544ms)
    Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 12.941286ms)
    Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 13.055726ms)
    Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 13.103984ms)
    Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 13.16752ms)
    Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.443413ms)
    Sep  6 11:08:24.320: INFO: (7) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 13.565328ms)
    Sep  6 11:08:24.321: INFO: (7) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 13.948975ms)
    Sep  6 11:08:24.325: INFO: (8) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 4.67447ms)
    Sep  6 11:08:24.327: INFO: (8) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.207571ms)
    Sep  6 11:08:24.328: INFO: (8) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 6.335889ms)
    Sep  6 11:08:24.328: INFO: (8) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.475697ms)
    Sep  6 11:08:24.328: INFO: (8) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.483091ms)
    Sep  6 11:08:24.329: INFO: (8) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.560767ms)
    Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 9.801782ms)
    Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 9.701575ms)
    Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 9.870745ms)
    Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.83625ms)
    Sep  6 11:08:24.331: INFO: (8) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 9.626726ms)
    Sep  6 11:08:24.332: INFO: (8) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 10.798414ms)
    Sep  6 11:08:24.332: INFO: (8) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.040125ms)
    Sep  6 11:08:24.333: INFO: (8) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 11.825852ms)
    Sep  6 11:08:24.334: INFO: (8) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.64658ms)
    Sep  6 11:08:24.334: INFO: (8) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 12.998629ms)
    Sep  6 11:08:24.341: INFO: (9) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 6.740869ms)
    Sep  6 11:08:24.341: INFO: (9) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.85742ms)
    Sep  6 11:08:24.341: INFO: (9) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 6.94969ms)
    Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 9.847682ms)
    Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 9.843675ms)
    Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.912875ms)
    Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.98695ms)
    Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 10.062203ms)
    Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 9.959512ms)
    Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 9.965241ms)
    Sep  6 11:08:24.344: INFO: (9) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.970896ms)
    Sep  6 11:08:24.345: INFO: (9) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 10.310577ms)
    Sep  6 11:08:24.346: INFO: (9) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.709814ms)
    Sep  6 11:08:24.347: INFO: (9) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.350133ms)
    Sep  6 11:08:24.347: INFO: (9) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 13.183069ms)
    Sep  6 11:08:24.348: INFO: (9) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.845793ms)
    Sep  6 11:08:24.354: INFO: (10) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 6.04612ms)
    Sep  6 11:08:24.357: INFO: (10) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.27668ms)
    Sep  6 11:08:24.358: INFO: (10) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 8.942147ms)
    Sep  6 11:08:24.358: INFO: (10) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.616072ms)
    Sep  6 11:08:24.358: INFO: (10) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 8.233604ms)
    Sep  6 11:08:24.358: INFO: (10) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 8.716129ms)
    Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 9.809232ms)
    Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 10.086803ms)
    Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.506973ms)
    Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 10.692197ms)
    Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.806736ms)
    Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 9.952093ms)
    Sep  6 11:08:24.359: INFO: (10) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 10.61605ms)
    Sep  6 11:08:24.360: INFO: (10) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 11.019624ms)
    Sep  6 11:08:24.360: INFO: (10) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.250578ms)
    Sep  6 11:08:24.360: INFO: (10) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.195821ms)
    Sep  6 11:08:24.366: INFO: (11) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 5.730931ms)
    Sep  6 11:08:24.367: INFO: (11) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.000464ms)
    Sep  6 11:08:24.367: INFO: (11) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 6.191795ms)
    Sep  6 11:08:24.367: INFO: (11) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 5.923954ms)
    Sep  6 11:08:24.368: INFO: (11) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.521166ms)
    Sep  6 11:08:24.369: INFO: (11) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.104411ms)
    Sep  6 11:08:24.369: INFO: (11) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 8.442174ms)
    Sep  6 11:08:24.369: INFO: (11) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 8.769385ms)
    Sep  6 11:08:24.369: INFO: (11) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.524908ms)
    Sep  6 11:08:24.372: INFO: (11) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 10.516729ms)
    Sep  6 11:08:24.372: INFO: (11) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.361438ms)
    Sep  6 11:08:24.372: INFO: (11) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 11.112576ms)
    Sep  6 11:08:24.372: INFO: (11) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 11.26692ms)
    Sep  6 11:08:24.373: INFO: (11) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 11.786901ms)
    Sep  6 11:08:24.374: INFO: (11) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 12.993782ms)
    Sep  6 11:08:24.376: INFO: (11) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 14.623937ms)
    Sep  6 11:08:24.382: INFO: (12) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 6.101804ms)
    Sep  6 11:08:24.382: INFO: (12) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 5.774113ms)
    Sep  6 11:08:24.383: INFO: (12) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 5.699359ms)
    Sep  6 11:08:24.383: INFO: (12) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 7.727183ms)
    Sep  6 11:08:24.383: INFO: (12) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.798953ms)
    Sep  6 11:08:24.384: INFO: (12) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 7.381111ms)
    Sep  6 11:08:24.384: INFO: (12) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.04894ms)
    Sep  6 11:08:24.386: INFO: (12) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 10.17539ms)
    Sep  6 11:08:24.389: INFO: (12) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 12.516616ms)
    Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 14.134929ms)
    Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 14.406973ms)
    Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 12.857973ms)
    Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 13.103215ms)
    Sep  6 11:08:24.390: INFO: (12) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 13.415794ms)
    Sep  6 11:08:24.391: INFO: (12) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 14.056548ms)
    Sep  6 11:08:24.391: INFO: (12) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 15.745247ms)
    Sep  6 11:08:24.398: INFO: (13) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 6.962987ms)
    Sep  6 11:08:24.398: INFO: (13) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.022351ms)
    Sep  6 11:08:24.400: INFO: (13) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.258717ms)
    Sep  6 11:08:24.400: INFO: (13) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.138493ms)
    Sep  6 11:08:24.400: INFO: (13) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 7.97518ms)
    Sep  6 11:08:24.400: INFO: (13) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 8.377223ms)
    Sep  6 11:08:24.402: INFO: (13) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 10.535091ms)
    Sep  6 11:08:24.402: INFO: (13) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.501571ms)
    Sep  6 11:08:24.402: INFO: (13) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 10.701869ms)
    Sep  6 11:08:24.403: INFO: (13) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.409494ms)
    Sep  6 11:08:24.404: INFO: (13) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 12.886394ms)
    Sep  6 11:08:24.404: INFO: (13) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 12.964558ms)
    Sep  6 11:08:24.405: INFO: (13) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 12.789109ms)
    Sep  6 11:08:24.405: INFO: (13) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 13.326462ms)
    Sep  6 11:08:24.406: INFO: (13) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 14.605193ms)
    Sep  6 11:08:24.407: INFO: (13) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 14.954234ms)
    Sep  6 11:08:24.413: INFO: (14) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 6.021757ms)
    Sep  6 11:08:24.413: INFO: (14) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.041672ms)
    Sep  6 11:08:24.413: INFO: (14) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 6.64447ms)
    Sep  6 11:08:24.415: INFO: (14) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.184866ms)
    Sep  6 11:08:24.415: INFO: (14) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 8.215791ms)
    Sep  6 11:08:24.415: INFO: (14) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.274483ms)
    Sep  6 11:08:24.416: INFO: (14) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 9.377255ms)
    Sep  6 11:08:24.417: INFO: (14) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 10.524674ms)
    Sep  6 11:08:24.417: INFO: (14) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.66461ms)
    Sep  6 11:08:24.418: INFO: (14) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 11.412209ms)
    Sep  6 11:08:24.418: INFO: (14) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.630284ms)
    Sep  6 11:08:24.419: INFO: (14) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.941613ms)
    Sep  6 11:08:24.419: INFO: (14) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 11.913104ms)
    Sep  6 11:08:24.420: INFO: (14) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.852243ms)
    Sep  6 11:08:24.420: INFO: (14) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 13.093174ms)
    Sep  6 11:08:24.420: INFO: (14) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 13.74385ms)
    Sep  6 11:08:24.426: INFO: (15) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 4.75544ms)
    Sep  6 11:08:24.426: INFO: (15) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 5.407921ms)
    Sep  6 11:08:24.428: INFO: (15) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 6.916666ms)
    Sep  6 11:08:24.428: INFO: (15) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 7.128205ms)
    Sep  6 11:08:24.428: INFO: (15) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 7.063857ms)
    Sep  6 11:08:24.428: INFO: (15) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 7.034224ms)
    Sep  6 11:08:24.429: INFO: (15) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 8.128984ms)
    Sep  6 11:08:24.429: INFO: (15) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 8.202525ms)
    Sep  6 11:08:24.429: INFO: (15) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 8.451978ms)
    Sep  6 11:08:24.430: INFO: (15) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 9.092037ms)
    Sep  6 11:08:24.431: INFO: (15) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 10.328885ms)
    Sep  6 11:08:24.431: INFO: (15) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.655166ms)
    Sep  6 11:08:24.431: INFO: (15) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.825875ms)
    Sep  6 11:08:24.432: INFO: (15) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 10.879623ms)
    Sep  6 11:08:24.433: INFO: (15) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 12.014994ms)
    Sep  6 11:08:24.433: INFO: (15) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 12.314371ms)
    Sep  6 11:08:24.439: INFO: (16) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 5.664985ms)
    Sep  6 11:08:24.439: INFO: (16) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 5.878022ms)
    Sep  6 11:08:24.440: INFO: (16) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 6.936175ms)
    Sep  6 11:08:24.440: INFO: (16) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 6.910944ms)
    Sep  6 11:08:24.441: INFO: (16) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 7.528606ms)
    Sep  6 11:08:24.441: INFO: (16) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 8.011539ms)
    Sep  6 11:08:24.442: INFO: (16) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 8.394515ms)
    Sep  6 11:08:24.443: INFO: (16) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 9.360303ms)
    Sep  6 11:08:24.443: INFO: (16) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 9.698192ms)
    Sep  6 11:08:24.444: INFO: (16) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 10.595563ms)
    Sep  6 11:08:24.444: INFO: (16) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 10.968164ms)
    Sep  6 11:08:24.446: INFO: (16) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 12.580573ms)
    Sep  6 11:08:24.446: INFO: (16) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 12.541722ms)
    Sep  6 11:08:24.446: INFO: (16) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 12.599379ms)
    Sep  6 11:08:24.447: INFO: (16) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 13.602446ms)
    Sep  6 11:08:24.448: INFO: (16) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 14.488494ms)
    Sep  6 11:08:24.452: INFO: (17) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 4.638782ms)
    Sep  6 11:08:24.455: INFO: (17) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.293458ms)
    Sep  6 11:08:24.455: INFO: (17) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.39522ms)
    Sep  6 11:08:24.457: INFO: (17) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 8.912565ms)
    Sep  6 11:08:24.457: INFO: (17) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 9.011552ms)
    Sep  6 11:08:24.457: INFO: (17) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.189627ms)
    Sep  6 11:08:24.457: INFO: (17) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.278892ms)
    Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 9.920688ms)
    Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 9.988816ms)
    Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 10.663769ms)
    Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 10.667138ms)
    Sep  6 11:08:24.458: INFO: (17) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 10.825662ms)
    Sep  6 11:08:24.459: INFO: (17) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.608501ms)
    Sep  6 11:08:24.459: INFO: (17) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 11.546624ms)
    Sep  6 11:08:24.459: INFO: (17) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 11.611118ms)
    Sep  6 11:08:24.460: INFO: (17) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.427023ms)
    Sep  6 11:08:24.465: INFO: (18) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 5.068073ms)
    Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 7.641968ms)
    Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 7.594096ms)
    Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 7.682141ms)
    Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 7.731089ms)
    Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 7.730033ms)
    Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.763852ms)
    Sep  6 11:08:24.468: INFO: (18) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 7.758128ms)
    Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 9.765931ms)
    Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 9.846147ms)
    Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 10.031714ms)
    Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 9.866788ms)
    Sep  6 11:08:24.470: INFO: (18) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 10.029693ms)
    Sep  6 11:08:24.472: INFO: (18) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.517236ms)
    Sep  6 11:08:24.473: INFO: (18) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 12.749041ms)
    Sep  6 11:08:24.473: INFO: (18) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 12.866805ms)
    Sep  6 11:08:24.478: INFO: (19) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:462/proxy/: tls qux (200; 5.416291ms)
    Sep  6 11:08:24.479: INFO: (19) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:443/proxy/tlsrewritem... (200; 5.271755ms)
    Sep  6 11:08:24.479: INFO: (19) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 5.752734ms)
    Sep  6 11:08:24.480: INFO: (19) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">... (200; 5.656578ms)
    Sep  6 11:08:24.481: INFO: (19) /api/v1/namespaces/proxy-2373/pods/http:proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 7.237677ms)
    Sep  6 11:08:24.481: INFO: (19) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8/proxy/rewriteme">test</a> (200; 7.72738ms)
    Sep  6 11:08:24.481: INFO: (19) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:1080/proxy/rewriteme">test<... (200; 6.820633ms)
    Sep  6 11:08:24.482: INFO: (19) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname1/proxy/: foo (200; 9.340765ms)
    Sep  6 11:08:24.483: INFO: (19) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:160/proxy/: foo (200; 8.894535ms)
    Sep  6 11:08:24.483: INFO: (19) /api/v1/namespaces/proxy-2373/pods/https:proxy-service-7kskg-xhqd8:460/proxy/: tls baz (200; 9.816788ms)
    Sep  6 11:08:24.485: INFO: (19) /api/v1/namespaces/proxy-2373/pods/proxy-service-7kskg-xhqd8:162/proxy/: bar (200; 11.406218ms)
    Sep  6 11:08:24.485: INFO: (19) /api/v1/namespaces/proxy-2373/services/http:proxy-service-7kskg:portname2/proxy/: bar (200; 11.649064ms)
    Sep  6 11:08:24.485: INFO: (19) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname1/proxy/: foo (200; 11.67175ms)
    Sep  6 11:08:24.486: INFO: (19) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname1/proxy/: tls baz (200; 11.490887ms)
    Sep  6 11:08:24.486: INFO: (19) /api/v1/namespaces/proxy-2373/services/proxy-service-7kskg:portname2/proxy/: bar (200; 11.610585ms)
    Sep  6 11:08:24.486: INFO: (19) /api/v1/namespaces/proxy-2373/services/https:proxy-service-7kskg:tlsportname2/proxy/: tls qux (200; 12.276011ms)
    STEP: deleting ReplicationController proxy-service-7kskg in namespace proxy-2373, will wait for the garbage collector to delete the pods 09/06/23 11:08:24.486
    Sep  6 11:08:24.547: INFO: Deleting ReplicationController proxy-service-7kskg took: 7.400629ms
    Sep  6 11:08:24.648: INFO: Terminating ReplicationController proxy-service-7kskg pods took: 100.957989ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:08:26.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2373" for this suite. 09/06/23 11:08:26.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:08:26.778
Sep  6 11:08:26.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename dns 09/06/23 11:08:26.779
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:26.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:26.807
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 09/06/23 11:08:26.812
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-954 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-954;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-954 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-954;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-954.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-954.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-954.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-954.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-954.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-954.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-954.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-954.svc;check="$$(dig +notcp +noall +answer +search 16.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.16_tcp@PTR;sleep 1; done
 09/06/23 11:08:26.843
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-954 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-954;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-954 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-954;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-954.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-954.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-954.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-954.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-954.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-954.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-954.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-954.svc;check="$$(dig +notcp +noall +answer +search 16.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.16_tcp@PTR;sleep 1; done
 09/06/23 11:08:26.843
STEP: creating a pod to probe DNS 09/06/23 11:08:26.843
STEP: submitting the pod to kubernetes 09/06/23 11:08:26.843
Sep  6 11:08:26.866: INFO: Waiting up to 15m0s for pod "dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235" in namespace "dns-954" to be "running"
Sep  6 11:08:26.877: INFO: Pod "dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235": Phase="Pending", Reason="", readiness=false. Elapsed: 9.948359ms
Sep  6 11:08:28.886: INFO: Pod "dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235": Phase="Running", Reason="", readiness=true. Elapsed: 2.018557064s
Sep  6 11:08:28.886: INFO: Pod "dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235" satisfied condition "running"
STEP: retrieving the pod 09/06/23 11:08:28.886
STEP: looking for the results for each expected name from probers 09/06/23 11:08:28.893
Sep  6 11:08:28.898: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.902: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.907: INFO: Unable to read wheezy_udp@dns-test-service.dns-954 from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-954 from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.918: INFO: Unable to read wheezy_udp@dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.922: INFO: Unable to read wheezy_tcp@dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.927: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.930: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.945: INFO: Unable to read jessie_udp@dns-test-service from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.948: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.951: INFO: Unable to read jessie_udp@dns-test-service.dns-954 from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.954: INFO: Unable to read jessie_tcp@dns-test-service.dns-954 from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.957: INFO: Unable to read jessie_udp@dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.960: INFO: Unable to read jessie_tcp@dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.964: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.967: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
Sep  6 11:08:28.981: INFO: Lookups using dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-954 wheezy_tcp@dns-test-service.dns-954 wheezy_udp@dns-test-service.dns-954.svc wheezy_tcp@dns-test-service.dns-954.svc wheezy_udp@_http._tcp.dns-test-service.dns-954.svc wheezy_tcp@_http._tcp.dns-test-service.dns-954.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-954 jessie_tcp@dns-test-service.dns-954 jessie_udp@dns-test-service.dns-954.svc jessie_tcp@dns-test-service.dns-954.svc jessie_udp@_http._tcp.dns-test-service.dns-954.svc jessie_tcp@_http._tcp.dns-test-service.dns-954.svc]

Sep  6 11:08:34.111: INFO: DNS probes using dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235 succeeded

STEP: deleting the pod 09/06/23 11:08:34.111
STEP: deleting the test service 09/06/23 11:08:34.153
STEP: deleting the test headless service 09/06/23 11:08:34.23
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  6 11:08:34.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-954" for this suite. 09/06/23 11:08:34.282
------------------------------
• [SLOW TEST] [7.520 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:08:26.778
    Sep  6 11:08:26.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename dns 09/06/23 11:08:26.779
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:26.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:26.807
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 09/06/23 11:08:26.812
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-954 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-954;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-954 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-954;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-954.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-954.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-954.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-954.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-954.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-954.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-954.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-954.svc;check="$$(dig +notcp +noall +answer +search 16.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.16_tcp@PTR;sleep 1; done
     09/06/23 11:08:26.843
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-954 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-954;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-954 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-954;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-954.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-954.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-954.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-954.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-954.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-954.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-954.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-954.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-954.svc;check="$$(dig +notcp +noall +answer +search 16.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.16_tcp@PTR;sleep 1; done
     09/06/23 11:08:26.843
    STEP: creating a pod to probe DNS 09/06/23 11:08:26.843
    STEP: submitting the pod to kubernetes 09/06/23 11:08:26.843
    Sep  6 11:08:26.866: INFO: Waiting up to 15m0s for pod "dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235" in namespace "dns-954" to be "running"
    Sep  6 11:08:26.877: INFO: Pod "dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235": Phase="Pending", Reason="", readiness=false. Elapsed: 9.948359ms
    Sep  6 11:08:28.886: INFO: Pod "dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235": Phase="Running", Reason="", readiness=true. Elapsed: 2.018557064s
    Sep  6 11:08:28.886: INFO: Pod "dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 11:08:28.886
    STEP: looking for the results for each expected name from probers 09/06/23 11:08:28.893
    Sep  6 11:08:28.898: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.902: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.907: INFO: Unable to read wheezy_udp@dns-test-service.dns-954 from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-954 from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.918: INFO: Unable to read wheezy_udp@dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.922: INFO: Unable to read wheezy_tcp@dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.927: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.930: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.945: INFO: Unable to read jessie_udp@dns-test-service from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.948: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.951: INFO: Unable to read jessie_udp@dns-test-service.dns-954 from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.954: INFO: Unable to read jessie_tcp@dns-test-service.dns-954 from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.957: INFO: Unable to read jessie_udp@dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.960: INFO: Unable to read jessie_tcp@dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.964: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.967: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-954.svc from pod dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235: the server could not find the requested resource (get pods dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235)
    Sep  6 11:08:28.981: INFO: Lookups using dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-954 wheezy_tcp@dns-test-service.dns-954 wheezy_udp@dns-test-service.dns-954.svc wheezy_tcp@dns-test-service.dns-954.svc wheezy_udp@_http._tcp.dns-test-service.dns-954.svc wheezy_tcp@_http._tcp.dns-test-service.dns-954.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-954 jessie_tcp@dns-test-service.dns-954 jessie_udp@dns-test-service.dns-954.svc jessie_tcp@dns-test-service.dns-954.svc jessie_udp@_http._tcp.dns-test-service.dns-954.svc jessie_tcp@_http._tcp.dns-test-service.dns-954.svc]

    Sep  6 11:08:34.111: INFO: DNS probes using dns-954/dns-test-69a7e4ac-22a9-4486-b08e-9e9d9e6f3235 succeeded

    STEP: deleting the pod 09/06/23 11:08:34.111
    STEP: deleting the test service 09/06/23 11:08:34.153
    STEP: deleting the test headless service 09/06/23 11:08:34.23
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:08:34.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-954" for this suite. 09/06/23 11:08:34.282
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:08:34.298
Sep  6 11:08:34.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename statefulset 09/06/23 11:08:34.299
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:34.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:34.328
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3025 09/06/23 11:08:34.333
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 09/06/23 11:08:34.346
STEP: Creating pod with conflicting port in namespace statefulset-3025 09/06/23 11:08:34.355
STEP: Waiting until pod test-pod will start running in namespace statefulset-3025 09/06/23 11:08:34.371
Sep  6 11:08:34.371: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3025" to be "running"
Sep  6 11:08:34.379: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.390797ms
Sep  6 11:08:36.426: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0554199s
Sep  6 11:08:38.393: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021675036s
Sep  6 11:08:40.396: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.024641933s
Sep  6 11:08:40.396: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3025 09/06/23 11:08:40.396
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3025 09/06/23 11:08:40.414
Sep  6 11:08:40.442: INFO: Observed stateful pod in namespace: statefulset-3025, name: ss-0, uid: 88f2ccd0-35be-4b69-899d-f90e8e8ff528, status phase: Pending. Waiting for statefulset controller to delete.
Sep  6 11:08:40.459: INFO: Observed stateful pod in namespace: statefulset-3025, name: ss-0, uid: 88f2ccd0-35be-4b69-899d-f90e8e8ff528, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 11:08:40.470: INFO: Observed stateful pod in namespace: statefulset-3025, name: ss-0, uid: 88f2ccd0-35be-4b69-899d-f90e8e8ff528, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 11:08:40.477: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3025
STEP: Removing pod with conflicting port in namespace statefulset-3025 09/06/23 11:08:40.477
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3025 and will be in running state 09/06/23 11:08:40.517
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  6 11:08:42.538: INFO: Deleting all statefulset in ns statefulset-3025
Sep  6 11:08:42.550: INFO: Scaling statefulset ss to 0
Sep  6 11:08:52.594: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:08:52.606: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:08:52.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3025" for this suite. 09/06/23 11:08:52.655
------------------------------
• [SLOW TEST] [18.367 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:08:34.298
    Sep  6 11:08:34.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename statefulset 09/06/23 11:08:34.299
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:34.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:34.328
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3025 09/06/23 11:08:34.333
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 09/06/23 11:08:34.346
    STEP: Creating pod with conflicting port in namespace statefulset-3025 09/06/23 11:08:34.355
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3025 09/06/23 11:08:34.371
    Sep  6 11:08:34.371: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3025" to be "running"
    Sep  6 11:08:34.379: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.390797ms
    Sep  6 11:08:36.426: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0554199s
    Sep  6 11:08:38.393: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021675036s
    Sep  6 11:08:40.396: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.024641933s
    Sep  6 11:08:40.396: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3025 09/06/23 11:08:40.396
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3025 09/06/23 11:08:40.414
    Sep  6 11:08:40.442: INFO: Observed stateful pod in namespace: statefulset-3025, name: ss-0, uid: 88f2ccd0-35be-4b69-899d-f90e8e8ff528, status phase: Pending. Waiting for statefulset controller to delete.
    Sep  6 11:08:40.459: INFO: Observed stateful pod in namespace: statefulset-3025, name: ss-0, uid: 88f2ccd0-35be-4b69-899d-f90e8e8ff528, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  6 11:08:40.470: INFO: Observed stateful pod in namespace: statefulset-3025, name: ss-0, uid: 88f2ccd0-35be-4b69-899d-f90e8e8ff528, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  6 11:08:40.477: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3025
    STEP: Removing pod with conflicting port in namespace statefulset-3025 09/06/23 11:08:40.477
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3025 and will be in running state 09/06/23 11:08:40.517
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  6 11:08:42.538: INFO: Deleting all statefulset in ns statefulset-3025
    Sep  6 11:08:42.550: INFO: Scaling statefulset ss to 0
    Sep  6 11:08:52.594: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 11:08:52.606: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:08:52.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3025" for this suite. 09/06/23 11:08:52.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:08:52.666
Sep  6 11:08:52.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 11:08:52.666
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:52.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:52.689
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 11:08:52.706
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:08:53.091
STEP: Deploying the webhook pod 09/06/23 11:08:53.098
STEP: Wait for the deployment to be ready 09/06/23 11:08:53.112
Sep  6 11:08:53.119: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/06/23 11:08:55.16
STEP: Verifying the service has paired with the endpoint 09/06/23 11:08:55.191
Sep  6 11:08:56.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 09/06/23 11:08:56.196
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/06/23 11:08:56.198
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/06/23 11:08:56.198
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/06/23 11:08:56.198
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/06/23 11:08:56.199
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/06/23 11:08:56.199
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/06/23 11:08:56.2
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:08:56.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1452" for this suite. 09/06/23 11:08:56.277
STEP: Destroying namespace "webhook-1452-markers" for this suite. 09/06/23 11:08:56.291
------------------------------
• [3.642 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:08:52.666
    Sep  6 11:08:52.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 11:08:52.666
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:52.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:52.689
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 11:08:52.706
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:08:53.091
    STEP: Deploying the webhook pod 09/06/23 11:08:53.098
    STEP: Wait for the deployment to be ready 09/06/23 11:08:53.112
    Sep  6 11:08:53.119: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/06/23 11:08:55.16
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:08:55.191
    Sep  6 11:08:56.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 09/06/23 11:08:56.196
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/06/23 11:08:56.198
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/06/23 11:08:56.198
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/06/23 11:08:56.198
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/06/23 11:08:56.199
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/06/23 11:08:56.199
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/06/23 11:08:56.2
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:08:56.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1452" for this suite. 09/06/23 11:08:56.277
    STEP: Destroying namespace "webhook-1452-markers" for this suite. 09/06/23 11:08:56.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:08:56.31
Sep  6 11:08:56.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename proxy 09/06/23 11:08:56.31
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:56.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:56.34
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Sep  6 11:08:56.348: INFO: Creating pod...
Sep  6 11:08:56.363: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3407" to be "running"
Sep  6 11:08:56.371: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.603924ms
Sep  6 11:08:58.387: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.0236388s
Sep  6 11:08:58.388: INFO: Pod "agnhost" satisfied condition "running"
Sep  6 11:08:58.388: INFO: Creating service...
Sep  6 11:08:58.417: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/DELETE
Sep  6 11:08:58.427: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  6 11:08:58.427: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/GET
Sep  6 11:08:58.434: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  6 11:08:58.434: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/HEAD
Sep  6 11:08:58.439: INFO: http.Client request:HEAD | StatusCode:200
Sep  6 11:08:58.440: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/OPTIONS
Sep  6 11:08:58.444: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  6 11:08:58.444: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/PATCH
Sep  6 11:08:58.449: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  6 11:08:58.449: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/POST
Sep  6 11:08:58.455: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  6 11:08:58.455: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/PUT
Sep  6 11:08:58.460: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  6 11:08:58.460: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/DELETE
Sep  6 11:08:58.466: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  6 11:08:58.466: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/GET
Sep  6 11:08:58.472: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  6 11:08:58.472: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/HEAD
Sep  6 11:08:58.478: INFO: http.Client request:HEAD | StatusCode:200
Sep  6 11:08:58.478: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/OPTIONS
Sep  6 11:08:58.486: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  6 11:08:58.486: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/PATCH
Sep  6 11:08:58.494: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  6 11:08:58.494: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/POST
Sep  6 11:08:58.501: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  6 11:08:58.501: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/PUT
Sep  6 11:08:58.509: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  6 11:08:58.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3407" for this suite. 09/06/23 11:08:58.514
------------------------------
• [2.213 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:08:56.31
    Sep  6 11:08:56.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename proxy 09/06/23 11:08:56.31
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:56.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:56.34
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Sep  6 11:08:56.348: INFO: Creating pod...
    Sep  6 11:08:56.363: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3407" to be "running"
    Sep  6 11:08:56.371: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.603924ms
    Sep  6 11:08:58.387: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.0236388s
    Sep  6 11:08:58.388: INFO: Pod "agnhost" satisfied condition "running"
    Sep  6 11:08:58.388: INFO: Creating service...
    Sep  6 11:08:58.417: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/DELETE
    Sep  6 11:08:58.427: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  6 11:08:58.427: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/GET
    Sep  6 11:08:58.434: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  6 11:08:58.434: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/HEAD
    Sep  6 11:08:58.439: INFO: http.Client request:HEAD | StatusCode:200
    Sep  6 11:08:58.440: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/OPTIONS
    Sep  6 11:08:58.444: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  6 11:08:58.444: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/PATCH
    Sep  6 11:08:58.449: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  6 11:08:58.449: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/POST
    Sep  6 11:08:58.455: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  6 11:08:58.455: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/pods/agnhost/proxy/some/path/with/PUT
    Sep  6 11:08:58.460: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  6 11:08:58.460: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/DELETE
    Sep  6 11:08:58.466: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  6 11:08:58.466: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/GET
    Sep  6 11:08:58.472: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  6 11:08:58.472: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/HEAD
    Sep  6 11:08:58.478: INFO: http.Client request:HEAD | StatusCode:200
    Sep  6 11:08:58.478: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/OPTIONS
    Sep  6 11:08:58.486: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  6 11:08:58.486: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/PATCH
    Sep  6 11:08:58.494: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  6 11:08:58.494: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/POST
    Sep  6 11:08:58.501: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  6 11:08:58.501: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3407/services/test-service/proxy/some/path/with/PUT
    Sep  6 11:08:58.509: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:08:58.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3407" for this suite. 09/06/23 11:08:58.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:08:58.524
Sep  6 11:08:58.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename containers 09/06/23 11:08:58.524
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:58.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:58.55
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Sep  6 11:08:58.563: INFO: Waiting up to 5m0s for pod "client-containers-1688abb2-c9ec-4804-94ed-6c37eed014ae" in namespace "containers-8732" to be "running"
Sep  6 11:08:58.571: INFO: Pod "client-containers-1688abb2-c9ec-4804-94ed-6c37eed014ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025944ms
Sep  6 11:09:00.591: INFO: Pod "client-containers-1688abb2-c9ec-4804-94ed-6c37eed014ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.027078674s
Sep  6 11:09:00.591: INFO: Pod "client-containers-1688abb2-c9ec-4804-94ed-6c37eed014ae" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:00.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8732" for this suite. 09/06/23 11:09:00.63
------------------------------
• [2.118 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:08:58.524
    Sep  6 11:08:58.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename containers 09/06/23 11:08:58.524
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:08:58.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:08:58.55
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Sep  6 11:08:58.563: INFO: Waiting up to 5m0s for pod "client-containers-1688abb2-c9ec-4804-94ed-6c37eed014ae" in namespace "containers-8732" to be "running"
    Sep  6 11:08:58.571: INFO: Pod "client-containers-1688abb2-c9ec-4804-94ed-6c37eed014ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025944ms
    Sep  6 11:09:00.591: INFO: Pod "client-containers-1688abb2-c9ec-4804-94ed-6c37eed014ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.027078674s
    Sep  6 11:09:00.591: INFO: Pod "client-containers-1688abb2-c9ec-4804-94ed-6c37eed014ae" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:00.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8732" for this suite. 09/06/23 11:09:00.63
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:00.645
Sep  6 11:09:00.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 11:09:00.646
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:00.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:00.666
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 09/06/23 11:09:00.668
STEP: Ensuring ResourceQuota status is calculated 09/06/23 11:09:00.674
STEP: Creating a ResourceQuota with not best effort scope 09/06/23 11:09:02.687
STEP: Ensuring ResourceQuota status is calculated 09/06/23 11:09:02.699
STEP: Creating a best-effort pod 09/06/23 11:09:04.713
STEP: Ensuring resource quota with best effort scope captures the pod usage 09/06/23 11:09:04.748
STEP: Ensuring resource quota with not best effort ignored the pod usage 09/06/23 11:09:06.754
STEP: Deleting the pod 09/06/23 11:09:08.758
STEP: Ensuring resource quota status released the pod usage 09/06/23 11:09:08.775
STEP: Creating a not best-effort pod 09/06/23 11:09:10.788
STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/06/23 11:09:10.812
STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/06/23 11:09:12.822
STEP: Deleting the pod 09/06/23 11:09:15.06
STEP: Ensuring resource quota status released the pod usage 09/06/23 11:09:16.533
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:18.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4587" for this suite. 09/06/23 11:09:18.554
------------------------------
• [SLOW TEST] [17.945 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:00.645
    Sep  6 11:09:00.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 11:09:00.646
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:00.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:00.666
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 09/06/23 11:09:00.668
    STEP: Ensuring ResourceQuota status is calculated 09/06/23 11:09:00.674
    STEP: Creating a ResourceQuota with not best effort scope 09/06/23 11:09:02.687
    STEP: Ensuring ResourceQuota status is calculated 09/06/23 11:09:02.699
    STEP: Creating a best-effort pod 09/06/23 11:09:04.713
    STEP: Ensuring resource quota with best effort scope captures the pod usage 09/06/23 11:09:04.748
    STEP: Ensuring resource quota with not best effort ignored the pod usage 09/06/23 11:09:06.754
    STEP: Deleting the pod 09/06/23 11:09:08.758
    STEP: Ensuring resource quota status released the pod usage 09/06/23 11:09:08.775
    STEP: Creating a not best-effort pod 09/06/23 11:09:10.788
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/06/23 11:09:10.812
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/06/23 11:09:12.822
    STEP: Deleting the pod 09/06/23 11:09:15.06
    STEP: Ensuring resource quota status released the pod usage 09/06/23 11:09:16.533
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:18.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4587" for this suite. 09/06/23 11:09:18.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:18.591
Sep  6 11:09:18.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename containers 09/06/23 11:09:18.592
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:18.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:18.681
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 09/06/23 11:09:18.684
Sep  6 11:09:18.754: INFO: Waiting up to 5m0s for pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada" in namespace "containers-2381" to be "Succeeded or Failed"
Sep  6 11:09:18.785: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada": Phase="Pending", Reason="", readiness=false. Elapsed: 31.157487ms
Sep  6 11:09:20.789: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035316607s
Sep  6 11:09:22.797: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042634226s
Sep  6 11:09:24.796: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041808402s
STEP: Saw pod success 09/06/23 11:09:24.796
Sep  6 11:09:24.796: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada" satisfied condition "Succeeded or Failed"
Sep  6 11:09:24.803: INFO: Trying to get logs from node kube-3 pod client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada container agnhost-container: <nil>
STEP: delete the pod 09/06/23 11:09:24.813
Sep  6 11:09:24.829: INFO: Waiting for pod client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada to disappear
Sep  6 11:09:24.833: INFO: Pod client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:24.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2381" for this suite. 09/06/23 11:09:24.837
------------------------------
• [SLOW TEST] [6.254 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:18.591
    Sep  6 11:09:18.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename containers 09/06/23 11:09:18.592
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:18.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:18.681
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 09/06/23 11:09:18.684
    Sep  6 11:09:18.754: INFO: Waiting up to 5m0s for pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada" in namespace "containers-2381" to be "Succeeded or Failed"
    Sep  6 11:09:18.785: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada": Phase="Pending", Reason="", readiness=false. Elapsed: 31.157487ms
    Sep  6 11:09:20.789: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035316607s
    Sep  6 11:09:22.797: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042634226s
    Sep  6 11:09:24.796: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041808402s
    STEP: Saw pod success 09/06/23 11:09:24.796
    Sep  6 11:09:24.796: INFO: Pod "client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada" satisfied condition "Succeeded or Failed"
    Sep  6 11:09:24.803: INFO: Trying to get logs from node kube-3 pod client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 11:09:24.813
    Sep  6 11:09:24.829: INFO: Waiting for pod client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada to disappear
    Sep  6 11:09:24.833: INFO: Pod client-containers-93dd37c2-ff8d-4b18-ac05-199392e50ada no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:24.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2381" for this suite. 09/06/23 11:09:24.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:24.846
Sep  6 11:09:24.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename deployment 09/06/23 11:09:24.847
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:24.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:24.864
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Sep  6 11:09:24.876: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  6 11:09:29.901: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/06/23 11:09:29.902
Sep  6 11:09:29.902: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/06/23 11:09:29.939
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  6 11:09:29.960: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5570  ed327e7b-20f2-4291-988a-42a01fb00e03 27566 1 2023-09-06 11:09:29 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-06 11:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c74fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 11:09:29.974: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5570  68060a50-b27a-44c8-b512-944ec6c61ede 27568 1 2023-09-06 11:09:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ed327e7b-20f2-4291-988a-42a01fb00e03 0xc004c75467 0xc004c75468}] [] [{kube-controller-manager Update apps/v1 2023-09-06 11:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed327e7b-20f2-4291-988a-42a01fb00e03\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c754f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 11:09:29.974: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  6 11:09:29.974: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5570  89186781-391a-4e0d-a451-8f3dac3712bb 27567 1 2023-09-06 11:09:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment ed327e7b-20f2-4291-988a-42a01fb00e03 0xc004c75337 0xc004c75338}] [] [{e2e.test Update apps/v1 2023-09-06 11:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 11:09:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-06 11:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"ed327e7b-20f2-4291-988a-42a01fb00e03\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c753f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 11:09:29.982: INFO: Pod "test-cleanup-controller-trw7s" is available:
&Pod{ObjectMeta:{test-cleanup-controller-trw7s test-cleanup-controller- deployment-5570  96bef571-0e79-4592-8c91-d0263e2926d3 27558 0 2023-09-06 11:09:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b0f6c55105ed88aeff7667226a719ef1592190fa08c2ebc86c5742d6c879e497 cni.projectcalico.org/podIP:10.233.99.95/32 cni.projectcalico.org/podIPs:10.233.99.95/32] [{apps/v1 ReplicaSet test-cleanup-controller 89186781-391a-4e0d-a451-8f3dac3712bb 0xc004c75ab7 0xc004c75ab8}] [] [{kube-controller-manager Update v1 2023-09-06 11:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89186781-391a-4e0d-a451-8f3dac3712bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8v2nl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8v2nl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.95,StartTime:2023-09-06 11:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cd6f4b56b77a635b75797c26602b3efe9fe9fab79a01009049430d1fcc8168dc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:09:29.982: INFO: Pod "test-cleanup-deployment-7698ff6f6b-xp7gh" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-xp7gh test-cleanup-deployment-7698ff6f6b- deployment-5570  0ec44e9f-b71f-45d9-9a8b-a26dcde7789b 27571 0 2023-09-06 11:09:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 68060a50-b27a-44c8-b512-944ec6c61ede 0xc004c75cd7 0xc004c75cd8}] [] [{kube-controller-manager Update v1 2023-09-06 11:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68060a50-b27a-44c8-b512-944ec6c61ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fb8qj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fb8qj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:29.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5570" for this suite. 09/06/23 11:09:29.999
------------------------------
• [SLOW TEST] [5.175 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:24.846
    Sep  6 11:09:24.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename deployment 09/06/23 11:09:24.847
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:24.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:24.864
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Sep  6 11:09:24.876: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Sep  6 11:09:29.901: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/06/23 11:09:29.902
    Sep  6 11:09:29.902: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/06/23 11:09:29.939
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  6 11:09:29.960: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5570  ed327e7b-20f2-4291-988a-42a01fb00e03 27566 1 2023-09-06 11:09:29 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-06 11:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c74fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  6 11:09:29.974: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5570  68060a50-b27a-44c8-b512-944ec6c61ede 27568 1 2023-09-06 11:09:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ed327e7b-20f2-4291-988a-42a01fb00e03 0xc004c75467 0xc004c75468}] [] [{kube-controller-manager Update apps/v1 2023-09-06 11:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed327e7b-20f2-4291-988a-42a01fb00e03\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c754f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 11:09:29.974: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Sep  6 11:09:29.974: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5570  89186781-391a-4e0d-a451-8f3dac3712bb 27567 1 2023-09-06 11:09:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment ed327e7b-20f2-4291-988a-42a01fb00e03 0xc004c75337 0xc004c75338}] [] [{e2e.test Update apps/v1 2023-09-06 11:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 11:09:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-06 11:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"ed327e7b-20f2-4291-988a-42a01fb00e03\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c753f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 11:09:29.982: INFO: Pod "test-cleanup-controller-trw7s" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-trw7s test-cleanup-controller- deployment-5570  96bef571-0e79-4592-8c91-d0263e2926d3 27558 0 2023-09-06 11:09:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b0f6c55105ed88aeff7667226a719ef1592190fa08c2ebc86c5742d6c879e497 cni.projectcalico.org/podIP:10.233.99.95/32 cni.projectcalico.org/podIPs:10.233.99.95/32] [{apps/v1 ReplicaSet test-cleanup-controller 89186781-391a-4e0d-a451-8f3dac3712bb 0xc004c75ab7 0xc004c75ab8}] [] [{kube-controller-manager Update v1 2023-09-06 11:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89186781-391a-4e0d-a451-8f3dac3712bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8v2nl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8v2nl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.95,StartTime:2023-09-06 11:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cd6f4b56b77a635b75797c26602b3efe9fe9fab79a01009049430d1fcc8168dc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:09:29.982: INFO: Pod "test-cleanup-deployment-7698ff6f6b-xp7gh" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-xp7gh test-cleanup-deployment-7698ff6f6b- deployment-5570  0ec44e9f-b71f-45d9-9a8b-a26dcde7789b 27571 0 2023-09-06 11:09:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 68060a50-b27a-44c8-b512-944ec6c61ede 0xc004c75cd7 0xc004c75cd8}] [] [{kube-controller-manager Update v1 2023-09-06 11:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68060a50-b27a-44c8-b512-944ec6c61ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fb8qj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fb8qj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:29.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5570" for this suite. 09/06/23 11:09:29.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:30.022
Sep  6 11:09:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 11:09:30.025
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:30.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:30.055
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-9ccb4888-b5d3-444b-b2c5-2bf725d0de32 09/06/23 11:09:30.06
STEP: Creating the pod 09/06/23 11:09:30.065
Sep  6 11:09:30.075: INFO: Waiting up to 5m0s for pod "pod-configmaps-85112698-1f28-4f54-bba9-9eddd63893ce" in namespace "configmap-9134" to be "running"
Sep  6 11:09:30.083: INFO: Pod "pod-configmaps-85112698-1f28-4f54-bba9-9eddd63893ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.250336ms
Sep  6 11:09:32.087: INFO: Pod "pod-configmaps-85112698-1f28-4f54-bba9-9eddd63893ce": Phase="Running", Reason="", readiness=false. Elapsed: 2.011651222s
Sep  6 11:09:32.087: INFO: Pod "pod-configmaps-85112698-1f28-4f54-bba9-9eddd63893ce" satisfied condition "running"
STEP: Waiting for pod with text data 09/06/23 11:09:32.087
STEP: Waiting for pod with binary data 09/06/23 11:09:32.096
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:32.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9134" for this suite. 09/06/23 11:09:32.108
------------------------------
• [2.094 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:30.022
    Sep  6 11:09:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 11:09:30.025
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:30.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:30.055
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-9ccb4888-b5d3-444b-b2c5-2bf725d0de32 09/06/23 11:09:30.06
    STEP: Creating the pod 09/06/23 11:09:30.065
    Sep  6 11:09:30.075: INFO: Waiting up to 5m0s for pod "pod-configmaps-85112698-1f28-4f54-bba9-9eddd63893ce" in namespace "configmap-9134" to be "running"
    Sep  6 11:09:30.083: INFO: Pod "pod-configmaps-85112698-1f28-4f54-bba9-9eddd63893ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.250336ms
    Sep  6 11:09:32.087: INFO: Pod "pod-configmaps-85112698-1f28-4f54-bba9-9eddd63893ce": Phase="Running", Reason="", readiness=false. Elapsed: 2.011651222s
    Sep  6 11:09:32.087: INFO: Pod "pod-configmaps-85112698-1f28-4f54-bba9-9eddd63893ce" satisfied condition "running"
    STEP: Waiting for pod with text data 09/06/23 11:09:32.087
    STEP: Waiting for pod with binary data 09/06/23 11:09:32.096
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:32.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9134" for this suite. 09/06/23 11:09:32.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:32.117
Sep  6 11:09:32.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 11:09:32.118
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:32.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:32.136
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-29a3e8d1-c7e8-4c96-a151-fe762f1e0afa 09/06/23 11:09:32.139
STEP: Creating a pod to test consume secrets 09/06/23 11:09:32.144
Sep  6 11:09:32.153: INFO: Waiting up to 5m0s for pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1" in namespace "secrets-35" to be "Succeeded or Failed"
Sep  6 11:09:32.161: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.355674ms
Sep  6 11:09:34.165: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012035222s
Sep  6 11:09:36.196: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04230952s
Sep  6 11:09:38.179: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026100157s
STEP: Saw pod success 09/06/23 11:09:38.18
Sep  6 11:09:38.180: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1" satisfied condition "Succeeded or Failed"
Sep  6 11:09:38.192: INFO: Trying to get logs from node kube-2 pod pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1 container secret-volume-test: <nil>
STEP: delete the pod 09/06/23 11:09:38.225
Sep  6 11:09:38.259: INFO: Waiting for pod pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1 to disappear
Sep  6 11:09:38.267: INFO: Pod pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:38.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-35" for this suite. 09/06/23 11:09:38.269
------------------------------
• [SLOW TEST] [6.159 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:32.117
    Sep  6 11:09:32.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 11:09:32.118
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:32.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:32.136
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-29a3e8d1-c7e8-4c96-a151-fe762f1e0afa 09/06/23 11:09:32.139
    STEP: Creating a pod to test consume secrets 09/06/23 11:09:32.144
    Sep  6 11:09:32.153: INFO: Waiting up to 5m0s for pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1" in namespace "secrets-35" to be "Succeeded or Failed"
    Sep  6 11:09:32.161: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.355674ms
    Sep  6 11:09:34.165: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012035222s
    Sep  6 11:09:36.196: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04230952s
    Sep  6 11:09:38.179: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026100157s
    STEP: Saw pod success 09/06/23 11:09:38.18
    Sep  6 11:09:38.180: INFO: Pod "pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1" satisfied condition "Succeeded or Failed"
    Sep  6 11:09:38.192: INFO: Trying to get logs from node kube-2 pod pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1 container secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 11:09:38.225
    Sep  6 11:09:38.259: INFO: Waiting for pod pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1 to disappear
    Sep  6 11:09:38.267: INFO: Pod pod-secrets-25e60601-072a-4a9c-971e-2e9393ae8db1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:38.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-35" for this suite. 09/06/23 11:09:38.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:38.276
Sep  6 11:09:38.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 11:09:38.277
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:38.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:38.294
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 09/06/23 11:09:38.296
Sep  6 11:09:38.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9511 create -f -'
Sep  6 11:09:38.915: INFO: stderr: ""
Sep  6 11:09:38.915: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 09/06/23 11:09:38.915
Sep  6 11:09:38.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9511 diff -f -'
Sep  6 11:09:39.436: INFO: rc: 1
Sep  6 11:09:39.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9511 delete -f -'
Sep  6 11:09:39.537: INFO: stderr: ""
Sep  6 11:09:39.537: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:39.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9511" for this suite. 09/06/23 11:09:39.545
------------------------------
• [1.290 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:38.276
    Sep  6 11:09:38.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 11:09:38.277
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:38.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:38.294
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 09/06/23 11:09:38.296
    Sep  6 11:09:38.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9511 create -f -'
    Sep  6 11:09:38.915: INFO: stderr: ""
    Sep  6 11:09:38.915: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 09/06/23 11:09:38.915
    Sep  6 11:09:38.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9511 diff -f -'
    Sep  6 11:09:39.436: INFO: rc: 1
    Sep  6 11:09:39.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-9511 delete -f -'
    Sep  6 11:09:39.537: INFO: stderr: ""
    Sep  6 11:09:39.537: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:39.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9511" for this suite. 09/06/23 11:09:39.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:39.566
Sep  6 11:09:39.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename containers 09/06/23 11:09:39.567
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:39.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:39.595
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 09/06/23 11:09:39.597
Sep  6 11:09:39.607: INFO: Waiting up to 5m0s for pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f" in namespace "containers-6991" to be "Succeeded or Failed"
Sep  6 11:09:39.611: INFO: Pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333786ms
Sep  6 11:09:41.624: INFO: Pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016949882s
Sep  6 11:09:43.617: INFO: Pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00958548s
STEP: Saw pod success 09/06/23 11:09:43.617
Sep  6 11:09:43.617: INFO: Pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f" satisfied condition "Succeeded or Failed"
Sep  6 11:09:43.620: INFO: Trying to get logs from node kube-3 pod client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f container agnhost-container: <nil>
STEP: delete the pod 09/06/23 11:09:43.634
Sep  6 11:09:43.656: INFO: Waiting for pod client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f to disappear
Sep  6 11:09:43.659: INFO: Pod client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:43.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6991" for this suite. 09/06/23 11:09:43.663
------------------------------
• [4.105 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:39.566
    Sep  6 11:09:39.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename containers 09/06/23 11:09:39.567
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:39.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:39.595
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 09/06/23 11:09:39.597
    Sep  6 11:09:39.607: INFO: Waiting up to 5m0s for pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f" in namespace "containers-6991" to be "Succeeded or Failed"
    Sep  6 11:09:39.611: INFO: Pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333786ms
    Sep  6 11:09:41.624: INFO: Pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016949882s
    Sep  6 11:09:43.617: INFO: Pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00958548s
    STEP: Saw pod success 09/06/23 11:09:43.617
    Sep  6 11:09:43.617: INFO: Pod "client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f" satisfied condition "Succeeded or Failed"
    Sep  6 11:09:43.620: INFO: Trying to get logs from node kube-3 pod client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 11:09:43.634
    Sep  6 11:09:43.656: INFO: Waiting for pod client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f to disappear
    Sep  6 11:09:43.659: INFO: Pod client-containers-50e0bc56-f7fa-4660-8151-34cf0be1688f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:43.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6991" for this suite. 09/06/23 11:09:43.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:43.672
Sep  6 11:09:43.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename events 09/06/23 11:09:43.673
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:43.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:43.694
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 09/06/23 11:09:43.697
STEP: listing events in all namespaces 09/06/23 11:09:43.702
STEP: listing events in test namespace 09/06/23 11:09:43.706
STEP: listing events with field selection filtering on source 09/06/23 11:09:43.709
STEP: listing events with field selection filtering on reportingController 09/06/23 11:09:43.712
STEP: getting the test event 09/06/23 11:09:43.714
STEP: patching the test event 09/06/23 11:09:43.716
STEP: getting the test event 09/06/23 11:09:43.726
STEP: updating the test event 09/06/23 11:09:43.729
STEP: getting the test event 09/06/23 11:09:43.738
STEP: deleting the test event 09/06/23 11:09:43.74
STEP: listing events in all namespaces 09/06/23 11:09:43.748
STEP: listing events in test namespace 09/06/23 11:09:43.752
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:43.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6310" for this suite. 09/06/23 11:09:43.758
------------------------------
• [0.093 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:43.672
    Sep  6 11:09:43.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename events 09/06/23 11:09:43.673
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:43.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:43.694
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 09/06/23 11:09:43.697
    STEP: listing events in all namespaces 09/06/23 11:09:43.702
    STEP: listing events in test namespace 09/06/23 11:09:43.706
    STEP: listing events with field selection filtering on source 09/06/23 11:09:43.709
    STEP: listing events with field selection filtering on reportingController 09/06/23 11:09:43.712
    STEP: getting the test event 09/06/23 11:09:43.714
    STEP: patching the test event 09/06/23 11:09:43.716
    STEP: getting the test event 09/06/23 11:09:43.726
    STEP: updating the test event 09/06/23 11:09:43.729
    STEP: getting the test event 09/06/23 11:09:43.738
    STEP: deleting the test event 09/06/23 11:09:43.74
    STEP: listing events in all namespaces 09/06/23 11:09:43.748
    STEP: listing events in test namespace 09/06/23 11:09:43.752
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:43.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6310" for this suite. 09/06/23 11:09:43.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:43.767
Sep  6 11:09:43.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:09:43.768
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:43.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:43.786
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:09:43.787
Sep  6 11:09:43.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb" in namespace "projected-8785" to be "Succeeded or Failed"
Sep  6 11:09:43.807: INFO: Pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.731135ms
Sep  6 11:09:45.821: INFO: Pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021993377s
Sep  6 11:09:47.823: INFO: Pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023390534s
STEP: Saw pod success 09/06/23 11:09:47.823
Sep  6 11:09:47.823: INFO: Pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb" satisfied condition "Succeeded or Failed"
Sep  6 11:09:47.834: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb container client-container: <nil>
STEP: delete the pod 09/06/23 11:09:47.854
Sep  6 11:09:47.879: INFO: Waiting for pod downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb to disappear
Sep  6 11:09:47.883: INFO: Pod downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:47.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8785" for this suite. 09/06/23 11:09:47.887
------------------------------
• [4.127 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:43.767
    Sep  6 11:09:43.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:09:43.768
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:43.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:43.786
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:09:43.787
    Sep  6 11:09:43.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb" in namespace "projected-8785" to be "Succeeded or Failed"
    Sep  6 11:09:43.807: INFO: Pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.731135ms
    Sep  6 11:09:45.821: INFO: Pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021993377s
    Sep  6 11:09:47.823: INFO: Pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023390534s
    STEP: Saw pod success 09/06/23 11:09:47.823
    Sep  6 11:09:47.823: INFO: Pod "downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb" satisfied condition "Succeeded or Failed"
    Sep  6 11:09:47.834: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb container client-container: <nil>
    STEP: delete the pod 09/06/23 11:09:47.854
    Sep  6 11:09:47.879: INFO: Waiting for pod downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb to disappear
    Sep  6 11:09:47.883: INFO: Pod downwardapi-volume-242ba7fd-a433-4149-9a9c-28b3dec6d8eb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:47.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8785" for this suite. 09/06/23 11:09:47.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:47.894
Sep  6 11:09:47.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 11:09:47.895
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:47.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:47.913
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-0915eec6-dd2a-4450-b609-37924128a5d1 09/06/23 11:09:47.918
STEP: Creating the pod 09/06/23 11:09:47.922
Sep  6 11:09:47.932: INFO: Waiting up to 5m0s for pod "pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748" in namespace "configmap-695" to be "running and ready"
Sep  6 11:09:47.938: INFO: Pod "pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748": Phase="Pending", Reason="", readiness=false. Elapsed: 5.927727ms
Sep  6 11:09:47.938: INFO: The phase of Pod pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:09:49.951: INFO: Pod "pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748": Phase="Running", Reason="", readiness=true. Elapsed: 2.01929874s
Sep  6 11:09:49.951: INFO: The phase of Pod pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748 is Running (Ready = true)
Sep  6 11:09:49.951: INFO: Pod "pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-0915eec6-dd2a-4450-b609-37924128a5d1 09/06/23 11:09:49.962
STEP: waiting to observe update in volume 09/06/23 11:09:49.969
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:52.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-695" for this suite. 09/06/23 11:09:52.014
------------------------------
• [4.140 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:47.894
    Sep  6 11:09:47.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 11:09:47.895
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:47.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:47.913
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-0915eec6-dd2a-4450-b609-37924128a5d1 09/06/23 11:09:47.918
    STEP: Creating the pod 09/06/23 11:09:47.922
    Sep  6 11:09:47.932: INFO: Waiting up to 5m0s for pod "pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748" in namespace "configmap-695" to be "running and ready"
    Sep  6 11:09:47.938: INFO: Pod "pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748": Phase="Pending", Reason="", readiness=false. Elapsed: 5.927727ms
    Sep  6 11:09:47.938: INFO: The phase of Pod pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:09:49.951: INFO: Pod "pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748": Phase="Running", Reason="", readiness=true. Elapsed: 2.01929874s
    Sep  6 11:09:49.951: INFO: The phase of Pod pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748 is Running (Ready = true)
    Sep  6 11:09:49.951: INFO: Pod "pod-configmaps-b582ee3b-5872-4bac-b6b1-4d0a41530748" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-0915eec6-dd2a-4450-b609-37924128a5d1 09/06/23 11:09:49.962
    STEP: waiting to observe update in volume 09/06/23 11:09:49.969
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:52.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-695" for this suite. 09/06/23 11:09:52.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:52.035
Sep  6 11:09:52.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename var-expansion 09/06/23 11:09:52.036
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:52.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:52.057
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 09/06/23 11:09:52.059
Sep  6 11:09:52.072: INFO: Waiting up to 5m0s for pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c" in namespace "var-expansion-4710" to be "Succeeded or Failed"
Sep  6 11:09:52.076: INFO: Pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000243ms
Sep  6 11:09:54.097: INFO: Pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02580736s
Sep  6 11:09:56.087: INFO: Pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015296038s
STEP: Saw pod success 09/06/23 11:09:56.087
Sep  6 11:09:56.088: INFO: Pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c" satisfied condition "Succeeded or Failed"
Sep  6 11:09:56.094: INFO: Trying to get logs from node kube-3 pod var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c container dapi-container: <nil>
STEP: delete the pod 09/06/23 11:09:56.107
Sep  6 11:09:56.127: INFO: Waiting for pod var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c to disappear
Sep  6 11:09:56.131: INFO: Pod var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4710" for this suite. 09/06/23 11:09:56.134
------------------------------
• [4.106 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:52.035
    Sep  6 11:09:52.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename var-expansion 09/06/23 11:09:52.036
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:52.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:52.057
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 09/06/23 11:09:52.059
    Sep  6 11:09:52.072: INFO: Waiting up to 5m0s for pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c" in namespace "var-expansion-4710" to be "Succeeded or Failed"
    Sep  6 11:09:52.076: INFO: Pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000243ms
    Sep  6 11:09:54.097: INFO: Pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02580736s
    Sep  6 11:09:56.087: INFO: Pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015296038s
    STEP: Saw pod success 09/06/23 11:09:56.087
    Sep  6 11:09:56.088: INFO: Pod "var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c" satisfied condition "Succeeded or Failed"
    Sep  6 11:09:56.094: INFO: Trying to get logs from node kube-3 pod var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c container dapi-container: <nil>
    STEP: delete the pod 09/06/23 11:09:56.107
    Sep  6 11:09:56.127: INFO: Waiting for pod var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c to disappear
    Sep  6 11:09:56.131: INFO: Pod var-expansion-6bb22089-b87d-4e61-8fb5-af41d5cb416c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4710" for this suite. 09/06/23 11:09:56.134
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:56.142
Sep  6 11:09:56.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 11:09:56.143
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:56.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:56.173
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 09/06/23 11:09:56.175
Sep  6 11:09:56.184: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d" in namespace "emptydir-7485" to be "running"
Sep  6 11:09:56.189: INFO: Pod "pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.87237ms
Sep  6 11:09:58.194: INFO: Pod "pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d": Phase="Running", Reason="", readiness=false. Elapsed: 2.009790444s
Sep  6 11:09:58.194: INFO: Pod "pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d" satisfied condition "running"
STEP: Reading file content from the nginx-container 09/06/23 11:09:58.194
Sep  6 11:09:58.194: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7485 PodName:pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:09:58.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:09:58.195: INFO: ExecWithOptions: Clientset creation
Sep  6 11:09:58.195: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-7485/pods/pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Sep  6 11:09:58.252: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 11:09:58.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7485" for this suite. 09/06/23 11:09:58.259
------------------------------
• [2.123 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:56.142
    Sep  6 11:09:56.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 11:09:56.143
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:56.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:56.173
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 09/06/23 11:09:56.175
    Sep  6 11:09:56.184: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d" in namespace "emptydir-7485" to be "running"
    Sep  6 11:09:56.189: INFO: Pod "pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.87237ms
    Sep  6 11:09:58.194: INFO: Pod "pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d": Phase="Running", Reason="", readiness=false. Elapsed: 2.009790444s
    Sep  6 11:09:58.194: INFO: Pod "pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d" satisfied condition "running"
    STEP: Reading file content from the nginx-container 09/06/23 11:09:58.194
    Sep  6 11:09:58.194: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7485 PodName:pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:09:58.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:09:58.195: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:09:58.195: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-7485/pods/pod-sharedvolume-312b1037-53a7-42ed-9f42-ba475efdb74d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Sep  6 11:09:58.252: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:09:58.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7485" for this suite. 09/06/23 11:09:58.259
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:09:58.265
Sep  6 11:09:58.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 11:09:58.266
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:58.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:58.289
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Sep  6 11:09:58.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 create -f -'
Sep  6 11:09:58.427: INFO: stderr: ""
Sep  6 11:09:58.427: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  6 11:09:58.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 create -f -'
Sep  6 11:09:58.607: INFO: stderr: ""
Sep  6 11:09:58.607: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/06/23 11:09:58.607
Sep  6 11:09:59.615: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:09:59.615: INFO: Found 0 / 1
Sep  6 11:10:00.623: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:10:00.623: INFO: Found 1 / 1
Sep  6 11:10:00.623: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 11:10:00.647: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:10:00.647: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 11:10:00.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe pod agnhost-primary-9p7lr'
Sep  6 11:10:00.817: INFO: stderr: ""
Sep  6 11:10:00.817: INFO: stdout: "Name:             agnhost-primary-9p7lr\nNamespace:        kubectl-1475\nPriority:         0\nService Account:  default\nNode:             kube-3/10.2.20.103\nStart Time:       Wed, 06 Sep 2023 11:09:58 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: bddcf4076df85a95fef5134ec642e4bd86eebfeb4eabae16391c1ce3420d4d13\n                  cni.projectcalico.org/podIP: 10.233.99.111/32\n                  cni.projectcalico.org/podIPs: 10.233.99.111/32\nStatus:           Running\nIP:               10.233.99.111\nIPs:\n  IP:           10.233.99.111\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://71ce773fc80c023ce2cc29681cd31e9e8787a081ac895b1bd3e9ce4add7d863c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 06 Sep 2023 11:09:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l9jt5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-l9jt5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1475/agnhost-primary-9p7lr to kube-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep  6 11:10:00.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe rc agnhost-primary'
Sep  6 11:10:00.908: INFO: stderr: ""
Sep  6 11:10:00.908: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1475\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-9p7lr\n"
Sep  6 11:10:00.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe service agnhost-primary'
Sep  6 11:10:00.991: INFO: stderr: ""
Sep  6 11:10:00.991: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1475\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.39.21\nIPs:               10.233.39.21\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.99.111:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  6 11:10:00.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe node kube-1'
Sep  6 11:10:01.074: INFO: stderr: ""
Sep  6 11:10:01.074: INFO: stdout: "Name:               kube-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kube-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.2.20.101/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.120.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 06 Sep 2023 09:52:12 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  kube-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 06 Sep 2023 11:09:55 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 06 Sep 2023 09:55:48 +0000   Wed, 06 Sep 2023 09:55:48 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 06 Sep 2023 11:09:55 +0000   Wed, 06 Sep 2023 09:52:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 06 Sep 2023 11:09:55 +0000   Wed, 06 Sep 2023 09:52:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 06 Sep 2023 11:09:55 +0000   Wed, 06 Sep 2023 09:52:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 06 Sep 2023 11:09:55 +0000   Wed, 06 Sep 2023 09:57:01 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.2.20.101\n  Hostname:    kube-1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  129064092Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4025600Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  118945466991\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3923200Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 843f623a384e4c269967d7f5f540437b\n  System UUID:                843f623a-384e-4c26-9967-d7f5f540437b\n  Boot ID:                    d83c0092-1927-49e3-a118-da846e105727\n  Kernel Version:             5.4.0-146-generic\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.1\n  Kubelet Version:            v1.26.5\n  Kube-Proxy Version:         v1.26.5\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-6dfcdfb99-6q4ng                    30m (0%)      1 (25%)     64M (1%)         256M (6%)      74m\n  kube-system                 calico-node-pkqgc                                          150m (3%)     300m (7%)   64M (1%)         500M (12%)     75m\n  kube-system                 coredns-645b46f4b6-hq55k                                   100m (2%)     0 (0%)      70Mi (1%)        300Mi (7%)     74m\n  kube-system                 kube-apiserver-kube-1                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         77m\n  kube-system                 kube-controller-manager-kube-1                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         77m\n  kube-system                 kube-proxy-fjqk6                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 kube-scheduler-kube-1                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         77m\n  kube-system                 nodelocaldns-74qn2                                         100m (2%)     0 (0%)      70Mi (1%)        200Mi (5%)     74m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                930m (23%)      1300m (32%)\n  memory             274800640 (6%)  1280288k (31%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  RegisteredNode  24m   node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal  RegisteredNode  100s  node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n"
Sep  6 11:10:01.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe namespace kubectl-1475'
Sep  6 11:10:01.144: INFO: stderr: ""
Sep  6 11:10:01.144: INFO: stdout: "Name:         kubectl-1475\nLabels:       e2e-framework=kubectl\n              e2e-run=4a83d354-710a-403a-981f-71e69f687fbc\n              kubernetes.io/metadata.name=kubectl-1475\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 11:10:01.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1475" for this suite. 09/06/23 11:10:01.149
------------------------------
• [2.896 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:09:58.265
    Sep  6 11:09:58.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 11:09:58.266
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:09:58.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:09:58.289
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Sep  6 11:09:58.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 create -f -'
    Sep  6 11:09:58.427: INFO: stderr: ""
    Sep  6 11:09:58.427: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Sep  6 11:09:58.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 create -f -'
    Sep  6 11:09:58.607: INFO: stderr: ""
    Sep  6 11:09:58.607: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/06/23 11:09:58.607
    Sep  6 11:09:59.615: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 11:09:59.615: INFO: Found 0 / 1
    Sep  6 11:10:00.623: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 11:10:00.623: INFO: Found 1 / 1
    Sep  6 11:10:00.623: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  6 11:10:00.647: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 11:10:00.647: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  6 11:10:00.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe pod agnhost-primary-9p7lr'
    Sep  6 11:10:00.817: INFO: stderr: ""
    Sep  6 11:10:00.817: INFO: stdout: "Name:             agnhost-primary-9p7lr\nNamespace:        kubectl-1475\nPriority:         0\nService Account:  default\nNode:             kube-3/10.2.20.103\nStart Time:       Wed, 06 Sep 2023 11:09:58 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: bddcf4076df85a95fef5134ec642e4bd86eebfeb4eabae16391c1ce3420d4d13\n                  cni.projectcalico.org/podIP: 10.233.99.111/32\n                  cni.projectcalico.org/podIPs: 10.233.99.111/32\nStatus:           Running\nIP:               10.233.99.111\nIPs:\n  IP:           10.233.99.111\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://71ce773fc80c023ce2cc29681cd31e9e8787a081ac895b1bd3e9ce4add7d863c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 06 Sep 2023 11:09:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l9jt5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-l9jt5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1475/agnhost-primary-9p7lr to kube-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Sep  6 11:10:00.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe rc agnhost-primary'
    Sep  6 11:10:00.908: INFO: stderr: ""
    Sep  6 11:10:00.908: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1475\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-9p7lr\n"
    Sep  6 11:10:00.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe service agnhost-primary'
    Sep  6 11:10:00.991: INFO: stderr: ""
    Sep  6 11:10:00.991: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1475\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.39.21\nIPs:               10.233.39.21\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.99.111:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Sep  6 11:10:00.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe node kube-1'
    Sep  6 11:10:01.074: INFO: stderr: ""
    Sep  6 11:10:01.074: INFO: stdout: "Name:               kube-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kube-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.2.20.101/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.120.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 06 Sep 2023 09:52:12 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  kube-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 06 Sep 2023 11:09:55 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 06 Sep 2023 09:55:48 +0000   Wed, 06 Sep 2023 09:55:48 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 06 Sep 2023 11:09:55 +0000   Wed, 06 Sep 2023 09:52:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 06 Sep 2023 11:09:55 +0000   Wed, 06 Sep 2023 09:52:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 06 Sep 2023 11:09:55 +0000   Wed, 06 Sep 2023 09:52:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 06 Sep 2023 11:09:55 +0000   Wed, 06 Sep 2023 09:57:01 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.2.20.101\n  Hostname:    kube-1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  129064092Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4025600Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  118945466991\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3923200Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 843f623a384e4c269967d7f5f540437b\n  System UUID:                843f623a-384e-4c26-9967-d7f5f540437b\n  Boot ID:                    d83c0092-1927-49e3-a118-da846e105727\n  Kernel Version:             5.4.0-146-generic\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.1\n  Kubelet Version:            v1.26.5\n  Kube-Proxy Version:         v1.26.5\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-6dfcdfb99-6q4ng                    30m (0%)      1 (25%)     64M (1%)         256M (6%)      74m\n  kube-system                 calico-node-pkqgc                                          150m (3%)     300m (7%)   64M (1%)         500M (12%)     75m\n  kube-system                 coredns-645b46f4b6-hq55k                                   100m (2%)     0 (0%)      70Mi (1%)        300Mi (7%)     74m\n  kube-system                 kube-apiserver-kube-1                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         77m\n  kube-system                 kube-controller-manager-kube-1                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         77m\n  kube-system                 kube-proxy-fjqk6                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 kube-scheduler-kube-1                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         77m\n  kube-system                 nodelocaldns-74qn2                                         100m (2%)     0 (0%)      70Mi (1%)        200Mi (5%)     74m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                930m (23%)      1300m (32%)\n  memory             274800640 (6%)  1280288k (31%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  RegisteredNode  24m   node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal  RegisteredNode  100s  node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n"
    Sep  6 11:10:01.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-1475 describe namespace kubectl-1475'
    Sep  6 11:10:01.144: INFO: stderr: ""
    Sep  6 11:10:01.144: INFO: stdout: "Name:         kubectl-1475\nLabels:       e2e-framework=kubectl\n              e2e-run=4a83d354-710a-403a-981f-71e69f687fbc\n              kubernetes.io/metadata.name=kubectl-1475\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:10:01.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1475" for this suite. 09/06/23 11:10:01.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:10:01.167
Sep  6 11:10:01.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:10:01.167
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:01.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:01.188
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-563afd79-2bf9-4dd6-876a-7dc6f9e6cc2c 09/06/23 11:10:01.191
STEP: Creating a pod to test consume configMaps 09/06/23 11:10:01.196
Sep  6 11:10:01.210: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285" in namespace "projected-218" to be "Succeeded or Failed"
Sep  6 11:10:01.214: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8951ms
Sep  6 11:10:03.304: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094509504s
Sep  6 11:10:05.219: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Running", Reason="", readiness=false. Elapsed: 4.008904061s
Sep  6 11:10:07.224: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Running", Reason="", readiness=false. Elapsed: 6.014523622s
Sep  6 11:10:09.226: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016664479s
STEP: Saw pod success 09/06/23 11:10:09.227
Sep  6 11:10:09.227: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285" satisfied condition "Succeeded or Failed"
Sep  6 11:10:09.244: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285 container agnhost-container: <nil>
STEP: delete the pod 09/06/23 11:10:09.267
Sep  6 11:10:09.284: INFO: Waiting for pod pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285 to disappear
Sep  6 11:10:09.288: INFO: Pod pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:10:09.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-218" for this suite. 09/06/23 11:10:09.292
------------------------------
• [SLOW TEST] [8.132 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:10:01.167
    Sep  6 11:10:01.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:10:01.167
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:01.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:01.188
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-563afd79-2bf9-4dd6-876a-7dc6f9e6cc2c 09/06/23 11:10:01.191
    STEP: Creating a pod to test consume configMaps 09/06/23 11:10:01.196
    Sep  6 11:10:01.210: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285" in namespace "projected-218" to be "Succeeded or Failed"
    Sep  6 11:10:01.214: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8951ms
    Sep  6 11:10:03.304: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094509504s
    Sep  6 11:10:05.219: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Running", Reason="", readiness=false. Elapsed: 4.008904061s
    Sep  6 11:10:07.224: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Running", Reason="", readiness=false. Elapsed: 6.014523622s
    Sep  6 11:10:09.226: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016664479s
    STEP: Saw pod success 09/06/23 11:10:09.227
    Sep  6 11:10:09.227: INFO: Pod "pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285" satisfied condition "Succeeded or Failed"
    Sep  6 11:10:09.244: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285 container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 11:10:09.267
    Sep  6 11:10:09.284: INFO: Waiting for pod pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285 to disappear
    Sep  6 11:10:09.288: INFO: Pod pod-projected-configmaps-7219f813-ec97-44a6-85c4-a500ae54d285 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:10:09.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-218" for this suite. 09/06/23 11:10:09.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:10:09.299
Sep  6 11:10:09.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:10:09.3
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:09.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:09.322
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 09/06/23 11:10:09.323
Sep  6 11:10:09.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: rename a version 09/06/23 11:10:18.322
STEP: check the new version name is served 09/06/23 11:10:18.342
STEP: check the old version name is removed 09/06/23 11:10:20.026
STEP: check the other version is not changed 09/06/23 11:10:20.797
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:10:23.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-124" for this suite. 09/06/23 11:10:23.809
------------------------------
• [SLOW TEST] [14.518 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:10:09.299
    Sep  6 11:10:09.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:10:09.3
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:09.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:09.322
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 09/06/23 11:10:09.323
    Sep  6 11:10:09.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: rename a version 09/06/23 11:10:18.322
    STEP: check the new version name is served 09/06/23 11:10:18.342
    STEP: check the old version name is removed 09/06/23 11:10:20.026
    STEP: check the other version is not changed 09/06/23 11:10:20.797
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:10:23.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-124" for this suite. 09/06/23 11:10:23.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:10:23.819
Sep  6 11:10:23.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 11:10:23.82
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:23.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:23.842
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 11:10:23.857
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:10:24.117
STEP: Deploying the webhook pod 09/06/23 11:10:24.125
STEP: Wait for the deployment to be ready 09/06/23 11:10:24.146
Sep  6 11:10:24.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 11:10:26.2
STEP: Verifying the service has paired with the endpoint 09/06/23 11:10:26.219
Sep  6 11:10:27.220: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/06/23 11:10:27.235
STEP: create a pod that should be updated by the webhook 09/06/23 11:10:27.26
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:10:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4267" for this suite. 09/06/23 11:10:27.373
STEP: Destroying namespace "webhook-4267-markers" for this suite. 09/06/23 11:10:27.394
------------------------------
• [3.594 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:10:23.819
    Sep  6 11:10:23.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 11:10:23.82
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:23.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:23.842
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 11:10:23.857
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:10:24.117
    STEP: Deploying the webhook pod 09/06/23 11:10:24.125
    STEP: Wait for the deployment to be ready 09/06/23 11:10:24.146
    Sep  6 11:10:24.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 11:10:26.2
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:10:26.219
    Sep  6 11:10:27.220: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/06/23 11:10:27.235
    STEP: create a pod that should be updated by the webhook 09/06/23 11:10:27.26
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:10:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4267" for this suite. 09/06/23 11:10:27.373
    STEP: Destroying namespace "webhook-4267-markers" for this suite. 09/06/23 11:10:27.394
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:10:27.413
Sep  6 11:10:27.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 11:10:27.414
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:27.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:27.46
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 11:10:27.484
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:10:28.051
STEP: Deploying the webhook pod 09/06/23 11:10:28.056
STEP: Wait for the deployment to be ready 09/06/23 11:10:28.072
Sep  6 11:10:28.082: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 11:10:30.12
STEP: Verifying the service has paired with the endpoint 09/06/23 11:10:30.158
Sep  6 11:10:31.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Sep  6 11:10:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-133-crds.webhook.example.com via the AdmissionRegistration API 09/06/23 11:10:36.69
STEP: Creating a custom resource that should be mutated by the webhook 09/06/23 11:10:36.707
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:10:39.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2594" for this suite. 09/06/23 11:10:39.422
STEP: Destroying namespace "webhook-2594-markers" for this suite. 09/06/23 11:10:39.441
------------------------------
• [SLOW TEST] [12.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:10:27.413
    Sep  6 11:10:27.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 11:10:27.414
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:27.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:27.46
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 11:10:27.484
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:10:28.051
    STEP: Deploying the webhook pod 09/06/23 11:10:28.056
    STEP: Wait for the deployment to be ready 09/06/23 11:10:28.072
    Sep  6 11:10:28.082: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 11:10:30.12
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:10:30.158
    Sep  6 11:10:31.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Sep  6 11:10:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-133-crds.webhook.example.com via the AdmissionRegistration API 09/06/23 11:10:36.69
    STEP: Creating a custom resource that should be mutated by the webhook 09/06/23 11:10:36.707
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:10:39.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2594" for this suite. 09/06/23 11:10:39.422
    STEP: Destroying namespace "webhook-2594-markers" for this suite. 09/06/23 11:10:39.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:10:39.473
Sep  6 11:10:39.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 11:10:39.474
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:39.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:39.548
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 09/06/23 11:10:39.553
Sep  6 11:10:39.573: INFO: Waiting up to 5m0s for pod "pod-7sjng" in namespace "pods-8713" to be "running"
Sep  6 11:10:39.583: INFO: Pod "pod-7sjng": Phase="Pending", Reason="", readiness=false. Elapsed: 9.401941ms
Sep  6 11:10:41.598: INFO: Pod "pod-7sjng": Phase="Running", Reason="", readiness=true. Elapsed: 2.024583415s
Sep  6 11:10:41.598: INFO: Pod "pod-7sjng" satisfied condition "running"
STEP: patching /status 09/06/23 11:10:41.598
Sep  6 11:10:41.640: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 11:10:41.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8713" for this suite. 09/06/23 11:10:41.648
------------------------------
• [2.197 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:10:39.473
    Sep  6 11:10:39.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 11:10:39.474
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:39.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:39.548
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 09/06/23 11:10:39.553
    Sep  6 11:10:39.573: INFO: Waiting up to 5m0s for pod "pod-7sjng" in namespace "pods-8713" to be "running"
    Sep  6 11:10:39.583: INFO: Pod "pod-7sjng": Phase="Pending", Reason="", readiness=false. Elapsed: 9.401941ms
    Sep  6 11:10:41.598: INFO: Pod "pod-7sjng": Phase="Running", Reason="", readiness=true. Elapsed: 2.024583415s
    Sep  6 11:10:41.598: INFO: Pod "pod-7sjng" satisfied condition "running"
    STEP: patching /status 09/06/23 11:10:41.598
    Sep  6 11:10:41.640: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:10:41.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8713" for this suite. 09/06/23 11:10:41.648
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:10:41.672
Sep  6 11:10:41.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:10:41.673
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:41.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:41.715
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6509 09/06/23 11:10:41.752
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/06/23 11:10:41.823
STEP: creating service externalsvc in namespace services-6509 09/06/23 11:10:41.823
STEP: creating replication controller externalsvc in namespace services-6509 09/06/23 11:10:41.845
I0906 11:10:41.855874      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6509, replica count: 2
I0906 11:10:44.907325      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 09/06/23 11:10:44.91
Sep  6 11:10:44.926: INFO: Creating new exec pod
Sep  6 11:10:44.943: INFO: Waiting up to 5m0s for pod "execpodk964k" in namespace "services-6509" to be "running"
Sep  6 11:10:44.949: INFO: Pod "execpodk964k": Phase="Pending", Reason="", readiness=false. Elapsed: 5.992201ms
Sep  6 11:10:46.957: INFO: Pod "execpodk964k": Phase="Running", Reason="", readiness=true. Elapsed: 2.014145132s
Sep  6 11:10:46.957: INFO: Pod "execpodk964k" satisfied condition "running"
Sep  6 11:10:46.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-6509 exec execpodk964k -- /bin/sh -x -c nslookup clusterip-service.services-6509.svc.cluster.local'
Sep  6 11:10:47.134: INFO: stderr: "+ nslookup clusterip-service.services-6509.svc.cluster.local\n"
Sep  6 11:10:47.134: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-6509.svc.cluster.local\tcanonical name = externalsvc.services-6509.svc.cluster.local.\nName:\texternalsvc.services-6509.svc.cluster.local\nAddress: 10.233.52.219\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6509, will wait for the garbage collector to delete the pods 09/06/23 11:10:47.134
Sep  6 11:10:47.198: INFO: Deleting ReplicationController externalsvc took: 8.379669ms
Sep  6 11:10:47.299: INFO: Terminating ReplicationController externalsvc pods took: 101.161466ms
Sep  6 11:10:50.028: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:10:50.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6509" for this suite. 09/06/23 11:10:50.178
------------------------------
• [SLOW TEST] [8.550 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:10:41.672
    Sep  6 11:10:41.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:10:41.673
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:41.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:41.715
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6509 09/06/23 11:10:41.752
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/06/23 11:10:41.823
    STEP: creating service externalsvc in namespace services-6509 09/06/23 11:10:41.823
    STEP: creating replication controller externalsvc in namespace services-6509 09/06/23 11:10:41.845
    I0906 11:10:41.855874      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6509, replica count: 2
    I0906 11:10:44.907325      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 09/06/23 11:10:44.91
    Sep  6 11:10:44.926: INFO: Creating new exec pod
    Sep  6 11:10:44.943: INFO: Waiting up to 5m0s for pod "execpodk964k" in namespace "services-6509" to be "running"
    Sep  6 11:10:44.949: INFO: Pod "execpodk964k": Phase="Pending", Reason="", readiness=false. Elapsed: 5.992201ms
    Sep  6 11:10:46.957: INFO: Pod "execpodk964k": Phase="Running", Reason="", readiness=true. Elapsed: 2.014145132s
    Sep  6 11:10:46.957: INFO: Pod "execpodk964k" satisfied condition "running"
    Sep  6 11:10:46.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-6509 exec execpodk964k -- /bin/sh -x -c nslookup clusterip-service.services-6509.svc.cluster.local'
    Sep  6 11:10:47.134: INFO: stderr: "+ nslookup clusterip-service.services-6509.svc.cluster.local\n"
    Sep  6 11:10:47.134: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-6509.svc.cluster.local\tcanonical name = externalsvc.services-6509.svc.cluster.local.\nName:\texternalsvc.services-6509.svc.cluster.local\nAddress: 10.233.52.219\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6509, will wait for the garbage collector to delete the pods 09/06/23 11:10:47.134
    Sep  6 11:10:47.198: INFO: Deleting ReplicationController externalsvc took: 8.379669ms
    Sep  6 11:10:47.299: INFO: Terminating ReplicationController externalsvc pods took: 101.161466ms
    Sep  6 11:10:50.028: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:10:50.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6509" for this suite. 09/06/23 11:10:50.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:10:50.223
Sep  6 11:10:50.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:10:50.224
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:50.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:50.332
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 09/06/23 11:10:50.334
Sep  6 11:10:50.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: mark a version not serverd 09/06/23 11:10:59.393
STEP: check the unserved version gets removed 09/06/23 11:10:59.418
STEP: check the other version is not changed 09/06/23 11:11:01.045
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:04.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1137" for this suite. 09/06/23 11:11:04.058
------------------------------
• [SLOW TEST] [13.844 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:10:50.223
    Sep  6 11:10:50.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:10:50.224
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:10:50.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:10:50.332
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 09/06/23 11:10:50.334
    Sep  6 11:10:50.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: mark a version not serverd 09/06/23 11:10:59.393
    STEP: check the unserved version gets removed 09/06/23 11:10:59.418
    STEP: check the other version is not changed 09/06/23 11:11:01.045
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:04.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1137" for this suite. 09/06/23 11:11:04.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:04.069
Sep  6 11:11:04.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:11:04.07
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:04.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:04.1
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:11:04.103
Sep  6 11:11:04.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb" in namespace "projected-7513" to be "Succeeded or Failed"
Sep  6 11:11:04.114: INFO: Pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.837602ms
Sep  6 11:11:06.131: INFO: Pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021473816s
Sep  6 11:11:08.130: INFO: Pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020598708s
STEP: Saw pod success 09/06/23 11:11:08.131
Sep  6 11:11:08.132: INFO: Pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb" satisfied condition "Succeeded or Failed"
Sep  6 11:11:08.144: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb container client-container: <nil>
STEP: delete the pod 09/06/23 11:11:08.188
Sep  6 11:11:08.213: INFO: Waiting for pod downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb to disappear
Sep  6 11:11:08.217: INFO: Pod downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:08.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7513" for this suite. 09/06/23 11:11:08.222
------------------------------
• [4.160 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:04.069
    Sep  6 11:11:04.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:11:04.07
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:04.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:04.1
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:11:04.103
    Sep  6 11:11:04.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb" in namespace "projected-7513" to be "Succeeded or Failed"
    Sep  6 11:11:04.114: INFO: Pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.837602ms
    Sep  6 11:11:06.131: INFO: Pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021473816s
    Sep  6 11:11:08.130: INFO: Pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020598708s
    STEP: Saw pod success 09/06/23 11:11:08.131
    Sep  6 11:11:08.132: INFO: Pod "downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb" satisfied condition "Succeeded or Failed"
    Sep  6 11:11:08.144: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb container client-container: <nil>
    STEP: delete the pod 09/06/23 11:11:08.188
    Sep  6 11:11:08.213: INFO: Waiting for pod downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb to disappear
    Sep  6 11:11:08.217: INFO: Pod downwardapi-volume-4d98c530-1546-4563-bf8f-fff9c448e3fb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:08.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7513" for this suite. 09/06/23 11:11:08.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:08.231
Sep  6 11:11:08.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename deployment 09/06/23 11:11:08.232
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:08.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:08.257
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 09/06/23 11:11:08.262
STEP: waiting for Deployment to be created 09/06/23 11:11:08.267
STEP: waiting for all Replicas to be Ready 09/06/23 11:11:08.269
Sep  6 11:11:08.270: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 11:11:08.270: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 11:11:08.283: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 11:11:08.283: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 11:11:08.386: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 11:11:08.386: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 11:11:08.424: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 11:11:08.424: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 11:11:09.542: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  6 11:11:09.542: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  6 11:11:09.886: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 09/06/23 11:11:09.886
W0906 11:11:09.898307      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  6 11:11:09.909: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 09/06/23 11:11:09.909
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:09.921: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:09.921: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:09.947: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:09.947: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:09.988: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:09.988: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:09.998: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:09.998: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:11.556: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:11.557: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:11.593: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
STEP: listing Deployments 09/06/23 11:11:11.593
Sep  6 11:11:11.597: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 09/06/23 11:11:11.597
Sep  6 11:11:11.610: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 09/06/23 11:11:11.61
Sep  6 11:11:11.635: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:11.656: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:11.710: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:11.747: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:11.758: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:12.682: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:13.587: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:13.651: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:13.668: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 11:11:17.554: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 09/06/23 11:11:17.93
STEP: fetching the DeploymentStatus 09/06/23 11:11:17.936
Sep  6 11:11:17.975: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:17.975: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 3
Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
Sep  6 11:11:17.977: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 3
STEP: deleting the Deployment 09/06/23 11:11:17.977
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
Sep  6 11:11:18.216: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  6 11:11:18.265: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:18.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8845" for this suite. 09/06/23 11:11:18.521
------------------------------
• [SLOW TEST] [10.460 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:08.231
    Sep  6 11:11:08.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename deployment 09/06/23 11:11:08.232
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:08.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:08.257
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 09/06/23 11:11:08.262
    STEP: waiting for Deployment to be created 09/06/23 11:11:08.267
    STEP: waiting for all Replicas to be Ready 09/06/23 11:11:08.269
    Sep  6 11:11:08.270: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  6 11:11:08.270: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  6 11:11:08.283: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  6 11:11:08.283: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  6 11:11:08.386: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  6 11:11:08.386: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  6 11:11:08.424: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  6 11:11:08.424: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  6 11:11:09.542: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  6 11:11:09.542: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  6 11:11:09.886: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 09/06/23 11:11:09.886
    W0906 11:11:09.898307      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  6 11:11:09.909: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 09/06/23 11:11:09.909
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 0
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:09.914: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:09.921: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:09.921: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:09.947: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:09.947: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:09.988: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:09.988: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:09.998: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:09.998: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:11.556: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:11.557: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:11.593: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    STEP: listing Deployments 09/06/23 11:11:11.593
    Sep  6 11:11:11.597: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 09/06/23 11:11:11.597
    Sep  6 11:11:11.610: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 09/06/23 11:11:11.61
    Sep  6 11:11:11.635: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:11.656: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:11.710: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:11.747: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:11.758: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:12.682: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:13.587: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:13.651: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:13.668: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  6 11:11:17.554: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 09/06/23 11:11:17.93
    STEP: fetching the DeploymentStatus 09/06/23 11:11:17.936
    Sep  6 11:11:17.975: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:17.975: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 1
    Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 3
    Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:17.976: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 2
    Sep  6 11:11:17.977: INFO: observed Deployment test-deployment in namespace deployment-8845 with ReadyReplicas 3
    STEP: deleting the Deployment 09/06/23 11:11:17.977
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    Sep  6 11:11:18.216: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  6 11:11:18.265: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:18.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8845" for this suite. 09/06/23 11:11:18.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:18.695
Sep  6 11:11:18.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 11:11:18.696
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:18.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:18.817
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 09/06/23 11:11:18.823
STEP: Creating a ResourceQuota 09/06/23 11:11:23.828
STEP: Ensuring resource quota status is calculated 09/06/23 11:11:23.834
STEP: Creating a Service 09/06/23 11:11:25.839
STEP: Creating a NodePort Service 09/06/23 11:11:25.863
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/06/23 11:11:25.901
STEP: Ensuring resource quota status captures service creation 09/06/23 11:11:25.938
STEP: Deleting Services 09/06/23 11:11:27.945
STEP: Ensuring resource quota status released usage 09/06/23 11:11:28.009
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:30.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6745" for this suite. 09/06/23 11:11:30.034
------------------------------
• [SLOW TEST] [11.356 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:18.695
    Sep  6 11:11:18.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 11:11:18.696
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:18.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:18.817
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 09/06/23 11:11:18.823
    STEP: Creating a ResourceQuota 09/06/23 11:11:23.828
    STEP: Ensuring resource quota status is calculated 09/06/23 11:11:23.834
    STEP: Creating a Service 09/06/23 11:11:25.839
    STEP: Creating a NodePort Service 09/06/23 11:11:25.863
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/06/23 11:11:25.901
    STEP: Ensuring resource quota status captures service creation 09/06/23 11:11:25.938
    STEP: Deleting Services 09/06/23 11:11:27.945
    STEP: Ensuring resource quota status released usage 09/06/23 11:11:28.009
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:30.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6745" for this suite. 09/06/23 11:11:30.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:30.056
Sep  6 11:11:30.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-lifecycle-hook 09/06/23 11:11:30.058
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:30.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:30.083
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/06/23 11:11:30.089
Sep  6 11:11:30.100: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9254" to be "running and ready"
Sep  6 11:11:30.109: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.450068ms
Sep  6 11:11:30.109: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:11:32.837: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.737249406s
Sep  6 11:11:32.837: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:11:34.122: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022539082s
Sep  6 11:11:34.122: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:11:36.123: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.023891103s
Sep  6 11:11:36.124: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  6 11:11:36.124: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 09/06/23 11:11:36.136
Sep  6 11:11:36.173: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9254" to be "running and ready"
Sep  6 11:11:36.231: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 58.31293ms
Sep  6 11:11:36.231: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:11:38.245: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.072098758s
Sep  6 11:11:38.246: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Sep  6 11:11:38.246: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/06/23 11:11:38.256
STEP: delete the pod with lifecycle hook 09/06/23 11:11:38.274
Sep  6 11:11:38.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 11:11:38.293: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 11:11:40.293: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 11:11:40.300: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 11:11:42.293: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 11:11:42.298: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:42.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9254" for this suite. 09/06/23 11:11:42.304
------------------------------
• [SLOW TEST] [12.257 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:30.056
    Sep  6 11:11:30.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/06/23 11:11:30.058
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:30.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:30.083
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/06/23 11:11:30.089
    Sep  6 11:11:30.100: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9254" to be "running and ready"
    Sep  6 11:11:30.109: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.450068ms
    Sep  6 11:11:30.109: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:11:32.837: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.737249406s
    Sep  6 11:11:32.837: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:11:34.122: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022539082s
    Sep  6 11:11:34.122: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:11:36.123: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.023891103s
    Sep  6 11:11:36.124: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  6 11:11:36.124: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 09/06/23 11:11:36.136
    Sep  6 11:11:36.173: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9254" to be "running and ready"
    Sep  6 11:11:36.231: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 58.31293ms
    Sep  6 11:11:36.231: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:11:38.245: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.072098758s
    Sep  6 11:11:38.246: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Sep  6 11:11:38.246: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/06/23 11:11:38.256
    STEP: delete the pod with lifecycle hook 09/06/23 11:11:38.274
    Sep  6 11:11:38.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  6 11:11:38.293: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  6 11:11:40.293: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  6 11:11:40.300: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  6 11:11:42.293: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  6 11:11:42.298: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:42.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9254" for this suite. 09/06/23 11:11:42.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:42.313
Sep  6 11:11:42.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 11:11:42.315
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:42.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:42.34
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Sep  6 11:11:42.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:47.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8081" for this suite. 09/06/23 11:11:47.903
------------------------------
• [SLOW TEST] [5.603 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:42.313
    Sep  6 11:11:42.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 11:11:42.315
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:42.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:42.34
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Sep  6 11:11:42.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:47.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8081" for this suite. 09/06/23 11:11:47.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:47.921
Sep  6 11:11:47.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 11:11:47.922
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:47.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:47.953
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 09/06/23 11:11:47.955
Sep  6 11:11:47.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2602 create -f -'
Sep  6 11:11:48.675: INFO: stderr: ""
Sep  6 11:11:48.675: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/06/23 11:11:48.675
Sep  6 11:11:49.683: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:11:49.683: INFO: Found 0 / 1
Sep  6 11:11:50.680: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:11:50.680: INFO: Found 1 / 1
Sep  6 11:11:50.680: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 09/06/23 11:11:50.68
Sep  6 11:11:50.684: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:11:50.684: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 11:11:50.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2602 patch pod agnhost-primary-9tj2b -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  6 11:11:50.756: INFO: stderr: ""
Sep  6 11:11:50.756: INFO: stdout: "pod/agnhost-primary-9tj2b patched\n"
STEP: checking annotations 09/06/23 11:11:50.756
Sep  6 11:11:50.760: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:11:50.760: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:50.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2602" for this suite. 09/06/23 11:11:50.763
------------------------------
• [2.850 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:47.921
    Sep  6 11:11:47.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 11:11:47.922
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:47.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:47.953
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 09/06/23 11:11:47.955
    Sep  6 11:11:47.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2602 create -f -'
    Sep  6 11:11:48.675: INFO: stderr: ""
    Sep  6 11:11:48.675: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/06/23 11:11:48.675
    Sep  6 11:11:49.683: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 11:11:49.683: INFO: Found 0 / 1
    Sep  6 11:11:50.680: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 11:11:50.680: INFO: Found 1 / 1
    Sep  6 11:11:50.680: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 09/06/23 11:11:50.68
    Sep  6 11:11:50.684: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 11:11:50.684: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  6 11:11:50.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2602 patch pod agnhost-primary-9tj2b -p {"metadata":{"annotations":{"x":"y"}}}'
    Sep  6 11:11:50.756: INFO: stderr: ""
    Sep  6 11:11:50.756: INFO: stdout: "pod/agnhost-primary-9tj2b patched\n"
    STEP: checking annotations 09/06/23 11:11:50.756
    Sep  6 11:11:50.760: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  6 11:11:50.760: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:50.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2602" for this suite. 09/06/23 11:11:50.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:50.772
Sep  6 11:11:50.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 11:11:50.773
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:50.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:50.794
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:11:50.796
Sep  6 11:11:50.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7" in namespace "downward-api-1601" to be "Succeeded or Failed"
Sep  6 11:11:50.809: INFO: Pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.145483ms
Sep  6 11:11:52.825: INFO: Pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021018813s
Sep  6 11:11:54.824: INFO: Pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020592709s
STEP: Saw pod success 09/06/23 11:11:54.824
Sep  6 11:11:54.825: INFO: Pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7" satisfied condition "Succeeded or Failed"
Sep  6 11:11:54.839: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7 container client-container: <nil>
STEP: delete the pod 09/06/23 11:11:54.86
Sep  6 11:11:54.912: INFO: Waiting for pod downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7 to disappear
Sep  6 11:11:54.919: INFO: Pod downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:54.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1601" for this suite. 09/06/23 11:11:54.923
------------------------------
• [4.163 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:50.772
    Sep  6 11:11:50.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 11:11:50.773
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:50.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:50.794
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:11:50.796
    Sep  6 11:11:50.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7" in namespace "downward-api-1601" to be "Succeeded or Failed"
    Sep  6 11:11:50.809: INFO: Pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.145483ms
    Sep  6 11:11:52.825: INFO: Pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021018813s
    Sep  6 11:11:54.824: INFO: Pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020592709s
    STEP: Saw pod success 09/06/23 11:11:54.824
    Sep  6 11:11:54.825: INFO: Pod "downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7" satisfied condition "Succeeded or Failed"
    Sep  6 11:11:54.839: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7 container client-container: <nil>
    STEP: delete the pod 09/06/23 11:11:54.86
    Sep  6 11:11:54.912: INFO: Waiting for pod downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7 to disappear
    Sep  6 11:11:54.919: INFO: Pod downwardapi-volume-37d8b4a0-e456-4778-ad7d-b960b6d779d7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:54.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1601" for this suite. 09/06/23 11:11:54.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:54.935
Sep  6 11:11:54.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename podtemplate 09/06/23 11:11:54.936
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:54.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:54.961
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 09/06/23 11:11:54.963
Sep  6 11:11:54.969: INFO: created test-podtemplate-1
Sep  6 11:11:54.974: INFO: created test-podtemplate-2
Sep  6 11:11:54.979: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 09/06/23 11:11:54.979
STEP: delete collection of pod templates 09/06/23 11:11:54.982
Sep  6 11:11:54.982: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 09/06/23 11:11:54.999
Sep  6 11:11:54.999: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  6 11:11:55.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-919" for this suite. 09/06/23 11:11:55.004
------------------------------
• [0.075 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:54.935
    Sep  6 11:11:54.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename podtemplate 09/06/23 11:11:54.936
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:54.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:54.961
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 09/06/23 11:11:54.963
    Sep  6 11:11:54.969: INFO: created test-podtemplate-1
    Sep  6 11:11:54.974: INFO: created test-podtemplate-2
    Sep  6 11:11:54.979: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 09/06/23 11:11:54.979
    STEP: delete collection of pod templates 09/06/23 11:11:54.982
    Sep  6 11:11:54.982: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 09/06/23 11:11:54.999
    Sep  6 11:11:54.999: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:11:55.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-919" for this suite. 09/06/23 11:11:55.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:11:55.011
Sep  6 11:11:55.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename taint-single-pod 09/06/23 11:11:55.012
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:55.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:55.031
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Sep  6 11:11:55.033: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 11:12:55.078: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Sep  6 11:12:55.085: INFO: Starting informer...
STEP: Starting pod... 09/06/23 11:12:55.085
Sep  6 11:12:55.308: INFO: Pod is running on kube-3. Tainting Node
STEP: Trying to apply a taint on the Node 09/06/23 11:12:55.308
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/06/23 11:12:55.32
STEP: Waiting short time to make sure Pod is queued for deletion 09/06/23 11:12:55.324
Sep  6 11:12:55.324: INFO: Pod wasn't evicted. Proceeding
Sep  6 11:12:55.324: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/06/23 11:12:55.337
STEP: Waiting some time to make sure that toleration time passed. 09/06/23 11:12:55.342
Sep  6 11:14:10.346: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:14:10.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-216" for this suite. 09/06/23 11:14:10.353
------------------------------
• [SLOW TEST] [135.355 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:11:55.011
    Sep  6 11:11:55.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename taint-single-pod 09/06/23 11:11:55.012
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:11:55.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:11:55.031
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Sep  6 11:11:55.033: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  6 11:12:55.078: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Sep  6 11:12:55.085: INFO: Starting informer...
    STEP: Starting pod... 09/06/23 11:12:55.085
    Sep  6 11:12:55.308: INFO: Pod is running on kube-3. Tainting Node
    STEP: Trying to apply a taint on the Node 09/06/23 11:12:55.308
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/06/23 11:12:55.32
    STEP: Waiting short time to make sure Pod is queued for deletion 09/06/23 11:12:55.324
    Sep  6 11:12:55.324: INFO: Pod wasn't evicted. Proceeding
    Sep  6 11:12:55.324: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/06/23 11:12:55.337
    STEP: Waiting some time to make sure that toleration time passed. 09/06/23 11:12:55.342
    Sep  6 11:14:10.346: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:14:10.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-216" for this suite. 09/06/23 11:14:10.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:14:10.367
Sep  6 11:14:10.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename svcaccounts 09/06/23 11:14:10.369
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:10.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:10.392
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  09/06/23 11:14:10.394
Sep  6 11:14:10.402: INFO: Waiting up to 5m0s for pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf" in namespace "svcaccounts-2326" to be "Succeeded or Failed"
Sep  6 11:14:10.412: INFO: Pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051053ms
Sep  6 11:14:12.419: INFO: Pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016592759s
Sep  6 11:14:14.421: INFO: Pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018542507s
STEP: Saw pod success 09/06/23 11:14:14.421
Sep  6 11:14:14.421: INFO: Pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf" satisfied condition "Succeeded or Failed"
Sep  6 11:14:14.429: INFO: Trying to get logs from node kube-3 pod test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf container agnhost-container: <nil>
STEP: delete the pod 09/06/23 11:14:14.459
Sep  6 11:14:14.475: INFO: Waiting for pod test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf to disappear
Sep  6 11:14:14.479: INFO: Pod test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  6 11:14:14.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2326" for this suite. 09/06/23 11:14:14.483
------------------------------
• [4.126 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:14:10.367
    Sep  6 11:14:10.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename svcaccounts 09/06/23 11:14:10.369
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:10.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:10.392
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  09/06/23 11:14:10.394
    Sep  6 11:14:10.402: INFO: Waiting up to 5m0s for pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf" in namespace "svcaccounts-2326" to be "Succeeded or Failed"
    Sep  6 11:14:10.412: INFO: Pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051053ms
    Sep  6 11:14:12.419: INFO: Pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016592759s
    Sep  6 11:14:14.421: INFO: Pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018542507s
    STEP: Saw pod success 09/06/23 11:14:14.421
    Sep  6 11:14:14.421: INFO: Pod "test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf" satisfied condition "Succeeded or Failed"
    Sep  6 11:14:14.429: INFO: Trying to get logs from node kube-3 pod test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 11:14:14.459
    Sep  6 11:14:14.475: INFO: Waiting for pod test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf to disappear
    Sep  6 11:14:14.479: INFO: Pod test-pod-cb6ca934-197d-4d16-bdc7-e07b5e5e80bf no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:14:14.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2326" for this suite. 09/06/23 11:14:14.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:14:14.494
Sep  6 11:14:14.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename runtimeclass 09/06/23 11:14:14.495
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:14.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:14.52
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  6 11:14:14.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7762" for this suite. 09/06/23 11:14:14.532
------------------------------
• [0.047 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:14:14.494
    Sep  6 11:14:14.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename runtimeclass 09/06/23 11:14:14.495
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:14.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:14.52
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:14:14.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7762" for this suite. 09/06/23 11:14:14.532
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:14:14.541
Sep  6 11:14:14.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename gc 09/06/23 11:14:14.542
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:14.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:14.561
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 09/06/23 11:14:14.563
STEP: delete the rc 09/06/23 11:14:19.572
STEP: wait for all pods to be garbage collected 09/06/23 11:14:19.585
STEP: Gathering metrics 09/06/23 11:14:24.606
Sep  6 11:14:24.642: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Sep  6 11:14:24.646: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.839932ms
Sep  6 11:14:24.646: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Sep  6 11:14:24.646: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Sep  6 11:14:24.687: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  6 11:14:24.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1455" for this suite. 09/06/23 11:14:24.691
------------------------------
• [SLOW TEST] [10.167 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:14:14.541
    Sep  6 11:14:14.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename gc 09/06/23 11:14:14.542
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:14.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:14.561
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 09/06/23 11:14:14.563
    STEP: delete the rc 09/06/23 11:14:19.572
    STEP: wait for all pods to be garbage collected 09/06/23 11:14:19.585
    STEP: Gathering metrics 09/06/23 11:14:24.606
    Sep  6 11:14:24.642: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Sep  6 11:14:24.646: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.839932ms
    Sep  6 11:14:24.646: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Sep  6 11:14:24.646: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Sep  6 11:14:24.687: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:14:24.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1455" for this suite. 09/06/23 11:14:24.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:14:24.711
Sep  6 11:14:24.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename subpath 09/06/23 11:14:24.712
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:24.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:24.793
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/06/23 11:14:24.802
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-tm78 09/06/23 11:14:24.853
STEP: Creating a pod to test atomic-volume-subpath 09/06/23 11:14:24.853
Sep  6 11:14:24.879: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tm78" in namespace "subpath-787" to be "Succeeded or Failed"
Sep  6 11:14:24.923: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012503ms
Sep  6 11:14:26.937: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 2.057777382s
Sep  6 11:14:28.938: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 4.058991875s
Sep  6 11:14:30.935: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 6.055780518s
Sep  6 11:14:32.937: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 8.057604556s
Sep  6 11:14:34.938: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 10.059167163s
Sep  6 11:14:36.935: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 12.055747783s
Sep  6 11:14:38.939: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 14.059623066s
Sep  6 11:14:40.940: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 16.060891881s
Sep  6 11:14:42.936: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 18.057109597s
Sep  6 11:14:44.936: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 20.056682352s
Sep  6 11:14:46.939: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=false. Elapsed: 22.05954691s
Sep  6 11:14:48.937: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.057269695s
STEP: Saw pod success 09/06/23 11:14:48.937
Sep  6 11:14:48.937: INFO: Pod "pod-subpath-test-downwardapi-tm78" satisfied condition "Succeeded or Failed"
Sep  6 11:14:48.947: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-downwardapi-tm78 container test-container-subpath-downwardapi-tm78: <nil>
STEP: delete the pod 09/06/23 11:14:48.962
Sep  6 11:14:48.987: INFO: Waiting for pod pod-subpath-test-downwardapi-tm78 to disappear
Sep  6 11:14:48.990: INFO: Pod pod-subpath-test-downwardapi-tm78 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tm78 09/06/23 11:14:48.99
Sep  6 11:14:48.990: INFO: Deleting pod "pod-subpath-test-downwardapi-tm78" in namespace "subpath-787"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  6 11:14:48.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-787" for this suite. 09/06/23 11:14:48.996
------------------------------
• [SLOW TEST] [24.292 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:14:24.711
    Sep  6 11:14:24.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename subpath 09/06/23 11:14:24.712
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:24.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:24.793
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/06/23 11:14:24.802
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-tm78 09/06/23 11:14:24.853
    STEP: Creating a pod to test atomic-volume-subpath 09/06/23 11:14:24.853
    Sep  6 11:14:24.879: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tm78" in namespace "subpath-787" to be "Succeeded or Failed"
    Sep  6 11:14:24.923: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012503ms
    Sep  6 11:14:26.937: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 2.057777382s
    Sep  6 11:14:28.938: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 4.058991875s
    Sep  6 11:14:30.935: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 6.055780518s
    Sep  6 11:14:32.937: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 8.057604556s
    Sep  6 11:14:34.938: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 10.059167163s
    Sep  6 11:14:36.935: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 12.055747783s
    Sep  6 11:14:38.939: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 14.059623066s
    Sep  6 11:14:40.940: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 16.060891881s
    Sep  6 11:14:42.936: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 18.057109597s
    Sep  6 11:14:44.936: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=true. Elapsed: 20.056682352s
    Sep  6 11:14:46.939: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Running", Reason="", readiness=false. Elapsed: 22.05954691s
    Sep  6 11:14:48.937: INFO: Pod "pod-subpath-test-downwardapi-tm78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.057269695s
    STEP: Saw pod success 09/06/23 11:14:48.937
    Sep  6 11:14:48.937: INFO: Pod "pod-subpath-test-downwardapi-tm78" satisfied condition "Succeeded or Failed"
    Sep  6 11:14:48.947: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-downwardapi-tm78 container test-container-subpath-downwardapi-tm78: <nil>
    STEP: delete the pod 09/06/23 11:14:48.962
    Sep  6 11:14:48.987: INFO: Waiting for pod pod-subpath-test-downwardapi-tm78 to disappear
    Sep  6 11:14:48.990: INFO: Pod pod-subpath-test-downwardapi-tm78 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-tm78 09/06/23 11:14:48.99
    Sep  6 11:14:48.990: INFO: Deleting pod "pod-subpath-test-downwardapi-tm78" in namespace "subpath-787"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:14:48.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-787" for this suite. 09/06/23 11:14:48.996
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:14:49.004
Sep  6 11:14:49.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename gc 09/06/23 11:14:49.005
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:49.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:49.025
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 09/06/23 11:14:49.031
STEP: delete the rc 09/06/23 11:14:54.552
STEP: wait for the rc to be deleted 09/06/23 11:14:54.747
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/06/23 11:15:00.289
STEP: Gathering metrics 09/06/23 11:15:30.302
Sep  6 11:15:30.315: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Sep  6 11:15:30.318: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.796543ms
Sep  6 11:15:30.318: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Sep  6 11:15:30.318: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Sep  6 11:15:30.360: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  6 11:15:30.360: INFO: Deleting pod "simpletest.rc-2cvpk" in namespace "gc-722"
Sep  6 11:15:30.386: INFO: Deleting pod "simpletest.rc-2l7dq" in namespace "gc-722"
Sep  6 11:15:30.403: INFO: Deleting pod "simpletest.rc-2pq5g" in namespace "gc-722"
Sep  6 11:15:30.417: INFO: Deleting pod "simpletest.rc-2qbtb" in namespace "gc-722"
Sep  6 11:15:30.459: INFO: Deleting pod "simpletest.rc-2w2k9" in namespace "gc-722"
Sep  6 11:15:30.497: INFO: Deleting pod "simpletest.rc-4b5mz" in namespace "gc-722"
Sep  6 11:15:30.575: INFO: Deleting pod "simpletest.rc-4dgm9" in namespace "gc-722"
Sep  6 11:15:30.632: INFO: Deleting pod "simpletest.rc-4dj8p" in namespace "gc-722"
Sep  6 11:15:30.682: INFO: Deleting pod "simpletest.rc-4l9bp" in namespace "gc-722"
Sep  6 11:15:30.795: INFO: Deleting pod "simpletest.rc-4whlc" in namespace "gc-722"
Sep  6 11:15:30.887: INFO: Deleting pod "simpletest.rc-5c2bn" in namespace "gc-722"
Sep  6 11:15:30.951: INFO: Deleting pod "simpletest.rc-5wks4" in namespace "gc-722"
Sep  6 11:15:31.051: INFO: Deleting pod "simpletest.rc-65mxp" in namespace "gc-722"
Sep  6 11:15:32.616: INFO: Deleting pod "simpletest.rc-6pvph" in namespace "gc-722"
Sep  6 11:15:32.794: INFO: Deleting pod "simpletest.rc-6ttkd" in namespace "gc-722"
Sep  6 11:15:33.330: INFO: Deleting pod "simpletest.rc-7krn8" in namespace "gc-722"
Sep  6 11:15:33.704: INFO: Deleting pod "simpletest.rc-7vjg7" in namespace "gc-722"
Sep  6 11:15:33.824: INFO: Deleting pod "simpletest.rc-7wwl8" in namespace "gc-722"
Sep  6 11:15:34.016: INFO: Deleting pod "simpletest.rc-852bq" in namespace "gc-722"
Sep  6 11:15:34.103: INFO: Deleting pod "simpletest.rc-8mlrh" in namespace "gc-722"
Sep  6 11:15:34.163: INFO: Deleting pod "simpletest.rc-8tnpk" in namespace "gc-722"
Sep  6 11:15:34.254: INFO: Deleting pod "simpletest.rc-8w4l6" in namespace "gc-722"
Sep  6 11:15:34.310: INFO: Deleting pod "simpletest.rc-8wd9q" in namespace "gc-722"
Sep  6 11:15:34.398: INFO: Deleting pod "simpletest.rc-8wzzx" in namespace "gc-722"
Sep  6 11:15:34.471: INFO: Deleting pod "simpletest.rc-95qgl" in namespace "gc-722"
Sep  6 11:15:34.600: INFO: Deleting pod "simpletest.rc-9p5vq" in namespace "gc-722"
Sep  6 11:15:34.747: INFO: Deleting pod "simpletest.rc-b469s" in namespace "gc-722"
Sep  6 11:15:34.819: INFO: Deleting pod "simpletest.rc-b5m8p" in namespace "gc-722"
Sep  6 11:15:35.426: INFO: Deleting pod "simpletest.rc-bfssq" in namespace "gc-722"
Sep  6 11:15:35.516: INFO: Deleting pod "simpletest.rc-bzfsx" in namespace "gc-722"
Sep  6 11:15:37.133: INFO: Deleting pod "simpletest.rc-cjwcw" in namespace "gc-722"
Sep  6 11:15:37.635: INFO: Deleting pod "simpletest.rc-cxgzg" in namespace "gc-722"
Sep  6 11:15:37.944: INFO: Deleting pod "simpletest.rc-dt2sw" in namespace "gc-722"
Sep  6 11:15:38.254: INFO: Deleting pod "simpletest.rc-dtdfg" in namespace "gc-722"
Sep  6 11:15:38.379: INFO: Deleting pod "simpletest.rc-dxgg8" in namespace "gc-722"
Sep  6 11:15:38.539: INFO: Deleting pod "simpletest.rc-dxjhz" in namespace "gc-722"
Sep  6 11:15:38.610: INFO: Deleting pod "simpletest.rc-f7zdx" in namespace "gc-722"
Sep  6 11:15:38.673: INFO: Deleting pod "simpletest.rc-flp5t" in namespace "gc-722"
Sep  6 11:15:38.763: INFO: Deleting pod "simpletest.rc-g5bgm" in namespace "gc-722"
Sep  6 11:15:38.846: INFO: Deleting pod "simpletest.rc-gf2z8" in namespace "gc-722"
Sep  6 11:15:40.193: INFO: Deleting pod "simpletest.rc-gl9mf" in namespace "gc-722"
Sep  6 11:15:40.255: INFO: Deleting pod "simpletest.rc-gm6nd" in namespace "gc-722"
Sep  6 11:15:40.341: INFO: Deleting pod "simpletest.rc-gwvtm" in namespace "gc-722"
Sep  6 11:15:40.384: INFO: Deleting pod "simpletest.rc-hf2fp" in namespace "gc-722"
Sep  6 11:15:40.512: INFO: Deleting pod "simpletest.rc-hjnbd" in namespace "gc-722"
Sep  6 11:15:40.580: INFO: Deleting pod "simpletest.rc-hkjjd" in namespace "gc-722"
Sep  6 11:15:40.646: INFO: Deleting pod "simpletest.rc-jb4ph" in namespace "gc-722"
Sep  6 11:15:40.754: INFO: Deleting pod "simpletest.rc-jbfhp" in namespace "gc-722"
Sep  6 11:15:40.818: INFO: Deleting pod "simpletest.rc-jfsll" in namespace "gc-722"
Sep  6 11:15:40.897: INFO: Deleting pod "simpletest.rc-jgsmj" in namespace "gc-722"
Sep  6 11:15:41.000: INFO: Deleting pod "simpletest.rc-jn2s5" in namespace "gc-722"
Sep  6 11:15:41.165: INFO: Deleting pod "simpletest.rc-jrgq6" in namespace "gc-722"
Sep  6 11:15:41.262: INFO: Deleting pod "simpletest.rc-k2t6p" in namespace "gc-722"
Sep  6 11:15:41.412: INFO: Deleting pod "simpletest.rc-k94b6" in namespace "gc-722"
Sep  6 11:15:41.548: INFO: Deleting pod "simpletest.rc-ktlj9" in namespace "gc-722"
Sep  6 11:15:41.659: INFO: Deleting pod "simpletest.rc-kxchz" in namespace "gc-722"
Sep  6 11:15:41.814: INFO: Deleting pod "simpletest.rc-l75xq" in namespace "gc-722"
Sep  6 11:15:41.908: INFO: Deleting pod "simpletest.rc-lwc5d" in namespace "gc-722"
Sep  6 11:15:42.018: INFO: Deleting pod "simpletest.rc-m54kn" in namespace "gc-722"
Sep  6 11:15:42.100: INFO: Deleting pod "simpletest.rc-m5t2t" in namespace "gc-722"
Sep  6 11:15:42.160: INFO: Deleting pod "simpletest.rc-nd4wn" in namespace "gc-722"
Sep  6 11:15:42.272: INFO: Deleting pod "simpletest.rc-nlvf6" in namespace "gc-722"
Sep  6 11:15:43.384: INFO: Deleting pod "simpletest.rc-p27b5" in namespace "gc-722"
Sep  6 11:15:44.346: INFO: Deleting pod "simpletest.rc-p72pv" in namespace "gc-722"
Sep  6 11:15:44.939: INFO: Deleting pod "simpletest.rc-pdgqm" in namespace "gc-722"
Sep  6 11:15:45.112: INFO: Deleting pod "simpletest.rc-pj8x2" in namespace "gc-722"
Sep  6 11:15:45.237: INFO: Deleting pod "simpletest.rc-ptc7c" in namespace "gc-722"
Sep  6 11:15:45.456: INFO: Deleting pod "simpletest.rc-ptpzx" in namespace "gc-722"
Sep  6 11:15:45.505: INFO: Deleting pod "simpletest.rc-pwbjd" in namespace "gc-722"
Sep  6 11:15:45.606: INFO: Deleting pod "simpletest.rc-q5sc6" in namespace "gc-722"
Sep  6 11:15:45.656: INFO: Deleting pod "simpletest.rc-qc2xg" in namespace "gc-722"
Sep  6 11:15:45.704: INFO: Deleting pod "simpletest.rc-qfcx8" in namespace "gc-722"
Sep  6 11:15:45.769: INFO: Deleting pod "simpletest.rc-qjgsp" in namespace "gc-722"
Sep  6 11:15:45.877: INFO: Deleting pod "simpletest.rc-qssfs" in namespace "gc-722"
Sep  6 11:15:45.960: INFO: Deleting pod "simpletest.rc-r4k2m" in namespace "gc-722"
Sep  6 11:15:46.065: INFO: Deleting pod "simpletest.rc-r556v" in namespace "gc-722"
Sep  6 11:15:46.164: INFO: Deleting pod "simpletest.rc-rc7jk" in namespace "gc-722"
Sep  6 11:15:46.272: INFO: Deleting pod "simpletest.rc-rkwz8" in namespace "gc-722"
Sep  6 11:15:46.356: INFO: Deleting pod "simpletest.rc-rs7t4" in namespace "gc-722"
Sep  6 11:15:46.446: INFO: Deleting pod "simpletest.rc-rv5kl" in namespace "gc-722"
Sep  6 11:15:47.384: INFO: Deleting pod "simpletest.rc-s6qdc" in namespace "gc-722"
Sep  6 11:15:48.238: INFO: Deleting pod "simpletest.rc-sl7x2" in namespace "gc-722"
Sep  6 11:15:48.834: INFO: Deleting pod "simpletest.rc-swctl" in namespace "gc-722"
Sep  6 11:15:49.203: INFO: Deleting pod "simpletest.rc-swdcf" in namespace "gc-722"
Sep  6 11:15:49.359: INFO: Deleting pod "simpletest.rc-t67qr" in namespace "gc-722"
Sep  6 11:15:49.480: INFO: Deleting pod "simpletest.rc-tc2w6" in namespace "gc-722"
Sep  6 11:15:49.527: INFO: Deleting pod "simpletest.rc-ttf7c" in namespace "gc-722"
Sep  6 11:15:49.586: INFO: Deleting pod "simpletest.rc-v45x6" in namespace "gc-722"
Sep  6 11:15:49.653: INFO: Deleting pod "simpletest.rc-v46xn" in namespace "gc-722"
Sep  6 11:15:49.685: INFO: Deleting pod "simpletest.rc-vh2pl" in namespace "gc-722"
Sep  6 11:15:49.774: INFO: Deleting pod "simpletest.rc-vwttp" in namespace "gc-722"
Sep  6 11:15:49.915: INFO: Deleting pod "simpletest.rc-wx86r" in namespace "gc-722"
Sep  6 11:15:50.611: INFO: Deleting pod "simpletest.rc-wxbp2" in namespace "gc-722"
Sep  6 11:15:50.759: INFO: Deleting pod "simpletest.rc-x5tdq" in namespace "gc-722"
Sep  6 11:15:50.849: INFO: Deleting pod "simpletest.rc-x75hq" in namespace "gc-722"
Sep  6 11:15:50.893: INFO: Deleting pod "simpletest.rc-xgzcz" in namespace "gc-722"
Sep  6 11:15:51.042: INFO: Deleting pod "simpletest.rc-xjwnz" in namespace "gc-722"
Sep  6 11:15:51.141: INFO: Deleting pod "simpletest.rc-xq76n" in namespace "gc-722"
Sep  6 11:15:51.235: INFO: Deleting pod "simpletest.rc-ztld9" in namespace "gc-722"
Sep  6 11:15:51.324: INFO: Deleting pod "simpletest.rc-zx49s" in namespace "gc-722"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  6 11:15:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-722" for this suite. 09/06/23 11:15:51.42
------------------------------
• [SLOW TEST] [62.480 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:14:49.004
    Sep  6 11:14:49.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename gc 09/06/23 11:14:49.005
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:14:49.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:14:49.025
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 09/06/23 11:14:49.031
    STEP: delete the rc 09/06/23 11:14:54.552
    STEP: wait for the rc to be deleted 09/06/23 11:14:54.747
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/06/23 11:15:00.289
    STEP: Gathering metrics 09/06/23 11:15:30.302
    Sep  6 11:15:30.315: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Sep  6 11:15:30.318: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.796543ms
    Sep  6 11:15:30.318: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Sep  6 11:15:30.318: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Sep  6 11:15:30.360: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  6 11:15:30.360: INFO: Deleting pod "simpletest.rc-2cvpk" in namespace "gc-722"
    Sep  6 11:15:30.386: INFO: Deleting pod "simpletest.rc-2l7dq" in namespace "gc-722"
    Sep  6 11:15:30.403: INFO: Deleting pod "simpletest.rc-2pq5g" in namespace "gc-722"
    Sep  6 11:15:30.417: INFO: Deleting pod "simpletest.rc-2qbtb" in namespace "gc-722"
    Sep  6 11:15:30.459: INFO: Deleting pod "simpletest.rc-2w2k9" in namespace "gc-722"
    Sep  6 11:15:30.497: INFO: Deleting pod "simpletest.rc-4b5mz" in namespace "gc-722"
    Sep  6 11:15:30.575: INFO: Deleting pod "simpletest.rc-4dgm9" in namespace "gc-722"
    Sep  6 11:15:30.632: INFO: Deleting pod "simpletest.rc-4dj8p" in namespace "gc-722"
    Sep  6 11:15:30.682: INFO: Deleting pod "simpletest.rc-4l9bp" in namespace "gc-722"
    Sep  6 11:15:30.795: INFO: Deleting pod "simpletest.rc-4whlc" in namespace "gc-722"
    Sep  6 11:15:30.887: INFO: Deleting pod "simpletest.rc-5c2bn" in namespace "gc-722"
    Sep  6 11:15:30.951: INFO: Deleting pod "simpletest.rc-5wks4" in namespace "gc-722"
    Sep  6 11:15:31.051: INFO: Deleting pod "simpletest.rc-65mxp" in namespace "gc-722"
    Sep  6 11:15:32.616: INFO: Deleting pod "simpletest.rc-6pvph" in namespace "gc-722"
    Sep  6 11:15:32.794: INFO: Deleting pod "simpletest.rc-6ttkd" in namespace "gc-722"
    Sep  6 11:15:33.330: INFO: Deleting pod "simpletest.rc-7krn8" in namespace "gc-722"
    Sep  6 11:15:33.704: INFO: Deleting pod "simpletest.rc-7vjg7" in namespace "gc-722"
    Sep  6 11:15:33.824: INFO: Deleting pod "simpletest.rc-7wwl8" in namespace "gc-722"
    Sep  6 11:15:34.016: INFO: Deleting pod "simpletest.rc-852bq" in namespace "gc-722"
    Sep  6 11:15:34.103: INFO: Deleting pod "simpletest.rc-8mlrh" in namespace "gc-722"
    Sep  6 11:15:34.163: INFO: Deleting pod "simpletest.rc-8tnpk" in namespace "gc-722"
    Sep  6 11:15:34.254: INFO: Deleting pod "simpletest.rc-8w4l6" in namespace "gc-722"
    Sep  6 11:15:34.310: INFO: Deleting pod "simpletest.rc-8wd9q" in namespace "gc-722"
    Sep  6 11:15:34.398: INFO: Deleting pod "simpletest.rc-8wzzx" in namespace "gc-722"
    Sep  6 11:15:34.471: INFO: Deleting pod "simpletest.rc-95qgl" in namespace "gc-722"
    Sep  6 11:15:34.600: INFO: Deleting pod "simpletest.rc-9p5vq" in namespace "gc-722"
    Sep  6 11:15:34.747: INFO: Deleting pod "simpletest.rc-b469s" in namespace "gc-722"
    Sep  6 11:15:34.819: INFO: Deleting pod "simpletest.rc-b5m8p" in namespace "gc-722"
    Sep  6 11:15:35.426: INFO: Deleting pod "simpletest.rc-bfssq" in namespace "gc-722"
    Sep  6 11:15:35.516: INFO: Deleting pod "simpletest.rc-bzfsx" in namespace "gc-722"
    Sep  6 11:15:37.133: INFO: Deleting pod "simpletest.rc-cjwcw" in namespace "gc-722"
    Sep  6 11:15:37.635: INFO: Deleting pod "simpletest.rc-cxgzg" in namespace "gc-722"
    Sep  6 11:15:37.944: INFO: Deleting pod "simpletest.rc-dt2sw" in namespace "gc-722"
    Sep  6 11:15:38.254: INFO: Deleting pod "simpletest.rc-dtdfg" in namespace "gc-722"
    Sep  6 11:15:38.379: INFO: Deleting pod "simpletest.rc-dxgg8" in namespace "gc-722"
    Sep  6 11:15:38.539: INFO: Deleting pod "simpletest.rc-dxjhz" in namespace "gc-722"
    Sep  6 11:15:38.610: INFO: Deleting pod "simpletest.rc-f7zdx" in namespace "gc-722"
    Sep  6 11:15:38.673: INFO: Deleting pod "simpletest.rc-flp5t" in namespace "gc-722"
    Sep  6 11:15:38.763: INFO: Deleting pod "simpletest.rc-g5bgm" in namespace "gc-722"
    Sep  6 11:15:38.846: INFO: Deleting pod "simpletest.rc-gf2z8" in namespace "gc-722"
    Sep  6 11:15:40.193: INFO: Deleting pod "simpletest.rc-gl9mf" in namespace "gc-722"
    Sep  6 11:15:40.255: INFO: Deleting pod "simpletest.rc-gm6nd" in namespace "gc-722"
    Sep  6 11:15:40.341: INFO: Deleting pod "simpletest.rc-gwvtm" in namespace "gc-722"
    Sep  6 11:15:40.384: INFO: Deleting pod "simpletest.rc-hf2fp" in namespace "gc-722"
    Sep  6 11:15:40.512: INFO: Deleting pod "simpletest.rc-hjnbd" in namespace "gc-722"
    Sep  6 11:15:40.580: INFO: Deleting pod "simpletest.rc-hkjjd" in namespace "gc-722"
    Sep  6 11:15:40.646: INFO: Deleting pod "simpletest.rc-jb4ph" in namespace "gc-722"
    Sep  6 11:15:40.754: INFO: Deleting pod "simpletest.rc-jbfhp" in namespace "gc-722"
    Sep  6 11:15:40.818: INFO: Deleting pod "simpletest.rc-jfsll" in namespace "gc-722"
    Sep  6 11:15:40.897: INFO: Deleting pod "simpletest.rc-jgsmj" in namespace "gc-722"
    Sep  6 11:15:41.000: INFO: Deleting pod "simpletest.rc-jn2s5" in namespace "gc-722"
    Sep  6 11:15:41.165: INFO: Deleting pod "simpletest.rc-jrgq6" in namespace "gc-722"
    Sep  6 11:15:41.262: INFO: Deleting pod "simpletest.rc-k2t6p" in namespace "gc-722"
    Sep  6 11:15:41.412: INFO: Deleting pod "simpletest.rc-k94b6" in namespace "gc-722"
    Sep  6 11:15:41.548: INFO: Deleting pod "simpletest.rc-ktlj9" in namespace "gc-722"
    Sep  6 11:15:41.659: INFO: Deleting pod "simpletest.rc-kxchz" in namespace "gc-722"
    Sep  6 11:15:41.814: INFO: Deleting pod "simpletest.rc-l75xq" in namespace "gc-722"
    Sep  6 11:15:41.908: INFO: Deleting pod "simpletest.rc-lwc5d" in namespace "gc-722"
    Sep  6 11:15:42.018: INFO: Deleting pod "simpletest.rc-m54kn" in namespace "gc-722"
    Sep  6 11:15:42.100: INFO: Deleting pod "simpletest.rc-m5t2t" in namespace "gc-722"
    Sep  6 11:15:42.160: INFO: Deleting pod "simpletest.rc-nd4wn" in namespace "gc-722"
    Sep  6 11:15:42.272: INFO: Deleting pod "simpletest.rc-nlvf6" in namespace "gc-722"
    Sep  6 11:15:43.384: INFO: Deleting pod "simpletest.rc-p27b5" in namespace "gc-722"
    Sep  6 11:15:44.346: INFO: Deleting pod "simpletest.rc-p72pv" in namespace "gc-722"
    Sep  6 11:15:44.939: INFO: Deleting pod "simpletest.rc-pdgqm" in namespace "gc-722"
    Sep  6 11:15:45.112: INFO: Deleting pod "simpletest.rc-pj8x2" in namespace "gc-722"
    Sep  6 11:15:45.237: INFO: Deleting pod "simpletest.rc-ptc7c" in namespace "gc-722"
    Sep  6 11:15:45.456: INFO: Deleting pod "simpletest.rc-ptpzx" in namespace "gc-722"
    Sep  6 11:15:45.505: INFO: Deleting pod "simpletest.rc-pwbjd" in namespace "gc-722"
    Sep  6 11:15:45.606: INFO: Deleting pod "simpletest.rc-q5sc6" in namespace "gc-722"
    Sep  6 11:15:45.656: INFO: Deleting pod "simpletest.rc-qc2xg" in namespace "gc-722"
    Sep  6 11:15:45.704: INFO: Deleting pod "simpletest.rc-qfcx8" in namespace "gc-722"
    Sep  6 11:15:45.769: INFO: Deleting pod "simpletest.rc-qjgsp" in namespace "gc-722"
    Sep  6 11:15:45.877: INFO: Deleting pod "simpletest.rc-qssfs" in namespace "gc-722"
    Sep  6 11:15:45.960: INFO: Deleting pod "simpletest.rc-r4k2m" in namespace "gc-722"
    Sep  6 11:15:46.065: INFO: Deleting pod "simpletest.rc-r556v" in namespace "gc-722"
    Sep  6 11:15:46.164: INFO: Deleting pod "simpletest.rc-rc7jk" in namespace "gc-722"
    Sep  6 11:15:46.272: INFO: Deleting pod "simpletest.rc-rkwz8" in namespace "gc-722"
    Sep  6 11:15:46.356: INFO: Deleting pod "simpletest.rc-rs7t4" in namespace "gc-722"
    Sep  6 11:15:46.446: INFO: Deleting pod "simpletest.rc-rv5kl" in namespace "gc-722"
    Sep  6 11:15:47.384: INFO: Deleting pod "simpletest.rc-s6qdc" in namespace "gc-722"
    Sep  6 11:15:48.238: INFO: Deleting pod "simpletest.rc-sl7x2" in namespace "gc-722"
    Sep  6 11:15:48.834: INFO: Deleting pod "simpletest.rc-swctl" in namespace "gc-722"
    Sep  6 11:15:49.203: INFO: Deleting pod "simpletest.rc-swdcf" in namespace "gc-722"
    Sep  6 11:15:49.359: INFO: Deleting pod "simpletest.rc-t67qr" in namespace "gc-722"
    Sep  6 11:15:49.480: INFO: Deleting pod "simpletest.rc-tc2w6" in namespace "gc-722"
    Sep  6 11:15:49.527: INFO: Deleting pod "simpletest.rc-ttf7c" in namespace "gc-722"
    Sep  6 11:15:49.586: INFO: Deleting pod "simpletest.rc-v45x6" in namespace "gc-722"
    Sep  6 11:15:49.653: INFO: Deleting pod "simpletest.rc-v46xn" in namespace "gc-722"
    Sep  6 11:15:49.685: INFO: Deleting pod "simpletest.rc-vh2pl" in namespace "gc-722"
    Sep  6 11:15:49.774: INFO: Deleting pod "simpletest.rc-vwttp" in namespace "gc-722"
    Sep  6 11:15:49.915: INFO: Deleting pod "simpletest.rc-wx86r" in namespace "gc-722"
    Sep  6 11:15:50.611: INFO: Deleting pod "simpletest.rc-wxbp2" in namespace "gc-722"
    Sep  6 11:15:50.759: INFO: Deleting pod "simpletest.rc-x5tdq" in namespace "gc-722"
    Sep  6 11:15:50.849: INFO: Deleting pod "simpletest.rc-x75hq" in namespace "gc-722"
    Sep  6 11:15:50.893: INFO: Deleting pod "simpletest.rc-xgzcz" in namespace "gc-722"
    Sep  6 11:15:51.042: INFO: Deleting pod "simpletest.rc-xjwnz" in namespace "gc-722"
    Sep  6 11:15:51.141: INFO: Deleting pod "simpletest.rc-xq76n" in namespace "gc-722"
    Sep  6 11:15:51.235: INFO: Deleting pod "simpletest.rc-ztld9" in namespace "gc-722"
    Sep  6 11:15:51.324: INFO: Deleting pod "simpletest.rc-zx49s" in namespace "gc-722"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:15:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-722" for this suite. 09/06/23 11:15:51.42
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:15:51.484
Sep  6 11:15:51.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename endpointslice 09/06/23 11:15:51.485
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:15:51.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:15:51.599
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 09/06/23 11:15:51.607
STEP: getting /apis/discovery.k8s.io 09/06/23 11:15:51.635
STEP: getting /apis/discovery.k8s.iov1 09/06/23 11:15:51.639
STEP: creating 09/06/23 11:15:51.644
STEP: getting 09/06/23 11:15:55.06
STEP: listing 09/06/23 11:15:55.082
STEP: watching 09/06/23 11:15:55.09
Sep  6 11:15:55.090: INFO: starting watch
STEP: cluster-wide listing 09/06/23 11:15:55.097
STEP: cluster-wide watching 09/06/23 11:15:55.109
Sep  6 11:15:55.109: INFO: starting watch
STEP: patching 09/06/23 11:15:55.111
STEP: updating 09/06/23 11:15:55.133
Sep  6 11:15:55.154: INFO: waiting for watch events with expected annotations
Sep  6 11:15:55.154: INFO: saw patched and updated annotations
STEP: deleting 09/06/23 11:15:55.154
STEP: deleting a collection 09/06/23 11:15:55.211
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  6 11:15:55.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2762" for this suite. 09/06/23 11:15:55.269
------------------------------
• [3.807 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:15:51.484
    Sep  6 11:15:51.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename endpointslice 09/06/23 11:15:51.485
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:15:51.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:15:51.599
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 09/06/23 11:15:51.607
    STEP: getting /apis/discovery.k8s.io 09/06/23 11:15:51.635
    STEP: getting /apis/discovery.k8s.iov1 09/06/23 11:15:51.639
    STEP: creating 09/06/23 11:15:51.644
    STEP: getting 09/06/23 11:15:55.06
    STEP: listing 09/06/23 11:15:55.082
    STEP: watching 09/06/23 11:15:55.09
    Sep  6 11:15:55.090: INFO: starting watch
    STEP: cluster-wide listing 09/06/23 11:15:55.097
    STEP: cluster-wide watching 09/06/23 11:15:55.109
    Sep  6 11:15:55.109: INFO: starting watch
    STEP: patching 09/06/23 11:15:55.111
    STEP: updating 09/06/23 11:15:55.133
    Sep  6 11:15:55.154: INFO: waiting for watch events with expected annotations
    Sep  6 11:15:55.154: INFO: saw patched and updated annotations
    STEP: deleting 09/06/23 11:15:55.154
    STEP: deleting a collection 09/06/23 11:15:55.211
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:15:55.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2762" for this suite. 09/06/23 11:15:55.269
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:15:55.292
Sep  6 11:15:55.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename dns 09/06/23 11:15:55.293
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:15:55.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:15:55.346
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/06/23 11:15:55.35
Sep  6 11:15:55.367: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9284  5ad8e195-604a-42b4-a908-fe1d53fee0b7 31544 0 2023-09-06 11:15:55 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-06 11:15:55 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6g59r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6g59r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:15:55.367: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9284" to be "running and ready"
Sep  6 11:15:55.374: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.705664ms
Sep  6 11:15:55.374: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:15:57.401: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034597217s
Sep  6 11:15:57.401: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:15:59.378: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.010829693s
Sep  6 11:15:59.378: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Sep  6 11:15:59.378: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 09/06/23 11:15:59.378
Sep  6 11:15:59.378: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9284 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:15:59.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:15:59.378: INFO: ExecWithOptions: Clientset creation
Sep  6 11:15:59.378: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9284/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 09/06/23 11:15:59.456
Sep  6 11:15:59.456: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9284 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:15:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:15:59.457: INFO: ExecWithOptions: Clientset creation
Sep  6 11:15:59.457: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9284/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  6 11:15:59.540: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  6 11:15:59.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9284" for this suite. 09/06/23 11:15:59.59
------------------------------
• [4.311 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:15:55.292
    Sep  6 11:15:55.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename dns 09/06/23 11:15:55.293
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:15:55.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:15:55.346
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/06/23 11:15:55.35
    Sep  6 11:15:55.367: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9284  5ad8e195-604a-42b4-a908-fe1d53fee0b7 31544 0 2023-09-06 11:15:55 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-06 11:15:55 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6g59r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6g59r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:15:55.367: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9284" to be "running and ready"
    Sep  6 11:15:55.374: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.705664ms
    Sep  6 11:15:55.374: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:15:57.401: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034597217s
    Sep  6 11:15:57.401: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:15:59.378: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.010829693s
    Sep  6 11:15:59.378: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Sep  6 11:15:59.378: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 09/06/23 11:15:59.378
    Sep  6 11:15:59.378: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9284 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:15:59.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:15:59.378: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:15:59.378: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9284/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 09/06/23 11:15:59.456
    Sep  6 11:15:59.456: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9284 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:15:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:15:59.457: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:15:59.457: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9284/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  6 11:15:59.540: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:15:59.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9284" for this suite. 09/06/23 11:15:59.59
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:15:59.604
Sep  6 11:15:59.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 11:15:59.606
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:15:59.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:15:59.683
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:15:59.686
Sep  6 11:15:59.698: INFO: Waiting up to 5m0s for pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681" in namespace "downward-api-2291" to be "Succeeded or Failed"
Sep  6 11:15:59.701: INFO: Pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681": Phase="Pending", Reason="", readiness=false. Elapsed: 3.556141ms
Sep  6 11:16:01.708: INFO: Pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010258737s
Sep  6 11:16:03.707: INFO: Pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009435751s
STEP: Saw pod success 09/06/23 11:16:03.707
Sep  6 11:16:03.708: INFO: Pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681" satisfied condition "Succeeded or Failed"
Sep  6 11:16:03.712: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681 container client-container: <nil>
STEP: delete the pod 09/06/23 11:16:03.721
Sep  6 11:16:03.795: INFO: Waiting for pod downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681 to disappear
Sep  6 11:16:03.798: INFO: Pod downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 11:16:03.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2291" for this suite. 09/06/23 11:16:03.827
------------------------------
• [4.262 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:15:59.604
    Sep  6 11:15:59.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 11:15:59.606
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:15:59.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:15:59.683
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:15:59.686
    Sep  6 11:15:59.698: INFO: Waiting up to 5m0s for pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681" in namespace "downward-api-2291" to be "Succeeded or Failed"
    Sep  6 11:15:59.701: INFO: Pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681": Phase="Pending", Reason="", readiness=false. Elapsed: 3.556141ms
    Sep  6 11:16:01.708: INFO: Pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010258737s
    Sep  6 11:16:03.707: INFO: Pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009435751s
    STEP: Saw pod success 09/06/23 11:16:03.707
    Sep  6 11:16:03.708: INFO: Pod "downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681" satisfied condition "Succeeded or Failed"
    Sep  6 11:16:03.712: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681 container client-container: <nil>
    STEP: delete the pod 09/06/23 11:16:03.721
    Sep  6 11:16:03.795: INFO: Waiting for pod downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681 to disappear
    Sep  6 11:16:03.798: INFO: Pod downwardapi-volume-246c81f3-18f0-4bab-b188-9bef2c7d5681 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:16:03.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2291" for this suite. 09/06/23 11:16:03.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:16:03.875
Sep  6 11:16:03.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename disruption 09/06/23 11:16:03.877
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:03.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:03.946
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:16:03.949
Sep  6 11:16:03.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename disruption-2 09/06/23 11:16:03.95
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:04.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:04.066
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 09/06/23 11:16:04.142
STEP: Waiting for the pdb to be processed 09/06/23 11:16:06.168
STEP: Waiting for the pdb to be processed 09/06/23 11:16:08.205
STEP: listing a collection of PDBs across all namespaces 09/06/23 11:16:10.216
STEP: listing a collection of PDBs in namespace disruption-1278 09/06/23 11:16:10.221
STEP: deleting a collection of PDBs 09/06/23 11:16:10.23
STEP: Waiting for the PDB collection to be deleted 09/06/23 11:16:10.263
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Sep  6 11:16:10.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  6 11:16:10.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-9435" for this suite. 09/06/23 11:16:10.28
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1278" for this suite. 09/06/23 11:16:10.287
------------------------------
• [SLOW TEST] [6.421 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:16:03.875
    Sep  6 11:16:03.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename disruption 09/06/23 11:16:03.877
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:03.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:03.946
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:16:03.949
    Sep  6 11:16:03.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename disruption-2 09/06/23 11:16:03.95
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:04.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:04.066
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 09/06/23 11:16:04.142
    STEP: Waiting for the pdb to be processed 09/06/23 11:16:06.168
    STEP: Waiting for the pdb to be processed 09/06/23 11:16:08.205
    STEP: listing a collection of PDBs across all namespaces 09/06/23 11:16:10.216
    STEP: listing a collection of PDBs in namespace disruption-1278 09/06/23 11:16:10.221
    STEP: deleting a collection of PDBs 09/06/23 11:16:10.23
    STEP: Waiting for the PDB collection to be deleted 09/06/23 11:16:10.263
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:16:10.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:16:10.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-9435" for this suite. 09/06/23 11:16:10.28
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1278" for this suite. 09/06/23 11:16:10.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:16:10.301
Sep  6 11:16:10.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-webhook 09/06/23 11:16:10.301
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:10.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:10.319
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/06/23 11:16:10.321
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/06/23 11:16:10.839
STEP: Deploying the custom resource conversion webhook pod 09/06/23 11:16:10.847
STEP: Wait for the deployment to be ready 09/06/23 11:16:10.859
Sep  6 11:16:10.875: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 11:16:12.911
STEP: Verifying the service has paired with the endpoint 09/06/23 11:16:12.936
Sep  6 11:16:13.937: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Sep  6 11:16:13.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Creating a v1 custom resource 09/06/23 11:16:21.555
STEP: v2 custom resource should be converted 09/06/23 11:16:21.562
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:16:22.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6023" for this suite. 09/06/23 11:16:22.186
------------------------------
• [SLOW TEST] [11.909 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:16:10.301
    Sep  6 11:16:10.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-webhook 09/06/23 11:16:10.301
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:10.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:10.319
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/06/23 11:16:10.321
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/06/23 11:16:10.839
    STEP: Deploying the custom resource conversion webhook pod 09/06/23 11:16:10.847
    STEP: Wait for the deployment to be ready 09/06/23 11:16:10.859
    Sep  6 11:16:10.875: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 11:16:12.911
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:16:12.936
    Sep  6 11:16:13.937: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Sep  6 11:16:13.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Creating a v1 custom resource 09/06/23 11:16:21.555
    STEP: v2 custom resource should be converted 09/06/23 11:16:21.562
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:16:22.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6023" for this suite. 09/06/23 11:16:22.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:16:22.21
Sep  6 11:16:22.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename subpath 09/06/23 11:16:22.211
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:22.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:22.317
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/06/23 11:16:22.323
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-xtqd 09/06/23 11:16:22.351
STEP: Creating a pod to test atomic-volume-subpath 09/06/23 11:16:22.351
Sep  6 11:16:22.366: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xtqd" in namespace "subpath-2244" to be "Succeeded or Failed"
Sep  6 11:16:22.389: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.576404ms
Sep  6 11:16:24.402: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 2.036267918s
Sep  6 11:16:26.393: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 4.027271152s
Sep  6 11:16:28.394: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 6.027913944s
Sep  6 11:16:30.394: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 8.02777533s
Sep  6 11:16:33.095: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 10.729244513s
Sep  6 11:16:34.393: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 12.027296559s
Sep  6 11:16:36.397: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 14.030542672s
Sep  6 11:16:38.400: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 16.034458407s
Sep  6 11:16:40.396: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 18.029921682s
Sep  6 11:16:42.397: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 20.031286843s
Sep  6 11:16:44.399: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=false. Elapsed: 22.032680015s
Sep  6 11:16:46.398: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.03166474s
STEP: Saw pod success 09/06/23 11:16:46.398
Sep  6 11:16:46.398: INFO: Pod "pod-subpath-test-configmap-xtqd" satisfied condition "Succeeded or Failed"
Sep  6 11:16:46.403: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-configmap-xtqd container test-container-subpath-configmap-xtqd: <nil>
STEP: delete the pod 09/06/23 11:16:46.414
Sep  6 11:16:46.437: INFO: Waiting for pod pod-subpath-test-configmap-xtqd to disappear
Sep  6 11:16:46.442: INFO: Pod pod-subpath-test-configmap-xtqd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xtqd 09/06/23 11:16:46.442
Sep  6 11:16:46.442: INFO: Deleting pod "pod-subpath-test-configmap-xtqd" in namespace "subpath-2244"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  6 11:16:46.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2244" for this suite. 09/06/23 11:16:46.452
------------------------------
• [SLOW TEST] [24.250 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:16:22.21
    Sep  6 11:16:22.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename subpath 09/06/23 11:16:22.211
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:22.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:22.317
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/06/23 11:16:22.323
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-xtqd 09/06/23 11:16:22.351
    STEP: Creating a pod to test atomic-volume-subpath 09/06/23 11:16:22.351
    Sep  6 11:16:22.366: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xtqd" in namespace "subpath-2244" to be "Succeeded or Failed"
    Sep  6 11:16:22.389: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.576404ms
    Sep  6 11:16:24.402: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 2.036267918s
    Sep  6 11:16:26.393: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 4.027271152s
    Sep  6 11:16:28.394: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 6.027913944s
    Sep  6 11:16:30.394: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 8.02777533s
    Sep  6 11:16:33.095: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 10.729244513s
    Sep  6 11:16:34.393: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 12.027296559s
    Sep  6 11:16:36.397: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 14.030542672s
    Sep  6 11:16:38.400: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 16.034458407s
    Sep  6 11:16:40.396: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 18.029921682s
    Sep  6 11:16:42.397: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=true. Elapsed: 20.031286843s
    Sep  6 11:16:44.399: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Running", Reason="", readiness=false. Elapsed: 22.032680015s
    Sep  6 11:16:46.398: INFO: Pod "pod-subpath-test-configmap-xtqd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.03166474s
    STEP: Saw pod success 09/06/23 11:16:46.398
    Sep  6 11:16:46.398: INFO: Pod "pod-subpath-test-configmap-xtqd" satisfied condition "Succeeded or Failed"
    Sep  6 11:16:46.403: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-configmap-xtqd container test-container-subpath-configmap-xtqd: <nil>
    STEP: delete the pod 09/06/23 11:16:46.414
    Sep  6 11:16:46.437: INFO: Waiting for pod pod-subpath-test-configmap-xtqd to disappear
    Sep  6 11:16:46.442: INFO: Pod pod-subpath-test-configmap-xtqd no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-xtqd 09/06/23 11:16:46.442
    Sep  6 11:16:46.442: INFO: Deleting pod "pod-subpath-test-configmap-xtqd" in namespace "subpath-2244"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:16:46.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2244" for this suite. 09/06/23 11:16:46.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:16:46.464
Sep  6 11:16:46.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 11:16:46.465
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:46.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:46.484
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-8237/configmap-test-9c465589-97b0-45dd-9503-47c63f17eb12 09/06/23 11:16:46.486
STEP: Creating a pod to test consume configMaps 09/06/23 11:16:46.492
Sep  6 11:16:46.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef" in namespace "configmap-8237" to be "Succeeded or Failed"
Sep  6 11:16:46.506: INFO: Pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394853ms
Sep  6 11:16:48.515: INFO: Pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013261992s
Sep  6 11:16:50.517: INFO: Pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015073097s
STEP: Saw pod success 09/06/23 11:16:50.517
Sep  6 11:16:50.517: INFO: Pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef" satisfied condition "Succeeded or Failed"
Sep  6 11:16:50.527: INFO: Trying to get logs from node kube-3 pod pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef container env-test: <nil>
STEP: delete the pod 09/06/23 11:16:50.543
Sep  6 11:16:50.564: INFO: Waiting for pod pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef to disappear
Sep  6 11:16:50.567: INFO: Pod pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:16:50.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8237" for this suite. 09/06/23 11:16:50.57
------------------------------
• [4.113 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:16:46.464
    Sep  6 11:16:46.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 11:16:46.465
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:46.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:46.484
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-8237/configmap-test-9c465589-97b0-45dd-9503-47c63f17eb12 09/06/23 11:16:46.486
    STEP: Creating a pod to test consume configMaps 09/06/23 11:16:46.492
    Sep  6 11:16:46.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef" in namespace "configmap-8237" to be "Succeeded or Failed"
    Sep  6 11:16:46.506: INFO: Pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394853ms
    Sep  6 11:16:48.515: INFO: Pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013261992s
    Sep  6 11:16:50.517: INFO: Pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015073097s
    STEP: Saw pod success 09/06/23 11:16:50.517
    Sep  6 11:16:50.517: INFO: Pod "pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef" satisfied condition "Succeeded or Failed"
    Sep  6 11:16:50.527: INFO: Trying to get logs from node kube-3 pod pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef container env-test: <nil>
    STEP: delete the pod 09/06/23 11:16:50.543
    Sep  6 11:16:50.564: INFO: Waiting for pod pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef to disappear
    Sep  6 11:16:50.567: INFO: Pod pod-configmaps-7464082c-5e27-4848-b144-cf7e75b4b6ef no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:16:50.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8237" for this suite. 09/06/23 11:16:50.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:16:50.578
Sep  6 11:16:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename runtimeclass 09/06/23 11:16:50.579
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:50.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:50.598
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 09/06/23 11:16:50.6
STEP: getting /apis/node.k8s.io 09/06/23 11:16:50.602
STEP: getting /apis/node.k8s.io/v1 09/06/23 11:16:50.603
STEP: creating 09/06/23 11:16:50.604
STEP: watching 09/06/23 11:16:50.618
Sep  6 11:16:50.618: INFO: starting watch
STEP: getting 09/06/23 11:16:50.624
STEP: listing 09/06/23 11:16:50.627
STEP: patching 09/06/23 11:16:50.63
STEP: updating 09/06/23 11:16:50.636
Sep  6 11:16:50.642: INFO: waiting for watch events with expected annotations
STEP: deleting 09/06/23 11:16:50.642
STEP: deleting a collection 09/06/23 11:16:50.652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  6 11:16:50.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6925" for this suite. 09/06/23 11:16:50.671
------------------------------
• [0.098 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:16:50.578
    Sep  6 11:16:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename runtimeclass 09/06/23 11:16:50.579
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:50.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:50.598
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 09/06/23 11:16:50.6
    STEP: getting /apis/node.k8s.io 09/06/23 11:16:50.602
    STEP: getting /apis/node.k8s.io/v1 09/06/23 11:16:50.603
    STEP: creating 09/06/23 11:16:50.604
    STEP: watching 09/06/23 11:16:50.618
    Sep  6 11:16:50.618: INFO: starting watch
    STEP: getting 09/06/23 11:16:50.624
    STEP: listing 09/06/23 11:16:50.627
    STEP: patching 09/06/23 11:16:50.63
    STEP: updating 09/06/23 11:16:50.636
    Sep  6 11:16:50.642: INFO: waiting for watch events with expected annotations
    STEP: deleting 09/06/23 11:16:50.642
    STEP: deleting a collection 09/06/23 11:16:50.652
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:16:50.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6925" for this suite. 09/06/23 11:16:50.671
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:16:50.679
Sep  6 11:16:50.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pod-network-test 09/06/23 11:16:50.679
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:50.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:50.697
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8865 09/06/23 11:16:50.699
STEP: creating a selector 09/06/23 11:16:50.699
STEP: Creating the service pods in kubernetes 09/06/23 11:16:50.699
Sep  6 11:16:50.700: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  6 11:16:50.731: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8865" to be "running and ready"
Sep  6 11:16:50.745: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.678118ms
Sep  6 11:16:50.745: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:16:52.884: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.15290652s
Sep  6 11:16:52.884: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:16:54.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020946548s
Sep  6 11:16:54.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:16:56.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026973143s
Sep  6 11:16:56.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:16:58.757: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02662364s
Sep  6 11:16:58.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:17:00.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.027157947s
Sep  6 11:17:00.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:17:02.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.031265733s
Sep  6 11:17:02.762: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  6 11:17:02.762: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  6 11:17:02.774: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8865" to be "running and ready"
Sep  6 11:17:02.785: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 10.264697ms
Sep  6 11:17:02.785: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:17:04.798: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.023388679s
Sep  6 11:17:04.798: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:17:06.800: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.025621608s
Sep  6 11:17:06.800: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:17:08.798: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.023300651s
Sep  6 11:17:08.798: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:17:10.800: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.02588298s
Sep  6 11:17:10.800: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:17:12.800: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.02582116s
Sep  6 11:17:12.800: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  6 11:17:12.800: INFO: Pod "netserver-1" satisfied condition "running and ready"
Sep  6 11:17:12.814: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8865" to be "running and ready"
Sep  6 11:17:12.825: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.159577ms
Sep  6 11:17:12.825: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Sep  6 11:17:12.825: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 09/06/23 11:17:12.834
Sep  6 11:17:12.851: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8865" to be "running"
Sep  6 11:17:12.858: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.78078ms
Sep  6 11:17:14.870: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018810334s
Sep  6 11:17:14.870: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  6 11:17:14.880: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8865" to be "running"
Sep  6 11:17:14.887: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.053427ms
Sep  6 11:17:14.888: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  6 11:17:14.894: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep  6 11:17:14.894: INFO: Going to poll 10.233.120.116 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep  6 11:17:14.901: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.120.116:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8865 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:17:14.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:17:14.903: INFO: ExecWithOptions: Clientset creation
Sep  6 11:17:14.903: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8865/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.120.116%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  6 11:17:15.034: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  6 11:17:15.034: INFO: Going to poll 10.233.120.215 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep  6 11:17:15.038: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.120.215:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8865 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:17:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:17:15.039: INFO: ExecWithOptions: Clientset creation
Sep  6 11:17:15.039: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8865/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.120.215%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  6 11:17:15.133: INFO: Found all 1 expected endpoints: [netserver-1]
Sep  6 11:17:15.133: INFO: Going to poll 10.233.99.109 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep  6 11:17:15.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.99.109:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8865 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:17:15.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:17:15.139: INFO: ExecWithOptions: Clientset creation
Sep  6 11:17:15.139: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8865/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.99.109%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  6 11:17:15.192: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  6 11:17:15.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8865" for this suite. 09/06/23 11:17:15.198
------------------------------
• [SLOW TEST] [24.526 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:16:50.679
    Sep  6 11:16:50.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pod-network-test 09/06/23 11:16:50.679
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:16:50.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:16:50.697
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8865 09/06/23 11:16:50.699
    STEP: creating a selector 09/06/23 11:16:50.699
    STEP: Creating the service pods in kubernetes 09/06/23 11:16:50.699
    Sep  6 11:16:50.700: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  6 11:16:50.731: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8865" to be "running and ready"
    Sep  6 11:16:50.745: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.678118ms
    Sep  6 11:16:50.745: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:16:52.884: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.15290652s
    Sep  6 11:16:52.884: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:16:54.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020946548s
    Sep  6 11:16:54.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:16:56.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026973143s
    Sep  6 11:16:56.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:16:58.757: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02662364s
    Sep  6 11:16:58.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:17:00.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.027157947s
    Sep  6 11:17:00.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:17:02.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.031265733s
    Sep  6 11:17:02.762: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  6 11:17:02.762: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  6 11:17:02.774: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8865" to be "running and ready"
    Sep  6 11:17:02.785: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 10.264697ms
    Sep  6 11:17:02.785: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:17:04.798: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.023388679s
    Sep  6 11:17:04.798: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:17:06.800: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.025621608s
    Sep  6 11:17:06.800: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:17:08.798: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.023300651s
    Sep  6 11:17:08.798: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:17:10.800: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.02588298s
    Sep  6 11:17:10.800: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:17:12.800: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.02582116s
    Sep  6 11:17:12.800: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  6 11:17:12.800: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Sep  6 11:17:12.814: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8865" to be "running and ready"
    Sep  6 11:17:12.825: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.159577ms
    Sep  6 11:17:12.825: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Sep  6 11:17:12.825: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 09/06/23 11:17:12.834
    Sep  6 11:17:12.851: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8865" to be "running"
    Sep  6 11:17:12.858: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.78078ms
    Sep  6 11:17:14.870: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018810334s
    Sep  6 11:17:14.870: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  6 11:17:14.880: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8865" to be "running"
    Sep  6 11:17:14.887: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.053427ms
    Sep  6 11:17:14.888: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  6 11:17:14.894: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Sep  6 11:17:14.894: INFO: Going to poll 10.233.120.116 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Sep  6 11:17:14.901: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.120.116:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8865 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:17:14.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:17:14.903: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:17:14.903: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8865/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.120.116%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  6 11:17:15.034: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  6 11:17:15.034: INFO: Going to poll 10.233.120.215 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Sep  6 11:17:15.038: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.120.215:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8865 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:17:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:17:15.039: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:17:15.039: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8865/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.120.215%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  6 11:17:15.133: INFO: Found all 1 expected endpoints: [netserver-1]
    Sep  6 11:17:15.133: INFO: Going to poll 10.233.99.109 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Sep  6 11:17:15.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.99.109:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8865 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:17:15.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:17:15.139: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:17:15.139: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8865/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.99.109%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  6 11:17:15.192: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:17:15.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8865" for this suite. 09/06/23 11:17:15.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:17:15.208
Sep  6 11:17:15.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replicaset 09/06/23 11:17:15.209
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:17:15.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:17:15.232
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 09/06/23 11:17:15.238
STEP: Verify that the required pods have come up. 09/06/23 11:17:15.243
Sep  6 11:17:15.247: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 11:17:20.258: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/06/23 11:17:20.258
STEP: Getting /status 09/06/23 11:17:20.259
Sep  6 11:17:20.265: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 09/06/23 11:17:20.265
Sep  6 11:17:20.285: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 09/06/23 11:17:20.286
Sep  6 11:17:20.288: INFO: Observed &ReplicaSet event: ADDED
Sep  6 11:17:20.288: INFO: Observed &ReplicaSet event: MODIFIED
Sep  6 11:17:20.288: INFO: Observed &ReplicaSet event: MODIFIED
Sep  6 11:17:20.288: INFO: Observed &ReplicaSet event: MODIFIED
Sep  6 11:17:20.288: INFO: Found replicaset test-rs in namespace replicaset-7396 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  6 11:17:20.288: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 09/06/23 11:17:20.288
Sep  6 11:17:20.288: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  6 11:17:20.300: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 09/06/23 11:17:20.3
Sep  6 11:17:20.304: INFO: Observed &ReplicaSet event: ADDED
Sep  6 11:17:20.304: INFO: Observed &ReplicaSet event: MODIFIED
Sep  6 11:17:20.304: INFO: Observed &ReplicaSet event: MODIFIED
Sep  6 11:17:20.305: INFO: Observed &ReplicaSet event: MODIFIED
Sep  6 11:17:20.305: INFO: Observed replicaset test-rs in namespace replicaset-7396 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  6 11:17:20.305: INFO: Observed &ReplicaSet event: MODIFIED
Sep  6 11:17:20.305: INFO: Found replicaset test-rs in namespace replicaset-7396 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep  6 11:17:20.305: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:17:20.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7396" for this suite. 09/06/23 11:17:20.316
------------------------------
• [SLOW TEST] [5.198 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:17:15.208
    Sep  6 11:17:15.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replicaset 09/06/23 11:17:15.209
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:17:15.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:17:15.232
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 09/06/23 11:17:15.238
    STEP: Verify that the required pods have come up. 09/06/23 11:17:15.243
    Sep  6 11:17:15.247: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  6 11:17:20.258: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/06/23 11:17:20.258
    STEP: Getting /status 09/06/23 11:17:20.259
    Sep  6 11:17:20.265: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 09/06/23 11:17:20.265
    Sep  6 11:17:20.285: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 09/06/23 11:17:20.286
    Sep  6 11:17:20.288: INFO: Observed &ReplicaSet event: ADDED
    Sep  6 11:17:20.288: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  6 11:17:20.288: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  6 11:17:20.288: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  6 11:17:20.288: INFO: Found replicaset test-rs in namespace replicaset-7396 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  6 11:17:20.288: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 09/06/23 11:17:20.288
    Sep  6 11:17:20.288: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  6 11:17:20.300: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 09/06/23 11:17:20.3
    Sep  6 11:17:20.304: INFO: Observed &ReplicaSet event: ADDED
    Sep  6 11:17:20.304: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  6 11:17:20.304: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  6 11:17:20.305: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  6 11:17:20.305: INFO: Observed replicaset test-rs in namespace replicaset-7396 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  6 11:17:20.305: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  6 11:17:20.305: INFO: Found replicaset test-rs in namespace replicaset-7396 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Sep  6 11:17:20.305: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:17:20.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7396" for this suite. 09/06/23 11:17:20.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:17:20.406
Sep  6 11:17:20.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-watch 09/06/23 11:17:20.407
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:17:20.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:17:20.506
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Sep  6 11:17:20.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Creating first CR  09/06/23 11:17:28.119
Sep  6 11:17:28.125: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:28Z]] name:name1 resourceVersion:32756 uid:49ed3009-7f8e-41cf-9fea-4978239430c7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 09/06/23 11:17:38.125
Sep  6 11:17:38.142: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:38Z]] name:name2 resourceVersion:32777 uid:809088e5-00c4-4677-acef-0c7f501de10a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 09/06/23 11:17:48.142
Sep  6 11:17:48.162: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:48Z]] name:name1 resourceVersion:32798 uid:49ed3009-7f8e-41cf-9fea-4978239430c7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 09/06/23 11:17:58.165
Sep  6 11:17:58.230: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:58Z]] name:name2 resourceVersion:32819 uid:809088e5-00c4-4677-acef-0c7f501de10a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 09/06/23 11:18:08.233
Sep  6 11:18:08.251: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:48Z]] name:name1 resourceVersion:32837 uid:49ed3009-7f8e-41cf-9fea-4978239430c7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 09/06/23 11:18:18.253
Sep  6 11:18:19.367: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:58Z]] name:name2 resourceVersion:32859 uid:809088e5-00c4-4677-acef-0c7f501de10a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:18:29.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-3345" for this suite. 09/06/23 11:18:29.916
------------------------------
• [SLOW TEST] [69.526 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:17:20.406
    Sep  6 11:17:20.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-watch 09/06/23 11:17:20.407
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:17:20.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:17:20.506
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Sep  6 11:17:20.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Creating first CR  09/06/23 11:17:28.119
    Sep  6 11:17:28.125: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:28Z]] name:name1 resourceVersion:32756 uid:49ed3009-7f8e-41cf-9fea-4978239430c7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 09/06/23 11:17:38.125
    Sep  6 11:17:38.142: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:38Z]] name:name2 resourceVersion:32777 uid:809088e5-00c4-4677-acef-0c7f501de10a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 09/06/23 11:17:48.142
    Sep  6 11:17:48.162: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:48Z]] name:name1 resourceVersion:32798 uid:49ed3009-7f8e-41cf-9fea-4978239430c7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 09/06/23 11:17:58.165
    Sep  6 11:17:58.230: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:58Z]] name:name2 resourceVersion:32819 uid:809088e5-00c4-4677-acef-0c7f501de10a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 09/06/23 11:18:08.233
    Sep  6 11:18:08.251: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:48Z]] name:name1 resourceVersion:32837 uid:49ed3009-7f8e-41cf-9fea-4978239430c7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 09/06/23 11:18:18.253
    Sep  6 11:18:19.367: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-06T11:17:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-06T11:17:58Z]] name:name2 resourceVersion:32859 uid:809088e5-00c4-4677-acef-0c7f501de10a] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:18:29.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-3345" for this suite. 09/06/23 11:18:29.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:18:29.934
Sep  6 11:18:29.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename dns 09/06/23 11:18:29.935
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:18:29.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:18:29.964
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 09/06/23 11:18:29.967
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 186.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.186_tcp@PTR;sleep 1; done
 09/06/23 11:18:29.994
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 186.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.186_tcp@PTR;sleep 1; done
 09/06/23 11:18:29.994
STEP: creating a pod to probe DNS 09/06/23 11:18:29.994
STEP: submitting the pod to kubernetes 09/06/23 11:18:29.994
Sep  6 11:18:30.019: INFO: Waiting up to 15m0s for pod "dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d" in namespace "dns-1654" to be "running"
Sep  6 11:18:30.028: INFO: Pod "dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.634076ms
Sep  6 11:18:32.035: INFO: Pod "dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016324967s
Sep  6 11:18:32.035: INFO: Pod "dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d" satisfied condition "running"
STEP: retrieving the pod 09/06/23 11:18:32.035
STEP: looking for the results for each expected name from probers 09/06/23 11:18:32.039
Sep  6 11:18:32.048: INFO: Unable to read wheezy_udp@dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
Sep  6 11:18:32.055: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
Sep  6 11:18:32.063: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
Sep  6 11:18:32.067: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
Sep  6 11:18:32.088: INFO: Unable to read jessie_udp@dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
Sep  6 11:18:32.092: INFO: Unable to read jessie_tcp@dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
Sep  6 11:18:32.098: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
Sep  6 11:18:32.102: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
Sep  6 11:18:32.118: INFO: Lookups using dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d failed for: [wheezy_udp@dns-test-service.dns-1654.svc.cluster.local wheezy_tcp@dns-test-service.dns-1654.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local jessie_udp@dns-test-service.dns-1654.svc.cluster.local jessie_tcp@dns-test-service.dns-1654.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local]

Sep  6 11:18:37.173: INFO: DNS probes using dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d succeeded

STEP: deleting the pod 09/06/23 11:18:37.173
STEP: deleting the test service 09/06/23 11:18:37.215
STEP: deleting the test headless service 09/06/23 11:18:37.276
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  6 11:18:37.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1654" for this suite. 09/06/23 11:18:37.309
------------------------------
• [SLOW TEST] [7.384 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:18:29.934
    Sep  6 11:18:29.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename dns 09/06/23 11:18:29.935
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:18:29.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:18:29.964
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 09/06/23 11:18:29.967
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 186.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.186_tcp@PTR;sleep 1; done
     09/06/23 11:18:29.994
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 186.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.186_tcp@PTR;sleep 1; done
     09/06/23 11:18:29.994
    STEP: creating a pod to probe DNS 09/06/23 11:18:29.994
    STEP: submitting the pod to kubernetes 09/06/23 11:18:29.994
    Sep  6 11:18:30.019: INFO: Waiting up to 15m0s for pod "dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d" in namespace "dns-1654" to be "running"
    Sep  6 11:18:30.028: INFO: Pod "dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.634076ms
    Sep  6 11:18:32.035: INFO: Pod "dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016324967s
    Sep  6 11:18:32.035: INFO: Pod "dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 11:18:32.035
    STEP: looking for the results for each expected name from probers 09/06/23 11:18:32.039
    Sep  6 11:18:32.048: INFO: Unable to read wheezy_udp@dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
    Sep  6 11:18:32.055: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
    Sep  6 11:18:32.063: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
    Sep  6 11:18:32.067: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
    Sep  6 11:18:32.088: INFO: Unable to read jessie_udp@dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
    Sep  6 11:18:32.092: INFO: Unable to read jessie_tcp@dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
    Sep  6 11:18:32.098: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
    Sep  6 11:18:32.102: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local from pod dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d: the server could not find the requested resource (get pods dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d)
    Sep  6 11:18:32.118: INFO: Lookups using dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d failed for: [wheezy_udp@dns-test-service.dns-1654.svc.cluster.local wheezy_tcp@dns-test-service.dns-1654.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local jessie_udp@dns-test-service.dns-1654.svc.cluster.local jessie_tcp@dns-test-service.dns-1654.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1654.svc.cluster.local]

    Sep  6 11:18:37.173: INFO: DNS probes using dns-1654/dns-test-aa090c84-b577-46cc-8ee1-5f4e0f0bca0d succeeded

    STEP: deleting the pod 09/06/23 11:18:37.173
    STEP: deleting the test service 09/06/23 11:18:37.215
    STEP: deleting the test headless service 09/06/23 11:18:37.276
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:18:37.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1654" for this suite. 09/06/23 11:18:37.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:18:37.319
Sep  6 11:18:37.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-pred 09/06/23 11:18:37.319
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:18:37.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:18:37.348
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  6 11:18:37.351: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 11:18:37.359: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 11:18:37.363: INFO: 
Logging pods the apiserver thinks is on node kube-1 before test
Sep  6 11:18:37.373: INFO: calico-kube-controllers-6dfcdfb99-6q4ng from kube-system started at 2023-09-06 09:55:41 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 11:18:37.373: INFO: calico-node-pkqgc from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container calico-node ready: true, restart count 2
Sep  6 11:18:37.373: INFO: coredns-645b46f4b6-hq55k from kube-system started at 2023-09-06 09:55:53 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container coredns ready: true, restart count 0
Sep  6 11:18:37.373: INFO: kube-apiserver-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container kube-apiserver ready: true, restart count 2
Sep  6 11:18:37.373: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container kube-controller-manager ready: true, restart count 5
Sep  6 11:18:37.373: INFO: kube-proxy-fjqk6 from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 11:18:37.373: INFO: kube-scheduler-kube-1 from kube-system started at 2023-09-06 09:52:15 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container kube-scheduler ready: true, restart count 4
Sep  6 11:18:37.373: INFO: nodelocaldns-74qn2 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 11:18:37.373: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 11:18:37.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:18:37.373: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 11:18:37.373: INFO: 
Logging pods the apiserver thinks is on node kube-2 before test
Sep  6 11:18:37.380: INFO: calico-node-f57x2 from kube-system started at 2023-09-06 09:54:23 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container calico-node ready: true, restart count 2
Sep  6 11:18:37.380: INFO: coredns-645b46f4b6-9lpfv from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container coredns ready: true, restart count 0
Sep  6 11:18:37.380: INFO: dns-autoscaler-659b8c48cb-5h6w8 from kube-system started at 2023-09-06 09:55:57 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container autoscaler ready: true, restart count 0
Sep  6 11:18:37.380: INFO: kube-apiserver-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container kube-apiserver ready: true, restart count 1
Sep  6 11:18:37.380: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-09-06 09:53:08 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container kube-controller-manager ready: true, restart count 4
Sep  6 11:18:37.380: INFO: kube-proxy-7fxzk from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 11:18:37.380: INFO: kube-scheduler-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container kube-scheduler ready: true, restart count 4
Sep  6 11:18:37.380: INFO: nodelocaldns-jpj4c from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 11:18:37.380: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 11:18:37.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:18:37.380: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 11:18:37.380: INFO: 
Logging pods the apiserver thinks is on node kube-3 before test
Sep  6 11:18:37.389: INFO: calico-node-6w7db from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.389: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 11:18:37.389: INFO: kube-proxy-sfndk from kube-system started at 2023-09-06 09:54:02 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.389: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 11:18:37.389: INFO: nginx-proxy-kube-3 from kube-system started at 2023-09-06 09:53:42 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.389: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  6 11:18:37.389: INFO: nodelocaldns-c9bb4 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.389: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 11:18:37.389: INFO: sonobuoy from sonobuoy started at 2023-09-06 09:59:53 +0000 UTC (1 container statuses recorded)
Sep  6 11:18:37.389: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 11:18:37.389: INFO: sonobuoy-e2e-job-c7c8c161973b4a54 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 11:18:37.389: INFO: 	Container e2e ready: true, restart count 0
Sep  6 11:18:37.389: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:18:37.389: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 11:18:37.389: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:18:37.389: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/06/23 11:18:37.389
Sep  6 11:18:37.402: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3547" to be "running"
Sep  6 11:18:37.410: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074043ms
Sep  6 11:18:39.425: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.023252546s
Sep  6 11:18:39.425: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/06/23 11:18:39.435
STEP: Trying to apply a random label on the found node. 09/06/23 11:18:39.471
STEP: verifying the node has the label kubernetes.io/e2e-4e234ca6-0023-414f-82da-d1200e657bef 95 09/06/23 11:18:39.486
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/06/23 11:18:39.494
Sep  6 11:18:39.503: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3547" to be "not pending"
Sep  6 11:18:39.518: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.927112ms
Sep  6 11:18:41.550: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046860394s
Sep  6 11:18:43.522: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.019305947s
Sep  6 11:18:43.522: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.2.20.103 on the node which pod4 resides and expect not scheduled 09/06/23 11:18:43.522
Sep  6 11:18:43.534: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3547" to be "not pending"
Sep  6 11:18:43.543: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.402142ms
Sep  6 11:18:45.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020780191s
Sep  6 11:18:47.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018146556s
Sep  6 11:18:49.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024929823s
Sep  6 11:18:51.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016222697s
Sep  6 11:18:53.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019162427s
Sep  6 11:18:55.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018603492s
Sep  6 11:18:57.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.022012937s
Sep  6 11:18:59.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.01868671s
Sep  6 11:19:01.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.018983938s
Sep  6 11:19:03.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018184878s
Sep  6 11:19:05.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.022583074s
Sep  6 11:19:07.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01571084s
Sep  6 11:19:09.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021775119s
Sep  6 11:19:11.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.017800492s
Sep  6 11:19:13.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.023779051s
Sep  6 11:19:15.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.02180448s
Sep  6 11:19:17.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.025253158s
Sep  6 11:19:19.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.018845516s
Sep  6 11:19:21.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.019044177s
Sep  6 11:19:23.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01746202s
Sep  6 11:19:25.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.023405281s
Sep  6 11:19:27.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.020484216s
Sep  6 11:19:29.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.024257465s
Sep  6 11:19:31.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.016880675s
Sep  6 11:19:33.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.023108704s
Sep  6 11:19:35.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01952885s
Sep  6 11:19:37.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.02495381s
Sep  6 11:19:39.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.017987505s
Sep  6 11:19:41.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.018686497s
Sep  6 11:19:43.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.0182951s
Sep  6 11:19:45.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.022880385s
Sep  6 11:19:47.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01523166s
Sep  6 11:19:49.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023399616s
Sep  6 11:19:51.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017798285s
Sep  6 11:19:53.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.025016173s
Sep  6 11:19:55.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017762918s
Sep  6 11:19:57.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.02237171s
Sep  6 11:19:59.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.017911586s
Sep  6 11:20:01.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019650473s
Sep  6 11:20:03.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018077431s
Sep  6 11:20:05.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.022803054s
Sep  6 11:20:07.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.020591388s
Sep  6 11:20:09.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.020127592s
Sep  6 11:20:11.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014460782s
Sep  6 11:20:13.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021506342s
Sep  6 11:20:15.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019206645s
Sep  6 11:20:17.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.016781829s
Sep  6 11:20:19.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014210015s
Sep  6 11:20:21.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01960689s
Sep  6 11:20:23.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019486553s
Sep  6 11:20:25.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022161005s
Sep  6 11:20:27.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.019509632s
Sep  6 11:20:29.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.021279921s
Sep  6 11:20:31.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019456715s
Sep  6 11:20:33.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.022838377s
Sep  6 11:20:35.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.017051452s
Sep  6 11:20:37.560: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.025825485s
Sep  6 11:20:39.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.015588734s
Sep  6 11:20:41.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018772695s
Sep  6 11:20:43.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.020482799s
Sep  6 11:20:45.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.020981276s
Sep  6 11:20:47.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.018925152s
Sep  6 11:20:49.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025169191s
Sep  6 11:20:51.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.013577065s
Sep  6 11:20:53.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.024410764s
Sep  6 11:20:55.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013031313s
Sep  6 11:20:57.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.023889641s
Sep  6 11:20:59.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.020934605s
Sep  6 11:21:01.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.021739047s
Sep  6 11:21:03.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01551525s
Sep  6 11:21:05.719: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.185168218s
Sep  6 11:21:07.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.024236944s
Sep  6 11:21:09.577: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.042836s
Sep  6 11:21:11.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.014972379s
Sep  6 11:21:13.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.018383287s
Sep  6 11:21:15.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.021688299s
Sep  6 11:21:17.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.023875644s
Sep  6 11:21:19.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.016345403s
Sep  6 11:21:21.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.021883319s
Sep  6 11:21:23.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.022261962s
Sep  6 11:21:25.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.024051694s
Sep  6 11:21:27.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.015841741s
Sep  6 11:21:29.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.02273698s
Sep  6 11:21:31.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.015945396s
Sep  6 11:21:33.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.022722879s
Sep  6 11:21:35.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.018833512s
Sep  6 11:21:37.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.022278571s
Sep  6 11:21:39.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.019423542s
Sep  6 11:21:41.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.013414185s
Sep  6 11:21:43.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.019613903s
Sep  6 11:21:45.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.014492702s
Sep  6 11:21:47.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.013691913s
Sep  6 11:21:49.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.024828849s
Sep  6 11:21:51.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013361503s
Sep  6 11:21:53.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.022462366s
Sep  6 11:21:55.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.019933003s
Sep  6 11:21:57.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.022652178s
Sep  6 11:21:59.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.017686559s
Sep  6 11:22:01.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.016642094s
Sep  6 11:22:03.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.018226673s
Sep  6 11:22:05.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.022098427s
Sep  6 11:22:07.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.020894634s
Sep  6 11:22:09.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.022764207s
Sep  6 11:22:11.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.018555778s
Sep  6 11:22:13.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.025574358s
Sep  6 11:22:15.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.016580086s
Sep  6 11:22:17.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.013887162s
Sep  6 11:22:19.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.020491037s
Sep  6 11:22:21.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.022021407s
Sep  6 11:22:23.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.020988751s
Sep  6 11:22:25.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.022523408s
Sep  6 11:22:27.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.018155925s
Sep  6 11:22:29.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.025293055s
Sep  6 11:22:31.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.014287244s
Sep  6 11:22:33.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.021171807s
Sep  6 11:22:35.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.016039598s
Sep  6 11:22:37.565: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.031178s
Sep  6 11:22:39.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.02779246s
Sep  6 11:22:41.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.01658335s
Sep  6 11:22:43.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.017098244s
Sep  6 11:22:45.560: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.026511307s
Sep  6 11:22:47.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.020153583s
Sep  6 11:22:49.546: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.012694601s
Sep  6 11:22:51.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.017473254s
Sep  6 11:22:53.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.016501841s
Sep  6 11:22:55.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.01417305s
Sep  6 11:22:57.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.018225014s
Sep  6 11:22:59.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.01890474s
Sep  6 11:23:01.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.014679773s
Sep  6 11:23:03.546: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012687938s
Sep  6 11:23:05.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.024048197s
Sep  6 11:23:07.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.018596523s
Sep  6 11:23:09.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.01519224s
Sep  6 11:23:11.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016322653s
Sep  6 11:23:13.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.02211183s
Sep  6 11:23:15.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.015669343s
Sep  6 11:23:17.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.024407662s
Sep  6 11:23:19.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.016081246s
Sep  6 11:23:21.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.018210228s
Sep  6 11:23:23.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.015355518s
Sep  6 11:23:25.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.018018047s
Sep  6 11:23:27.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.019668865s
Sep  6 11:23:29.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.01450423s
Sep  6 11:23:31.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.017267827s
Sep  6 11:23:33.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.019906449s
Sep  6 11:23:35.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.019547459s
Sep  6 11:23:37.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.024566672s
Sep  6 11:23:39.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017808929s
Sep  6 11:23:41.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.017104488s
Sep  6 11:23:43.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017918688s
Sep  6 11:23:43.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.027120885s
STEP: removing the label kubernetes.io/e2e-4e234ca6-0023-414f-82da-d1200e657bef off the node kube-3 09/06/23 11:23:43.561
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4e234ca6-0023-414f-82da-d1200e657bef 09/06/23 11:23:43.586
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:23:43.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3547" for this suite. 09/06/23 11:23:43.597
------------------------------
• [SLOW TEST] [306.288 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:18:37.319
    Sep  6 11:18:37.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-pred 09/06/23 11:18:37.319
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:18:37.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:18:37.348
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  6 11:18:37.351: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  6 11:18:37.359: INFO: Waiting for terminating namespaces to be deleted...
    Sep  6 11:18:37.363: INFO: 
    Logging pods the apiserver thinks is on node kube-1 before test
    Sep  6 11:18:37.373: INFO: calico-kube-controllers-6dfcdfb99-6q4ng from kube-system started at 2023-09-06 09:55:41 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  6 11:18:37.373: INFO: calico-node-pkqgc from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container calico-node ready: true, restart count 2
    Sep  6 11:18:37.373: INFO: coredns-645b46f4b6-hq55k from kube-system started at 2023-09-06 09:55:53 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container coredns ready: true, restart count 0
    Sep  6 11:18:37.373: INFO: kube-apiserver-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container kube-apiserver ready: true, restart count 2
    Sep  6 11:18:37.373: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Sep  6 11:18:37.373: INFO: kube-proxy-fjqk6 from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 11:18:37.373: INFO: kube-scheduler-kube-1 from kube-system started at 2023-09-06 09:52:15 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container kube-scheduler ready: true, restart count 4
    Sep  6 11:18:37.373: INFO: nodelocaldns-74qn2 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 11:18:37.373: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 11:18:37.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 11:18:37.373: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  6 11:18:37.373: INFO: 
    Logging pods the apiserver thinks is on node kube-2 before test
    Sep  6 11:18:37.380: INFO: calico-node-f57x2 from kube-system started at 2023-09-06 09:54:23 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container calico-node ready: true, restart count 2
    Sep  6 11:18:37.380: INFO: coredns-645b46f4b6-9lpfv from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container coredns ready: true, restart count 0
    Sep  6 11:18:37.380: INFO: dns-autoscaler-659b8c48cb-5h6w8 from kube-system started at 2023-09-06 09:55:57 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  6 11:18:37.380: INFO: kube-apiserver-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container kube-apiserver ready: true, restart count 1
    Sep  6 11:18:37.380: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-09-06 09:53:08 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Sep  6 11:18:37.380: INFO: kube-proxy-7fxzk from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 11:18:37.380: INFO: kube-scheduler-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container kube-scheduler ready: true, restart count 4
    Sep  6 11:18:37.380: INFO: nodelocaldns-jpj4c from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 11:18:37.380: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 11:18:37.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 11:18:37.380: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  6 11:18:37.380: INFO: 
    Logging pods the apiserver thinks is on node kube-3 before test
    Sep  6 11:18:37.389: INFO: calico-node-6w7db from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.389: INFO: 	Container calico-node ready: true, restart count 0
    Sep  6 11:18:37.389: INFO: kube-proxy-sfndk from kube-system started at 2023-09-06 09:54:02 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.389: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 11:18:37.389: INFO: nginx-proxy-kube-3 from kube-system started at 2023-09-06 09:53:42 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.389: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  6 11:18:37.389: INFO: nodelocaldns-c9bb4 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.389: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 11:18:37.389: INFO: sonobuoy from sonobuoy started at 2023-09-06 09:59:53 +0000 UTC (1 container statuses recorded)
    Sep  6 11:18:37.389: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  6 11:18:37.389: INFO: sonobuoy-e2e-job-c7c8c161973b4a54 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 11:18:37.389: INFO: 	Container e2e ready: true, restart count 0
    Sep  6 11:18:37.389: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 11:18:37.389: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 11:18:37.389: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 11:18:37.389: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/06/23 11:18:37.389
    Sep  6 11:18:37.402: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3547" to be "running"
    Sep  6 11:18:37.410: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074043ms
    Sep  6 11:18:39.425: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.023252546s
    Sep  6 11:18:39.425: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/06/23 11:18:39.435
    STEP: Trying to apply a random label on the found node. 09/06/23 11:18:39.471
    STEP: verifying the node has the label kubernetes.io/e2e-4e234ca6-0023-414f-82da-d1200e657bef 95 09/06/23 11:18:39.486
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/06/23 11:18:39.494
    Sep  6 11:18:39.503: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3547" to be "not pending"
    Sep  6 11:18:39.518: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.927112ms
    Sep  6 11:18:41.550: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046860394s
    Sep  6 11:18:43.522: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.019305947s
    Sep  6 11:18:43.522: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.2.20.103 on the node which pod4 resides and expect not scheduled 09/06/23 11:18:43.522
    Sep  6 11:18:43.534: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3547" to be "not pending"
    Sep  6 11:18:43.543: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.402142ms
    Sep  6 11:18:45.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020780191s
    Sep  6 11:18:47.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018146556s
    Sep  6 11:18:49.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024929823s
    Sep  6 11:18:51.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016222697s
    Sep  6 11:18:53.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019162427s
    Sep  6 11:18:55.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018603492s
    Sep  6 11:18:57.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.022012937s
    Sep  6 11:18:59.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.01868671s
    Sep  6 11:19:01.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.018983938s
    Sep  6 11:19:03.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018184878s
    Sep  6 11:19:05.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.022583074s
    Sep  6 11:19:07.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01571084s
    Sep  6 11:19:09.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021775119s
    Sep  6 11:19:11.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.017800492s
    Sep  6 11:19:13.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.023779051s
    Sep  6 11:19:15.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.02180448s
    Sep  6 11:19:17.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.025253158s
    Sep  6 11:19:19.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.018845516s
    Sep  6 11:19:21.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.019044177s
    Sep  6 11:19:23.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01746202s
    Sep  6 11:19:25.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.023405281s
    Sep  6 11:19:27.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.020484216s
    Sep  6 11:19:29.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.024257465s
    Sep  6 11:19:31.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.016880675s
    Sep  6 11:19:33.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.023108704s
    Sep  6 11:19:35.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01952885s
    Sep  6 11:19:37.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.02495381s
    Sep  6 11:19:39.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.017987505s
    Sep  6 11:19:41.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.018686497s
    Sep  6 11:19:43.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.0182951s
    Sep  6 11:19:45.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.022880385s
    Sep  6 11:19:47.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01523166s
    Sep  6 11:19:49.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023399616s
    Sep  6 11:19:51.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017798285s
    Sep  6 11:19:53.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.025016173s
    Sep  6 11:19:55.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017762918s
    Sep  6 11:19:57.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.02237171s
    Sep  6 11:19:59.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.017911586s
    Sep  6 11:20:01.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019650473s
    Sep  6 11:20:03.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018077431s
    Sep  6 11:20:05.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.022803054s
    Sep  6 11:20:07.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.020591388s
    Sep  6 11:20:09.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.020127592s
    Sep  6 11:20:11.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014460782s
    Sep  6 11:20:13.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021506342s
    Sep  6 11:20:15.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019206645s
    Sep  6 11:20:17.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.016781829s
    Sep  6 11:20:19.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014210015s
    Sep  6 11:20:21.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01960689s
    Sep  6 11:20:23.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019486553s
    Sep  6 11:20:25.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022161005s
    Sep  6 11:20:27.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.019509632s
    Sep  6 11:20:29.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.021279921s
    Sep  6 11:20:31.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019456715s
    Sep  6 11:20:33.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.022838377s
    Sep  6 11:20:35.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.017051452s
    Sep  6 11:20:37.560: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.025825485s
    Sep  6 11:20:39.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.015588734s
    Sep  6 11:20:41.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018772695s
    Sep  6 11:20:43.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.020482799s
    Sep  6 11:20:45.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.020981276s
    Sep  6 11:20:47.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.018925152s
    Sep  6 11:20:49.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025169191s
    Sep  6 11:20:51.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.013577065s
    Sep  6 11:20:53.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.024410764s
    Sep  6 11:20:55.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013031313s
    Sep  6 11:20:57.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.023889641s
    Sep  6 11:20:59.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.020934605s
    Sep  6 11:21:01.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.021739047s
    Sep  6 11:21:03.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01551525s
    Sep  6 11:21:05.719: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.185168218s
    Sep  6 11:21:07.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.024236944s
    Sep  6 11:21:09.577: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.042836s
    Sep  6 11:21:11.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.014972379s
    Sep  6 11:21:13.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.018383287s
    Sep  6 11:21:15.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.021688299s
    Sep  6 11:21:17.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.023875644s
    Sep  6 11:21:19.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.016345403s
    Sep  6 11:21:21.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.021883319s
    Sep  6 11:21:23.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.022261962s
    Sep  6 11:21:25.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.024051694s
    Sep  6 11:21:27.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.015841741s
    Sep  6 11:21:29.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.02273698s
    Sep  6 11:21:31.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.015945396s
    Sep  6 11:21:33.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.022722879s
    Sep  6 11:21:35.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.018833512s
    Sep  6 11:21:37.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.022278571s
    Sep  6 11:21:39.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.019423542s
    Sep  6 11:21:41.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.013414185s
    Sep  6 11:21:43.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.019613903s
    Sep  6 11:21:45.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.014492702s
    Sep  6 11:21:47.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.013691913s
    Sep  6 11:21:49.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.024828849s
    Sep  6 11:21:51.547: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013361503s
    Sep  6 11:21:53.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.022462366s
    Sep  6 11:21:55.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.019933003s
    Sep  6 11:21:57.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.022652178s
    Sep  6 11:21:59.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.017686559s
    Sep  6 11:22:01.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.016642094s
    Sep  6 11:22:03.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.018226673s
    Sep  6 11:22:05.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.022098427s
    Sep  6 11:22:07.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.020894634s
    Sep  6 11:22:09.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.022764207s
    Sep  6 11:22:11.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.018555778s
    Sep  6 11:22:13.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.025574358s
    Sep  6 11:22:15.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.016580086s
    Sep  6 11:22:17.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.013887162s
    Sep  6 11:22:19.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.020491037s
    Sep  6 11:22:21.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.022021407s
    Sep  6 11:22:23.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.020988751s
    Sep  6 11:22:25.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.022523408s
    Sep  6 11:22:27.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.018155925s
    Sep  6 11:22:29.559: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.025293055s
    Sep  6 11:22:31.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.014287244s
    Sep  6 11:22:33.555: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.021171807s
    Sep  6 11:22:35.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.016039598s
    Sep  6 11:22:37.565: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.031178s
    Sep  6 11:22:39.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.02779246s
    Sep  6 11:22:41.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.01658335s
    Sep  6 11:22:43.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.017098244s
    Sep  6 11:22:45.560: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.026511307s
    Sep  6 11:22:47.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.020153583s
    Sep  6 11:22:49.546: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.012694601s
    Sep  6 11:22:51.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.017473254s
    Sep  6 11:22:53.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.016501841s
    Sep  6 11:22:55.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.01417305s
    Sep  6 11:22:57.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.018225014s
    Sep  6 11:22:59.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.01890474s
    Sep  6 11:23:01.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.014679773s
    Sep  6 11:23:03.546: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012687938s
    Sep  6 11:23:05.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.024048197s
    Sep  6 11:23:07.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.018596523s
    Sep  6 11:23:09.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.01519224s
    Sep  6 11:23:11.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016322653s
    Sep  6 11:23:13.556: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.02211183s
    Sep  6 11:23:15.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.015669343s
    Sep  6 11:23:17.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.024407662s
    Sep  6 11:23:19.550: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.016081246s
    Sep  6 11:23:21.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.018210228s
    Sep  6 11:23:23.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.015355518s
    Sep  6 11:23:25.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.018018047s
    Sep  6 11:23:27.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.019668865s
    Sep  6 11:23:29.548: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.01450423s
    Sep  6 11:23:31.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.017267827s
    Sep  6 11:23:33.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.019906449s
    Sep  6 11:23:35.553: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.019547459s
    Sep  6 11:23:37.558: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.024566672s
    Sep  6 11:23:39.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017808929s
    Sep  6 11:23:41.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.017104488s
    Sep  6 11:23:43.552: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017918688s
    Sep  6 11:23:43.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.027120885s
    STEP: removing the label kubernetes.io/e2e-4e234ca6-0023-414f-82da-d1200e657bef off the node kube-3 09/06/23 11:23:43.561
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-4e234ca6-0023-414f-82da-d1200e657bef 09/06/23 11:23:43.586
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:23:43.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3547" for this suite. 09/06/23 11:23:43.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:23:43.61
Sep  6 11:23:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:23:43.611
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:23:43.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:23:43.643
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-6b93ff1d-b6df-4ec7-a357-a7dde3d2494b 09/06/23 11:23:43.646
STEP: Creating a pod to test consume secrets 09/06/23 11:23:43.654
Sep  6 11:23:43.675: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d" in namespace "projected-8586" to be "Succeeded or Failed"
Sep  6 11:23:43.685: INFO: Pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.762806ms
Sep  6 11:23:45.697: INFO: Pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022416338s
Sep  6 11:23:47.698: INFO: Pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023304868s
STEP: Saw pod success 09/06/23 11:23:47.698
Sep  6 11:23:47.699: INFO: Pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d" satisfied condition "Succeeded or Failed"
Sep  6 11:23:47.710: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d container projected-secret-volume-test: <nil>
STEP: delete the pod 09/06/23 11:23:47.766
Sep  6 11:23:47.790: INFO: Waiting for pod pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d to disappear
Sep  6 11:23:47.794: INFO: Pod pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  6 11:23:47.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8586" for this suite. 09/06/23 11:23:47.799
------------------------------
• [4.196 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:23:43.61
    Sep  6 11:23:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:23:43.611
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:23:43.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:23:43.643
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-6b93ff1d-b6df-4ec7-a357-a7dde3d2494b 09/06/23 11:23:43.646
    STEP: Creating a pod to test consume secrets 09/06/23 11:23:43.654
    Sep  6 11:23:43.675: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d" in namespace "projected-8586" to be "Succeeded or Failed"
    Sep  6 11:23:43.685: INFO: Pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.762806ms
    Sep  6 11:23:45.697: INFO: Pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022416338s
    Sep  6 11:23:47.698: INFO: Pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023304868s
    STEP: Saw pod success 09/06/23 11:23:47.698
    Sep  6 11:23:47.699: INFO: Pod "pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d" satisfied condition "Succeeded or Failed"
    Sep  6 11:23:47.710: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 11:23:47.766
    Sep  6 11:23:47.790: INFO: Waiting for pod pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d to disappear
    Sep  6 11:23:47.794: INFO: Pod pod-projected-secrets-671e7a77-b212-4421-a0fa-edebb34e421d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:23:47.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8586" for this suite. 09/06/23 11:23:47.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:23:47.807
Sep  6 11:23:47.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename server-version 09/06/23 11:23:47.808
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:23:47.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:23:47.828
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 09/06/23 11:23:47.83
STEP: Confirm major version 09/06/23 11:23:47.83
Sep  6 11:23:47.830: INFO: Major version: 1
STEP: Confirm minor version 09/06/23 11:23:47.83
Sep  6 11:23:47.831: INFO: cleanMinorVersion: 26
Sep  6 11:23:47.831: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Sep  6 11:23:47.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-8903" for this suite. 09/06/23 11:23:47.834
------------------------------
• [0.033 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:23:47.807
    Sep  6 11:23:47.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename server-version 09/06/23 11:23:47.808
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:23:47.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:23:47.828
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 09/06/23 11:23:47.83
    STEP: Confirm major version 09/06/23 11:23:47.83
    Sep  6 11:23:47.830: INFO: Major version: 1
    STEP: Confirm minor version 09/06/23 11:23:47.83
    Sep  6 11:23:47.831: INFO: cleanMinorVersion: 26
    Sep  6 11:23:47.831: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:23:47.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-8903" for this suite. 09/06/23 11:23:47.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:23:47.842
Sep  6 11:23:47.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename watch 09/06/23 11:23:47.843
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:23:47.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:23:47.864
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 09/06/23 11:23:47.866
STEP: creating a new configmap 09/06/23 11:23:47.867
STEP: modifying the configmap once 09/06/23 11:23:47.871
STEP: changing the label value of the configmap 09/06/23 11:23:47.879
STEP: Expecting to observe a delete notification for the watched object 09/06/23 11:23:47.886
Sep  6 11:23:47.886: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33679 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:23:47.886: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33680 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:23:47.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33681 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 09/06/23 11:23:47.886
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/06/23 11:23:47.894
STEP: changing the label value of the configmap back 09/06/23 11:23:57.895
STEP: modifying the configmap a third time 09/06/23 11:23:57.925
STEP: deleting the configmap 09/06/23 11:23:57.941
STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/06/23 11:23:57.948
Sep  6 11:23:57.948: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33742 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:23:57.948: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33743 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:23:57.948: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33744 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  6 11:23:57.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-934" for this suite. 09/06/23 11:23:57.954
------------------------------
• [SLOW TEST] [10.120 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:23:47.842
    Sep  6 11:23:47.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename watch 09/06/23 11:23:47.843
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:23:47.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:23:47.864
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 09/06/23 11:23:47.866
    STEP: creating a new configmap 09/06/23 11:23:47.867
    STEP: modifying the configmap once 09/06/23 11:23:47.871
    STEP: changing the label value of the configmap 09/06/23 11:23:47.879
    STEP: Expecting to observe a delete notification for the watched object 09/06/23 11:23:47.886
    Sep  6 11:23:47.886: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33679 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 11:23:47.886: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33680 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 11:23:47.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33681 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 09/06/23 11:23:47.886
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/06/23 11:23:47.894
    STEP: changing the label value of the configmap back 09/06/23 11:23:57.895
    STEP: modifying the configmap a third time 09/06/23 11:23:57.925
    STEP: deleting the configmap 09/06/23 11:23:57.941
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/06/23 11:23:57.948
    Sep  6 11:23:57.948: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33742 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 11:23:57.948: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33743 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  6 11:23:57.948: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-934  db43acff-e6c1-4215-b54e-f8e93bfd8fad 33744 0 2023-09-06 11:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-06 11:23:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:23:57.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-934" for this suite. 09/06/23 11:23:57.954
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:23:57.963
Sep  6 11:23:57.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 11:23:57.964
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:23:57.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:23:57.987
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-f84147ef-aad8-4ae4-bd72-92b46a39cd35 09/06/23 11:23:57.989
STEP: Creating a pod to test consume configMaps 09/06/23 11:23:57.994
Sep  6 11:23:58.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d" in namespace "configmap-7320" to be "Succeeded or Failed"
Sep  6 11:23:58.009: INFO: Pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.463254ms
Sep  6 11:24:00.033: INFO: Pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027382331s
Sep  6 11:24:02.021: INFO: Pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015845979s
STEP: Saw pod success 09/06/23 11:24:02.021
Sep  6 11:24:02.022: INFO: Pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d" satisfied condition "Succeeded or Failed"
Sep  6 11:24:02.036: INFO: Trying to get logs from node kube-3 pod pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d container agnhost-container: <nil>
STEP: delete the pod 09/06/23 11:24:02.063
Sep  6 11:24:02.087: INFO: Waiting for pod pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d to disappear
Sep  6 11:24:02.090: INFO: Pod pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:02.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7320" for this suite. 09/06/23 11:24:02.094
------------------------------
• [4.138 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:23:57.963
    Sep  6 11:23:57.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 11:23:57.964
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:23:57.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:23:57.987
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-f84147ef-aad8-4ae4-bd72-92b46a39cd35 09/06/23 11:23:57.989
    STEP: Creating a pod to test consume configMaps 09/06/23 11:23:57.994
    Sep  6 11:23:58.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d" in namespace "configmap-7320" to be "Succeeded or Failed"
    Sep  6 11:23:58.009: INFO: Pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.463254ms
    Sep  6 11:24:00.033: INFO: Pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027382331s
    Sep  6 11:24:02.021: INFO: Pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015845979s
    STEP: Saw pod success 09/06/23 11:24:02.021
    Sep  6 11:24:02.022: INFO: Pod "pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d" satisfied condition "Succeeded or Failed"
    Sep  6 11:24:02.036: INFO: Trying to get logs from node kube-3 pod pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 11:24:02.063
    Sep  6 11:24:02.087: INFO: Waiting for pod pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d to disappear
    Sep  6 11:24:02.090: INFO: Pod pod-configmaps-a66ba2ec-806b-4fab-9eed-6e1d23c3c14d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:02.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7320" for this suite. 09/06/23 11:24:02.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:02.102
Sep  6 11:24:02.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename gc 09/06/23 11:24:02.103
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:02.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:02.123
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 09/06/23 11:24:02.125
STEP: Wait for the Deployment to create new ReplicaSet 09/06/23 11:24:02.13
STEP: delete the deployment 09/06/23 11:24:02.644
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/06/23 11:24:02.661
STEP: Gathering metrics 09/06/23 11:24:03.189
Sep  6 11:24:03.208: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Sep  6 11:24:03.211: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.323187ms
Sep  6 11:24:03.211: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Sep  6 11:24:03.211: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Sep  6 11:24:03.253: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:03.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2555" for this suite. 09/06/23 11:24:03.257
------------------------------
• [1.162 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:02.102
    Sep  6 11:24:02.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename gc 09/06/23 11:24:02.103
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:02.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:02.123
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 09/06/23 11:24:02.125
    STEP: Wait for the Deployment to create new ReplicaSet 09/06/23 11:24:02.13
    STEP: delete the deployment 09/06/23 11:24:02.644
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/06/23 11:24:02.661
    STEP: Gathering metrics 09/06/23 11:24:03.189
    Sep  6 11:24:03.208: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Sep  6 11:24:03.211: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.323187ms
    Sep  6 11:24:03.211: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Sep  6 11:24:03.211: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Sep  6 11:24:03.253: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:03.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2555" for this suite. 09/06/23 11:24:03.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:03.269
Sep  6 11:24:03.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename dns 09/06/23 11:24:03.27
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:03.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:03.292
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 09/06/23 11:24:03.294
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 09/06/23 11:24:03.301
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 09/06/23 11:24:03.301
STEP: creating a pod to probe DNS 09/06/23 11:24:03.301
STEP: submitting the pod to kubernetes 09/06/23 11:24:03.301
Sep  6 11:24:03.315: INFO: Waiting up to 15m0s for pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73" in namespace "dns-7066" to be "running"
Sep  6 11:24:03.319: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253136ms
Sep  6 11:24:05.468: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.153351151s
Sep  6 11:24:07.325: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009805693s
Sep  6 11:24:09.329: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73": Phase="Running", Reason="", readiness=true. Elapsed: 6.013504482s
Sep  6 11:24:09.329: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73" satisfied condition "running"
STEP: retrieving the pod 09/06/23 11:24:09.329
STEP: looking for the results for each expected name from probers 09/06/23 11:24:09.332
Sep  6 11:24:11.777: INFO: DNS probes using dns-7066/dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73 succeeded

STEP: deleting the pod 09/06/23 11:24:11.777
STEP: deleting the test headless service 09/06/23 11:24:11.814
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:11.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7066" for this suite. 09/06/23 11:24:11.904
------------------------------
• [SLOW TEST] [8.650 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:03.269
    Sep  6 11:24:03.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename dns 09/06/23 11:24:03.27
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:03.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:03.292
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 09/06/23 11:24:03.294
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     09/06/23 11:24:03.301
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     09/06/23 11:24:03.301
    STEP: creating a pod to probe DNS 09/06/23 11:24:03.301
    STEP: submitting the pod to kubernetes 09/06/23 11:24:03.301
    Sep  6 11:24:03.315: INFO: Waiting up to 15m0s for pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73" in namespace "dns-7066" to be "running"
    Sep  6 11:24:03.319: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253136ms
    Sep  6 11:24:05.468: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.153351151s
    Sep  6 11:24:07.325: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009805693s
    Sep  6 11:24:09.329: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73": Phase="Running", Reason="", readiness=true. Elapsed: 6.013504482s
    Sep  6 11:24:09.329: INFO: Pod "dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73" satisfied condition "running"
    STEP: retrieving the pod 09/06/23 11:24:09.329
    STEP: looking for the results for each expected name from probers 09/06/23 11:24:09.332
    Sep  6 11:24:11.777: INFO: DNS probes using dns-7066/dns-test-fd443acb-f7ff-448f-8a21-012e2df1cb73 succeeded

    STEP: deleting the pod 09/06/23 11:24:11.777
    STEP: deleting the test headless service 09/06/23 11:24:11.814
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:11.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7066" for this suite. 09/06/23 11:24:11.904
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:11.92
Sep  6 11:24:11.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename svcaccounts 09/06/23 11:24:11.92
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:11.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:11.96
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Sep  6 11:24:11.969: INFO: Got root ca configmap in namespace "svcaccounts-3532"
Sep  6 11:24:11.980: INFO: Deleted root ca configmap in namespace "svcaccounts-3532"
STEP: waiting for a new root ca configmap created 09/06/23 11:24:12.481
Sep  6 11:24:12.492: INFO: Recreated root ca configmap in namespace "svcaccounts-3532"
Sep  6 11:24:12.502: INFO: Updated root ca configmap in namespace "svcaccounts-3532"
STEP: waiting for the root ca configmap reconciled 09/06/23 11:24:13.003
Sep  6 11:24:13.007: INFO: Reconciled root ca configmap in namespace "svcaccounts-3532"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:13.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3532" for this suite. 09/06/23 11:24:13.011
------------------------------
• [1.101 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:11.92
    Sep  6 11:24:11.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename svcaccounts 09/06/23 11:24:11.92
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:11.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:11.96
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Sep  6 11:24:11.969: INFO: Got root ca configmap in namespace "svcaccounts-3532"
    Sep  6 11:24:11.980: INFO: Deleted root ca configmap in namespace "svcaccounts-3532"
    STEP: waiting for a new root ca configmap created 09/06/23 11:24:12.481
    Sep  6 11:24:12.492: INFO: Recreated root ca configmap in namespace "svcaccounts-3532"
    Sep  6 11:24:12.502: INFO: Updated root ca configmap in namespace "svcaccounts-3532"
    STEP: waiting for the root ca configmap reconciled 09/06/23 11:24:13.003
    Sep  6 11:24:13.007: INFO: Reconciled root ca configmap in namespace "svcaccounts-3532"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:13.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3532" for this suite. 09/06/23 11:24:13.011
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:13.021
Sep  6 11:24:13.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:24:13.023
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:13.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:13.056
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-8760 09/06/23 11:24:13.06
STEP: creating service affinity-clusterip in namespace services-8760 09/06/23 11:24:13.06
STEP: creating replication controller affinity-clusterip in namespace services-8760 09/06/23 11:24:13.071
I0906 11:24:13.084951      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8760, replica count: 3
I0906 11:24:16.135722      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:24:16.156: INFO: Creating new exec pod
Sep  6 11:24:16.169: INFO: Waiting up to 5m0s for pod "execpod-affinityxmw7p" in namespace "services-8760" to be "running"
Sep  6 11:24:16.174: INFO: Pod "execpod-affinityxmw7p": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377102ms
Sep  6 11:24:18.179: INFO: Pod "execpod-affinityxmw7p": Phase="Running", Reason="", readiness=true. Elapsed: 2.009556442s
Sep  6 11:24:18.179: INFO: Pod "execpod-affinityxmw7p" satisfied condition "running"
Sep  6 11:24:19.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8760 exec execpod-affinityxmw7p -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Sep  6 11:24:19.522: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  6 11:24:19.522: INFO: stdout: ""
Sep  6 11:24:19.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8760 exec execpod-affinityxmw7p -- /bin/sh -x -c nc -v -z -w 2 10.233.43.112 80'
Sep  6 11:24:19.630: INFO: stderr: "+ nc -v -z -w 2 10.233.43.112 80\nConnection to 10.233.43.112 80 port [tcp/http] succeeded!\n"
Sep  6 11:24:19.630: INFO: stdout: ""
Sep  6 11:24:19.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8760 exec execpod-affinityxmw7p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.43.112:80/ ; done'
Sep  6 11:24:19.825: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n"
Sep  6 11:24:19.825: INFO: stdout: "\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv"
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
Sep  6 11:24:19.825: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8760, will wait for the garbage collector to delete the pods 09/06/23 11:24:19.85
Sep  6 11:24:19.914: INFO: Deleting ReplicationController affinity-clusterip took: 7.580784ms
Sep  6 11:24:20.014: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.143184ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8760" for this suite. 09/06/23 11:24:22.752
------------------------------
• [SLOW TEST] [9.742 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:13.021
    Sep  6 11:24:13.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:24:13.023
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:13.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:13.056
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-8760 09/06/23 11:24:13.06
    STEP: creating service affinity-clusterip in namespace services-8760 09/06/23 11:24:13.06
    STEP: creating replication controller affinity-clusterip in namespace services-8760 09/06/23 11:24:13.071
    I0906 11:24:13.084951      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8760, replica count: 3
    I0906 11:24:16.135722      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 11:24:16.156: INFO: Creating new exec pod
    Sep  6 11:24:16.169: INFO: Waiting up to 5m0s for pod "execpod-affinityxmw7p" in namespace "services-8760" to be "running"
    Sep  6 11:24:16.174: INFO: Pod "execpod-affinityxmw7p": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377102ms
    Sep  6 11:24:18.179: INFO: Pod "execpod-affinityxmw7p": Phase="Running", Reason="", readiness=true. Elapsed: 2.009556442s
    Sep  6 11:24:18.179: INFO: Pod "execpod-affinityxmw7p" satisfied condition "running"
    Sep  6 11:24:19.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8760 exec execpod-affinityxmw7p -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Sep  6 11:24:19.522: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Sep  6 11:24:19.522: INFO: stdout: ""
    Sep  6 11:24:19.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8760 exec execpod-affinityxmw7p -- /bin/sh -x -c nc -v -z -w 2 10.233.43.112 80'
    Sep  6 11:24:19.630: INFO: stderr: "+ nc -v -z -w 2 10.233.43.112 80\nConnection to 10.233.43.112 80 port [tcp/http] succeeded!\n"
    Sep  6 11:24:19.630: INFO: stdout: ""
    Sep  6 11:24:19.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-8760 exec execpod-affinityxmw7p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.43.112:80/ ; done'
    Sep  6 11:24:19.825: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.112:80/\n"
    Sep  6 11:24:19.825: INFO: stdout: "\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv\naffinity-clusterip-5xvgv"
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Received response from host: affinity-clusterip-5xvgv
    Sep  6 11:24:19.825: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-8760, will wait for the garbage collector to delete the pods 09/06/23 11:24:19.85
    Sep  6 11:24:19.914: INFO: Deleting ReplicationController affinity-clusterip took: 7.580784ms
    Sep  6 11:24:20.014: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.143184ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8760" for this suite. 09/06/23 11:24:22.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:22.764
Sep  6 11:24:22.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 11:24:22.765
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:22.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:22.796
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/06/23 11:24:22.799
Sep  6 11:24:22.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-948 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Sep  6 11:24:22.876: INFO: stderr: ""
Sep  6 11:24:22.876: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 09/06/23 11:24:22.876
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Sep  6 11:24:22.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-948 delete pods e2e-test-httpd-pod'
Sep  6 11:24:25.588: INFO: stderr: ""
Sep  6 11:24:25.588: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:25.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-948" for this suite. 09/06/23 11:24:25.591
------------------------------
• [2.834 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:22.764
    Sep  6 11:24:22.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 11:24:22.765
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:22.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:22.796
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/06/23 11:24:22.799
    Sep  6 11:24:22.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-948 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Sep  6 11:24:22.876: INFO: stderr: ""
    Sep  6 11:24:22.876: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 09/06/23 11:24:22.876
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Sep  6 11:24:22.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-948 delete pods e2e-test-httpd-pod'
    Sep  6 11:24:25.588: INFO: stderr: ""
    Sep  6 11:24:25.588: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:25.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-948" for this suite. 09/06/23 11:24:25.591
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:25.599
Sep  6 11:24:25.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename namespaces 09/06/23 11:24:25.599
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:25.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:25.622
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5880" 09/06/23 11:24:25.626
Sep  6 11:24:25.636: INFO: Namespace "namespaces-5880" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"4a83d354-710a-403a-981f-71e69f687fbc", "kubernetes.io/metadata.name":"namespaces-5880", "namespaces-5880":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:25.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5880" for this suite. 09/06/23 11:24:25.639
------------------------------
• [0.046 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:25.599
    Sep  6 11:24:25.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename namespaces 09/06/23 11:24:25.599
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:25.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:25.622
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5880" 09/06/23 11:24:25.626
    Sep  6 11:24:25.636: INFO: Namespace "namespaces-5880" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"4a83d354-710a-403a-981f-71e69f687fbc", "kubernetes.io/metadata.name":"namespaces-5880", "namespaces-5880":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:25.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5880" for this suite. 09/06/23 11:24:25.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:25.651
Sep  6 11:24:25.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:24:25.652
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:25.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:25.674
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-1777 09/06/23 11:24:25.676
STEP: creating service affinity-clusterip-transition in namespace services-1777 09/06/23 11:24:25.676
STEP: creating replication controller affinity-clusterip-transition in namespace services-1777 09/06/23 11:24:25.689
I0906 11:24:25.703065      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1777, replica count: 3
I0906 11:24:28.754535      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:24:28.782: INFO: Creating new exec pod
Sep  6 11:24:28.804: INFO: Waiting up to 5m0s for pod "execpod-affinityvnh9d" in namespace "services-1777" to be "running"
Sep  6 11:24:28.811: INFO: Pod "execpod-affinityvnh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957094ms
Sep  6 11:24:30.816: INFO: Pod "execpod-affinityvnh9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011227779s
Sep  6 11:24:30.816: INFO: Pod "execpod-affinityvnh9d" satisfied condition "running"
Sep  6 11:24:31.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1777 exec execpod-affinityvnh9d -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Sep  6 11:24:32.055: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  6 11:24:32.055: INFO: stdout: ""
Sep  6 11:24:32.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1777 exec execpod-affinityvnh9d -- /bin/sh -x -c nc -v -z -w 2 10.233.41.251 80'
Sep  6 11:24:32.179: INFO: stderr: "+ nc -v -z -w 2 10.233.41.251 80\nConnection to 10.233.41.251 80 port [tcp/http] succeeded!\n"
Sep  6 11:24:32.179: INFO: stdout: ""
Sep  6 11:24:32.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1777 exec execpod-affinityvnh9d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.41.251:80/ ; done'
Sep  6 11:24:32.382: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n"
Sep  6 11:24:32.382: INFO: stdout: "\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5"
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
Sep  6 11:24:32.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1777 exec execpod-affinityvnh9d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.41.251:80/ ; done'
Sep  6 11:24:32.570: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n"
Sep  6 11:24:32.570: INFO: stdout: "\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt"
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
Sep  6 11:24:32.570: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1777, will wait for the garbage collector to delete the pods 09/06/23 11:24:32.594
Sep  6 11:24:32.661: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.858024ms
Sep  6 11:24:32.762: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.042749ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:34.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1777" for this suite. 09/06/23 11:24:34.799
------------------------------
• [SLOW TEST] [9.160 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:25.651
    Sep  6 11:24:25.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:24:25.652
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:25.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:25.674
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-1777 09/06/23 11:24:25.676
    STEP: creating service affinity-clusterip-transition in namespace services-1777 09/06/23 11:24:25.676
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1777 09/06/23 11:24:25.689
    I0906 11:24:25.703065      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1777, replica count: 3
    I0906 11:24:28.754535      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 11:24:28.782: INFO: Creating new exec pod
    Sep  6 11:24:28.804: INFO: Waiting up to 5m0s for pod "execpod-affinityvnh9d" in namespace "services-1777" to be "running"
    Sep  6 11:24:28.811: INFO: Pod "execpod-affinityvnh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957094ms
    Sep  6 11:24:30.816: INFO: Pod "execpod-affinityvnh9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011227779s
    Sep  6 11:24:30.816: INFO: Pod "execpod-affinityvnh9d" satisfied condition "running"
    Sep  6 11:24:31.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1777 exec execpod-affinityvnh9d -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Sep  6 11:24:32.055: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Sep  6 11:24:32.055: INFO: stdout: ""
    Sep  6 11:24:32.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1777 exec execpod-affinityvnh9d -- /bin/sh -x -c nc -v -z -w 2 10.233.41.251 80'
    Sep  6 11:24:32.179: INFO: stderr: "+ nc -v -z -w 2 10.233.41.251 80\nConnection to 10.233.41.251 80 port [tcp/http] succeeded!\n"
    Sep  6 11:24:32.179: INFO: stdout: ""
    Sep  6 11:24:32.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1777 exec execpod-affinityvnh9d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.41.251:80/ ; done'
    Sep  6 11:24:32.382: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n"
    Sep  6 11:24:32.382: INFO: stdout: "\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5\naffinity-clusterip-transition-qvdkr\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-g6vk5"
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-qvdkr
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.382: INFO: Received response from host: affinity-clusterip-transition-g6vk5
    Sep  6 11:24:32.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1777 exec execpod-affinityvnh9d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.41.251:80/ ; done'
    Sep  6 11:24:32.570: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.251:80/\n"
    Sep  6 11:24:32.570: INFO: stdout: "\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt\naffinity-clusterip-transition-lg8nt"
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Received response from host: affinity-clusterip-transition-lg8nt
    Sep  6 11:24:32.570: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1777, will wait for the garbage collector to delete the pods 09/06/23 11:24:32.594
    Sep  6 11:24:32.661: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.858024ms
    Sep  6 11:24:32.762: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.042749ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:34.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1777" for this suite. 09/06/23 11:24:34.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:34.814
Sep  6 11:24:34.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename var-expansion 09/06/23 11:24:34.816
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:34.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:34.847
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Sep  6 11:24:34.865: INFO: Waiting up to 2m0s for pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a" in namespace "var-expansion-2082" to be "container 0 failed with reason CreateContainerConfigError"
Sep  6 11:24:34.874: INFO: Pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.005982ms
Sep  6 11:24:36.880: INFO: Pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014554675s
Sep  6 11:24:36.880: INFO: Pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  6 11:24:36.880: INFO: Deleting pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a" in namespace "var-expansion-2082"
Sep  6 11:24:36.891: INFO: Wait up to 5m0s for pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:40.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2082" for this suite. 09/06/23 11:24:40.907
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:34.814
    Sep  6 11:24:34.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename var-expansion 09/06/23 11:24:34.816
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:34.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:34.847
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Sep  6 11:24:34.865: INFO: Waiting up to 2m0s for pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a" in namespace "var-expansion-2082" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  6 11:24:34.874: INFO: Pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.005982ms
    Sep  6 11:24:36.880: INFO: Pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014554675s
    Sep  6 11:24:36.880: INFO: Pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  6 11:24:36.880: INFO: Deleting pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a" in namespace "var-expansion-2082"
    Sep  6 11:24:36.891: INFO: Wait up to 5m0s for pod "var-expansion-91d838a5-12aa-49a6-86ca-b786afaf646a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:40.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2082" for this suite. 09/06/23 11:24:40.907
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:40.917
Sep  6 11:24:40.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename limitrange 09/06/23 11:24:40.918
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:40.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:40.945
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 09/06/23 11:24:40.948
STEP: Setting up watch 09/06/23 11:24:40.948
STEP: Submitting a LimitRange 09/06/23 11:24:41.052
STEP: Verifying LimitRange creation was observed 09/06/23 11:24:41.059
STEP: Fetching the LimitRange to ensure it has proper values 09/06/23 11:24:41.059
Sep  6 11:24:41.062: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  6 11:24:41.062: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 09/06/23 11:24:41.062
STEP: Ensuring Pod has resource requirements applied from LimitRange 09/06/23 11:24:41.07
Sep  6 11:24:41.073: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  6 11:24:41.074: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 09/06/23 11:24:41.074
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/06/23 11:24:41.084
Sep  6 11:24:41.090: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  6 11:24:41.090: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 09/06/23 11:24:41.09
STEP: Failing to create a Pod with more than max resources 09/06/23 11:24:41.092
STEP: Updating a LimitRange 09/06/23 11:24:41.095
STEP: Verifying LimitRange updating is effective 09/06/23 11:24:41.101
STEP: Creating a Pod with less than former min resources 09/06/23 11:24:43.112
STEP: Failing to create a Pod with more than max resources 09/06/23 11:24:43.123
STEP: Deleting a LimitRange 09/06/23 11:24:43.126
STEP: Verifying the LimitRange was deleted 09/06/23 11:24:43.14
Sep  6 11:24:48.148: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 09/06/23 11:24:48.148
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:48.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-1670" for this suite. 09/06/23 11:24:48.181
------------------------------
• [SLOW TEST] [7.271 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:40.917
    Sep  6 11:24:40.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename limitrange 09/06/23 11:24:40.918
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:40.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:40.945
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 09/06/23 11:24:40.948
    STEP: Setting up watch 09/06/23 11:24:40.948
    STEP: Submitting a LimitRange 09/06/23 11:24:41.052
    STEP: Verifying LimitRange creation was observed 09/06/23 11:24:41.059
    STEP: Fetching the LimitRange to ensure it has proper values 09/06/23 11:24:41.059
    Sep  6 11:24:41.062: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  6 11:24:41.062: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 09/06/23 11:24:41.062
    STEP: Ensuring Pod has resource requirements applied from LimitRange 09/06/23 11:24:41.07
    Sep  6 11:24:41.073: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  6 11:24:41.074: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 09/06/23 11:24:41.074
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/06/23 11:24:41.084
    Sep  6 11:24:41.090: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Sep  6 11:24:41.090: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 09/06/23 11:24:41.09
    STEP: Failing to create a Pod with more than max resources 09/06/23 11:24:41.092
    STEP: Updating a LimitRange 09/06/23 11:24:41.095
    STEP: Verifying LimitRange updating is effective 09/06/23 11:24:41.101
    STEP: Creating a Pod with less than former min resources 09/06/23 11:24:43.112
    STEP: Failing to create a Pod with more than max resources 09/06/23 11:24:43.123
    STEP: Deleting a LimitRange 09/06/23 11:24:43.126
    STEP: Verifying the LimitRange was deleted 09/06/23 11:24:43.14
    Sep  6 11:24:48.148: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 09/06/23 11:24:48.148
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:48.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-1670" for this suite. 09/06/23 11:24:48.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:48.192
Sep  6 11:24:48.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename disruption 09/06/23 11:24:48.192
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:48.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:48.218
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 09/06/23 11:24:48.221
STEP: Waiting for the pdb to be processed 09/06/23 11:24:48.226
STEP: First trying to evict a pod which shouldn't be evictable 09/06/23 11:24:50.252
STEP: Waiting for all pods to be running 09/06/23 11:24:50.252
Sep  6 11:24:50.275: INFO: pods: 0 < 3
STEP: locating a running pod 09/06/23 11:24:52.28
STEP: Updating the pdb to allow a pod to be evicted 09/06/23 11:24:52.291
STEP: Waiting for the pdb to be processed 09/06/23 11:24:52.302
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/06/23 11:24:54.314
STEP: Waiting for all pods to be running 09/06/23 11:24:54.314
STEP: Waiting for the pdb to observed all healthy pods 09/06/23 11:24:54.316
STEP: Patching the pdb to disallow a pod to be evicted 09/06/23 11:24:54.344
STEP: Waiting for the pdb to be processed 09/06/23 11:24:54.374
STEP: Waiting for all pods to be running 09/06/23 11:24:56.405
STEP: locating a running pod 09/06/23 11:24:56.422
STEP: Deleting the pdb to allow a pod to be evicted 09/06/23 11:24:56.446
STEP: Waiting for the pdb to be deleted 09/06/23 11:24:56.456
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/06/23 11:24:56.458
STEP: Waiting for all pods to be running 09/06/23 11:24:56.458
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  6 11:24:56.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6805" for this suite. 09/06/23 11:24:56.515
------------------------------
• [SLOW TEST] [8.339 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:48.192
    Sep  6 11:24:48.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename disruption 09/06/23 11:24:48.192
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:48.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:48.218
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 09/06/23 11:24:48.221
    STEP: Waiting for the pdb to be processed 09/06/23 11:24:48.226
    STEP: First trying to evict a pod which shouldn't be evictable 09/06/23 11:24:50.252
    STEP: Waiting for all pods to be running 09/06/23 11:24:50.252
    Sep  6 11:24:50.275: INFO: pods: 0 < 3
    STEP: locating a running pod 09/06/23 11:24:52.28
    STEP: Updating the pdb to allow a pod to be evicted 09/06/23 11:24:52.291
    STEP: Waiting for the pdb to be processed 09/06/23 11:24:52.302
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/06/23 11:24:54.314
    STEP: Waiting for all pods to be running 09/06/23 11:24:54.314
    STEP: Waiting for the pdb to observed all healthy pods 09/06/23 11:24:54.316
    STEP: Patching the pdb to disallow a pod to be evicted 09/06/23 11:24:54.344
    STEP: Waiting for the pdb to be processed 09/06/23 11:24:54.374
    STEP: Waiting for all pods to be running 09/06/23 11:24:56.405
    STEP: locating a running pod 09/06/23 11:24:56.422
    STEP: Deleting the pdb to allow a pod to be evicted 09/06/23 11:24:56.446
    STEP: Waiting for the pdb to be deleted 09/06/23 11:24:56.456
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/06/23 11:24:56.458
    STEP: Waiting for all pods to be running 09/06/23 11:24:56.458
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:24:56.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6805" for this suite. 09/06/23 11:24:56.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:24:56.533
Sep  6 11:24:56.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:24:56.534
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:56.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:56.579
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-7852 09/06/23 11:24:56.584
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[] 09/06/23 11:24:56.599
Sep  6 11:24:56.612: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep  6 11:24:57.642: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7852 09/06/23 11:24:57.643
Sep  6 11:24:57.680: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7852" to be "running and ready"
Sep  6 11:24:57.700: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.56827ms
Sep  6 11:24:57.700: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:24:59.705: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025121632s
Sep  6 11:24:59.705: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:25:01.707: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027417409s
Sep  6 11:25:01.707: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  6 11:25:01.707: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[pod1:[80]] 09/06/23 11:25:01.712
Sep  6 11:25:01.734: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 09/06/23 11:25:01.734
Sep  6 11:25:01.734: INFO: Creating new exec pod
Sep  6 11:25:01.749: INFO: Waiting up to 5m0s for pod "execpod2gt5j" in namespace "services-7852" to be "running"
Sep  6 11:25:01.757: INFO: Pod "execpod2gt5j": Phase="Pending", Reason="", readiness=false. Elapsed: 7.502781ms
Sep  6 11:25:03.767: INFO: Pod "execpod2gt5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.017613929s
Sep  6 11:25:03.767: INFO: Pod "execpod2gt5j" satisfied condition "running"
Sep  6 11:25:04.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  6 11:25:05.082: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  6 11:25:05.082: INFO: stdout: ""
Sep  6 11:25:05.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 10.233.58.186 80'
Sep  6 11:25:05.273: INFO: stderr: "+ nc -v -z -w 2 10.233.58.186 80\nConnection to 10.233.58.186 80 port [tcp/http] succeeded!\n"
Sep  6 11:25:05.273: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-7852 09/06/23 11:25:05.273
Sep  6 11:25:05.280: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7852" to be "running and ready"
Sep  6 11:25:05.284: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.530993ms
Sep  6 11:25:05.284: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:25:07.287: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007390694s
Sep  6 11:25:07.287: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  6 11:25:07.287: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[pod1:[80] pod2:[80]] 09/06/23 11:25:07.291
Sep  6 11:25:07.306: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 09/06/23 11:25:07.306
Sep  6 11:25:08.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  6 11:25:08.526: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  6 11:25:08.526: INFO: stdout: ""
Sep  6 11:25:08.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 10.233.58.186 80'
Sep  6 11:25:08.643: INFO: stderr: "+ nc -v -z -w 2 10.233.58.186 80\nConnection to 10.233.58.186 80 port [tcp/http] succeeded!\n"
Sep  6 11:25:08.643: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7852 09/06/23 11:25:08.643
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[pod2:[80]] 09/06/23 11:25:08.664
Sep  6 11:25:08.693: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 09/06/23 11:25:08.694
Sep  6 11:25:09.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  6 11:25:09.829: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  6 11:25:09.829: INFO: stdout: ""
Sep  6 11:25:09.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 10.233.58.186 80'
Sep  6 11:25:09.949: INFO: stderr: "+ nc -v -z -w 2 10.233.58.186 80\nConnection to 10.233.58.186 80 port [tcp/http] succeeded!\n"
Sep  6 11:25:09.949: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-7852 09/06/23 11:25:09.949
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[] 09/06/23 11:25:09.984
Sep  6 11:25:10.009: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:10.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7852" for this suite. 09/06/23 11:25:10.083
------------------------------
• [SLOW TEST] [13.562 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:24:56.533
    Sep  6 11:24:56.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:24:56.534
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:24:56.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:24:56.579
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-7852 09/06/23 11:24:56.584
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[] 09/06/23 11:24:56.599
    Sep  6 11:24:56.612: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Sep  6 11:24:57.642: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7852 09/06/23 11:24:57.643
    Sep  6 11:24:57.680: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7852" to be "running and ready"
    Sep  6 11:24:57.700: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.56827ms
    Sep  6 11:24:57.700: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:24:59.705: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025121632s
    Sep  6 11:24:59.705: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:25:01.707: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027417409s
    Sep  6 11:25:01.707: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  6 11:25:01.707: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[pod1:[80]] 09/06/23 11:25:01.712
    Sep  6 11:25:01.734: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 09/06/23 11:25:01.734
    Sep  6 11:25:01.734: INFO: Creating new exec pod
    Sep  6 11:25:01.749: INFO: Waiting up to 5m0s for pod "execpod2gt5j" in namespace "services-7852" to be "running"
    Sep  6 11:25:01.757: INFO: Pod "execpod2gt5j": Phase="Pending", Reason="", readiness=false. Elapsed: 7.502781ms
    Sep  6 11:25:03.767: INFO: Pod "execpod2gt5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.017613929s
    Sep  6 11:25:03.767: INFO: Pod "execpod2gt5j" satisfied condition "running"
    Sep  6 11:25:04.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  6 11:25:05.082: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  6 11:25:05.082: INFO: stdout: ""
    Sep  6 11:25:05.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 10.233.58.186 80'
    Sep  6 11:25:05.273: INFO: stderr: "+ nc -v -z -w 2 10.233.58.186 80\nConnection to 10.233.58.186 80 port [tcp/http] succeeded!\n"
    Sep  6 11:25:05.273: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-7852 09/06/23 11:25:05.273
    Sep  6 11:25:05.280: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7852" to be "running and ready"
    Sep  6 11:25:05.284: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.530993ms
    Sep  6 11:25:05.284: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:25:07.287: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007390694s
    Sep  6 11:25:07.287: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  6 11:25:07.287: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[pod1:[80] pod2:[80]] 09/06/23 11:25:07.291
    Sep  6 11:25:07.306: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 09/06/23 11:25:07.306
    Sep  6 11:25:08.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  6 11:25:08.526: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  6 11:25:08.526: INFO: stdout: ""
    Sep  6 11:25:08.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 10.233.58.186 80'
    Sep  6 11:25:08.643: INFO: stderr: "+ nc -v -z -w 2 10.233.58.186 80\nConnection to 10.233.58.186 80 port [tcp/http] succeeded!\n"
    Sep  6 11:25:08.643: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7852 09/06/23 11:25:08.643
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[pod2:[80]] 09/06/23 11:25:08.664
    Sep  6 11:25:08.693: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 09/06/23 11:25:08.694
    Sep  6 11:25:09.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  6 11:25:09.829: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  6 11:25:09.829: INFO: stdout: ""
    Sep  6 11:25:09.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-7852 exec execpod2gt5j -- /bin/sh -x -c nc -v -z -w 2 10.233.58.186 80'
    Sep  6 11:25:09.949: INFO: stderr: "+ nc -v -z -w 2 10.233.58.186 80\nConnection to 10.233.58.186 80 port [tcp/http] succeeded!\n"
    Sep  6 11:25:09.949: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-7852 09/06/23 11:25:09.949
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7852 to expose endpoints map[] 09/06/23 11:25:09.984
    Sep  6 11:25:10.009: INFO: successfully validated that service endpoint-test2 in namespace services-7852 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:10.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7852" for this suite. 09/06/23 11:25:10.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:10.094
Sep  6 11:25:10.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-probe 09/06/23 11:25:10.097
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:10.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:10.133
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Sep  6 11:25:10.156: INFO: Waiting up to 5m0s for pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace" in namespace "container-probe-6305" to be "running and ready"
Sep  6 11:25:10.174: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Pending", Reason="", readiness=false. Elapsed: 18.560247ms
Sep  6 11:25:10.174: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:25:12.184: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 2.028033415s
Sep  6 11:25:12.184: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:14.189: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 4.033038457s
Sep  6 11:25:14.189: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:16.187: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 6.031343061s
Sep  6 11:25:16.187: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:18.190: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 8.033827066s
Sep  6 11:25:18.190: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:20.181: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 10.024895497s
Sep  6 11:25:20.181: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:22.191: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 12.035583528s
Sep  6 11:25:22.191: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:24.186: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 14.030184741s
Sep  6 11:25:24.186: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:26.187: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 16.031540484s
Sep  6 11:25:26.187: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:28.184: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 18.028148206s
Sep  6 11:25:28.184: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:30.179: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 20.022869857s
Sep  6 11:25:30.179: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
Sep  6 11:25:32.190: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=true. Elapsed: 22.034065169s
Sep  6 11:25:32.190: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = true)
Sep  6 11:25:32.190: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace" satisfied condition "running and ready"
Sep  6 11:25:32.204: INFO: Container started at 2023-09-06 11:25:10 +0000 UTC, pod became ready at 2023-09-06 11:25:30 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:32.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6305" for this suite. 09/06/23 11:25:32.217
------------------------------
• [SLOW TEST] [22.139 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:10.094
    Sep  6 11:25:10.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-probe 09/06/23 11:25:10.097
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:10.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:10.133
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Sep  6 11:25:10.156: INFO: Waiting up to 5m0s for pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace" in namespace "container-probe-6305" to be "running and ready"
    Sep  6 11:25:10.174: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Pending", Reason="", readiness=false. Elapsed: 18.560247ms
    Sep  6 11:25:10.174: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:25:12.184: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 2.028033415s
    Sep  6 11:25:12.184: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:14.189: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 4.033038457s
    Sep  6 11:25:14.189: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:16.187: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 6.031343061s
    Sep  6 11:25:16.187: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:18.190: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 8.033827066s
    Sep  6 11:25:18.190: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:20.181: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 10.024895497s
    Sep  6 11:25:20.181: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:22.191: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 12.035583528s
    Sep  6 11:25:22.191: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:24.186: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 14.030184741s
    Sep  6 11:25:24.186: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:26.187: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 16.031540484s
    Sep  6 11:25:26.187: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:28.184: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 18.028148206s
    Sep  6 11:25:28.184: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:30.179: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=false. Elapsed: 20.022869857s
    Sep  6 11:25:30.179: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = false)
    Sep  6 11:25:32.190: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace": Phase="Running", Reason="", readiness=true. Elapsed: 22.034065169s
    Sep  6 11:25:32.190: INFO: The phase of Pod test-webserver-f127aff2-9eac-4286-85ed-66094a814ace is Running (Ready = true)
    Sep  6 11:25:32.190: INFO: Pod "test-webserver-f127aff2-9eac-4286-85ed-66094a814ace" satisfied condition "running and ready"
    Sep  6 11:25:32.204: INFO: Container started at 2023-09-06 11:25:10 +0000 UTC, pod became ready at 2023-09-06 11:25:30 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:32.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6305" for this suite. 09/06/23 11:25:32.217
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:32.237
Sep  6 11:25:32.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 11:25:32.239
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:32.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:32.268
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 09/06/23 11:25:32.27
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/06/23 11:25:32.271
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/06/23 11:25:32.271
STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/06/23 11:25:32.271
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/06/23 11:25:32.271
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/06/23 11:25:32.272
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/06/23 11:25:32.272
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:32.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3854" for this suite. 09/06/23 11:25:32.278
------------------------------
• [0.047 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:32.237
    Sep  6 11:25:32.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 11:25:32.239
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:32.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:32.268
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 09/06/23 11:25:32.27
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/06/23 11:25:32.271
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/06/23 11:25:32.271
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/06/23 11:25:32.271
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/06/23 11:25:32.271
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/06/23 11:25:32.272
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/06/23 11:25:32.272
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:32.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3854" for this suite. 09/06/23 11:25:32.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:32.288
Sep  6 11:25:32.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 11:25:32.289
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:32.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:32.308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 11:25:32.327
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:25:32.645
STEP: Deploying the webhook pod 09/06/23 11:25:32.653
STEP: Wait for the deployment to be ready 09/06/23 11:25:32.67
Sep  6 11:25:32.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 11:25:34.708
STEP: Verifying the service has paired with the endpoint 09/06/23 11:25:34.726
Sep  6 11:25:35.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Sep  6 11:25:35.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3153-crds.webhook.example.com via the AdmissionRegistration API 09/06/23 11:25:41.244
Sep  6 11:25:41.331: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version 09/06/23 11:25:41.44
STEP: Patching Custom Resource Definition to set v2 as storage 09/06/23 11:25:43.526
STEP: Patching the custom resource while v2 is storage version 09/06/23 11:25:43.564
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:44.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7989" for this suite. 09/06/23 11:25:44.381
STEP: Destroying namespace "webhook-7989-markers" for this suite. 09/06/23 11:25:44.39
------------------------------
• [SLOW TEST] [12.115 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:32.288
    Sep  6 11:25:32.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 11:25:32.289
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:32.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:32.308
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 11:25:32.327
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:25:32.645
    STEP: Deploying the webhook pod 09/06/23 11:25:32.653
    STEP: Wait for the deployment to be ready 09/06/23 11:25:32.67
    Sep  6 11:25:32.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 11:25:34.708
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:25:34.726
    Sep  6 11:25:35.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Sep  6 11:25:35.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3153-crds.webhook.example.com via the AdmissionRegistration API 09/06/23 11:25:41.244
    Sep  6 11:25:41.331: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource while v1 is storage version 09/06/23 11:25:41.44
    STEP: Patching Custom Resource Definition to set v2 as storage 09/06/23 11:25:43.526
    STEP: Patching the custom resource while v2 is storage version 09/06/23 11:25:43.564
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:44.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7989" for this suite. 09/06/23 11:25:44.381
    STEP: Destroying namespace "webhook-7989-markers" for this suite. 09/06/23 11:25:44.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:44.404
Sep  6 11:25:44.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename security-context 09/06/23 11:25:44.405
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:44.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:44.453
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/06/23 11:25:44.465
Sep  6 11:25:44.493: INFO: Waiting up to 5m0s for pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a" in namespace "security-context-9749" to be "Succeeded or Failed"
Sep  6 11:25:44.501: INFO: Pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.280085ms
Sep  6 11:25:46.516: INFO: Pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022439551s
Sep  6 11:25:48.515: INFO: Pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021264244s
STEP: Saw pod success 09/06/23 11:25:48.515
Sep  6 11:25:48.516: INFO: Pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a" satisfied condition "Succeeded or Failed"
Sep  6 11:25:48.530: INFO: Trying to get logs from node kube-3 pod security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a container test-container: <nil>
STEP: delete the pod 09/06/23 11:25:48.576
Sep  6 11:25:48.597: INFO: Waiting for pod security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a to disappear
Sep  6 11:25:48.600: INFO: Pod security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:48.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9749" for this suite. 09/06/23 11:25:48.604
------------------------------
• [4.205 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:44.404
    Sep  6 11:25:44.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename security-context 09/06/23 11:25:44.405
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:44.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:44.453
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/06/23 11:25:44.465
    Sep  6 11:25:44.493: INFO: Waiting up to 5m0s for pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a" in namespace "security-context-9749" to be "Succeeded or Failed"
    Sep  6 11:25:44.501: INFO: Pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.280085ms
    Sep  6 11:25:46.516: INFO: Pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022439551s
    Sep  6 11:25:48.515: INFO: Pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021264244s
    STEP: Saw pod success 09/06/23 11:25:48.515
    Sep  6 11:25:48.516: INFO: Pod "security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a" satisfied condition "Succeeded or Failed"
    Sep  6 11:25:48.530: INFO: Trying to get logs from node kube-3 pod security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a container test-container: <nil>
    STEP: delete the pod 09/06/23 11:25:48.576
    Sep  6 11:25:48.597: INFO: Waiting for pod security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a to disappear
    Sep  6 11:25:48.600: INFO: Pod security-context-5ab7bdff-ccf3-4178-98d6-b7cd46cf882a no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:48.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9749" for this suite. 09/06/23 11:25:48.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:48.611
Sep  6 11:25:48.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename endpointslice 09/06/23 11:25:48.612
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:48.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:48.634
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:50.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8159" for this suite. 09/06/23 11:25:50.723
------------------------------
• [2.122 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:48.611
    Sep  6 11:25:48.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename endpointslice 09/06/23 11:25:48.612
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:48.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:48.634
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:50.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8159" for this suite. 09/06/23 11:25:50.723
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:50.734
Sep  6 11:25:50.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename runtimeclass 09/06/23 11:25:50.734
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:50.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:50.761
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Sep  6 11:25:50.784: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2620 to be scheduled
Sep  6 11:25:50.789: INFO: 1 pods are not scheduled: [runtimeclass-2620/test-runtimeclass-runtimeclass-2620-preconfigured-handler-46rns(38727bfb-6e73-4a5e-add3-7d7b7a1dd7a4)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:52.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2620" for this suite. 09/06/23 11:25:52.807
------------------------------
• [2.085 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:50.734
    Sep  6 11:25:50.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename runtimeclass 09/06/23 11:25:50.734
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:50.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:50.761
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Sep  6 11:25:50.784: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2620 to be scheduled
    Sep  6 11:25:50.789: INFO: 1 pods are not scheduled: [runtimeclass-2620/test-runtimeclass-runtimeclass-2620-preconfigured-handler-46rns(38727bfb-6e73-4a5e-add3-7d7b7a1dd7a4)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:52.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2620" for this suite. 09/06/23 11:25:52.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:52.823
Sep  6 11:25:52.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename init-container 09/06/23 11:25:52.824
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:52.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:52.854
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 09/06/23 11:25:52.856
Sep  6 11:25:52.856: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:58.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-969" for this suite. 09/06/23 11:25:58.045
------------------------------
• [SLOW TEST] [5.233 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:52.823
    Sep  6 11:25:52.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename init-container 09/06/23 11:25:52.824
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:52.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:52.854
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 09/06/23 11:25:52.856
    Sep  6 11:25:52.856: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:58.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-969" for this suite. 09/06/23 11:25:58.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:58.058
Sep  6 11:25:58.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 11:25:58.059
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:58.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:58.083
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 09/06/23 11:25:58.088
Sep  6 11:25:58.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-4105 api-versions'
Sep  6 11:25:58.164: INFO: stderr: ""
Sep  6 11:25:58.164: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 11:25:58.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4105" for this suite. 09/06/23 11:25:58.168
------------------------------
• [0.119 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:58.058
    Sep  6 11:25:58.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 11:25:58.059
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:58.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:58.083
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 09/06/23 11:25:58.088
    Sep  6 11:25:58.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-4105 api-versions'
    Sep  6 11:25:58.164: INFO: stderr: ""
    Sep  6 11:25:58.164: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:25:58.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4105" for this suite. 09/06/23 11:25:58.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:25:58.178
Sep  6 11:25:58.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 11:25:58.179
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:58.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:58.2
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 09/06/23 11:25:58.202
STEP: Creating a ResourceQuota 09/06/23 11:26:03.207
STEP: Ensuring resource quota status is calculated 09/06/23 11:26:03.215
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:06.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6920" for this suite. 09/06/23 11:26:06.377
------------------------------
• [SLOW TEST] [8.281 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:25:58.178
    Sep  6 11:25:58.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 11:25:58.179
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:25:58.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:25:58.2
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 09/06/23 11:25:58.202
    STEP: Creating a ResourceQuota 09/06/23 11:26:03.207
    STEP: Ensuring resource quota status is calculated 09/06/23 11:26:03.215
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:06.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6920" for this suite. 09/06/23 11:26:06.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:06.459
Sep  6 11:26:06.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename security-context 09/06/23 11:26:06.461
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:06.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:06.566
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/06/23 11:26:06.569
Sep  6 11:26:06.691: INFO: Waiting up to 5m0s for pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195" in namespace "security-context-446" to be "Succeeded or Failed"
Sep  6 11:26:06.762: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195": Phase="Pending", Reason="", readiness=false. Elapsed: 71.203786ms
Sep  6 11:26:08.775: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083914477s
Sep  6 11:26:10.775: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083870364s
Sep  6 11:26:12.771: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080766909s
STEP: Saw pod success 09/06/23 11:26:12.772
Sep  6 11:26:12.772: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195" satisfied condition "Succeeded or Failed"
Sep  6 11:26:12.780: INFO: Trying to get logs from node kube-3 pod security-context-a54d4778-cb92-4e25-bebf-1a9c98792195 container test-container: <nil>
STEP: delete the pod 09/06/23 11:26:12.791
Sep  6 11:26:12.812: INFO: Waiting for pod security-context-a54d4778-cb92-4e25-bebf-1a9c98792195 to disappear
Sep  6 11:26:12.815: INFO: Pod security-context-a54d4778-cb92-4e25-bebf-1a9c98792195 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:12.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-446" for this suite. 09/06/23 11:26:12.819
------------------------------
• [SLOW TEST] [6.367 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:06.459
    Sep  6 11:26:06.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename security-context 09/06/23 11:26:06.461
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:06.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:06.566
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/06/23 11:26:06.569
    Sep  6 11:26:06.691: INFO: Waiting up to 5m0s for pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195" in namespace "security-context-446" to be "Succeeded or Failed"
    Sep  6 11:26:06.762: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195": Phase="Pending", Reason="", readiness=false. Elapsed: 71.203786ms
    Sep  6 11:26:08.775: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083914477s
    Sep  6 11:26:10.775: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083870364s
    Sep  6 11:26:12.771: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080766909s
    STEP: Saw pod success 09/06/23 11:26:12.772
    Sep  6 11:26:12.772: INFO: Pod "security-context-a54d4778-cb92-4e25-bebf-1a9c98792195" satisfied condition "Succeeded or Failed"
    Sep  6 11:26:12.780: INFO: Trying to get logs from node kube-3 pod security-context-a54d4778-cb92-4e25-bebf-1a9c98792195 container test-container: <nil>
    STEP: delete the pod 09/06/23 11:26:12.791
    Sep  6 11:26:12.812: INFO: Waiting for pod security-context-a54d4778-cb92-4e25-bebf-1a9c98792195 to disappear
    Sep  6 11:26:12.815: INFO: Pod security-context-a54d4778-cb92-4e25-bebf-1a9c98792195 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:12.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-446" for this suite. 09/06/23 11:26:12.819
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:12.827
Sep  6 11:26:12.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename daemonsets 09/06/23 11:26:12.827
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:12.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:12.848
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 09/06/23 11:26:12.868
STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 11:26:12.873
Sep  6 11:26:12.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:26:12.880: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:26:13.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:26:13.888: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:26:14.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 11:26:14.917: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 09/06/23 11:26:14.932
STEP: DeleteCollection of the DaemonSets 09/06/23 11:26:14.948
STEP: Verify that ReplicaSets have been deleted 09/06/23 11:26:14.966
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Sep  6 11:26:14.984: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35220"},"items":null}

Sep  6 11:26:14.993: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35220"},"items":[{"metadata":{"name":"daemon-set-6pnxb","generateName":"daemon-set-","namespace":"daemonsets-8838","uid":"75ffe579-dc60-45f3-8bba-5f932643b6b1","resourceVersion":"35212","creationTimestamp":"2023-09-06T11:26:12Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d810251a0e5507ce275dde2afbefdef1a6bb8b0eabc781d8567105f1160aff04","cni.projectcalico.org/podIP":"10.233.99.88/32","cni.projectcalico.org/podIPs":"10.233.99.88/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8bacda5e-a024-432f-bfab-34ed0c9ca28a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bacda5e-a024-432f-bfab-34ed0c9ca28a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wdl8t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wdl8t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"}],"hostIP":"10.2.20.103","podIP":"10.233.99.88","podIPs":[{"ip":"10.233.99.88"}],"startTime":"2023-09-06T11:26:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-06T11:26:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0a2f0f7422e4ec2c673c780e12fb7e6104387964aa53c04fedf43d440f5f5c1a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lxkwp","generateName":"daemon-set-","namespace":"daemonsets-8838","uid":"e308e798-c355-4b43-9bd3-dd7f66782f33","resourceVersion":"35210","creationTimestamp":"2023-09-06T11:26:12Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6c6970734332b14499e930602249d7b1de38b08edb823aee14b5cb4886cbdd28","cni.projectcalico.org/podIP":"10.233.120.219/32","cni.projectcalico.org/podIPs":"10.233.120.219/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8bacda5e-a024-432f-bfab-34ed0c9ca28a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bacda5e-a024-432f-bfab-34ed0c9ca28a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-27s52","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-27s52","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"}],"hostIP":"10.2.20.102","podIP":"10.233.120.219","podIPs":[{"ip":"10.233.120.219"}],"startTime":"2023-09-06T11:26:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-06T11:26:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://677c67d972bdc617e724cb4de283757f02b8f472b09be6bfda455b4e643e6333","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tgwcd","generateName":"daemon-set-","namespace":"daemonsets-8838","uid":"e72ad32f-682a-49e0-968c-98890b2c4b02","resourceVersion":"35208","creationTimestamp":"2023-09-06T11:26:12Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"726555ae9afce0e3df081bce7667705295f9507bcde733da69542b3c6a6f8048","cni.projectcalico.org/podIP":"10.233.120.69/32","cni.projectcalico.org/podIPs":"10.233.120.69/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8bacda5e-a024-432f-bfab-34ed0c9ca28a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bacda5e-a024-432f-bfab-34ed0c9ca28a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mfqm2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mfqm2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"}],"hostIP":"10.2.20.101","podIP":"10.233.120.69","podIPs":[{"ip":"10.233.120.69"}],"startTime":"2023-09-06T11:26:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-06T11:26:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3be1aa96f7e37f64e815b243c0229191d5b3c8cda6f11c8f21b06db39a5df2af","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:15.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8838" for this suite. 09/06/23 11:26:15.027
------------------------------
• [2.206 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:12.827
    Sep  6 11:26:12.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename daemonsets 09/06/23 11:26:12.827
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:12.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:12.848
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 09/06/23 11:26:12.868
    STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 11:26:12.873
    Sep  6 11:26:12.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:26:12.880: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:26:13.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:26:13.888: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:26:14.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 11:26:14.917: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 09/06/23 11:26:14.932
    STEP: DeleteCollection of the DaemonSets 09/06/23 11:26:14.948
    STEP: Verify that ReplicaSets have been deleted 09/06/23 11:26:14.966
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Sep  6 11:26:14.984: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35220"},"items":null}

    Sep  6 11:26:14.993: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35220"},"items":[{"metadata":{"name":"daemon-set-6pnxb","generateName":"daemon-set-","namespace":"daemonsets-8838","uid":"75ffe579-dc60-45f3-8bba-5f932643b6b1","resourceVersion":"35212","creationTimestamp":"2023-09-06T11:26:12Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d810251a0e5507ce275dde2afbefdef1a6bb8b0eabc781d8567105f1160aff04","cni.projectcalico.org/podIP":"10.233.99.88/32","cni.projectcalico.org/podIPs":"10.233.99.88/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8bacda5e-a024-432f-bfab-34ed0c9ca28a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bacda5e-a024-432f-bfab-34ed0c9ca28a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wdl8t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wdl8t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"}],"hostIP":"10.2.20.103","podIP":"10.233.99.88","podIPs":[{"ip":"10.233.99.88"}],"startTime":"2023-09-06T11:26:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-06T11:26:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0a2f0f7422e4ec2c673c780e12fb7e6104387964aa53c04fedf43d440f5f5c1a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lxkwp","generateName":"daemon-set-","namespace":"daemonsets-8838","uid":"e308e798-c355-4b43-9bd3-dd7f66782f33","resourceVersion":"35210","creationTimestamp":"2023-09-06T11:26:12Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6c6970734332b14499e930602249d7b1de38b08edb823aee14b5cb4886cbdd28","cni.projectcalico.org/podIP":"10.233.120.219/32","cni.projectcalico.org/podIPs":"10.233.120.219/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8bacda5e-a024-432f-bfab-34ed0c9ca28a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bacda5e-a024-432f-bfab-34ed0c9ca28a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-27s52","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-27s52","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"}],"hostIP":"10.2.20.102","podIP":"10.233.120.219","podIPs":[{"ip":"10.233.120.219"}],"startTime":"2023-09-06T11:26:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-06T11:26:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://677c67d972bdc617e724cb4de283757f02b8f472b09be6bfda455b4e643e6333","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tgwcd","generateName":"daemon-set-","namespace":"daemonsets-8838","uid":"e72ad32f-682a-49e0-968c-98890b2c4b02","resourceVersion":"35208","creationTimestamp":"2023-09-06T11:26:12Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"726555ae9afce0e3df081bce7667705295f9507bcde733da69542b3c6a6f8048","cni.projectcalico.org/podIP":"10.233.120.69/32","cni.projectcalico.org/podIPs":"10.233.120.69/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8bacda5e-a024-432f-bfab-34ed0c9ca28a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bacda5e-a024-432f-bfab-34ed0c9ca28a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-06T11:26:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mfqm2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mfqm2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-06T11:26:12Z"}],"hostIP":"10.2.20.101","podIP":"10.233.120.69","podIPs":[{"ip":"10.233.120.69"}],"startTime":"2023-09-06T11:26:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-06T11:26:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3be1aa96f7e37f64e815b243c0229191d5b3c8cda6f11c8f21b06db39a5df2af","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:15.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8838" for this suite. 09/06/23 11:26:15.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:15.035
Sep  6 11:26:15.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replicaset 09/06/23 11:26:15.036
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:15.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:15.064
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/06/23 11:26:15.066
Sep  6 11:26:15.076: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 11:26:20.084: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/06/23 11:26:20.084
STEP: getting scale subresource 09/06/23 11:26:20.084
STEP: updating a scale subresource 09/06/23 11:26:20.089
STEP: verifying the replicaset Spec.Replicas was modified 09/06/23 11:26:20.097
STEP: Patch a scale subresource 09/06/23 11:26:20.1
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:20.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8513" for this suite. 09/06/23 11:26:20.129
------------------------------
• [SLOW TEST] [5.114 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:15.035
    Sep  6 11:26:15.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replicaset 09/06/23 11:26:15.036
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:15.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:15.064
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/06/23 11:26:15.066
    Sep  6 11:26:15.076: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  6 11:26:20.084: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/06/23 11:26:20.084
    STEP: getting scale subresource 09/06/23 11:26:20.084
    STEP: updating a scale subresource 09/06/23 11:26:20.089
    STEP: verifying the replicaset Spec.Replicas was modified 09/06/23 11:26:20.097
    STEP: Patch a scale subresource 09/06/23 11:26:20.1
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:20.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8513" for this suite. 09/06/23 11:26:20.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:20.149
Sep  6 11:26:20.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:26:20.15
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:20.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:20.188
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-ecefcb67-3992-4d4a-b2c6-143cb8da2912 09/06/23 11:26:20.191
STEP: Creating a pod to test consume configMaps 09/06/23 11:26:20.201
Sep  6 11:26:20.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8" in namespace "projected-2504" to be "Succeeded or Failed"
Sep  6 11:26:20.224: INFO: Pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.555834ms
Sep  6 11:26:22.236: INFO: Pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018443164s
Sep  6 11:26:24.241: INFO: Pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02325116s
STEP: Saw pod success 09/06/23 11:26:24.243
Sep  6 11:26:24.243: INFO: Pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8" satisfied condition "Succeeded or Failed"
Sep  6 11:26:24.254: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8 container projected-configmap-volume-test: <nil>
STEP: delete the pod 09/06/23 11:26:24.28
Sep  6 11:26:24.309: INFO: Waiting for pod pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8 to disappear
Sep  6 11:26:24.314: INFO: Pod pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:24.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2504" for this suite. 09/06/23 11:26:24.321
------------------------------
• [4.181 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:20.149
    Sep  6 11:26:20.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:26:20.15
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:20.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:20.188
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-ecefcb67-3992-4d4a-b2c6-143cb8da2912 09/06/23 11:26:20.191
    STEP: Creating a pod to test consume configMaps 09/06/23 11:26:20.201
    Sep  6 11:26:20.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8" in namespace "projected-2504" to be "Succeeded or Failed"
    Sep  6 11:26:20.224: INFO: Pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.555834ms
    Sep  6 11:26:22.236: INFO: Pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018443164s
    Sep  6 11:26:24.241: INFO: Pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02325116s
    STEP: Saw pod success 09/06/23 11:26:24.243
    Sep  6 11:26:24.243: INFO: Pod "pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8" satisfied condition "Succeeded or Failed"
    Sep  6 11:26:24.254: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 09/06/23 11:26:24.28
    Sep  6 11:26:24.309: INFO: Waiting for pod pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8 to disappear
    Sep  6 11:26:24.314: INFO: Pod pod-projected-configmaps-b36094e3-0c5b-4909-92d9-0248ed7b8dc8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:24.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2504" for this suite. 09/06/23 11:26:24.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:24.332
Sep  6 11:26:24.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename proxy 09/06/23 11:26:24.333
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:24.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:24.353
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Sep  6 11:26:24.355: INFO: Creating pod...
Sep  6 11:26:24.367: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8901" to be "running"
Sep  6 11:26:24.375: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.612254ms
Sep  6 11:26:26.384: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.016500355s
Sep  6 11:26:26.384: INFO: Pod "agnhost" satisfied condition "running"
Sep  6 11:26:26.384: INFO: Creating service...
Sep  6 11:26:26.401: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=DELETE
Sep  6 11:26:26.413: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  6 11:26:26.413: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=OPTIONS
Sep  6 11:26:26.421: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  6 11:26:26.421: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=PATCH
Sep  6 11:26:26.426: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  6 11:26:26.426: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=POST
Sep  6 11:26:26.432: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  6 11:26:26.432: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=PUT
Sep  6 11:26:26.438: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  6 11:26:26.438: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=DELETE
Sep  6 11:26:26.448: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  6 11:26:26.448: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=OPTIONS
Sep  6 11:26:26.470: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  6 11:26:26.470: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=PATCH
Sep  6 11:26:26.477: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  6 11:26:26.477: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=POST
Sep  6 11:26:26.487: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  6 11:26:26.488: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=PUT
Sep  6 11:26:26.496: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  6 11:26:26.496: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=GET
Sep  6 11:26:26.499: INFO: http.Client request:GET StatusCode:301
Sep  6 11:26:26.499: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=GET
Sep  6 11:26:26.507: INFO: http.Client request:GET StatusCode:301
Sep  6 11:26:26.507: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=HEAD
Sep  6 11:26:26.511: INFO: http.Client request:HEAD StatusCode:301
Sep  6 11:26:26.511: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=HEAD
Sep  6 11:26:26.516: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:26.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8901" for this suite. 09/06/23 11:26:26.524
------------------------------
• [2.201 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:24.332
    Sep  6 11:26:24.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename proxy 09/06/23 11:26:24.333
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:24.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:24.353
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Sep  6 11:26:24.355: INFO: Creating pod...
    Sep  6 11:26:24.367: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8901" to be "running"
    Sep  6 11:26:24.375: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.612254ms
    Sep  6 11:26:26.384: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.016500355s
    Sep  6 11:26:26.384: INFO: Pod "agnhost" satisfied condition "running"
    Sep  6 11:26:26.384: INFO: Creating service...
    Sep  6 11:26:26.401: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=DELETE
    Sep  6 11:26:26.413: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  6 11:26:26.413: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=OPTIONS
    Sep  6 11:26:26.421: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  6 11:26:26.421: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=PATCH
    Sep  6 11:26:26.426: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  6 11:26:26.426: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=POST
    Sep  6 11:26:26.432: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  6 11:26:26.432: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=PUT
    Sep  6 11:26:26.438: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  6 11:26:26.438: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=DELETE
    Sep  6 11:26:26.448: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  6 11:26:26.448: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Sep  6 11:26:26.470: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  6 11:26:26.470: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=PATCH
    Sep  6 11:26:26.477: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  6 11:26:26.477: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=POST
    Sep  6 11:26:26.487: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  6 11:26:26.488: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=PUT
    Sep  6 11:26:26.496: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  6 11:26:26.496: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=GET
    Sep  6 11:26:26.499: INFO: http.Client request:GET StatusCode:301
    Sep  6 11:26:26.499: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=GET
    Sep  6 11:26:26.507: INFO: http.Client request:GET StatusCode:301
    Sep  6 11:26:26.507: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/pods/agnhost/proxy?method=HEAD
    Sep  6 11:26:26.511: INFO: http.Client request:HEAD StatusCode:301
    Sep  6 11:26:26.511: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8901/services/e2e-proxy-test-service/proxy?method=HEAD
    Sep  6 11:26:26.516: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:26.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8901" for this suite. 09/06/23 11:26:26.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:26.545
Sep  6 11:26:26.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 11:26:26.546
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:26.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:26.586
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 11:26:26.611
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:26:27.26
STEP: Deploying the webhook pod 09/06/23 11:26:27.268
STEP: Wait for the deployment to be ready 09/06/23 11:26:27.282
Sep  6 11:26:27.291: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 11:26:29.3
STEP: Verifying the service has paired with the endpoint 09/06/23 11:26:29.313
Sep  6 11:26:30.314: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/06/23 11:26:30.346
STEP: Registering slow webhook via the AdmissionRegistration API 09/06/23 11:26:30.347
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/06/23 11:26:30.693
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/06/23 11:26:31.825
STEP: Registering slow webhook via the AdmissionRegistration API 09/06/23 11:26:31.825
Sep  6 11:26:31.910: INFO: Waiting for webhook configuration to be ready...
STEP: Having no error when timeout is longer than webhook latency 09/06/23 11:26:33.121
STEP: Registering slow webhook via the AdmissionRegistration API 09/06/23 11:26:33.121
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/06/23 11:26:38.219
STEP: Registering slow webhook via the AdmissionRegistration API 09/06/23 11:26:38.22
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:43.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9728" for this suite. 09/06/23 11:26:43.334
STEP: Destroying namespace "webhook-9728-markers" for this suite. 09/06/23 11:26:43.356
------------------------------
• [SLOW TEST] [16.835 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:26.545
    Sep  6 11:26:26.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 11:26:26.546
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:26.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:26.586
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 11:26:26.611
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:26:27.26
    STEP: Deploying the webhook pod 09/06/23 11:26:27.268
    STEP: Wait for the deployment to be ready 09/06/23 11:26:27.282
    Sep  6 11:26:27.291: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 11:26:29.3
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:26:29.313
    Sep  6 11:26:30.314: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/06/23 11:26:30.346
    STEP: Registering slow webhook via the AdmissionRegistration API 09/06/23 11:26:30.347
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/06/23 11:26:30.693
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/06/23 11:26:31.825
    STEP: Registering slow webhook via the AdmissionRegistration API 09/06/23 11:26:31.825
    Sep  6 11:26:31.910: INFO: Waiting for webhook configuration to be ready...
    STEP: Having no error when timeout is longer than webhook latency 09/06/23 11:26:33.121
    STEP: Registering slow webhook via the AdmissionRegistration API 09/06/23 11:26:33.121
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/06/23 11:26:38.219
    STEP: Registering slow webhook via the AdmissionRegistration API 09/06/23 11:26:38.22
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:43.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9728" for this suite. 09/06/23 11:26:43.334
    STEP: Destroying namespace "webhook-9728-markers" for this suite. 09/06/23 11:26:43.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:43.384
Sep  6 11:26:43.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename prestop 09/06/23 11:26:43.387
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:43.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:43.455
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1656 09/06/23 11:26:43.461
STEP: Waiting for pods to come up. 09/06/23 11:26:43.477
Sep  6 11:26:43.477: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1656" to be "running"
Sep  6 11:26:43.493: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 15.46186ms
Sep  6 11:26:45.505: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.027480259s
Sep  6 11:26:45.505: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1656 09/06/23 11:26:45.517
Sep  6 11:26:45.529: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1656" to be "running"
Sep  6 11:26:45.535: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.712386ms
Sep  6 11:26:47.542: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.013184488s
Sep  6 11:26:47.542: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 09/06/23 11:26:47.542
Sep  6 11:26:52.564: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 09/06/23 11:26:52.564
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-1656" for this suite. 09/06/23 11:26:52.597
------------------------------
• [SLOW TEST] [9.225 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:43.384
    Sep  6 11:26:43.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename prestop 09/06/23 11:26:43.387
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:43.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:43.455
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1656 09/06/23 11:26:43.461
    STEP: Waiting for pods to come up. 09/06/23 11:26:43.477
    Sep  6 11:26:43.477: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1656" to be "running"
    Sep  6 11:26:43.493: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 15.46186ms
    Sep  6 11:26:45.505: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.027480259s
    Sep  6 11:26:45.505: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1656 09/06/23 11:26:45.517
    Sep  6 11:26:45.529: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1656" to be "running"
    Sep  6 11:26:45.535: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.712386ms
    Sep  6 11:26:47.542: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.013184488s
    Sep  6 11:26:47.542: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 09/06/23 11:26:47.542
    Sep  6 11:26:52.564: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 09/06/23 11:26:52.564
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-1656" for this suite. 09/06/23 11:26:52.597
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:52.609
Sep  6 11:26:52.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 11:26:52.61
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:52.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:52.656
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-587/secret-test-58a1e231-9f9b-4c3e-9a4c-72a4ec691307 09/06/23 11:26:52.661
STEP: Creating a pod to test consume secrets 09/06/23 11:26:52.667
Sep  6 11:26:52.676: INFO: Waiting up to 5m0s for pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd" in namespace "secrets-587" to be "Succeeded or Failed"
Sep  6 11:26:52.681: INFO: Pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.513319ms
Sep  6 11:26:54.695: INFO: Pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019600539s
Sep  6 11:26:56.695: INFO: Pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019312207s
STEP: Saw pod success 09/06/23 11:26:56.695
Sep  6 11:26:56.696: INFO: Pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd" satisfied condition "Succeeded or Failed"
Sep  6 11:26:56.705: INFO: Trying to get logs from node kube-3 pod pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd container env-test: <nil>
STEP: delete the pod 09/06/23 11:26:56.723
Sep  6 11:26:56.860: INFO: Waiting for pod pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd to disappear
Sep  6 11:26:56.877: INFO: Pod pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 11:26:56.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-587" for this suite. 09/06/23 11:26:56.885
------------------------------
• [4.292 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:52.609
    Sep  6 11:26:52.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 11:26:52.61
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:52.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:52.656
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-587/secret-test-58a1e231-9f9b-4c3e-9a4c-72a4ec691307 09/06/23 11:26:52.661
    STEP: Creating a pod to test consume secrets 09/06/23 11:26:52.667
    Sep  6 11:26:52.676: INFO: Waiting up to 5m0s for pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd" in namespace "secrets-587" to be "Succeeded or Failed"
    Sep  6 11:26:52.681: INFO: Pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.513319ms
    Sep  6 11:26:54.695: INFO: Pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019600539s
    Sep  6 11:26:56.695: INFO: Pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019312207s
    STEP: Saw pod success 09/06/23 11:26:56.695
    Sep  6 11:26:56.696: INFO: Pod "pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd" satisfied condition "Succeeded or Failed"
    Sep  6 11:26:56.705: INFO: Trying to get logs from node kube-3 pod pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd container env-test: <nil>
    STEP: delete the pod 09/06/23 11:26:56.723
    Sep  6 11:26:56.860: INFO: Waiting for pod pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd to disappear
    Sep  6 11:26:56.877: INFO: Pod pod-configmaps-f5e3b1ee-c9e2-4fae-940d-002fe80c5ebd no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:26:56.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-587" for this suite. 09/06/23 11:26:56.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:26:56.908
Sep  6 11:26:56.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replicaset 09/06/23 11:26:56.91
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:57.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:57.01
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 09/06/23 11:26:57.012
STEP: Verify that the required pods have come up 09/06/23 11:26:57.037
Sep  6 11:26:57.042: INFO: Pod name sample-pod: Found 0 pods out of 3
Sep  6 11:27:02.046: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 09/06/23 11:27:02.046
Sep  6 11:27:02.050: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 09/06/23 11:27:02.05
STEP: DeleteCollection of the ReplicaSets 09/06/23 11:27:02.055
STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/06/23 11:27:02.065
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:02.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6065" for this suite. 09/06/23 11:27:02.076
------------------------------
• [SLOW TEST] [5.212 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:26:56.908
    Sep  6 11:26:56.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replicaset 09/06/23 11:26:56.91
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:26:57.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:26:57.01
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 09/06/23 11:26:57.012
    STEP: Verify that the required pods have come up 09/06/23 11:26:57.037
    Sep  6 11:26:57.042: INFO: Pod name sample-pod: Found 0 pods out of 3
    Sep  6 11:27:02.046: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 09/06/23 11:27:02.046
    Sep  6 11:27:02.050: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 09/06/23 11:27:02.05
    STEP: DeleteCollection of the ReplicaSets 09/06/23 11:27:02.055
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/06/23 11:27:02.065
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:02.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6065" for this suite. 09/06/23 11:27:02.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:02.127
Sep  6 11:27:02.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 11:27:02.128
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:02.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:02.205
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 09/06/23 11:27:02.209
STEP: submitting the pod to kubernetes 09/06/23 11:27:02.209
Sep  6 11:27:02.239: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec" in namespace "pods-9078" to be "running and ready"
Sep  6 11:27:02.250: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.136271ms
Sep  6 11:27:02.250: INFO: The phase of Pod pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:27:04.267: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Running", Reason="", readiness=true. Elapsed: 2.028033563s
Sep  6 11:27:04.267: INFO: The phase of Pod pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec is Running (Ready = true)
Sep  6 11:27:04.267: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/06/23 11:27:04.28
STEP: updating the pod 09/06/23 11:27:04.289
Sep  6 11:27:04.834: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec"
Sep  6 11:27:04.834: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec" in namespace "pods-9078" to be "terminated with reason DeadlineExceeded"
Sep  6 11:27:04.837: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Running", Reason="", readiness=true. Elapsed: 3.770886ms
Sep  6 11:27:06.852: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Running", Reason="", readiness=true. Elapsed: 2.018791716s
Sep  6 11:27:08.845: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Running", Reason="", readiness=false. Elapsed: 4.011090893s
Sep  6 11:27:10.852: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.018272012s
Sep  6 11:27:10.852: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:10.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9078" for this suite. 09/06/23 11:27:10.869
------------------------------
• [SLOW TEST] [8.766 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:02.127
    Sep  6 11:27:02.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 11:27:02.128
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:02.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:02.205
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 09/06/23 11:27:02.209
    STEP: submitting the pod to kubernetes 09/06/23 11:27:02.209
    Sep  6 11:27:02.239: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec" in namespace "pods-9078" to be "running and ready"
    Sep  6 11:27:02.250: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.136271ms
    Sep  6 11:27:02.250: INFO: The phase of Pod pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:27:04.267: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Running", Reason="", readiness=true. Elapsed: 2.028033563s
    Sep  6 11:27:04.267: INFO: The phase of Pod pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec is Running (Ready = true)
    Sep  6 11:27:04.267: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/06/23 11:27:04.28
    STEP: updating the pod 09/06/23 11:27:04.289
    Sep  6 11:27:04.834: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec"
    Sep  6 11:27:04.834: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec" in namespace "pods-9078" to be "terminated with reason DeadlineExceeded"
    Sep  6 11:27:04.837: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Running", Reason="", readiness=true. Elapsed: 3.770886ms
    Sep  6 11:27:06.852: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Running", Reason="", readiness=true. Elapsed: 2.018791716s
    Sep  6 11:27:08.845: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Running", Reason="", readiness=false. Elapsed: 4.011090893s
    Sep  6 11:27:10.852: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.018272012s
    Sep  6 11:27:10.852: INFO: Pod "pod-update-activedeadlineseconds-3d21fda1-42fc-4e97-8384-f50435eebdec" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:10.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9078" for this suite. 09/06/23 11:27:10.869
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:10.893
Sep  6 11:27:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 11:27:10.894
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:10.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:10.916
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 09/06/23 11:27:10.919
Sep  6 11:27:10.927: INFO: created test-pod-1
Sep  6 11:27:10.939: INFO: created test-pod-2
Sep  6 11:27:10.949: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 09/06/23 11:27:10.95
Sep  6 11:27:10.950: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6111' to be running and ready
Sep  6 11:27:10.983: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  6 11:27:10.983: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  6 11:27:10.983: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  6 11:27:10.983: INFO: 0 / 3 pods in namespace 'pods-6111' are running and ready (0 seconds elapsed)
Sep  6 11:27:10.983: INFO: expected 0 pod replicas in namespace 'pods-6111', 0 are Running and Ready.
Sep  6 11:27:10.983: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
Sep  6 11:27:10.983: INFO: test-pod-1  kube-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC  }]
Sep  6 11:27:10.983: INFO: test-pod-2  kube-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC  }]
Sep  6 11:27:10.983: INFO: test-pod-3  kube-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC  }]
Sep  6 11:27:10.983: INFO: 
Sep  6 11:27:13.030: INFO: 3 / 3 pods in namespace 'pods-6111' are running and ready (2 seconds elapsed)
Sep  6 11:27:13.030: INFO: expected 0 pod replicas in namespace 'pods-6111', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 09/06/23 11:27:13.071
Sep  6 11:27:13.085: INFO: Pod quantity 3 is different from expected quantity 0
Sep  6 11:27:14.092: INFO: Pod quantity 3 is different from expected quantity 0
Sep  6 11:27:15.100: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:16.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6111" for this suite. 09/06/23 11:27:16.094
------------------------------
• [SLOW TEST] [5.211 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:10.893
    Sep  6 11:27:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 11:27:10.894
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:10.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:10.916
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 09/06/23 11:27:10.919
    Sep  6 11:27:10.927: INFO: created test-pod-1
    Sep  6 11:27:10.939: INFO: created test-pod-2
    Sep  6 11:27:10.949: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 09/06/23 11:27:10.95
    Sep  6 11:27:10.950: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6111' to be running and ready
    Sep  6 11:27:10.983: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  6 11:27:10.983: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  6 11:27:10.983: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  6 11:27:10.983: INFO: 0 / 3 pods in namespace 'pods-6111' are running and ready (0 seconds elapsed)
    Sep  6 11:27:10.983: INFO: expected 0 pod replicas in namespace 'pods-6111', 0 are Running and Ready.
    Sep  6 11:27:10.983: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
    Sep  6 11:27:10.983: INFO: test-pod-1  kube-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC  }]
    Sep  6 11:27:10.983: INFO: test-pod-2  kube-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC  }]
    Sep  6 11:27:10.983: INFO: test-pod-3  kube-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:27:10 +0000 UTC  }]
    Sep  6 11:27:10.983: INFO: 
    Sep  6 11:27:13.030: INFO: 3 / 3 pods in namespace 'pods-6111' are running and ready (2 seconds elapsed)
    Sep  6 11:27:13.030: INFO: expected 0 pod replicas in namespace 'pods-6111', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 09/06/23 11:27:13.071
    Sep  6 11:27:13.085: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  6 11:27:14.092: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  6 11:27:15.100: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:16.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6111" for this suite. 09/06/23 11:27:16.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:16.106
Sep  6 11:27:16.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 11:27:16.107
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:16.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:16.135
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-9772/configmap-test-cb56d744-0e4d-406d-b8f3-729bd643e628 09/06/23 11:27:16.138
STEP: Creating a pod to test consume configMaps 09/06/23 11:27:16.145
Sep  6 11:27:16.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f" in namespace "configmap-9772" to be "Succeeded or Failed"
Sep  6 11:27:16.159: INFO: Pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.849312ms
Sep  6 11:27:18.185: INFO: Pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030882203s
Sep  6 11:27:20.167: INFO: Pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012587443s
STEP: Saw pod success 09/06/23 11:27:20.167
Sep  6 11:27:20.167: INFO: Pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f" satisfied condition "Succeeded or Failed"
Sep  6 11:27:20.170: INFO: Trying to get logs from node kube-3 pod pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f container env-test: <nil>
STEP: delete the pod 09/06/23 11:27:20.176
Sep  6 11:27:20.317: INFO: Waiting for pod pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f to disappear
Sep  6 11:27:20.319: INFO: Pod pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:20.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9772" for this suite. 09/06/23 11:27:20.323
------------------------------
• [4.235 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:16.106
    Sep  6 11:27:16.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 11:27:16.107
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:16.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:16.135
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-9772/configmap-test-cb56d744-0e4d-406d-b8f3-729bd643e628 09/06/23 11:27:16.138
    STEP: Creating a pod to test consume configMaps 09/06/23 11:27:16.145
    Sep  6 11:27:16.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f" in namespace "configmap-9772" to be "Succeeded or Failed"
    Sep  6 11:27:16.159: INFO: Pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.849312ms
    Sep  6 11:27:18.185: INFO: Pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030882203s
    Sep  6 11:27:20.167: INFO: Pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012587443s
    STEP: Saw pod success 09/06/23 11:27:20.167
    Sep  6 11:27:20.167: INFO: Pod "pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f" satisfied condition "Succeeded or Failed"
    Sep  6 11:27:20.170: INFO: Trying to get logs from node kube-3 pod pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f container env-test: <nil>
    STEP: delete the pod 09/06/23 11:27:20.176
    Sep  6 11:27:20.317: INFO: Waiting for pod pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f to disappear
    Sep  6 11:27:20.319: INFO: Pod pod-configmaps-c76cb8d5-a677-4ae9-b17c-4f2791888a5f no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:20.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9772" for this suite. 09/06/23 11:27:20.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:20.342
Sep  6 11:27:20.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:27:20.343
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:20.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:20.437
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/06/23 11:27:20.468
Sep  6 11:27:20.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/06/23 11:27:31.987
Sep  6 11:27:31.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:27:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:50.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3228" for this suite. 09/06/23 11:27:50.008
------------------------------
• [SLOW TEST] [29.675 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:20.342
    Sep  6 11:27:20.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:27:20.343
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:20.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:20.437
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/06/23 11:27:20.468
    Sep  6 11:27:20.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/06/23 11:27:31.987
    Sep  6 11:27:31.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:27:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:50.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3228" for this suite. 09/06/23 11:27:50.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:50.018
Sep  6 11:27:50.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sysctl 09/06/23 11:27:50.019
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:50.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:50.042
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/06/23 11:27:50.044
STEP: Watching for error events or started pod 09/06/23 11:27:50.054
STEP: Waiting for pod completion 09/06/23 11:27:52.06
Sep  6 11:27:52.060: INFO: Waiting up to 3m0s for pod "sysctl-27cbd6a3-9d9b-4b4a-8960-cc74c2c41eef" in namespace "sysctl-6100" to be "completed"
Sep  6 11:27:52.065: INFO: Pod "sysctl-27cbd6a3-9d9b-4b4a-8960-cc74c2c41eef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.805746ms
Sep  6 11:27:54.069: INFO: Pod "sysctl-27cbd6a3-9d9b-4b4a-8960-cc74c2c41eef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008614302s
Sep  6 11:27:54.069: INFO: Pod "sysctl-27cbd6a3-9d9b-4b4a-8960-cc74c2c41eef" satisfied condition "completed"
STEP: Checking that the pod succeeded 09/06/23 11:27:54.071
STEP: Getting logs from the pod 09/06/23 11:27:54.071
STEP: Checking that the sysctl is actually updated 09/06/23 11:27:54.077
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:54.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-6100" for this suite. 09/06/23 11:27:54.081
------------------------------
• [4.070 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:50.018
    Sep  6 11:27:50.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sysctl 09/06/23 11:27:50.019
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:50.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:50.042
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/06/23 11:27:50.044
    STEP: Watching for error events or started pod 09/06/23 11:27:50.054
    STEP: Waiting for pod completion 09/06/23 11:27:52.06
    Sep  6 11:27:52.060: INFO: Waiting up to 3m0s for pod "sysctl-27cbd6a3-9d9b-4b4a-8960-cc74c2c41eef" in namespace "sysctl-6100" to be "completed"
    Sep  6 11:27:52.065: INFO: Pod "sysctl-27cbd6a3-9d9b-4b4a-8960-cc74c2c41eef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.805746ms
    Sep  6 11:27:54.069: INFO: Pod "sysctl-27cbd6a3-9d9b-4b4a-8960-cc74c2c41eef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008614302s
    Sep  6 11:27:54.069: INFO: Pod "sysctl-27cbd6a3-9d9b-4b4a-8960-cc74c2c41eef" satisfied condition "completed"
    STEP: Checking that the pod succeeded 09/06/23 11:27:54.071
    STEP: Getting logs from the pod 09/06/23 11:27:54.071
    STEP: Checking that the sysctl is actually updated 09/06/23 11:27:54.077
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:54.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-6100" for this suite. 09/06/23 11:27:54.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:54.09
Sep  6 11:27:54.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename tables 09/06/23 11:27:54.091
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:54.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:54.112
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:54.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-6987" for this suite. 09/06/23 11:27:54.119
------------------------------
• [0.035 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:54.09
    Sep  6 11:27:54.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename tables 09/06/23 11:27:54.091
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:54.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:54.112
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:54.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-6987" for this suite. 09/06/23 11:27:54.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:54.127
Sep  6 11:27:54.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replication-controller 09/06/23 11:27:54.128
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:54.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:54.149
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Sep  6 11:27:54.150: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/06/23 11:27:55.168
STEP: Checking rc "condition-test" has the desired failure condition set 09/06/23 11:27:55.173
STEP: Scaling down rc "condition-test" to satisfy pod quota 09/06/23 11:27:56.187
Sep  6 11:27:56.290: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 09/06/23 11:27:56.29
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:56.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3071" for this suite. 09/06/23 11:27:56.355
------------------------------
• [2.256 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:54.127
    Sep  6 11:27:54.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replication-controller 09/06/23 11:27:54.128
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:54.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:54.149
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Sep  6 11:27:54.150: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/06/23 11:27:55.168
    STEP: Checking rc "condition-test" has the desired failure condition set 09/06/23 11:27:55.173
    STEP: Scaling down rc "condition-test" to satisfy pod quota 09/06/23 11:27:56.187
    Sep  6 11:27:56.290: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 09/06/23 11:27:56.29
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:56.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3071" for this suite. 09/06/23 11:27:56.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:56.383
Sep  6 11:27:56.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename namespaces 09/06/23 11:27:56.384
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:56.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:56.485
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-n2zdd" 09/06/23 11:27:56.488
Sep  6 11:27:56.570: INFO: Namespace "e2e-ns-n2zdd-9542" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-n2zdd-9542" 09/06/23 11:27:56.57
Sep  6 11:27:56.638: INFO: Namespace "e2e-ns-n2zdd-9542" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-n2zdd-9542" 09/06/23 11:27:56.638
Sep  6 11:27:56.711: INFO: Namespace "e2e-ns-n2zdd-9542" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:56.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1105" for this suite. 09/06/23 11:27:56.716
STEP: Destroying namespace "e2e-ns-n2zdd-9542" for this suite. 09/06/23 11:27:56.729
------------------------------
• [0.368 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:56.383
    Sep  6 11:27:56.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename namespaces 09/06/23 11:27:56.384
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:56.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:56.485
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-n2zdd" 09/06/23 11:27:56.488
    Sep  6 11:27:56.570: INFO: Namespace "e2e-ns-n2zdd-9542" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-n2zdd-9542" 09/06/23 11:27:56.57
    Sep  6 11:27:56.638: INFO: Namespace "e2e-ns-n2zdd-9542" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-n2zdd-9542" 09/06/23 11:27:56.638
    Sep  6 11:27:56.711: INFO: Namespace "e2e-ns-n2zdd-9542" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:56.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1105" for this suite. 09/06/23 11:27:56.716
    STEP: Destroying namespace "e2e-ns-n2zdd-9542" for this suite. 09/06/23 11:27:56.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:56.753
Sep  6 11:27:56.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename disruption 09/06/23 11:27:56.754
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:56.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:56.791
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 09/06/23 11:27:56.804
STEP: Waiting for all pods to be running 09/06/23 11:27:56.868
Sep  6 11:27:56.878: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  6 11:27:58.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4631" for this suite. 09/06/23 11:27:58.912
------------------------------
• [2.173 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:56.753
    Sep  6 11:27:56.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename disruption 09/06/23 11:27:56.754
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:56.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:56.791
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 09/06/23 11:27:56.804
    STEP: Waiting for all pods to be running 09/06/23 11:27:56.868
    Sep  6 11:27:56.878: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:27:58.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4631" for this suite. 09/06/23 11:27:58.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:27:58.927
Sep  6 11:27:58.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replication-controller 09/06/23 11:27:58.928
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:59.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:59.008
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 09/06/23 11:27:59.01
STEP: When the matched label of one of its pods change 09/06/23 11:27:59.016
Sep  6 11:27:59.019: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  6 11:28:04.025: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 09/06/23 11:28:04.06
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  6 11:28:05.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-752" for this suite. 09/06/23 11:28:05.083
------------------------------
• [SLOW TEST] [6.163 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:27:58.927
    Sep  6 11:27:58.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replication-controller 09/06/23 11:27:58.928
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:27:59.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:27:59.008
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 09/06/23 11:27:59.01
    STEP: When the matched label of one of its pods change 09/06/23 11:27:59.016
    Sep  6 11:27:59.019: INFO: Pod name pod-release: Found 0 pods out of 1
    Sep  6 11:28:04.025: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/06/23 11:28:04.06
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:28:05.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-752" for this suite. 09/06/23 11:28:05.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:28:05.092
Sep  6 11:28:05.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-lifecycle-hook 09/06/23 11:28:05.093
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:05.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:05.116
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/06/23 11:28:05.12
Sep  6 11:28:05.129: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-519" to be "running and ready"
Sep  6 11:28:05.133: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343619ms
Sep  6 11:28:05.133: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:28:07.149: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.019427819s
Sep  6 11:28:07.149: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  6 11:28:07.149: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 09/06/23 11:28:07.159
Sep  6 11:28:07.182: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-519" to be "running and ready"
Sep  6 11:28:07.196: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.382808ms
Sep  6 11:28:07.196: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:28:09.212: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029720973s
Sep  6 11:28:09.212: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Sep  6 11:28:09.212: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/06/23 11:28:09.218
STEP: delete the pod with lifecycle hook 09/06/23 11:28:09.237
Sep  6 11:28:09.249: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 11:28:09.254: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 11:28:11.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 11:28:11.269: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 11:28:13.256: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 11:28:13.275: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  6 11:28:13.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-519" for this suite. 09/06/23 11:28:13.288
------------------------------
• [SLOW TEST] [8.211 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:28:05.092
    Sep  6 11:28:05.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/06/23 11:28:05.093
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:05.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:05.116
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/06/23 11:28:05.12
    Sep  6 11:28:05.129: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-519" to be "running and ready"
    Sep  6 11:28:05.133: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343619ms
    Sep  6 11:28:05.133: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:28:07.149: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.019427819s
    Sep  6 11:28:07.149: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  6 11:28:07.149: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 09/06/23 11:28:07.159
    Sep  6 11:28:07.182: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-519" to be "running and ready"
    Sep  6 11:28:07.196: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.382808ms
    Sep  6 11:28:07.196: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:28:09.212: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029720973s
    Sep  6 11:28:09.212: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Sep  6 11:28:09.212: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/06/23 11:28:09.218
    STEP: delete the pod with lifecycle hook 09/06/23 11:28:09.237
    Sep  6 11:28:09.249: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  6 11:28:09.254: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  6 11:28:11.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  6 11:28:11.269: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  6 11:28:13.256: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  6 11:28:13.275: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:28:13.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-519" for this suite. 09/06/23 11:28:13.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:28:13.309
Sep  6 11:28:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 11:28:13.31
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:13.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:13.333
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-e29be0ff-bf61-4ae2-b615-ff780f1f84d6 09/06/23 11:28:13.336
STEP: Creating a pod to test consume secrets 09/06/23 11:28:13.341
Sep  6 11:28:13.424: INFO: Waiting up to 5m0s for pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74" in namespace "secrets-9118" to be "Succeeded or Failed"
Sep  6 11:28:13.516: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74": Phase="Pending", Reason="", readiness=false. Elapsed: 91.619549ms
Sep  6 11:28:15.536: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111895621s
Sep  6 11:28:17.529: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10509005s
Sep  6 11:28:19.520: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095507622s
STEP: Saw pod success 09/06/23 11:28:19.52
Sep  6 11:28:19.520: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74" satisfied condition "Succeeded or Failed"
Sep  6 11:28:19.523: INFO: Trying to get logs from node kube-3 pod pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74 container secret-volume-test: <nil>
STEP: delete the pod 09/06/23 11:28:19.531
Sep  6 11:28:19.547: INFO: Waiting for pod pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74 to disappear
Sep  6 11:28:19.549: INFO: Pod pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 11:28:19.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9118" for this suite. 09/06/23 11:28:19.553
------------------------------
• [SLOW TEST] [6.263 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:28:13.309
    Sep  6 11:28:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 11:28:13.31
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:13.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:13.333
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-e29be0ff-bf61-4ae2-b615-ff780f1f84d6 09/06/23 11:28:13.336
    STEP: Creating a pod to test consume secrets 09/06/23 11:28:13.341
    Sep  6 11:28:13.424: INFO: Waiting up to 5m0s for pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74" in namespace "secrets-9118" to be "Succeeded or Failed"
    Sep  6 11:28:13.516: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74": Phase="Pending", Reason="", readiness=false. Elapsed: 91.619549ms
    Sep  6 11:28:15.536: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111895621s
    Sep  6 11:28:17.529: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10509005s
    Sep  6 11:28:19.520: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095507622s
    STEP: Saw pod success 09/06/23 11:28:19.52
    Sep  6 11:28:19.520: INFO: Pod "pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74" satisfied condition "Succeeded or Failed"
    Sep  6 11:28:19.523: INFO: Trying to get logs from node kube-3 pod pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74 container secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 11:28:19.531
    Sep  6 11:28:19.547: INFO: Waiting for pod pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74 to disappear
    Sep  6 11:28:19.549: INFO: Pod pod-secrets-86824a81-10ec-4b3e-b7e1-15e61e4c8a74 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:28:19.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9118" for this suite. 09/06/23 11:28:19.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:28:19.573
Sep  6 11:28:19.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-lifecycle-hook 09/06/23 11:28:19.574
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:19.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:19.605
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/06/23 11:28:19.615
Sep  6 11:28:19.623: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8218" to be "running and ready"
Sep  6 11:28:19.628: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098266ms
Sep  6 11:28:19.628: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:28:21.634: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010721101s
Sep  6 11:28:21.634: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  6 11:28:21.634: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 09/06/23 11:28:21.642
Sep  6 11:28:21.650: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8218" to be "running and ready"
Sep  6 11:28:21.658: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.575605ms
Sep  6 11:28:21.658: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:28:23.671: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.020944955s
Sep  6 11:28:23.671: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Sep  6 11:28:23.671: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/06/23 11:28:23.68
Sep  6 11:28:23.703: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 11:28:23.707: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 11:28:25.708: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 11:28:25.713: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 11:28:27.709: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 11:28:27.723: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 09/06/23 11:28:27.723
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  6 11:28:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8218" for this suite. 09/06/23 11:28:27.76
------------------------------
• [SLOW TEST] [8.202 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:28:19.573
    Sep  6 11:28:19.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/06/23 11:28:19.574
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:19.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:19.605
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/06/23 11:28:19.615
    Sep  6 11:28:19.623: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8218" to be "running and ready"
    Sep  6 11:28:19.628: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098266ms
    Sep  6 11:28:19.628: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:28:21.634: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010721101s
    Sep  6 11:28:21.634: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  6 11:28:21.634: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 09/06/23 11:28:21.642
    Sep  6 11:28:21.650: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8218" to be "running and ready"
    Sep  6 11:28:21.658: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.575605ms
    Sep  6 11:28:21.658: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:28:23.671: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.020944955s
    Sep  6 11:28:23.671: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Sep  6 11:28:23.671: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/06/23 11:28:23.68
    Sep  6 11:28:23.703: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  6 11:28:23.707: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  6 11:28:25.708: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  6 11:28:25.713: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  6 11:28:27.709: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  6 11:28:27.723: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 09/06/23 11:28:27.723
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:28:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8218" for this suite. 09/06/23 11:28:27.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:28:27.78
Sep  6 11:28:27.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replication-controller 09/06/23 11:28:27.781
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:27.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:27.807
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-99vlq" 09/06/23 11:28:27.809
Sep  6 11:28:27.815: INFO: Get Replication Controller "e2e-rc-99vlq" to confirm replicas
Sep  6 11:28:28.819: INFO: Get Replication Controller "e2e-rc-99vlq" to confirm replicas
Sep  6 11:28:28.825: INFO: Found 1 replicas for "e2e-rc-99vlq" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-99vlq" 09/06/23 11:28:28.825
STEP: Updating a scale subresource 09/06/23 11:28:28.833
STEP: Verifying replicas where modified for replication controller "e2e-rc-99vlq" 09/06/23 11:28:28.843
Sep  6 11:28:28.844: INFO: Get Replication Controller "e2e-rc-99vlq" to confirm replicas
Sep  6 11:28:29.848: INFO: Get Replication Controller "e2e-rc-99vlq" to confirm replicas
Sep  6 11:28:29.858: INFO: Found 2 replicas for "e2e-rc-99vlq" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  6 11:28:29.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7318" for this suite. 09/06/23 11:28:29.863
------------------------------
• [2.090 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:28:27.78
    Sep  6 11:28:27.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replication-controller 09/06/23 11:28:27.781
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:27.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:27.807
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-99vlq" 09/06/23 11:28:27.809
    Sep  6 11:28:27.815: INFO: Get Replication Controller "e2e-rc-99vlq" to confirm replicas
    Sep  6 11:28:28.819: INFO: Get Replication Controller "e2e-rc-99vlq" to confirm replicas
    Sep  6 11:28:28.825: INFO: Found 1 replicas for "e2e-rc-99vlq" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-99vlq" 09/06/23 11:28:28.825
    STEP: Updating a scale subresource 09/06/23 11:28:28.833
    STEP: Verifying replicas where modified for replication controller "e2e-rc-99vlq" 09/06/23 11:28:28.843
    Sep  6 11:28:28.844: INFO: Get Replication Controller "e2e-rc-99vlq" to confirm replicas
    Sep  6 11:28:29.848: INFO: Get Replication Controller "e2e-rc-99vlq" to confirm replicas
    Sep  6 11:28:29.858: INFO: Found 2 replicas for "e2e-rc-99vlq" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:28:29.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7318" for this suite. 09/06/23 11:28:29.863
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:28:29.871
Sep  6 11:28:29.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:28:29.873
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:29.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:29.896
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Sep  6 11:28:29.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/06/23 11:28:38.006
Sep  6 11:28:38.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 --namespace=crd-publish-openapi-1530 create -f -'
Sep  6 11:28:38.510: INFO: stderr: ""
Sep  6 11:28:38.510: INFO: stdout: "e2e-test-crd-publish-openapi-1172-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  6 11:28:38.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 --namespace=crd-publish-openapi-1530 delete e2e-test-crd-publish-openapi-1172-crds test-cr'
Sep  6 11:28:38.605: INFO: stderr: ""
Sep  6 11:28:38.605: INFO: stdout: "e2e-test-crd-publish-openapi-1172-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  6 11:28:38.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 --namespace=crd-publish-openapi-1530 apply -f -'
Sep  6 11:28:38.749: INFO: stderr: ""
Sep  6 11:28:38.749: INFO: stdout: "e2e-test-crd-publish-openapi-1172-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  6 11:28:38.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 --namespace=crd-publish-openapi-1530 delete e2e-test-crd-publish-openapi-1172-crds test-cr'
Sep  6 11:28:38.810: INFO: stderr: ""
Sep  6 11:28:38.810: INFO: stdout: "e2e-test-crd-publish-openapi-1172-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/06/23 11:28:38.81
Sep  6 11:28:38.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 explain e2e-test-crd-publish-openapi-1172-crds'
Sep  6 11:28:38.964: INFO: stderr: ""
Sep  6 11:28:38.964: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1172-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:28:41.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1530" for this suite. 09/06/23 11:28:41.053
------------------------------
• [SLOW TEST] [11.193 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:28:29.871
    Sep  6 11:28:29.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:28:29.873
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:29.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:29.896
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Sep  6 11:28:29.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/06/23 11:28:38.006
    Sep  6 11:28:38.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 --namespace=crd-publish-openapi-1530 create -f -'
    Sep  6 11:28:38.510: INFO: stderr: ""
    Sep  6 11:28:38.510: INFO: stdout: "e2e-test-crd-publish-openapi-1172-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  6 11:28:38.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 --namespace=crd-publish-openapi-1530 delete e2e-test-crd-publish-openapi-1172-crds test-cr'
    Sep  6 11:28:38.605: INFO: stderr: ""
    Sep  6 11:28:38.605: INFO: stdout: "e2e-test-crd-publish-openapi-1172-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Sep  6 11:28:38.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 --namespace=crd-publish-openapi-1530 apply -f -'
    Sep  6 11:28:38.749: INFO: stderr: ""
    Sep  6 11:28:38.749: INFO: stdout: "e2e-test-crd-publish-openapi-1172-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  6 11:28:38.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 --namespace=crd-publish-openapi-1530 delete e2e-test-crd-publish-openapi-1172-crds test-cr'
    Sep  6 11:28:38.810: INFO: stderr: ""
    Sep  6 11:28:38.810: INFO: stdout: "e2e-test-crd-publish-openapi-1172-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/06/23 11:28:38.81
    Sep  6 11:28:38.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-1530 explain e2e-test-crd-publish-openapi-1172-crds'
    Sep  6 11:28:38.964: INFO: stderr: ""
    Sep  6 11:28:38.964: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1172-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:28:41.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1530" for this suite. 09/06/23 11:28:41.053
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:28:41.064
Sep  6 11:28:41.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pod-network-test 09/06/23 11:28:41.064
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:41.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:41.088
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9715 09/06/23 11:28:41.09
STEP: creating a selector 09/06/23 11:28:41.09
STEP: Creating the service pods in kubernetes 09/06/23 11:28:41.09
Sep  6 11:28:41.090: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  6 11:28:41.131: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9715" to be "running and ready"
Sep  6 11:28:41.140: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.667583ms
Sep  6 11:28:41.140: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:28:43.153: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021162107s
Sep  6 11:28:43.153: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:28:45.145: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013218361s
Sep  6 11:28:45.145: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:28:47.155: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023181676s
Sep  6 11:28:47.155: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:28:49.153: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.021595783s
Sep  6 11:28:49.153: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:28:51.155: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023474862s
Sep  6 11:28:51.155: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  6 11:28:53.153: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.020991757s
Sep  6 11:28:53.153: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  6 11:28:53.153: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  6 11:28:53.160: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9715" to be "running and ready"
Sep  6 11:28:53.167: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.33105ms
Sep  6 11:28:53.167: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:28:55.180: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.01944878s
Sep  6 11:28:55.180: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:28:57.184: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.023834707s
Sep  6 11:28:57.184: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:28:59.184: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.023657892s
Sep  6 11:28:59.184: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:29:01.173: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.012571491s
Sep  6 11:29:01.173: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  6 11:29:03.173: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.012929997s
Sep  6 11:29:03.173: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  6 11:29:03.173: INFO: Pod "netserver-1" satisfied condition "running and ready"
Sep  6 11:29:03.179: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9715" to be "running and ready"
Sep  6 11:29:03.182: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.128566ms
Sep  6 11:29:03.182: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Sep  6 11:29:03.182: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 09/06/23 11:29:03.185
Sep  6 11:29:03.191: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9715" to be "running"
Sep  6 11:29:03.194: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148714ms
Sep  6 11:29:05.208: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017777489s
Sep  6 11:29:05.208: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  6 11:29:05.218: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep  6 11:29:05.218: INFO: Breadth first check of 10.233.120.77 on host 10.2.20.101...
Sep  6 11:29:05.226: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.122:9080/dial?request=hostname&protocol=http&host=10.233.120.77&port=8083&tries=1'] Namespace:pod-network-test-9715 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:29:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:29:05.228: INFO: ExecWithOptions: Clientset creation
Sep  6 11:29:05.228: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9715/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.120.77%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  6 11:29:05.340: INFO: Waiting for responses: map[]
Sep  6 11:29:05.340: INFO: reached 10.233.120.77 after 0/1 tries
Sep  6 11:29:05.340: INFO: Breadth first check of 10.233.120.206 on host 10.2.20.102...
Sep  6 11:29:05.344: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.122:9080/dial?request=hostname&protocol=http&host=10.233.120.206&port=8083&tries=1'] Namespace:pod-network-test-9715 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:29:05.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:29:05.345: INFO: ExecWithOptions: Clientset creation
Sep  6 11:29:05.345: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9715/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.120.206%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  6 11:29:05.412: INFO: Waiting for responses: map[]
Sep  6 11:29:05.412: INFO: reached 10.233.120.206 after 0/1 tries
Sep  6 11:29:05.413: INFO: Breadth first check of 10.233.99.76 on host 10.2.20.103...
Sep  6 11:29:05.416: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.122:9080/dial?request=hostname&protocol=http&host=10.233.99.76&port=8083&tries=1'] Namespace:pod-network-test-9715 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:29:05.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:29:05.417: INFO: ExecWithOptions: Clientset creation
Sep  6 11:29:05.417: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9715/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.99.76%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  6 11:29:05.469: INFO: Waiting for responses: map[]
Sep  6 11:29:05.469: INFO: reached 10.233.99.76 after 0/1 tries
Sep  6 11:29:05.469: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  6 11:29:05.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9715" for this suite. 09/06/23 11:29:05.474
------------------------------
• [SLOW TEST] [24.417 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:28:41.064
    Sep  6 11:28:41.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pod-network-test 09/06/23 11:28:41.064
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:28:41.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:28:41.088
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9715 09/06/23 11:28:41.09
    STEP: creating a selector 09/06/23 11:28:41.09
    STEP: Creating the service pods in kubernetes 09/06/23 11:28:41.09
    Sep  6 11:28:41.090: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  6 11:28:41.131: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9715" to be "running and ready"
    Sep  6 11:28:41.140: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.667583ms
    Sep  6 11:28:41.140: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:28:43.153: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021162107s
    Sep  6 11:28:43.153: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:28:45.145: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013218361s
    Sep  6 11:28:45.145: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:28:47.155: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023181676s
    Sep  6 11:28:47.155: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:28:49.153: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.021595783s
    Sep  6 11:28:49.153: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:28:51.155: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023474862s
    Sep  6 11:28:51.155: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  6 11:28:53.153: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.020991757s
    Sep  6 11:28:53.153: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  6 11:28:53.153: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  6 11:28:53.160: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9715" to be "running and ready"
    Sep  6 11:28:53.167: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.33105ms
    Sep  6 11:28:53.167: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:28:55.180: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.01944878s
    Sep  6 11:28:55.180: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:28:57.184: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.023834707s
    Sep  6 11:28:57.184: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:28:59.184: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.023657892s
    Sep  6 11:28:59.184: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:29:01.173: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.012571491s
    Sep  6 11:29:01.173: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  6 11:29:03.173: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.012929997s
    Sep  6 11:29:03.173: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  6 11:29:03.173: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Sep  6 11:29:03.179: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9715" to be "running and ready"
    Sep  6 11:29:03.182: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.128566ms
    Sep  6 11:29:03.182: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Sep  6 11:29:03.182: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 09/06/23 11:29:03.185
    Sep  6 11:29:03.191: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9715" to be "running"
    Sep  6 11:29:03.194: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148714ms
    Sep  6 11:29:05.208: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017777489s
    Sep  6 11:29:05.208: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  6 11:29:05.218: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Sep  6 11:29:05.218: INFO: Breadth first check of 10.233.120.77 on host 10.2.20.101...
    Sep  6 11:29:05.226: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.122:9080/dial?request=hostname&protocol=http&host=10.233.120.77&port=8083&tries=1'] Namespace:pod-network-test-9715 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:29:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:29:05.228: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:29:05.228: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9715/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.120.77%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  6 11:29:05.340: INFO: Waiting for responses: map[]
    Sep  6 11:29:05.340: INFO: reached 10.233.120.77 after 0/1 tries
    Sep  6 11:29:05.340: INFO: Breadth first check of 10.233.120.206 on host 10.2.20.102...
    Sep  6 11:29:05.344: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.122:9080/dial?request=hostname&protocol=http&host=10.233.120.206&port=8083&tries=1'] Namespace:pod-network-test-9715 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:29:05.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:29:05.345: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:29:05.345: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9715/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.120.206%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  6 11:29:05.412: INFO: Waiting for responses: map[]
    Sep  6 11:29:05.412: INFO: reached 10.233.120.206 after 0/1 tries
    Sep  6 11:29:05.413: INFO: Breadth first check of 10.233.99.76 on host 10.2.20.103...
    Sep  6 11:29:05.416: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.122:9080/dial?request=hostname&protocol=http&host=10.233.99.76&port=8083&tries=1'] Namespace:pod-network-test-9715 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:29:05.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:29:05.417: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:29:05.417: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9715/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.99.76%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  6 11:29:05.469: INFO: Waiting for responses: map[]
    Sep  6 11:29:05.469: INFO: reached 10.233.99.76 after 0/1 tries
    Sep  6 11:29:05.469: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:29:05.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9715" for this suite. 09/06/23 11:29:05.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:29:05.482
Sep  6 11:29:05.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename deployment 09/06/23 11:29:05.483
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:05.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:05.5
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Sep  6 11:29:05.502: INFO: Creating deployment "webserver-deployment"
Sep  6 11:29:05.508: INFO: Waiting for observed generation 1
Sep  6 11:29:07.876: INFO: Waiting for all required pods to come up
Sep  6 11:29:07.991: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 09/06/23 11:29:07.991
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wj4q6" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hn985" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mdct2" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pmx26" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rkmmv" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7fhhs" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nvqtk" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-46jfd" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gh8t5" in namespace "deployment-5519" to be "running"
Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2jh8k" in namespace "deployment-5519" to be "running"
Sep  6 11:29:08.001: INFO: Pod "webserver-deployment-7f5969cbc7-rkmmv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.392321ms
Sep  6 11:29:08.001: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.134784ms
Sep  6 11:29:08.001: INFO: Pod "webserver-deployment-7f5969cbc7-pmx26": Phase="Pending", Reason="", readiness=false. Elapsed: 9.667206ms
Sep  6 11:29:08.013: INFO: Pod "webserver-deployment-7f5969cbc7-wj4q6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.15772ms
Sep  6 11:29:08.014: INFO: Pod "webserver-deployment-7f5969cbc7-hn985": Phase="Pending", Reason="", readiness=false. Elapsed: 23.08291ms
Sep  6 11:29:08.014: INFO: Pod "webserver-deployment-7f5969cbc7-mdct2": Phase="Pending", Reason="", readiness=false. Elapsed: 23.064629ms
Sep  6 11:29:08.014: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs": Phase="Pending", Reason="", readiness=false. Elapsed: 23.014836ms
Sep  6 11:29:08.379: INFO: Pod "webserver-deployment-7f5969cbc7-nvqtk": Phase="Pending", Reason="", readiness=false. Elapsed: 387.184728ms
Sep  6 11:29:08.379: INFO: Pod "webserver-deployment-7f5969cbc7-46jfd": Phase="Pending", Reason="", readiness=false. Elapsed: 387.081998ms
Sep  6 11:29:08.379: INFO: Pod "webserver-deployment-7f5969cbc7-2jh8k": Phase="Pending", Reason="", readiness=false. Elapsed: 387.014445ms
Sep  6 11:29:10.009: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01781856s
Sep  6 11:29:10.010: INFO: Pod "webserver-deployment-7f5969cbc7-pmx26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019081604s
Sep  6 11:29:10.010: INFO: Pod "webserver-deployment-7f5969cbc7-rkmmv": Phase="Running", Reason="", readiness=true. Elapsed: 2.019229529s
Sep  6 11:29:10.010: INFO: Pod "webserver-deployment-7f5969cbc7-rkmmv" satisfied condition "running"
Sep  6 11:29:10.023: INFO: Pod "webserver-deployment-7f5969cbc7-wj4q6": Phase="Running", Reason="", readiness=true. Elapsed: 2.031774738s
Sep  6 11:29:10.023: INFO: Pod "webserver-deployment-7f5969cbc7-wj4q6" satisfied condition "running"
Sep  6 11:29:10.026: INFO: Pod "webserver-deployment-7f5969cbc7-mdct2": Phase="Running", Reason="", readiness=true. Elapsed: 2.03480327s
Sep  6 11:29:10.026: INFO: Pod "webserver-deployment-7f5969cbc7-mdct2" satisfied condition "running"
Sep  6 11:29:10.026: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034943925s
Sep  6 11:29:10.030: INFO: Pod "webserver-deployment-7f5969cbc7-hn985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039056586s
Sep  6 11:29:10.392: INFO: Pod "webserver-deployment-7f5969cbc7-2jh8k": Phase="Running", Reason="", readiness=true. Elapsed: 2.40019217s
Sep  6 11:29:10.392: INFO: Pod "webserver-deployment-7f5969cbc7-2jh8k" satisfied condition "running"
Sep  6 11:29:10.393: INFO: Pod "webserver-deployment-7f5969cbc7-nvqtk": Phase="Running", Reason="", readiness=true. Elapsed: 2.401945329s
Sep  6 11:29:10.393: INFO: Pod "webserver-deployment-7f5969cbc7-nvqtk" satisfied condition "running"
Sep  6 11:29:10.394: INFO: Pod "webserver-deployment-7f5969cbc7-46jfd": Phase="Running", Reason="", readiness=true. Elapsed: 2.402643831s
Sep  6 11:29:10.394: INFO: Pod "webserver-deployment-7f5969cbc7-46jfd" satisfied condition "running"
Sep  6 11:29:12.021: INFO: Pod "webserver-deployment-7f5969cbc7-hn985": Phase="Running", Reason="", readiness=true. Elapsed: 4.029579746s
Sep  6 11:29:12.021: INFO: Pod "webserver-deployment-7f5969cbc7-hn985" satisfied condition "running"
Sep  6 11:29:12.021: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5": Phase="Running", Reason="", readiness=true. Elapsed: 4.029042548s
Sep  6 11:29:12.021: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5" satisfied condition "running"
Sep  6 11:29:12.176: INFO: Pod "webserver-deployment-7f5969cbc7-pmx26": Phase="Running", Reason="", readiness=true. Elapsed: 4.184919264s
Sep  6 11:29:12.176: INFO: Pod "webserver-deployment-7f5969cbc7-pmx26" satisfied condition "running"
Sep  6 11:29:12.176: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs": Phase="Running", Reason="", readiness=true. Elapsed: 4.184891704s
Sep  6 11:29:12.176: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs" satisfied condition "running"
Sep  6 11:29:12.176: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  6 11:29:12.333: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  6 11:29:12.482: INFO: Updating deployment webserver-deployment
Sep  6 11:29:12.483: INFO: Waiting for observed generation 2
Sep  6 11:29:14.543: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  6 11:29:14.552: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  6 11:29:14.558: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  6 11:29:14.572: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  6 11:29:14.572: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  6 11:29:14.577: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  6 11:29:14.586: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  6 11:29:14.587: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  6 11:29:14.600: INFO: Updating deployment webserver-deployment
Sep  6 11:29:14.600: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  6 11:29:14.612: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  6 11:29:14.616: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  6 11:29:16.704: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5519  52ca9868-7ddb-4a99-bfc8-d3bfdfc05655 37169 3 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b2c4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-06 11:29:14 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-06 11:29:14 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  6 11:29:16.735: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5519  d008de23-79cd-4ffd-b9d8-40d29e32e5a9 37168 3 2023-09-06 11:29:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 52ca9868-7ddb-4a99-bfc8-d3bfdfc05655 0xc0037db517 0xc0037db518}] [] [{kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52ca9868-7ddb-4a99-bfc8-d3bfdfc05655\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037db5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 11:29:16.735: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  6 11:29:16.735: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5519  169c3a4a-8f0f-4687-813c-734f50dd6c76 37160 3 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 52ca9868-7ddb-4a99-bfc8-d3bfdfc05655 0xc0037db427 0xc0037db428}] [] [{kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52ca9868-7ddb-4a99-bfc8-d3bfdfc05655\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037db4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  6 11:29:16.763: INFO: Pod "webserver-deployment-7f5969cbc7-2jh8k" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2jh8k webserver-deployment-7f5969cbc7- deployment-5519  61b56161-f021-44d3-8803-49f2bd12beba 36921 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ede1311768dc2a97b7cbb6699b1fea6e00d8d9253a3ea6a499a7e9232f65687c cni.projectcalico.org/podIP:10.233.120.199/32 cni.projectcalico.org/podIPs:10.233.120.199/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2c8f7 0xc003b2c8f8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvqdl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvqdl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.199,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://91abb988c6c3ed2c1ff5e4ea135fb79444c0f454887975145ecb209b41f1c186,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-5jfn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5jfn9 webserver-deployment-7f5969cbc7- deployment-5519  44fca14c-bba3-4479-91ae-6ec95fd7d795 37245 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2cb27 0xc003b2cb28}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2h7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2h7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-67sv7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-67sv7 webserver-deployment-7f5969cbc7- deployment-5519  a8370165-d0c1-4269-818c-3c127ed07d18 37239 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2cd07 0xc003b2cd08}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnbbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnbbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7fhhs webserver-deployment-7f5969cbc7- deployment-5519  fe4dcfa9-fc79-4308-bda6-13956b815a3a 36931 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:277a026e932e40bb04487bb6d097a556200506eb4ae9b387ef30ffe57ee279ac cni.projectcalico.org/podIP:10.233.99.123/32 cni.projectcalico.org/podIPs:10.233.99.123/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2cf17 0xc003b2cf18}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5l64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5l64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.123,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://286fc88fb5c157c09a553fae5502980ffb723cf87ede55ed40212c59faca2ea9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-8mqdd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8mqdd webserver-deployment-7f5969cbc7- deployment-5519  dc34724e-aa53-497d-b04c-6d3897a71aa5 37216 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d127 0xc003b2d128}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq6xg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq6xg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-dbrwf" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dbrwf webserver-deployment-7f5969cbc7- deployment-5519  fc3ea3db-524b-47cb-a720-e48be65dc349 37197 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:76532e207fe5eec81e77acc9d47a4efb71bd8eebdc51d3f45ead4f83344e0e4d cni.projectcalico.org/podIP:10.233.99.89/32 cni.projectcalico.org/podIPs:10.233.99.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d317 0xc003b2d318}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcrm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcrm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.765: INFO: Pod "webserver-deployment-7f5969cbc7-dzh9b" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dzh9b webserver-deployment-7f5969cbc7- deployment-5519  84e1e900-dde6-42c2-9093-cceb204ad017 37211 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a8c0b9ad8d0884819ed1451146ca653953e3c06e07a59ada983065935faf2b23 cni.projectcalico.org/podIP:10.233.120.226/32 cni.projectcalico.org/podIPs:10.233.120.226/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d507 0xc003b2d508}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8bvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8bvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.774: INFO: Pod "webserver-deployment-7f5969cbc7-fjdcq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fjdcq webserver-deployment-7f5969cbc7- deployment-5519  443f0584-ed7a-4a8c-9dac-f58ffc7d8e5e 37198 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fbd5f09c9c20828c1cb1d8ffbf9c93dbd3e698f1c2906e5aab662669253c6b65 cni.projectcalico.org/podIP:10.233.120.220/32 cni.projectcalico.org/podIPs:10.233.120.220/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d6f7 0xc003b2d6f8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4f6xk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4f6xk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.775: INFO: Pod "webserver-deployment-7f5969cbc7-fx9kl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fx9kl webserver-deployment-7f5969cbc7- deployment-5519  01b088d7-d9e9-431c-9acd-e739d333a3e6 37137 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d8e7 0xc003b2d8e8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpmxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpmxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.777: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gh8t5 webserver-deployment-7f5969cbc7- deployment-5519  b6e508a5-8d3e-40b1-a6e4-868af0aed9c0 36940 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:268d236ed56cec3edcbfd7d65289d78238dcd3f8b8fbf23394842b516a0a7580 cni.projectcalico.org/podIP:10.233.120.127/32 cni.projectcalico.org/podIPs:10.233.120.127/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2da50 0xc003b2da51}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cthz5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cthz5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.127,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d8e3fef0af7ea13ca2eb18d988b1290705f9b0f12996fcda227dbfb8881908ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.778: INFO: Pod "webserver-deployment-7f5969cbc7-hn985" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hn985 webserver-deployment-7f5969cbc7- deployment-5519  40bdcfae-6db2-4989-91ec-6266848b2ca4 36936 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4847c5be703b62aca940e12e1eb4717cccd299496598f7951329ce57c7cc0e6c cni.projectcalico.org/podIP:10.233.120.78/32 cni.projectcalico.org/podIPs:10.233.120.78/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2dc77 0xc003b2dc78}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76hm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76hm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.78,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://85089ae81bfbcdfa5a657a7bf5a31a7730802aea7f673f7342295f8555e33746,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.778: INFO: Pod "webserver-deployment-7f5969cbc7-jwk4c" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jwk4c webserver-deployment-7f5969cbc7- deployment-5519  4cbbf55b-01e8-474f-90c2-a0505c48e8a7 37251 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cd5bd412f49308dbc96e392f9f3401d2067844da47cc31f08724f804bae0df9f cni.projectcalico.org/podIP:10.233.120.73/32 cni.projectcalico.org/podIPs:10.233.120.73/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2dea7 0xc003b2dea8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6wrf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6wrf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.778: INFO: Pod "webserver-deployment-7f5969cbc7-k44sj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k44sj webserver-deployment-7f5969cbc7- deployment-5519  a9ce183a-330c-4242-9639-0d8a225ee7bc 37218 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0fba77dfb476ef092ef7d3043eaf69efdb7b2a1f5577d23e8d0083325c776837 cni.projectcalico.org/podIP:10.233.120.117/32 cni.projectcalico.org/podIPs:10.233.120.117/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0097 0xc0040a0098}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6npp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6npp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.778: INFO: Pod "webserver-deployment-7f5969cbc7-mdct2" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mdct2 webserver-deployment-7f5969cbc7- deployment-5519  11a6d913-52a1-4655-806b-845a7fdb00bc 36915 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bd6b5b06e1d10d840d910c888e6fa85f0fbc3a1b41b270b5632833bb646af4ea cni.projectcalico.org/podIP:10.233.120.255/32 cni.projectcalico.org/podIPs:10.233.120.255/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0287 0xc0040a0288}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x9ctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x9ctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.255,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fd6695372f56336adc5bfcaad299eacee8726592500121364a2b015718854588,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.779: INFO: Pod "webserver-deployment-7f5969cbc7-n4h6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n4h6c webserver-deployment-7f5969cbc7- deployment-5519  349aca08-1484-48ad-9d57-aeb05f3dcd28 37186 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2bba99a40ccca9fa75efed7e745b9f0df1d0df0af7b2a045bfe679f41f109902 cni.projectcalico.org/podIP:10.233.99.68/32 cni.projectcalico.org/podIPs:10.233.99.68/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a04c7 0xc0040a04c8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vpsx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vpsx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.779: INFO: Pod "webserver-deployment-7f5969cbc7-nvqtk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nvqtk webserver-deployment-7f5969cbc7- deployment-5519  5967c9f4-d10d-41d0-9f2f-3d2ef5b54c9e 36933 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f8e373c70cb9518f4cb9a186662aa1a9aec8da7e50090bb95dd40346d9e9df21 cni.projectcalico.org/podIP:10.233.120.71/32 cni.projectcalico.org/podIPs:10.233.120.71/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a06d7 0xc0040a06d8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxxrt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxxrt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.71,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f1ab19601b3c1fde0bc754cb9166e0d5d43a6a6c3e1e83d5785b0776b1f4fe1b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.779: INFO: Pod "webserver-deployment-7f5969cbc7-p2ncz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p2ncz webserver-deployment-7f5969cbc7- deployment-5519  63a3cf4f-7ef0-4e55-826c-488dffa00a29 37242 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:65cf69abcd3e4f2dfff98546640533a7eb418b9855b92ef1cf96ef104dacfe11 cni.projectcalico.org/podIP:10.233.99.95/32 cni.projectcalico.org/podIPs:10.233.99.95/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0907 0xc0040a0908}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqvlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqvlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.780: INFO: Pod "webserver-deployment-7f5969cbc7-rkmmv" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rkmmv webserver-deployment-7f5969cbc7- deployment-5519  c5c1f0b4-ba93-4d92-931a-f429f05a0cf0 36918 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:313d19484bea8279b45dce72b40a124b63afb51e257741b6e7944bf23313039c cni.projectcalico.org/podIP:10.233.120.200/32 cni.projectcalico.org/podIPs:10.233.120.200/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0a90 0xc0040a0a91}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nt6t5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nt6t5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.200,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f2f3bab1538bd0bc3cdea70e159ecceca2735ed4f2c02d2f147bccfd8fb7675f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.780: INFO: Pod "webserver-deployment-7f5969cbc7-v9qqt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v9qqt webserver-deployment-7f5969cbc7- deployment-5519  e6309fa6-634b-4408-a26f-19483aa84491 37167 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0ca7 0xc0040a0ca8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gf5l7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gf5l7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.780: INFO: Pod "webserver-deployment-7f5969cbc7-wj4q6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wj4q6 webserver-deployment-7f5969cbc7- deployment-5519  f386a0ce-cdec-4787-a73f-6cd2968284bb 36871 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4e8e5f92b394d7f87d1a385c1fe94565f80c394d3749c5c59d5da2cd211407e0 cni.projectcalico.org/podIP:10.233.99.126/32 cni.projectcalico.org/podIPs:10.233.99.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0f37 0xc0040a0f38}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djsnl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djsnl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.126,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fb72f54883bfaac7cdcf53b8efe7bc7c8781d4876e19a36170c4f7da0a5b486b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.780: INFO: Pod "webserver-deployment-d9f79cb5-2ks2z" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2ks2z webserver-deployment-d9f79cb5- deployment-5519  b9b59f37-160b-4a72-90a0-f925ee65a2f2 37237 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0040a1187 0xc0040a1188}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p79d4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p79d4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.781: INFO: Pod "webserver-deployment-d9f79cb5-6jlrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6jlrt webserver-deployment-d9f79cb5- deployment-5519  31daabb1-0adc-4a68-bb45-dbffa61ddd59 37257 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:043020a389bcaca665643424adbd71e8b2b74c7608294dece39e04ef8363fad8 cni.projectcalico.org/podIP:10.233.99.94/32 cni.projectcalico.org/podIPs:10.233.99.94/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0040a19cf 0xc0040a1b70}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhlzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhlzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.781: INFO: Pod "webserver-deployment-d9f79cb5-78lfd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-78lfd webserver-deployment-d9f79cb5- deployment-5519  0e0abcdc-2947-45f1-ac94-b8a4e18df428 37171 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0040a1f3f 0xc0040a1f50}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmlg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmlg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.782: INFO: Pod "webserver-deployment-d9f79cb5-78z66" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-78z66 webserver-deployment-d9f79cb5- deployment-5519  41841883-e663-4ab6-90ef-f37039041508 37246 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:eb93892aa9ce79e2f506bfd34d6d95f83acbb11be173e4423128a1638ff61fd1 cni.projectcalico.org/podIP:10.233.120.227/32 cni.projectcalico.org/podIPs:10.233.120.227/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0032c8ccf 0xc0032c8d50}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fktgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fktgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.782: INFO: Pod "webserver-deployment-d9f79cb5-9rtm7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9rtm7 webserver-deployment-d9f79cb5- deployment-5519  830a636d-84a6-4d51-8f53-7452542a4817 37076 0 2023-09-06 11:29:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1226a563880031c5343306c40e3ad874918e84b201b500ca9ea4a23f4484df2f cni.projectcalico.org/podIP:10.233.120.225/32 cni.projectcalico.org/podIPs:10.233.120.225/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0032c978f 0xc0032c97c0}] [] [{calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8w4z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8w4z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.782: INFO: Pod "webserver-deployment-d9f79cb5-g7wxp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g7wxp webserver-deployment-d9f79cb5- deployment-5519  5f146eac-e1a8-49cf-aca1-94765e932ae8 37200 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:73227af129e9e25de928e97ee7265a6b6efd0a1c48d39f0d5a05e2de393b99ff cni.projectcalico.org/podIP:10.233.120.120/32 cni.projectcalico.org/podIPs:10.233.120.120/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0032c9bbf 0xc0032c9bd0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnjhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnjhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.783: INFO: Pod "webserver-deployment-d9f79cb5-jl6hd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jl6hd webserver-deployment-d9f79cb5- deployment-5519  241ad4fe-45b8-4b9e-bc4e-7eccba6f7b23 37256 0 2023-09-06 11:29:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2609a36cd44b008c4bb69f5162380d33115775c78a15b5de8e99bfbacb1b0c61 cni.projectcalico.org/podIP:10.233.120.79/32 cni.projectcalico.org/podIPs:10.233.120.79/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0032c9ecf 0xc0032c9f40}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztzd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztzd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.79,StartTime:2023-09-06 11:29:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.783: INFO: Pod "webserver-deployment-d9f79cb5-mdfjz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mdfjz webserver-deployment-d9f79cb5- deployment-5519  0918821a-1d64-4968-b69e-089ad2f53da2 37258 0 2023-09-06 11:29:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b9741f332c41f5057f8a7fccbb1144f0c3e365f6d36efb492a170602126fb063 cni.projectcalico.org/podIP:10.233.120.209/32 cni.projectcalico.org/podIPs:10.233.120.209/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d41cf 0xc0048d41e0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tp89l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tp89l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.209,StartTime:2023-09-06 11:29:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.783: INFO: Pod "webserver-deployment-d9f79cb5-nscrd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nscrd webserver-deployment-d9f79cb5- deployment-5519  cb472f1b-fb38-4ec1-a73f-b04b16fd6012 37151 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d43ff 0xc0048d4410}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6mmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6mmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.783: INFO: Pod "webserver-deployment-d9f79cb5-qth2k" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qth2k webserver-deployment-d9f79cb5- deployment-5519  da459104-833a-40a3-8b7c-ea0cfe7b9905 37077 0 2023-09-06 11:29:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f1f6427795b7b64baa3940c94b58b8de5255f966d9a2731cc8f8310d798a857d cni.projectcalico.org/podIP:10.233.99.120/32 cni.projectcalico.org/podIPs:10.233.99.120/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d456f 0xc0048d45a0}] [] [{calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvcbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvcbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.784: INFO: Pod "webserver-deployment-d9f79cb5-sk9vw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sk9vw webserver-deployment-d9f79cb5- deployment-5519  f5b55986-f79f-4185-85a2-394f776b799c 37058 0 2023-09-06 11:29:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ae6078bca47ebfd812c9b8afc063c03e2ba8aecb5c008a1a4351b5c2f14d9a8d cni.projectcalico.org/podIP:10.233.99.117/32 cni.projectcalico.org/podIPs:10.233.99.117/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d478f 0xc0048d47c0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbxm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbxm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.785: INFO: Pod "webserver-deployment-d9f79cb5-w8xtd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w8xtd webserver-deployment-d9f79cb5- deployment-5519  9b6971af-bbc4-4bfa-ae6b-a56d262e58a1 37231 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a4fe4753e390996a1c433e15a391914c745b9d27f5629bfad8a78b90f643c466 cni.projectcalico.org/podIP:10.233.120.70/32 cni.projectcalico.org/podIPs:10.233.120.70/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d49af 0xc0048d49e0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58grx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58grx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:29:16.785: INFO: Pod "webserver-deployment-d9f79cb5-zkhvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zkhvj webserver-deployment-d9f79cb5- deployment-5519  13a97a3d-8d98-4095-9222-464e0536f09a 37164 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d4bcf 0xc0048d4be0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85fns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85fns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  6 11:29:16.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5519" for this suite. 09/06/23 11:29:16.806
------------------------------
• [SLOW TEST] [11.367 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:29:05.482
    Sep  6 11:29:05.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename deployment 09/06/23 11:29:05.483
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:05.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:05.5
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Sep  6 11:29:05.502: INFO: Creating deployment "webserver-deployment"
    Sep  6 11:29:05.508: INFO: Waiting for observed generation 1
    Sep  6 11:29:07.876: INFO: Waiting for all required pods to come up
    Sep  6 11:29:07.991: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 09/06/23 11:29:07.991
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wj4q6" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hn985" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mdct2" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pmx26" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rkmmv" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7fhhs" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nvqtk" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-46jfd" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gh8t5" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:07.991: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2jh8k" in namespace "deployment-5519" to be "running"
    Sep  6 11:29:08.001: INFO: Pod "webserver-deployment-7f5969cbc7-rkmmv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.392321ms
    Sep  6 11:29:08.001: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.134784ms
    Sep  6 11:29:08.001: INFO: Pod "webserver-deployment-7f5969cbc7-pmx26": Phase="Pending", Reason="", readiness=false. Elapsed: 9.667206ms
    Sep  6 11:29:08.013: INFO: Pod "webserver-deployment-7f5969cbc7-wj4q6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.15772ms
    Sep  6 11:29:08.014: INFO: Pod "webserver-deployment-7f5969cbc7-hn985": Phase="Pending", Reason="", readiness=false. Elapsed: 23.08291ms
    Sep  6 11:29:08.014: INFO: Pod "webserver-deployment-7f5969cbc7-mdct2": Phase="Pending", Reason="", readiness=false. Elapsed: 23.064629ms
    Sep  6 11:29:08.014: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs": Phase="Pending", Reason="", readiness=false. Elapsed: 23.014836ms
    Sep  6 11:29:08.379: INFO: Pod "webserver-deployment-7f5969cbc7-nvqtk": Phase="Pending", Reason="", readiness=false. Elapsed: 387.184728ms
    Sep  6 11:29:08.379: INFO: Pod "webserver-deployment-7f5969cbc7-46jfd": Phase="Pending", Reason="", readiness=false. Elapsed: 387.081998ms
    Sep  6 11:29:08.379: INFO: Pod "webserver-deployment-7f5969cbc7-2jh8k": Phase="Pending", Reason="", readiness=false. Elapsed: 387.014445ms
    Sep  6 11:29:10.009: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01781856s
    Sep  6 11:29:10.010: INFO: Pod "webserver-deployment-7f5969cbc7-pmx26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019081604s
    Sep  6 11:29:10.010: INFO: Pod "webserver-deployment-7f5969cbc7-rkmmv": Phase="Running", Reason="", readiness=true. Elapsed: 2.019229529s
    Sep  6 11:29:10.010: INFO: Pod "webserver-deployment-7f5969cbc7-rkmmv" satisfied condition "running"
    Sep  6 11:29:10.023: INFO: Pod "webserver-deployment-7f5969cbc7-wj4q6": Phase="Running", Reason="", readiness=true. Elapsed: 2.031774738s
    Sep  6 11:29:10.023: INFO: Pod "webserver-deployment-7f5969cbc7-wj4q6" satisfied condition "running"
    Sep  6 11:29:10.026: INFO: Pod "webserver-deployment-7f5969cbc7-mdct2": Phase="Running", Reason="", readiness=true. Elapsed: 2.03480327s
    Sep  6 11:29:10.026: INFO: Pod "webserver-deployment-7f5969cbc7-mdct2" satisfied condition "running"
    Sep  6 11:29:10.026: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034943925s
    Sep  6 11:29:10.030: INFO: Pod "webserver-deployment-7f5969cbc7-hn985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039056586s
    Sep  6 11:29:10.392: INFO: Pod "webserver-deployment-7f5969cbc7-2jh8k": Phase="Running", Reason="", readiness=true. Elapsed: 2.40019217s
    Sep  6 11:29:10.392: INFO: Pod "webserver-deployment-7f5969cbc7-2jh8k" satisfied condition "running"
    Sep  6 11:29:10.393: INFO: Pod "webserver-deployment-7f5969cbc7-nvqtk": Phase="Running", Reason="", readiness=true. Elapsed: 2.401945329s
    Sep  6 11:29:10.393: INFO: Pod "webserver-deployment-7f5969cbc7-nvqtk" satisfied condition "running"
    Sep  6 11:29:10.394: INFO: Pod "webserver-deployment-7f5969cbc7-46jfd": Phase="Running", Reason="", readiness=true. Elapsed: 2.402643831s
    Sep  6 11:29:10.394: INFO: Pod "webserver-deployment-7f5969cbc7-46jfd" satisfied condition "running"
    Sep  6 11:29:12.021: INFO: Pod "webserver-deployment-7f5969cbc7-hn985": Phase="Running", Reason="", readiness=true. Elapsed: 4.029579746s
    Sep  6 11:29:12.021: INFO: Pod "webserver-deployment-7f5969cbc7-hn985" satisfied condition "running"
    Sep  6 11:29:12.021: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5": Phase="Running", Reason="", readiness=true. Elapsed: 4.029042548s
    Sep  6 11:29:12.021: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5" satisfied condition "running"
    Sep  6 11:29:12.176: INFO: Pod "webserver-deployment-7f5969cbc7-pmx26": Phase="Running", Reason="", readiness=true. Elapsed: 4.184919264s
    Sep  6 11:29:12.176: INFO: Pod "webserver-deployment-7f5969cbc7-pmx26" satisfied condition "running"
    Sep  6 11:29:12.176: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs": Phase="Running", Reason="", readiness=true. Elapsed: 4.184891704s
    Sep  6 11:29:12.176: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs" satisfied condition "running"
    Sep  6 11:29:12.176: INFO: Waiting for deployment "webserver-deployment" to complete
    Sep  6 11:29:12.333: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Sep  6 11:29:12.482: INFO: Updating deployment webserver-deployment
    Sep  6 11:29:12.483: INFO: Waiting for observed generation 2
    Sep  6 11:29:14.543: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Sep  6 11:29:14.552: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Sep  6 11:29:14.558: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  6 11:29:14.572: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Sep  6 11:29:14.572: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Sep  6 11:29:14.577: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  6 11:29:14.586: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Sep  6 11:29:14.587: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Sep  6 11:29:14.600: INFO: Updating deployment webserver-deployment
    Sep  6 11:29:14.600: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Sep  6 11:29:14.612: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Sep  6 11:29:14.616: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  6 11:29:16.704: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5519  52ca9868-7ddb-4a99-bfc8-d3bfdfc05655 37169 3 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b2c4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-06 11:29:14 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-06 11:29:14 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Sep  6 11:29:16.735: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5519  d008de23-79cd-4ffd-b9d8-40d29e32e5a9 37168 3 2023-09-06 11:29:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 52ca9868-7ddb-4a99-bfc8-d3bfdfc05655 0xc0037db517 0xc0037db518}] [] [{kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52ca9868-7ddb-4a99-bfc8-d3bfdfc05655\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037db5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 11:29:16.735: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Sep  6 11:29:16.735: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5519  169c3a4a-8f0f-4687-813c-734f50dd6c76 37160 3 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 52ca9868-7ddb-4a99-bfc8-d3bfdfc05655 0xc0037db427 0xc0037db428}] [] [{kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52ca9868-7ddb-4a99-bfc8-d3bfdfc05655\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037db4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Sep  6 11:29:16.763: INFO: Pod "webserver-deployment-7f5969cbc7-2jh8k" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2jh8k webserver-deployment-7f5969cbc7- deployment-5519  61b56161-f021-44d3-8803-49f2bd12beba 36921 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ede1311768dc2a97b7cbb6699b1fea6e00d8d9253a3ea6a499a7e9232f65687c cni.projectcalico.org/podIP:10.233.120.199/32 cni.projectcalico.org/podIPs:10.233.120.199/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2c8f7 0xc003b2c8f8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvqdl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvqdl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.199,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://91abb988c6c3ed2c1ff5e4ea135fb79444c0f454887975145ecb209b41f1c186,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-5jfn9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5jfn9 webserver-deployment-7f5969cbc7- deployment-5519  44fca14c-bba3-4479-91ae-6ec95fd7d795 37245 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2cb27 0xc003b2cb28}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2h7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2h7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-67sv7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-67sv7 webserver-deployment-7f5969cbc7- deployment-5519  a8370165-d0c1-4269-818c-3c127ed07d18 37239 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2cd07 0xc003b2cd08}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnbbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnbbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-7fhhs" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7fhhs webserver-deployment-7f5969cbc7- deployment-5519  fe4dcfa9-fc79-4308-bda6-13956b815a3a 36931 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:277a026e932e40bb04487bb6d097a556200506eb4ae9b387ef30ffe57ee279ac cni.projectcalico.org/podIP:10.233.99.123/32 cni.projectcalico.org/podIPs:10.233.99.123/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2cf17 0xc003b2cf18}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5l64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5l64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.123,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://286fc88fb5c157c09a553fae5502980ffb723cf87ede55ed40212c59faca2ea9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-8mqdd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8mqdd webserver-deployment-7f5969cbc7- deployment-5519  dc34724e-aa53-497d-b04c-6d3897a71aa5 37216 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d127 0xc003b2d128}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq6xg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq6xg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.764: INFO: Pod "webserver-deployment-7f5969cbc7-dbrwf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dbrwf webserver-deployment-7f5969cbc7- deployment-5519  fc3ea3db-524b-47cb-a720-e48be65dc349 37197 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:76532e207fe5eec81e77acc9d47a4efb71bd8eebdc51d3f45ead4f83344e0e4d cni.projectcalico.org/podIP:10.233.99.89/32 cni.projectcalico.org/podIPs:10.233.99.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d317 0xc003b2d318}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcrm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcrm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.765: INFO: Pod "webserver-deployment-7f5969cbc7-dzh9b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dzh9b webserver-deployment-7f5969cbc7- deployment-5519  84e1e900-dde6-42c2-9093-cceb204ad017 37211 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a8c0b9ad8d0884819ed1451146ca653953e3c06e07a59ada983065935faf2b23 cni.projectcalico.org/podIP:10.233.120.226/32 cni.projectcalico.org/podIPs:10.233.120.226/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d507 0xc003b2d508}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8bvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8bvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.774: INFO: Pod "webserver-deployment-7f5969cbc7-fjdcq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fjdcq webserver-deployment-7f5969cbc7- deployment-5519  443f0584-ed7a-4a8c-9dac-f58ffc7d8e5e 37198 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fbd5f09c9c20828c1cb1d8ffbf9c93dbd3e698f1c2906e5aab662669253c6b65 cni.projectcalico.org/podIP:10.233.120.220/32 cni.projectcalico.org/podIPs:10.233.120.220/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d6f7 0xc003b2d6f8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4f6xk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4f6xk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.775: INFO: Pod "webserver-deployment-7f5969cbc7-fx9kl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fx9kl webserver-deployment-7f5969cbc7- deployment-5519  01b088d7-d9e9-431c-9acd-e739d333a3e6 37137 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2d8e7 0xc003b2d8e8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpmxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpmxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.777: INFO: Pod "webserver-deployment-7f5969cbc7-gh8t5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gh8t5 webserver-deployment-7f5969cbc7- deployment-5519  b6e508a5-8d3e-40b1-a6e4-868af0aed9c0 36940 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:268d236ed56cec3edcbfd7d65289d78238dcd3f8b8fbf23394842b516a0a7580 cni.projectcalico.org/podIP:10.233.120.127/32 cni.projectcalico.org/podIPs:10.233.120.127/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2da50 0xc003b2da51}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cthz5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cthz5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.127,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d8e3fef0af7ea13ca2eb18d988b1290705f9b0f12996fcda227dbfb8881908ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.778: INFO: Pod "webserver-deployment-7f5969cbc7-hn985" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hn985 webserver-deployment-7f5969cbc7- deployment-5519  40bdcfae-6db2-4989-91ec-6266848b2ca4 36936 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4847c5be703b62aca940e12e1eb4717cccd299496598f7951329ce57c7cc0e6c cni.projectcalico.org/podIP:10.233.120.78/32 cni.projectcalico.org/podIPs:10.233.120.78/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2dc77 0xc003b2dc78}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76hm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76hm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.78,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://85089ae81bfbcdfa5a657a7bf5a31a7730802aea7f673f7342295f8555e33746,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.778: INFO: Pod "webserver-deployment-7f5969cbc7-jwk4c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jwk4c webserver-deployment-7f5969cbc7- deployment-5519  4cbbf55b-01e8-474f-90c2-a0505c48e8a7 37251 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cd5bd412f49308dbc96e392f9f3401d2067844da47cc31f08724f804bae0df9f cni.projectcalico.org/podIP:10.233.120.73/32 cni.projectcalico.org/podIPs:10.233.120.73/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc003b2dea7 0xc003b2dea8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6wrf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6wrf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.778: INFO: Pod "webserver-deployment-7f5969cbc7-k44sj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k44sj webserver-deployment-7f5969cbc7- deployment-5519  a9ce183a-330c-4242-9639-0d8a225ee7bc 37218 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0fba77dfb476ef092ef7d3043eaf69efdb7b2a1f5577d23e8d0083325c776837 cni.projectcalico.org/podIP:10.233.120.117/32 cni.projectcalico.org/podIPs:10.233.120.117/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0097 0xc0040a0098}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6npp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6npp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.778: INFO: Pod "webserver-deployment-7f5969cbc7-mdct2" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mdct2 webserver-deployment-7f5969cbc7- deployment-5519  11a6d913-52a1-4655-806b-845a7fdb00bc 36915 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bd6b5b06e1d10d840d910c888e6fa85f0fbc3a1b41b270b5632833bb646af4ea cni.projectcalico.org/podIP:10.233.120.255/32 cni.projectcalico.org/podIPs:10.233.120.255/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0287 0xc0040a0288}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x9ctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x9ctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.255,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fd6695372f56336adc5bfcaad299eacee8726592500121364a2b015718854588,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.779: INFO: Pod "webserver-deployment-7f5969cbc7-n4h6c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n4h6c webserver-deployment-7f5969cbc7- deployment-5519  349aca08-1484-48ad-9d57-aeb05f3dcd28 37186 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2bba99a40ccca9fa75efed7e745b9f0df1d0df0af7b2a045bfe679f41f109902 cni.projectcalico.org/podIP:10.233.99.68/32 cni.projectcalico.org/podIPs:10.233.99.68/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a04c7 0xc0040a04c8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vpsx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vpsx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.779: INFO: Pod "webserver-deployment-7f5969cbc7-nvqtk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nvqtk webserver-deployment-7f5969cbc7- deployment-5519  5967c9f4-d10d-41d0-9f2f-3d2ef5b54c9e 36933 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f8e373c70cb9518f4cb9a186662aa1a9aec8da7e50090bb95dd40346d9e9df21 cni.projectcalico.org/podIP:10.233.120.71/32 cni.projectcalico.org/podIPs:10.233.120.71/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a06d7 0xc0040a06d8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxxrt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxxrt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.71,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f1ab19601b3c1fde0bc754cb9166e0d5d43a6a6c3e1e83d5785b0776b1f4fe1b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.779: INFO: Pod "webserver-deployment-7f5969cbc7-p2ncz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p2ncz webserver-deployment-7f5969cbc7- deployment-5519  63a3cf4f-7ef0-4e55-826c-488dffa00a29 37242 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:65cf69abcd3e4f2dfff98546640533a7eb418b9855b92ef1cf96ef104dacfe11 cni.projectcalico.org/podIP:10.233.99.95/32 cni.projectcalico.org/podIPs:10.233.99.95/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0907 0xc0040a0908}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqvlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqvlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.780: INFO: Pod "webserver-deployment-7f5969cbc7-rkmmv" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rkmmv webserver-deployment-7f5969cbc7- deployment-5519  c5c1f0b4-ba93-4d92-931a-f429f05a0cf0 36918 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:313d19484bea8279b45dce72b40a124b63afb51e257741b6e7944bf23313039c cni.projectcalico.org/podIP:10.233.120.200/32 cni.projectcalico.org/podIPs:10.233.120.200/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0a90 0xc0040a0a91}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nt6t5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nt6t5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.200,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f2f3bab1538bd0bc3cdea70e159ecceca2735ed4f2c02d2f147bccfd8fb7675f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.780: INFO: Pod "webserver-deployment-7f5969cbc7-v9qqt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v9qqt webserver-deployment-7f5969cbc7- deployment-5519  e6309fa6-634b-4408-a26f-19483aa84491 37167 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0ca7 0xc0040a0ca8}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gf5l7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gf5l7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.780: INFO: Pod "webserver-deployment-7f5969cbc7-wj4q6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wj4q6 webserver-deployment-7f5969cbc7- deployment-5519  f386a0ce-cdec-4787-a73f-6cd2968284bb 36871 0 2023-09-06 11:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4e8e5f92b394d7f87d1a385c1fe94565f80c394d3749c5c59d5da2cd211407e0 cni.projectcalico.org/podIP:10.233.99.126/32 cni.projectcalico.org/podIPs:10.233.99.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 169c3a4a-8f0f-4687-813c-734f50dd6c76 0xc0040a0f37 0xc0040a0f38}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"169c3a4a-8f0f-4687-813c-734f50dd6c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djsnl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djsnl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.126,StartTime:2023-09-06 11:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-06 11:29:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fb72f54883bfaac7cdcf53b8efe7bc7c8781d4876e19a36170c4f7da0a5b486b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.780: INFO: Pod "webserver-deployment-d9f79cb5-2ks2z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2ks2z webserver-deployment-d9f79cb5- deployment-5519  b9b59f37-160b-4a72-90a0-f925ee65a2f2 37237 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0040a1187 0xc0040a1188}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p79d4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p79d4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.781: INFO: Pod "webserver-deployment-d9f79cb5-6jlrt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6jlrt webserver-deployment-d9f79cb5- deployment-5519  31daabb1-0adc-4a68-bb45-dbffa61ddd59 37257 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:043020a389bcaca665643424adbd71e8b2b74c7608294dece39e04ef8363fad8 cni.projectcalico.org/podIP:10.233.99.94/32 cni.projectcalico.org/podIPs:10.233.99.94/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0040a19cf 0xc0040a1b70}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhlzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhlzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.781: INFO: Pod "webserver-deployment-d9f79cb5-78lfd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-78lfd webserver-deployment-d9f79cb5- deployment-5519  0e0abcdc-2947-45f1-ac94-b8a4e18df428 37171 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0040a1f3f 0xc0040a1f50}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmlg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmlg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.782: INFO: Pod "webserver-deployment-d9f79cb5-78z66" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-78z66 webserver-deployment-d9f79cb5- deployment-5519  41841883-e663-4ab6-90ef-f37039041508 37246 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:eb93892aa9ce79e2f506bfd34d6d95f83acbb11be173e4423128a1638ff61fd1 cni.projectcalico.org/podIP:10.233.120.227/32 cni.projectcalico.org/podIPs:10.233.120.227/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0032c8ccf 0xc0032c8d50}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fktgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fktgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.782: INFO: Pod "webserver-deployment-d9f79cb5-9rtm7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9rtm7 webserver-deployment-d9f79cb5- deployment-5519  830a636d-84a6-4d51-8f53-7452542a4817 37076 0 2023-09-06 11:29:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1226a563880031c5343306c40e3ad874918e84b201b500ca9ea4a23f4484df2f cni.projectcalico.org/podIP:10.233.120.225/32 cni.projectcalico.org/podIPs:10.233.120.225/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0032c978f 0xc0032c97c0}] [] [{calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8w4z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8w4z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-09-06 11:29:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.782: INFO: Pod "webserver-deployment-d9f79cb5-g7wxp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g7wxp webserver-deployment-d9f79cb5- deployment-5519  5f146eac-e1a8-49cf-aca1-94765e932ae8 37200 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:73227af129e9e25de928e97ee7265a6b6efd0a1c48d39f0d5a05e2de393b99ff cni.projectcalico.org/podIP:10.233.120.120/32 cni.projectcalico.org/podIPs:10.233.120.120/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0032c9bbf 0xc0032c9bd0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnjhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnjhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.783: INFO: Pod "webserver-deployment-d9f79cb5-jl6hd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jl6hd webserver-deployment-d9f79cb5- deployment-5519  241ad4fe-45b8-4b9e-bc4e-7eccba6f7b23 37256 0 2023-09-06 11:29:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2609a36cd44b008c4bb69f5162380d33115775c78a15b5de8e99bfbacb1b0c61 cni.projectcalico.org/podIP:10.233.120.79/32 cni.projectcalico.org/podIPs:10.233.120.79/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0032c9ecf 0xc0032c9f40}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztzd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztzd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.79,StartTime:2023-09-06 11:29:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.783: INFO: Pod "webserver-deployment-d9f79cb5-mdfjz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mdfjz webserver-deployment-d9f79cb5- deployment-5519  0918821a-1d64-4968-b69e-089ad2f53da2 37258 0 2023-09-06 11:29:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b9741f332c41f5057f8a7fccbb1144f0c3e365f6d36efb492a170602126fb063 cni.projectcalico.org/podIP:10.233.120.209/32 cni.projectcalico.org/podIPs:10.233.120.209/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d41cf 0xc0048d41e0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tp89l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tp89l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.209,StartTime:2023-09-06 11:29:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.783: INFO: Pod "webserver-deployment-d9f79cb5-nscrd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nscrd webserver-deployment-d9f79cb5- deployment-5519  cb472f1b-fb38-4ec1-a73f-b04b16fd6012 37151 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d43ff 0xc0048d4410}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6mmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6mmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.783: INFO: Pod "webserver-deployment-d9f79cb5-qth2k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qth2k webserver-deployment-d9f79cb5- deployment-5519  da459104-833a-40a3-8b7c-ea0cfe7b9905 37077 0 2023-09-06 11:29:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f1f6427795b7b64baa3940c94b58b8de5255f966d9a2731cc8f8310d798a857d cni.projectcalico.org/podIP:10.233.99.120/32 cni.projectcalico.org/podIPs:10.233.99.120/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d456f 0xc0048d45a0}] [] [{calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvcbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvcbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.784: INFO: Pod "webserver-deployment-d9f79cb5-sk9vw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sk9vw webserver-deployment-d9f79cb5- deployment-5519  f5b55986-f79f-4185-85a2-394f776b799c 37058 0 2023-09-06 11:29:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ae6078bca47ebfd812c9b8afc063c03e2ba8aecb5c008a1a4351b5c2f14d9a8d cni.projectcalico.org/podIP:10.233.99.117/32 cni.projectcalico.org/podIPs:10.233.99.117/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d478f 0xc0048d47c0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-06 11:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbxm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbxm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.785: INFO: Pod "webserver-deployment-d9f79cb5-w8xtd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w8xtd webserver-deployment-d9f79cb5- deployment-5519  9b6971af-bbc4-4bfa-ae6b-a56d262e58a1 37231 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a4fe4753e390996a1c433e15a391914c745b9d27f5629bfad8a78b90f643c466 cni.projectcalico.org/podIP:10.233.120.70/32 cni.projectcalico.org/podIPs:10.233.120.70/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d49af 0xc0048d49e0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-06 11:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58grx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58grx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  6 11:29:16.785: INFO: Pod "webserver-deployment-d9f79cb5-zkhvj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zkhvj webserver-deployment-d9f79cb5- deployment-5519  13a97a3d-8d98-4095-9222-464e0536f09a 37164 0 2023-09-06 11:29:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d008de23-79cd-4ffd-b9d8-40d29e32e5a9 0xc0048d4bcf 0xc0048d4be0}] [] [{kube-controller-manager Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d008de23-79cd-4ffd-b9d8-40d29e32e5a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-06 11:29:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85fns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85fns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-06 11:29:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-09-06 11:29:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:29:16.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5519" for this suite. 09/06/23 11:29:16.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:29:16.867
Sep  6 11:29:16.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 11:29:16.871
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:18.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:18.465
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 09/06/23 11:29:18.472
STEP: Getting a ResourceQuota 09/06/23 11:29:18.496
STEP: Updating a ResourceQuota 09/06/23 11:29:18.552
STEP: Verifying a ResourceQuota was modified 09/06/23 11:29:18.579
STEP: Deleting a ResourceQuota 09/06/23 11:29:18.603
STEP: Verifying the deleted ResourceQuota 09/06/23 11:29:18.632
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 11:29:18.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4362" for this suite. 09/06/23 11:29:18.671
------------------------------
• [1.852 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:29:16.867
    Sep  6 11:29:16.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 11:29:16.871
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:18.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:18.465
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 09/06/23 11:29:18.472
    STEP: Getting a ResourceQuota 09/06/23 11:29:18.496
    STEP: Updating a ResourceQuota 09/06/23 11:29:18.552
    STEP: Verifying a ResourceQuota was modified 09/06/23 11:29:18.579
    STEP: Deleting a ResourceQuota 09/06/23 11:29:18.603
    STEP: Verifying the deleted ResourceQuota 09/06/23 11:29:18.632
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:29:18.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4362" for this suite. 09/06/23 11:29:18.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:29:18.72
Sep  6 11:29:18.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:29:18.721
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:18.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:18.775
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Sep  6 11:29:18.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/06/23 11:29:26.012
Sep  6 11:29:26.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 --namespace=crd-publish-openapi-6470 create -f -'
Sep  6 11:29:26.847: INFO: stderr: ""
Sep  6 11:29:26.847: INFO: stdout: "e2e-test-crd-publish-openapi-1460-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  6 11:29:26.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 --namespace=crd-publish-openapi-6470 delete e2e-test-crd-publish-openapi-1460-crds test-cr'
Sep  6 11:29:26.946: INFO: stderr: ""
Sep  6 11:29:26.946: INFO: stdout: "e2e-test-crd-publish-openapi-1460-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  6 11:29:26.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 --namespace=crd-publish-openapi-6470 apply -f -'
Sep  6 11:29:27.289: INFO: stderr: ""
Sep  6 11:29:27.289: INFO: stdout: "e2e-test-crd-publish-openapi-1460-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  6 11:29:27.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 --namespace=crd-publish-openapi-6470 delete e2e-test-crd-publish-openapi-1460-crds test-cr'
Sep  6 11:29:27.462: INFO: stderr: ""
Sep  6 11:29:27.462: INFO: stdout: "e2e-test-crd-publish-openapi-1460-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/06/23 11:29:27.462
Sep  6 11:29:27.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 explain e2e-test-crd-publish-openapi-1460-crds'
Sep  6 11:29:28.158: INFO: stderr: ""
Sep  6 11:29:28.158: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1460-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:29:29.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6470" for this suite. 09/06/23 11:29:29.837
------------------------------
• [SLOW TEST] [11.124 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:29:18.72
    Sep  6 11:29:18.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-publish-openapi 09/06/23 11:29:18.721
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:18.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:18.775
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Sep  6 11:29:18.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/06/23 11:29:26.012
    Sep  6 11:29:26.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 --namespace=crd-publish-openapi-6470 create -f -'
    Sep  6 11:29:26.847: INFO: stderr: ""
    Sep  6 11:29:26.847: INFO: stdout: "e2e-test-crd-publish-openapi-1460-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  6 11:29:26.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 --namespace=crd-publish-openapi-6470 delete e2e-test-crd-publish-openapi-1460-crds test-cr'
    Sep  6 11:29:26.946: INFO: stderr: ""
    Sep  6 11:29:26.946: INFO: stdout: "e2e-test-crd-publish-openapi-1460-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Sep  6 11:29:26.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 --namespace=crd-publish-openapi-6470 apply -f -'
    Sep  6 11:29:27.289: INFO: stderr: ""
    Sep  6 11:29:27.289: INFO: stdout: "e2e-test-crd-publish-openapi-1460-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  6 11:29:27.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 --namespace=crd-publish-openapi-6470 delete e2e-test-crd-publish-openapi-1460-crds test-cr'
    Sep  6 11:29:27.462: INFO: stderr: ""
    Sep  6 11:29:27.462: INFO: stdout: "e2e-test-crd-publish-openapi-1460-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/06/23 11:29:27.462
    Sep  6 11:29:27.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=crd-publish-openapi-6470 explain e2e-test-crd-publish-openapi-1460-crds'
    Sep  6 11:29:28.158: INFO: stderr: ""
    Sep  6 11:29:28.158: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1460-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:29:29.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6470" for this suite. 09/06/23 11:29:29.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:29:29.845
Sep  6 11:29:29.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 11:29:29.846
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:29.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:29.866
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-89265c0e-0cbc-4854-bf58-77b35301d51a 09/06/23 11:29:29.868
STEP: Creating a pod to test consume configMaps 09/06/23 11:29:29.873
Sep  6 11:29:29.882: INFO: Waiting up to 5m0s for pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432" in namespace "configmap-8238" to be "Succeeded or Failed"
Sep  6 11:29:29.904: INFO: Pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432": Phase="Pending", Reason="", readiness=false. Elapsed: 21.535156ms
Sep  6 11:29:31.917: INFO: Pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034897608s
Sep  6 11:29:33.916: INFO: Pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033643896s
STEP: Saw pod success 09/06/23 11:29:33.916
Sep  6 11:29:33.916: INFO: Pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432" satisfied condition "Succeeded or Failed"
Sep  6 11:29:33.927: INFO: Trying to get logs from node kube-3 pod pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432 container agnhost-container: <nil>
STEP: delete the pod 09/06/23 11:29:33.946
Sep  6 11:29:33.967: INFO: Waiting for pod pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432 to disappear
Sep  6 11:29:33.970: INFO: Pod pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:29:33.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8238" for this suite. 09/06/23 11:29:33.974
------------------------------
• [4.137 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:29:29.845
    Sep  6 11:29:29.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 11:29:29.846
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:29.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:29.866
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-89265c0e-0cbc-4854-bf58-77b35301d51a 09/06/23 11:29:29.868
    STEP: Creating a pod to test consume configMaps 09/06/23 11:29:29.873
    Sep  6 11:29:29.882: INFO: Waiting up to 5m0s for pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432" in namespace "configmap-8238" to be "Succeeded or Failed"
    Sep  6 11:29:29.904: INFO: Pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432": Phase="Pending", Reason="", readiness=false. Elapsed: 21.535156ms
    Sep  6 11:29:31.917: INFO: Pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034897608s
    Sep  6 11:29:33.916: INFO: Pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033643896s
    STEP: Saw pod success 09/06/23 11:29:33.916
    Sep  6 11:29:33.916: INFO: Pod "pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432" satisfied condition "Succeeded or Failed"
    Sep  6 11:29:33.927: INFO: Trying to get logs from node kube-3 pod pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432 container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 11:29:33.946
    Sep  6 11:29:33.967: INFO: Waiting for pod pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432 to disappear
    Sep  6 11:29:33.970: INFO: Pod pod-configmaps-419a87eb-0cfd-499e-a2a4-d7d105417432 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:29:33.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8238" for this suite. 09/06/23 11:29:33.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:29:33.982
Sep  6 11:29:33.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-preemption 09/06/23 11:29:33.983
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:33.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:34.004
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  6 11:29:34.019: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 11:30:34.089: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 09/06/23 11:30:34.098
Sep  6 11:30:34.138: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  6 11:30:34.145: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  6 11:30:34.184: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  6 11:30:34.194: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Sep  6 11:30:34.246: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Sep  6 11:30:34.267: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/06/23 11:30:34.267
Sep  6 11:30:34.267: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-996" to be "running"
Sep  6 11:30:34.282: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.64855ms
Sep  6 11:30:37.531: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264305262s
Sep  6 11:30:38.463: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195951201s
Sep  6 11:30:40.288: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.020906596s
Sep  6 11:30:40.288: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  6 11:30:40.288: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
Sep  6 11:30:40.291: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.324749ms
Sep  6 11:30:40.291: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  6 11:30:40.291: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
Sep  6 11:30:40.294: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.825343ms
Sep  6 11:30:40.294: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  6 11:30:40.294: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
Sep  6 11:30:40.297: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.560912ms
Sep  6 11:30:40.297: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  6 11:30:40.297: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
Sep  6 11:30:40.299: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.481207ms
Sep  6 11:30:40.299: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  6 11:30:40.299: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
Sep  6 11:30:40.302: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.530971ms
Sep  6 11:30:40.302: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/06/23 11:30:40.302
Sep  6 11:30:40.306: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-996" to be "running"
Sep  6 11:30:40.310: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43451ms
Sep  6 11:30:42.317: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010735338s
Sep  6 11:30:44.317: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011069238s
Sep  6 11:30:44.318: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:30:44.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-996" for this suite. 09/06/23 11:30:44.403
------------------------------
• [SLOW TEST] [70.428 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:29:33.982
    Sep  6 11:29:33.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-preemption 09/06/23 11:29:33.983
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:29:33.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:29:34.004
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  6 11:29:34.019: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  6 11:30:34.089: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 09/06/23 11:30:34.098
    Sep  6 11:30:34.138: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  6 11:30:34.145: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  6 11:30:34.184: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  6 11:30:34.194: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Sep  6 11:30:34.246: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Sep  6 11:30:34.267: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/06/23 11:30:34.267
    Sep  6 11:30:34.267: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-996" to be "running"
    Sep  6 11:30:34.282: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.64855ms
    Sep  6 11:30:37.531: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264305262s
    Sep  6 11:30:38.463: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195951201s
    Sep  6 11:30:40.288: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.020906596s
    Sep  6 11:30:40.288: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  6 11:30:40.288: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
    Sep  6 11:30:40.291: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.324749ms
    Sep  6 11:30:40.291: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  6 11:30:40.291: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
    Sep  6 11:30:40.294: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.825343ms
    Sep  6 11:30:40.294: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  6 11:30:40.294: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
    Sep  6 11:30:40.297: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.560912ms
    Sep  6 11:30:40.297: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  6 11:30:40.297: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
    Sep  6 11:30:40.299: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.481207ms
    Sep  6 11:30:40.299: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  6 11:30:40.299: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-996" to be "running"
    Sep  6 11:30:40.302: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.530971ms
    Sep  6 11:30:40.302: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/06/23 11:30:40.302
    Sep  6 11:30:40.306: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-996" to be "running"
    Sep  6 11:30:40.310: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43451ms
    Sep  6 11:30:42.317: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010735338s
    Sep  6 11:30:44.317: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011069238s
    Sep  6 11:30:44.318: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:30:44.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-996" for this suite. 09/06/23 11:30:44.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:30:44.424
Sep  6 11:30:44.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 11:30:44.425
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:44.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:44.444
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Sep  6 11:30:44.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: creating the pod 09/06/23 11:30:44.447
STEP: submitting the pod to kubernetes 09/06/23 11:30:44.447
Sep  6 11:30:44.455: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a" in namespace "pods-3090" to be "running and ready"
Sep  6 11:30:44.460: INFO: Pod "pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917078ms
Sep  6 11:30:44.460: INFO: The phase of Pod pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:30:46.474: INFO: Pod "pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a": Phase="Running", Reason="", readiness=true. Elapsed: 2.018473773s
Sep  6 11:30:46.474: INFO: The phase of Pod pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a is Running (Ready = true)
Sep  6 11:30:46.474: INFO: Pod "pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 11:30:46.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3090" for this suite. 09/06/23 11:30:46.661
------------------------------
• [2.245 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:30:44.424
    Sep  6 11:30:44.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 11:30:44.425
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:44.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:44.444
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Sep  6 11:30:44.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: creating the pod 09/06/23 11:30:44.447
    STEP: submitting the pod to kubernetes 09/06/23 11:30:44.447
    Sep  6 11:30:44.455: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a" in namespace "pods-3090" to be "running and ready"
    Sep  6 11:30:44.460: INFO: Pod "pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917078ms
    Sep  6 11:30:44.460: INFO: The phase of Pod pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:30:46.474: INFO: Pod "pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a": Phase="Running", Reason="", readiness=true. Elapsed: 2.018473773s
    Sep  6 11:30:46.474: INFO: The phase of Pod pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a is Running (Ready = true)
    Sep  6 11:30:46.474: INFO: Pod "pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:30:46.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3090" for this suite. 09/06/23 11:30:46.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:30:46.669
Sep  6 11:30:46.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:30:46.67
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:46.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:46.689
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:30:46.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4681" for this suite. 09/06/23 11:30:46.699
------------------------------
• [0.037 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:30:46.669
    Sep  6 11:30:46.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:30:46.67
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:46.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:46.689
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:30:46.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4681" for this suite. 09/06/23 11:30:46.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:30:46.707
Sep  6 11:30:46.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename endpointslice 09/06/23 11:30:46.708
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:46.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:46.729
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Sep  6 11:30:46.740: INFO: Endpoints addresses: [10.2.20.101 10.2.20.102] , ports: [6443]
Sep  6 11:30:46.740: INFO: EndpointSlices addresses: [10.2.20.101 10.2.20.102] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  6 11:30:46.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9070" for this suite. 09/06/23 11:30:46.744
------------------------------
• [0.044 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:30:46.707
    Sep  6 11:30:46.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename endpointslice 09/06/23 11:30:46.708
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:46.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:46.729
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Sep  6 11:30:46.740: INFO: Endpoints addresses: [10.2.20.101 10.2.20.102] , ports: [6443]
    Sep  6 11:30:46.740: INFO: EndpointSlices addresses: [10.2.20.101 10.2.20.102] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:30:46.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9070" for this suite. 09/06/23 11:30:46.744
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:30:46.752
Sep  6 11:30:46.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename sched-pred 09/06/23 11:30:46.753
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:46.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:46.771
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  6 11:30:46.772: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 11:30:46.778: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 11:30:46.781: INFO: 
Logging pods the apiserver thinks is on node kube-1 before test
Sep  6 11:30:46.787: INFO: calico-kube-controllers-6dfcdfb99-6q4ng from kube-system started at 2023-09-06 09:55:41 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 11:30:46.787: INFO: calico-node-pkqgc from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container calico-node ready: true, restart count 2
Sep  6 11:30:46.787: INFO: coredns-645b46f4b6-hq55k from kube-system started at 2023-09-06 09:55:53 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container coredns ready: true, restart count 0
Sep  6 11:30:46.787: INFO: kube-apiserver-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container kube-apiserver ready: true, restart count 2
Sep  6 11:30:46.787: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container kube-controller-manager ready: true, restart count 5
Sep  6 11:30:46.787: INFO: kube-proxy-fjqk6 from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 11:30:46.787: INFO: kube-scheduler-kube-1 from kube-system started at 2023-09-06 09:52:15 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container kube-scheduler ready: true, restart count 4
Sep  6 11:30:46.787: INFO: nodelocaldns-74qn2 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 11:30:46.787: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
Sep  6 11:30:46.787: INFO: preemptor-pod from sched-preemption-996 started at 2023-09-06 11:30:42 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container preemptor-pod ready: true, restart count 0
Sep  6 11:30:46.787: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 11:30:46.787: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:30:46.787: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 11:30:46.787: INFO: 
Logging pods the apiserver thinks is on node kube-2 before test
Sep  6 11:30:46.794: INFO: calico-node-f57x2 from kube-system started at 2023-09-06 09:54:23 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container calico-node ready: true, restart count 2
Sep  6 11:30:46.794: INFO: coredns-645b46f4b6-9lpfv from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container coredns ready: true, restart count 0
Sep  6 11:30:46.794: INFO: dns-autoscaler-659b8c48cb-5h6w8 from kube-system started at 2023-09-06 09:55:57 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container autoscaler ready: true, restart count 0
Sep  6 11:30:46.794: INFO: kube-apiserver-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container kube-apiserver ready: true, restart count 1
Sep  6 11:30:46.794: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-09-06 09:53:08 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container kube-controller-manager ready: true, restart count 4
Sep  6 11:30:46.794: INFO: kube-proxy-7fxzk from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 11:30:46.794: INFO: kube-scheduler-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container kube-scheduler ready: true, restart count 4
Sep  6 11:30:46.794: INFO: nodelocaldns-jpj4c from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 11:30:46.794: INFO: pod1-0-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container pod1-0-sched-preemption-medium-priority ready: true, restart count 0
Sep  6 11:30:46.794: INFO: pod1-1-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container pod1-1-sched-preemption-medium-priority ready: true, restart count 0
Sep  6 11:30:46.794: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 11:30:46.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:30:46.794: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 11:30:46.794: INFO: 
Logging pods the apiserver thinks is on node kube-3 before test
Sep  6 11:30:46.800: INFO: calico-node-6w7db from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 11:30:46.800: INFO: kube-proxy-sfndk from kube-system started at 2023-09-06 09:54:02 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 11:30:46.800: INFO: nginx-proxy-kube-3 from kube-system started at 2023-09-06 09:53:42 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  6 11:30:46.800: INFO: nodelocaldns-c9bb4 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container node-cache ready: true, restart count 0
Sep  6 11:30:46.800: INFO: pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a from pods-3090 started at 2023-09-06 11:30:44 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container main ready: true, restart count 0
Sep  6 11:30:46.800: INFO: pod2-0-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container pod2-0-sched-preemption-medium-priority ready: true, restart count 0
Sep  6 11:30:46.800: INFO: pod2-1-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container pod2-1-sched-preemption-medium-priority ready: true, restart count 0
Sep  6 11:30:46.800: INFO: sonobuoy from sonobuoy started at 2023-09-06 09:59:53 +0000 UTC (1 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 11:30:46.800: INFO: sonobuoy-e2e-job-c7c8c161973b4a54 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container e2e ready: true, restart count 0
Sep  6 11:30:46.800: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:30:46.800: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
Sep  6 11:30:46.800: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:30:46.800: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node kube-1 09/06/23 11:30:46.818
STEP: verifying the node has the label node kube-2 09/06/23 11:30:46.837
STEP: verifying the node has the label node kube-3 09/06/23 11:30:46.853
Sep  6 11:30:46.879: INFO: Pod calico-kube-controllers-6dfcdfb99-6q4ng requesting resource cpu=30m on Node kube-1
Sep  6 11:30:46.879: INFO: Pod calico-node-6w7db requesting resource cpu=150m on Node kube-3
Sep  6 11:30:46.879: INFO: Pod calico-node-f57x2 requesting resource cpu=150m on Node kube-2
Sep  6 11:30:46.879: INFO: Pod calico-node-pkqgc requesting resource cpu=150m on Node kube-1
Sep  6 11:30:46.879: INFO: Pod coredns-645b46f4b6-9lpfv requesting resource cpu=100m on Node kube-2
Sep  6 11:30:46.879: INFO: Pod coredns-645b46f4b6-hq55k requesting resource cpu=100m on Node kube-1
Sep  6 11:30:46.879: INFO: Pod dns-autoscaler-659b8c48cb-5h6w8 requesting resource cpu=20m on Node kube-2
Sep  6 11:30:46.879: INFO: Pod kube-apiserver-kube-1 requesting resource cpu=250m on Node kube-1
Sep  6 11:30:46.879: INFO: Pod kube-apiserver-kube-2 requesting resource cpu=250m on Node kube-2
Sep  6 11:30:46.879: INFO: Pod kube-controller-manager-kube-1 requesting resource cpu=200m on Node kube-1
Sep  6 11:30:46.880: INFO: Pod kube-controller-manager-kube-2 requesting resource cpu=200m on Node kube-2
Sep  6 11:30:46.880: INFO: Pod kube-proxy-7fxzk requesting resource cpu=0m on Node kube-2
Sep  6 11:30:46.880: INFO: Pod kube-proxy-fjqk6 requesting resource cpu=0m on Node kube-1
Sep  6 11:30:46.880: INFO: Pod kube-proxy-sfndk requesting resource cpu=0m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod kube-scheduler-kube-1 requesting resource cpu=100m on Node kube-1
Sep  6 11:30:46.880: INFO: Pod kube-scheduler-kube-2 requesting resource cpu=100m on Node kube-2
Sep  6 11:30:46.880: INFO: Pod nginx-proxy-kube-3 requesting resource cpu=25m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod nodelocaldns-74qn2 requesting resource cpu=100m on Node kube-1
Sep  6 11:30:46.880: INFO: Pod nodelocaldns-c9bb4 requesting resource cpu=100m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod nodelocaldns-jpj4c requesting resource cpu=100m on Node kube-2
Sep  6 11:30:46.880: INFO: Pod pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a requesting resource cpu=0m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod pod0-1-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-1
Sep  6 11:30:46.880: INFO: Pod pod1-0-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-2
Sep  6 11:30:46.880: INFO: Pod pod1-1-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-2
Sep  6 11:30:46.880: INFO: Pod pod2-0-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod pod2-1-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod preemptor-pod requesting resource cpu=0m on Node kube-1
Sep  6 11:30:46.880: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod sonobuoy-e2e-job-c7c8c161973b4a54 requesting resource cpu=0m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h requesting resource cpu=0m on Node kube-3
Sep  6 11:30:46.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg requesting resource cpu=0m on Node kube-1
Sep  6 11:30:46.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 requesting resource cpu=0m on Node kube-2
STEP: Starting Pods to consume most of the cluster CPU. 09/06/23 11:30:46.88
Sep  6 11:30:46.880: INFO: Creating a pod which consumes cpu=2149m on Node kube-1
Sep  6 11:30:46.896: INFO: Creating a pod which consumes cpu=2156m on Node kube-2
Sep  6 11:30:46.907: INFO: Creating a pod which consumes cpu=2607m on Node kube-3
Sep  6 11:30:46.923: INFO: Waiting up to 5m0s for pod "filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c" in namespace "sched-pred-7253" to be "running"
Sep  6 11:30:46.954: INFO: Pod "filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.695945ms
Sep  6 11:30:48.967: INFO: Pod "filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c": Phase="Running", Reason="", readiness=true. Elapsed: 2.044431438s
Sep  6 11:30:48.967: INFO: Pod "filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c" satisfied condition "running"
Sep  6 11:30:48.967: INFO: Waiting up to 5m0s for pod "filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa" in namespace "sched-pred-7253" to be "running"
Sep  6 11:30:48.981: INFO: Pod "filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa": Phase="Running", Reason="", readiness=true. Elapsed: 13.795862ms
Sep  6 11:30:48.981: INFO: Pod "filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa" satisfied condition "running"
Sep  6 11:30:48.981: INFO: Waiting up to 5m0s for pod "filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984" in namespace "sched-pred-7253" to be "running"
Sep  6 11:30:48.993: INFO: Pod "filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984": Phase="Running", Reason="", readiness=true. Elapsed: 11.555251ms
Sep  6 11:30:48.993: INFO: Pod "filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 09/06/23 11:30:48.993
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa.17824be2b55d34f5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7253/filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa to kube-2] 09/06/23 11:30:48.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa.17824be2e2825cfa], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/06/23 11:30:48.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa.17824be2e4a683db], Reason = [Created], Message = [Created container filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa] 09/06/23 11:30:48.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa.17824be2ebf78cf8], Reason = [Started], Message = [Started container filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa] 09/06/23 11:30:48.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984.17824be2b696f24d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7253/filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984 to kube-3] 09/06/23 11:30:48.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984.17824be2e1d8d547], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/06/23 11:30:48.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984.17824be2e37f39e1], Reason = [Created], Message = [Created container filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984] 09/06/23 11:30:48.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984.17824be2eb02bcd8], Reason = [Started], Message = [Started container filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984] 09/06/23 11:30:49
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c.17824be2b3f65aef], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7253/filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c to kube-1] 09/06/23 11:30:49
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c.17824be2e326f743], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/06/23 11:30:49
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c.17824be2e49d36a1], Reason = [Created], Message = [Created container filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c] 09/06/23 11:30:49
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c.17824be2eccb019c], Reason = [Started], Message = [Started container filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c] 09/06/23 11:30:49
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17824be33199668a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 09/06/23 11:30:49.012
STEP: removing the label node off the node kube-1 09/06/23 11:30:50.018
STEP: verifying the node doesn't have the label node 09/06/23 11:30:50.064
STEP: removing the label node off the node kube-2 09/06/23 11:30:50.08
STEP: verifying the node doesn't have the label node 09/06/23 11:30:50.129
STEP: removing the label node off the node kube-3 09/06/23 11:30:50.165
STEP: verifying the node doesn't have the label node 09/06/23 11:30:50.398
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:30:50.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7253" for this suite. 09/06/23 11:30:50.453
------------------------------
• [3.740 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:30:46.752
    Sep  6 11:30:46.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename sched-pred 09/06/23 11:30:46.753
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:46.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:46.771
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  6 11:30:46.772: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  6 11:30:46.778: INFO: Waiting for terminating namespaces to be deleted...
    Sep  6 11:30:46.781: INFO: 
    Logging pods the apiserver thinks is on node kube-1 before test
    Sep  6 11:30:46.787: INFO: calico-kube-controllers-6dfcdfb99-6q4ng from kube-system started at 2023-09-06 09:55:41 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  6 11:30:46.787: INFO: calico-node-pkqgc from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container calico-node ready: true, restart count 2
    Sep  6 11:30:46.787: INFO: coredns-645b46f4b6-hq55k from kube-system started at 2023-09-06 09:55:53 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container coredns ready: true, restart count 0
    Sep  6 11:30:46.787: INFO: kube-apiserver-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container kube-apiserver ready: true, restart count 2
    Sep  6 11:30:46.787: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-09-06 09:52:16 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Sep  6 11:30:46.787: INFO: kube-proxy-fjqk6 from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 11:30:46.787: INFO: kube-scheduler-kube-1 from kube-system started at 2023-09-06 09:52:15 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container kube-scheduler ready: true, restart count 4
    Sep  6 11:30:46.787: INFO: nodelocaldns-74qn2 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 11:30:46.787: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
    Sep  6 11:30:46.787: INFO: preemptor-pod from sched-preemption-996 started at 2023-09-06 11:30:42 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container preemptor-pod ready: true, restart count 0
    Sep  6 11:30:46.787: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 11:30:46.787: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 11:30:46.787: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  6 11:30:46.787: INFO: 
    Logging pods the apiserver thinks is on node kube-2 before test
    Sep  6 11:30:46.794: INFO: calico-node-f57x2 from kube-system started at 2023-09-06 09:54:23 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container calico-node ready: true, restart count 2
    Sep  6 11:30:46.794: INFO: coredns-645b46f4b6-9lpfv from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container coredns ready: true, restart count 0
    Sep  6 11:30:46.794: INFO: dns-autoscaler-659b8c48cb-5h6w8 from kube-system started at 2023-09-06 09:55:57 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  6 11:30:46.794: INFO: kube-apiserver-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container kube-apiserver ready: true, restart count 1
    Sep  6 11:30:46.794: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-09-06 09:53:08 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Sep  6 11:30:46.794: INFO: kube-proxy-7fxzk from kube-system started at 2023-09-06 09:54:00 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 11:30:46.794: INFO: kube-scheduler-kube-2 from kube-system started at 2023-09-06 09:52:42 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container kube-scheduler ready: true, restart count 4
    Sep  6 11:30:46.794: INFO: nodelocaldns-jpj4c from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 11:30:46.794: INFO: pod1-0-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container pod1-0-sched-preemption-medium-priority ready: true, restart count 0
    Sep  6 11:30:46.794: INFO: pod1-1-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container pod1-1-sched-preemption-medium-priority ready: true, restart count 0
    Sep  6 11:30:46.794: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 11:30:46.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 11:30:46.794: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  6 11:30:46.794: INFO: 
    Logging pods the apiserver thinks is on node kube-3 before test
    Sep  6 11:30:46.800: INFO: calico-node-6w7db from kube-system started at 2023-09-06 09:54:24 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container calico-node ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: kube-proxy-sfndk from kube-system started at 2023-09-06 09:54:02 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: nginx-proxy-kube-3 from kube-system started at 2023-09-06 09:53:42 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: nodelocaldns-c9bb4 from kube-system started at 2023-09-06 09:55:59 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container node-cache ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a from pods-3090 started at 2023-09-06 11:30:44 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container main ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: pod2-0-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container pod2-0-sched-preemption-medium-priority ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: pod2-1-sched-preemption-medium-priority from sched-preemption-996 started at 2023-09-06 11:30:34 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container pod2-1-sched-preemption-medium-priority ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: sonobuoy from sonobuoy started at 2023-09-06 09:59:53 +0000 UTC (1 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: sonobuoy-e2e-job-c7c8c161973b4a54 from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container e2e ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h from sonobuoy started at 2023-09-06 10:00:02 +0000 UTC (2 container statuses recorded)
    Sep  6 11:30:46.800: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  6 11:30:46.800: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node kube-1 09/06/23 11:30:46.818
    STEP: verifying the node has the label node kube-2 09/06/23 11:30:46.837
    STEP: verifying the node has the label node kube-3 09/06/23 11:30:46.853
    Sep  6 11:30:46.879: INFO: Pod calico-kube-controllers-6dfcdfb99-6q4ng requesting resource cpu=30m on Node kube-1
    Sep  6 11:30:46.879: INFO: Pod calico-node-6w7db requesting resource cpu=150m on Node kube-3
    Sep  6 11:30:46.879: INFO: Pod calico-node-f57x2 requesting resource cpu=150m on Node kube-2
    Sep  6 11:30:46.879: INFO: Pod calico-node-pkqgc requesting resource cpu=150m on Node kube-1
    Sep  6 11:30:46.879: INFO: Pod coredns-645b46f4b6-9lpfv requesting resource cpu=100m on Node kube-2
    Sep  6 11:30:46.879: INFO: Pod coredns-645b46f4b6-hq55k requesting resource cpu=100m on Node kube-1
    Sep  6 11:30:46.879: INFO: Pod dns-autoscaler-659b8c48cb-5h6w8 requesting resource cpu=20m on Node kube-2
    Sep  6 11:30:46.879: INFO: Pod kube-apiserver-kube-1 requesting resource cpu=250m on Node kube-1
    Sep  6 11:30:46.879: INFO: Pod kube-apiserver-kube-2 requesting resource cpu=250m on Node kube-2
    Sep  6 11:30:46.879: INFO: Pod kube-controller-manager-kube-1 requesting resource cpu=200m on Node kube-1
    Sep  6 11:30:46.880: INFO: Pod kube-controller-manager-kube-2 requesting resource cpu=200m on Node kube-2
    Sep  6 11:30:46.880: INFO: Pod kube-proxy-7fxzk requesting resource cpu=0m on Node kube-2
    Sep  6 11:30:46.880: INFO: Pod kube-proxy-fjqk6 requesting resource cpu=0m on Node kube-1
    Sep  6 11:30:46.880: INFO: Pod kube-proxy-sfndk requesting resource cpu=0m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod kube-scheduler-kube-1 requesting resource cpu=100m on Node kube-1
    Sep  6 11:30:46.880: INFO: Pod kube-scheduler-kube-2 requesting resource cpu=100m on Node kube-2
    Sep  6 11:30:46.880: INFO: Pod nginx-proxy-kube-3 requesting resource cpu=25m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod nodelocaldns-74qn2 requesting resource cpu=100m on Node kube-1
    Sep  6 11:30:46.880: INFO: Pod nodelocaldns-c9bb4 requesting resource cpu=100m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod nodelocaldns-jpj4c requesting resource cpu=100m on Node kube-2
    Sep  6 11:30:46.880: INFO: Pod pod-exec-websocket-1dc42924-c2d1-46f0-8f00-e39eb1d5420a requesting resource cpu=0m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod pod0-1-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-1
    Sep  6 11:30:46.880: INFO: Pod pod1-0-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-2
    Sep  6 11:30:46.880: INFO: Pod pod1-1-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-2
    Sep  6 11:30:46.880: INFO: Pod pod2-0-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod pod2-1-sched-preemption-medium-priority requesting resource cpu=0m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod preemptor-pod requesting resource cpu=0m on Node kube-1
    Sep  6 11:30:46.880: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod sonobuoy-e2e-job-c7c8c161973b4a54 requesting resource cpu=0m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-4jk4h requesting resource cpu=0m on Node kube-3
    Sep  6 11:30:46.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-kxqtg requesting resource cpu=0m on Node kube-1
    Sep  6 11:30:46.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-c2db00c8440c40bf-sfg64 requesting resource cpu=0m on Node kube-2
    STEP: Starting Pods to consume most of the cluster CPU. 09/06/23 11:30:46.88
    Sep  6 11:30:46.880: INFO: Creating a pod which consumes cpu=2149m on Node kube-1
    Sep  6 11:30:46.896: INFO: Creating a pod which consumes cpu=2156m on Node kube-2
    Sep  6 11:30:46.907: INFO: Creating a pod which consumes cpu=2607m on Node kube-3
    Sep  6 11:30:46.923: INFO: Waiting up to 5m0s for pod "filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c" in namespace "sched-pred-7253" to be "running"
    Sep  6 11:30:46.954: INFO: Pod "filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.695945ms
    Sep  6 11:30:48.967: INFO: Pod "filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c": Phase="Running", Reason="", readiness=true. Elapsed: 2.044431438s
    Sep  6 11:30:48.967: INFO: Pod "filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c" satisfied condition "running"
    Sep  6 11:30:48.967: INFO: Waiting up to 5m0s for pod "filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa" in namespace "sched-pred-7253" to be "running"
    Sep  6 11:30:48.981: INFO: Pod "filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa": Phase="Running", Reason="", readiness=true. Elapsed: 13.795862ms
    Sep  6 11:30:48.981: INFO: Pod "filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa" satisfied condition "running"
    Sep  6 11:30:48.981: INFO: Waiting up to 5m0s for pod "filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984" in namespace "sched-pred-7253" to be "running"
    Sep  6 11:30:48.993: INFO: Pod "filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984": Phase="Running", Reason="", readiness=true. Elapsed: 11.555251ms
    Sep  6 11:30:48.993: INFO: Pod "filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 09/06/23 11:30:48.993
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa.17824be2b55d34f5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7253/filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa to kube-2] 09/06/23 11:30:48.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa.17824be2e2825cfa], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/06/23 11:30:48.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa.17824be2e4a683db], Reason = [Created], Message = [Created container filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa] 09/06/23 11:30:48.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa.17824be2ebf78cf8], Reason = [Started], Message = [Started container filler-pod-050527fc-ea28-44c9-93b6-8f79184d9caa] 09/06/23 11:30:48.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984.17824be2b696f24d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7253/filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984 to kube-3] 09/06/23 11:30:48.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984.17824be2e1d8d547], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/06/23 11:30:48.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984.17824be2e37f39e1], Reason = [Created], Message = [Created container filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984] 09/06/23 11:30:48.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984.17824be2eb02bcd8], Reason = [Started], Message = [Started container filler-pod-10f02573-0199-4f4b-9539-48f9e0c3a984] 09/06/23 11:30:49
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c.17824be2b3f65aef], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7253/filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c to kube-1] 09/06/23 11:30:49
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c.17824be2e326f743], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/06/23 11:30:49
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c.17824be2e49d36a1], Reason = [Created], Message = [Created container filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c] 09/06/23 11:30:49
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c.17824be2eccb019c], Reason = [Started], Message = [Started container filler-pod-d80e6452-a4d7-4421-81b3-6c84046e9e0c] 09/06/23 11:30:49
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17824be33199668a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 09/06/23 11:30:49.012
    STEP: removing the label node off the node kube-1 09/06/23 11:30:50.018
    STEP: verifying the node doesn't have the label node 09/06/23 11:30:50.064
    STEP: removing the label node off the node kube-2 09/06/23 11:30:50.08
    STEP: verifying the node doesn't have the label node 09/06/23 11:30:50.129
    STEP: removing the label node off the node kube-3 09/06/23 11:30:50.165
    STEP: verifying the node doesn't have the label node 09/06/23 11:30:50.398
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:30:50.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7253" for this suite. 09/06/23 11:30:50.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:30:50.493
Sep  6 11:30:50.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 11:30:50.494
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:50.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:50.541
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 09/06/23 11:30:50.571
STEP: watching for Pod to be ready 09/06/23 11:30:50.592
Sep  6 11:30:50.597: INFO: observed Pod pod-test in namespace pods-845 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep  6 11:30:50.603: INFO: observed Pod pod-test in namespace pods-845 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  }]
Sep  6 11:30:50.638: INFO: observed Pod pod-test in namespace pods-845 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  }]
Sep  6 11:30:51.216: INFO: observed Pod pod-test in namespace pods-845 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  }]
Sep  6 11:30:51.714: INFO: Found Pod pod-test in namespace pods-845 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 09/06/23 11:30:51.735
STEP: getting the Pod and ensuring that it's patched 09/06/23 11:30:51.828
STEP: replacing the Pod's status Ready condition to False 09/06/23 11:30:51.841
STEP: check the Pod again to ensure its Ready conditions are False 09/06/23 11:30:52.047
STEP: deleting the Pod via a Collection with a LabelSelector 09/06/23 11:30:52.048
STEP: watching for the Pod to be deleted 09/06/23 11:30:52.222
Sep  6 11:30:52.276: INFO: observed event type MODIFIED
Sep  6 11:30:53.696: INFO: observed event type MODIFIED
Sep  6 11:30:54.059: INFO: observed event type MODIFIED
Sep  6 11:30:54.694: INFO: observed event type MODIFIED
Sep  6 11:30:54.703: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 11:30:54.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-845" for this suite. 09/06/23 11:30:54.721
------------------------------
• [4.235 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:30:50.493
    Sep  6 11:30:50.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 11:30:50.494
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:50.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:50.541
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 09/06/23 11:30:50.571
    STEP: watching for Pod to be ready 09/06/23 11:30:50.592
    Sep  6 11:30:50.597: INFO: observed Pod pod-test in namespace pods-845 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Sep  6 11:30:50.603: INFO: observed Pod pod-test in namespace pods-845 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  }]
    Sep  6 11:30:50.638: INFO: observed Pod pod-test in namespace pods-845 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  }]
    Sep  6 11:30:51.216: INFO: observed Pod pod-test in namespace pods-845 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  }]
    Sep  6 11:30:51.714: INFO: Found Pod pod-test in namespace pods-845 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-06 11:30:50 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 09/06/23 11:30:51.735
    STEP: getting the Pod and ensuring that it's patched 09/06/23 11:30:51.828
    STEP: replacing the Pod's status Ready condition to False 09/06/23 11:30:51.841
    STEP: check the Pod again to ensure its Ready conditions are False 09/06/23 11:30:52.047
    STEP: deleting the Pod via a Collection with a LabelSelector 09/06/23 11:30:52.048
    STEP: watching for the Pod to be deleted 09/06/23 11:30:52.222
    Sep  6 11:30:52.276: INFO: observed event type MODIFIED
    Sep  6 11:30:53.696: INFO: observed event type MODIFIED
    Sep  6 11:30:54.059: INFO: observed event type MODIFIED
    Sep  6 11:30:54.694: INFO: observed event type MODIFIED
    Sep  6 11:30:54.703: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:30:54.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-845" for this suite. 09/06/23 11:30:54.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:30:54.73
Sep  6 11:30:54.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename crd-webhook 09/06/23 11:30:54.731
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:54.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:54.754
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/06/23 11:30:54.756
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/06/23 11:30:55.063
STEP: Deploying the custom resource conversion webhook pod 09/06/23 11:30:55.071
STEP: Wait for the deployment to be ready 09/06/23 11:30:55.085
Sep  6 11:30:55.094: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/06/23 11:30:57.104
STEP: Verifying the service has paired with the endpoint 09/06/23 11:30:57.115
Sep  6 11:30:58.116: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Sep  6 11:30:58.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Creating a v1 custom resource 09/06/23 11:31:05.735
STEP: Create a v2 custom resource 09/06/23 11:31:05.752
STEP: List CRs in v1 09/06/23 11:31:05.804
STEP: List CRs in v2 09/06/23 11:31:05.809
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:31:06.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4736" for this suite. 09/06/23 11:31:06.406
------------------------------
• [SLOW TEST] [11.686 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:30:54.73
    Sep  6 11:30:54.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename crd-webhook 09/06/23 11:30:54.731
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:30:54.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:30:54.754
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/06/23 11:30:54.756
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/06/23 11:30:55.063
    STEP: Deploying the custom resource conversion webhook pod 09/06/23 11:30:55.071
    STEP: Wait for the deployment to be ready 09/06/23 11:30:55.085
    Sep  6 11:30:55.094: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/06/23 11:30:57.104
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:30:57.115
    Sep  6 11:30:58.116: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Sep  6 11:30:58.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Creating a v1 custom resource 09/06/23 11:31:05.735
    STEP: Create a v2 custom resource 09/06/23 11:31:05.752
    STEP: List CRs in v1 09/06/23 11:31:05.804
    STEP: List CRs in v2 09/06/23 11:31:05.809
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:31:06.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4736" for this suite. 09/06/23 11:31:06.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:31:06.416
Sep  6 11:31:06.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 11:31:06.417
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:31:06.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:31:06.465
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 09/06/23 11:31:06.47
STEP: Creating a ResourceQuota 09/06/23 11:31:11.474
STEP: Ensuring resource quota status is calculated 09/06/23 11:31:11.481
STEP: Creating a Pod that fits quota 09/06/23 11:31:13.499
STEP: Ensuring ResourceQuota status captures the pod usage 09/06/23 11:31:13.533
STEP: Not allowing a pod to be created that exceeds remaining quota 09/06/23 11:31:15.54
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/06/23 11:31:15.544
STEP: Ensuring a pod cannot update its resource requirements 09/06/23 11:31:15.546
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/06/23 11:31:15.553
STEP: Deleting the pod 09/06/23 11:31:17.568
STEP: Ensuring resource quota status released the pod usage 09/06/23 11:31:17.804
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 11:31:19.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3192" for this suite. 09/06/23 11:31:19.834
------------------------------
• [SLOW TEST] [13.449 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:31:06.416
    Sep  6 11:31:06.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 11:31:06.417
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:31:06.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:31:06.465
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 09/06/23 11:31:06.47
    STEP: Creating a ResourceQuota 09/06/23 11:31:11.474
    STEP: Ensuring resource quota status is calculated 09/06/23 11:31:11.481
    STEP: Creating a Pod that fits quota 09/06/23 11:31:13.499
    STEP: Ensuring ResourceQuota status captures the pod usage 09/06/23 11:31:13.533
    STEP: Not allowing a pod to be created that exceeds remaining quota 09/06/23 11:31:15.54
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/06/23 11:31:15.544
    STEP: Ensuring a pod cannot update its resource requirements 09/06/23 11:31:15.546
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/06/23 11:31:15.553
    STEP: Deleting the pod 09/06/23 11:31:17.568
    STEP: Ensuring resource quota status released the pod usage 09/06/23 11:31:17.804
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:31:19.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3192" for this suite. 09/06/23 11:31:19.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:31:19.873
Sep  6 11:31:19.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:31:19.876
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:31:19.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:31:19.941
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1460 09/06/23 11:31:19.943
STEP: creating replication controller nodeport-test in namespace services-1460 09/06/23 11:31:20.027
I0906 11:31:20.043348      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1460, replica count: 2
I0906 11:31:23.094696      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:31:23.094: INFO: Creating new exec pod
Sep  6 11:31:23.101: INFO: Waiting up to 5m0s for pod "execpod7ktnd" in namespace "services-1460" to be "running"
Sep  6 11:31:23.111: INFO: Pod "execpod7ktnd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.397599ms
Sep  6 11:31:25.114: INFO: Pod "execpod7ktnd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012427091s
Sep  6 11:31:25.114: INFO: Pod "execpod7ktnd" satisfied condition "running"
Sep  6 11:31:26.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1460 exec execpod7ktnd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Sep  6 11:31:26.301: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  6 11:31:26.301: INFO: stdout: ""
Sep  6 11:31:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1460 exec execpod7ktnd -- /bin/sh -x -c nc -v -z -w 2 10.233.6.127 80'
Sep  6 11:31:26.424: INFO: stderr: "+ nc -v -z -w 2 10.233.6.127 80\nConnection to 10.233.6.127 80 port [tcp/http] succeeded!\n"
Sep  6 11:31:26.424: INFO: stdout: ""
Sep  6 11:31:26.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1460 exec execpod7ktnd -- /bin/sh -x -c nc -v -z -w 2 10.2.20.103 31576'
Sep  6 11:31:26.557: INFO: stderr: "+ nc -v -z -w 2 10.2.20.103 31576\nConnection to 10.2.20.103 31576 port [tcp/*] succeeded!\n"
Sep  6 11:31:26.557: INFO: stdout: ""
Sep  6 11:31:26.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1460 exec execpod7ktnd -- /bin/sh -x -c nc -v -z -w 2 10.2.20.101 31576'
Sep  6 11:31:26.691: INFO: stderr: "+ nc -v -z -w 2 10.2.20.101 31576\nConnection to 10.2.20.101 31576 port [tcp/*] succeeded!\n"
Sep  6 11:31:26.691: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:31:26.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1460" for this suite. 09/06/23 11:31:26.695
------------------------------
• [SLOW TEST] [6.830 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:31:19.873
    Sep  6 11:31:19.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:31:19.876
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:31:19.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:31:19.941
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1460 09/06/23 11:31:19.943
    STEP: creating replication controller nodeport-test in namespace services-1460 09/06/23 11:31:20.027
    I0906 11:31:20.043348      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1460, replica count: 2
    I0906 11:31:23.094696      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 11:31:23.094: INFO: Creating new exec pod
    Sep  6 11:31:23.101: INFO: Waiting up to 5m0s for pod "execpod7ktnd" in namespace "services-1460" to be "running"
    Sep  6 11:31:23.111: INFO: Pod "execpod7ktnd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.397599ms
    Sep  6 11:31:25.114: INFO: Pod "execpod7ktnd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012427091s
    Sep  6 11:31:25.114: INFO: Pod "execpod7ktnd" satisfied condition "running"
    Sep  6 11:31:26.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1460 exec execpod7ktnd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Sep  6 11:31:26.301: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  6 11:31:26.301: INFO: stdout: ""
    Sep  6 11:31:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1460 exec execpod7ktnd -- /bin/sh -x -c nc -v -z -w 2 10.233.6.127 80'
    Sep  6 11:31:26.424: INFO: stderr: "+ nc -v -z -w 2 10.233.6.127 80\nConnection to 10.233.6.127 80 port [tcp/http] succeeded!\n"
    Sep  6 11:31:26.424: INFO: stdout: ""
    Sep  6 11:31:26.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1460 exec execpod7ktnd -- /bin/sh -x -c nc -v -z -w 2 10.2.20.103 31576'
    Sep  6 11:31:26.557: INFO: stderr: "+ nc -v -z -w 2 10.2.20.103 31576\nConnection to 10.2.20.103 31576 port [tcp/*] succeeded!\n"
    Sep  6 11:31:26.557: INFO: stdout: ""
    Sep  6 11:31:26.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1460 exec execpod7ktnd -- /bin/sh -x -c nc -v -z -w 2 10.2.20.101 31576'
    Sep  6 11:31:26.691: INFO: stderr: "+ nc -v -z -w 2 10.2.20.101 31576\nConnection to 10.2.20.101 31576 port [tcp/*] succeeded!\n"
    Sep  6 11:31:26.691: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:31:26.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1460" for this suite. 09/06/23 11:31:26.695
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:31:26.703
Sep  6 11:31:26.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename var-expansion 09/06/23 11:31:26.704
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:31:26.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:31:26.726
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 09/06/23 11:31:26.728
Sep  6 11:31:26.736: INFO: Waiting up to 2m0s for pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" in namespace "var-expansion-4539" to be "running"
Sep  6 11:31:26.740: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000135ms
Sep  6 11:31:28.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019512571s
Sep  6 11:31:30.743: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00782675s
Sep  6 11:31:32.744: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008429798s
Sep  6 11:31:34.743: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007261s
Sep  6 11:31:36.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015938986s
Sep  6 11:31:38.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018437078s
Sep  6 11:31:40.744: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008664276s
Sep  6 11:31:42.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019121016s
Sep  6 11:31:44.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017108422s
Sep  6 11:31:46.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017521843s
Sep  6 11:31:48.746: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010293715s
Sep  6 11:31:50.747: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01175123s
Sep  6 11:31:52.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020311422s
Sep  6 11:31:54.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 28.020820752s
Sep  6 11:31:56.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 30.020560537s
Sep  6 11:31:58.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019289551s
Sep  6 11:32:00.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 34.017756714s
Sep  6 11:32:02.751: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015898744s
Sep  6 11:32:04.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 38.020124489s
Sep  6 11:32:06.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01804058s
Sep  6 11:32:08.745: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009307656s
Sep  6 11:32:10.750: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 44.014368132s
Sep  6 11:32:12.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016456767s
Sep  6 11:32:15.173: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 48.437652953s
Sep  6 11:32:16.782: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 50.045953875s
Sep  6 11:32:18.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 52.016161171s
Sep  6 11:32:20.745: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009583237s
Sep  6 11:32:22.749: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01323679s
Sep  6 11:32:24.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 58.019107475s
Sep  6 11:32:26.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.016002197s
Sep  6 11:32:28.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.018425567s
Sep  6 11:32:30.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.016780903s
Sep  6 11:32:32.744: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008045692s
Sep  6 11:32:34.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016469284s
Sep  6 11:32:36.745: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009461378s
Sep  6 11:32:38.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01968208s
Sep  6 11:32:40.750: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014517678s
Sep  6 11:32:42.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.016993363s
Sep  6 11:32:44.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016818493s
Sep  6 11:32:46.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.020263167s
Sep  6 11:32:48.757: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.021182361s
Sep  6 11:32:50.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.019882142s
Sep  6 11:32:52.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01665117s
Sep  6 11:32:54.757: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.021342037s
Sep  6 11:32:56.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.016743111s
Sep  6 11:32:58.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01593927s
Sep  6 11:33:00.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.017860881s
Sep  6 11:33:02.747: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011864662s
Sep  6 11:33:04.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.016365699s
Sep  6 11:33:06.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016839432s
Sep  6 11:33:08.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.018841926s
Sep  6 11:33:10.744: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008789245s
Sep  6 11:33:12.749: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01337269s
Sep  6 11:33:14.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018246542s
Sep  6 11:33:16.750: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014313631s
Sep  6 11:33:18.751: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015684619s
Sep  6 11:33:20.745: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009260165s
Sep  6 11:33:22.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.017094915s
Sep  6 11:33:24.757: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.02110541s
Sep  6 11:33:26.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017207033s
Sep  6 11:33:26.765: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.029635981s
STEP: updating the pod 09/06/23 11:33:26.765
Sep  6 11:33:27.300: INFO: Successfully updated pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036"
STEP: waiting for pod running 09/06/23 11:33:27.3
Sep  6 11:33:27.300: INFO: Waiting up to 2m0s for pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" in namespace "var-expansion-4539" to be "running"
Sep  6 11:33:27.311: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 10.975482ms
Sep  6 11:33:29.324: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Running", Reason="", readiness=true. Elapsed: 2.023733642s
Sep  6 11:33:29.324: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" satisfied condition "running"
STEP: deleting the pod gracefully 09/06/23 11:33:29.324
Sep  6 11:33:29.325: INFO: Deleting pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" in namespace "var-expansion-4539"
Sep  6 11:33:29.342: INFO: Wait up to 5m0s for pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  6 11:34:01.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4539" for this suite. 09/06/23 11:34:01.378
------------------------------
• [SLOW TEST] [154.692 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:31:26.703
    Sep  6 11:31:26.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename var-expansion 09/06/23 11:31:26.704
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:31:26.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:31:26.726
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 09/06/23 11:31:26.728
    Sep  6 11:31:26.736: INFO: Waiting up to 2m0s for pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" in namespace "var-expansion-4539" to be "running"
    Sep  6 11:31:26.740: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000135ms
    Sep  6 11:31:28.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019512571s
    Sep  6 11:31:30.743: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00782675s
    Sep  6 11:31:32.744: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008429798s
    Sep  6 11:31:34.743: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007261s
    Sep  6 11:31:36.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015938986s
    Sep  6 11:31:38.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018437078s
    Sep  6 11:31:40.744: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008664276s
    Sep  6 11:31:42.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019121016s
    Sep  6 11:31:44.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017108422s
    Sep  6 11:31:46.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017521843s
    Sep  6 11:31:48.746: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010293715s
    Sep  6 11:31:50.747: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01175123s
    Sep  6 11:31:52.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020311422s
    Sep  6 11:31:54.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 28.020820752s
    Sep  6 11:31:56.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 30.020560537s
    Sep  6 11:31:58.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019289551s
    Sep  6 11:32:00.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 34.017756714s
    Sep  6 11:32:02.751: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015898744s
    Sep  6 11:32:04.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 38.020124489s
    Sep  6 11:32:06.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01804058s
    Sep  6 11:32:08.745: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009307656s
    Sep  6 11:32:10.750: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 44.014368132s
    Sep  6 11:32:12.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016456767s
    Sep  6 11:32:15.173: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 48.437652953s
    Sep  6 11:32:16.782: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 50.045953875s
    Sep  6 11:32:18.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 52.016161171s
    Sep  6 11:32:20.745: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009583237s
    Sep  6 11:32:22.749: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01323679s
    Sep  6 11:32:24.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 58.019107475s
    Sep  6 11:32:26.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.016002197s
    Sep  6 11:32:28.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.018425567s
    Sep  6 11:32:30.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.016780903s
    Sep  6 11:32:32.744: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008045692s
    Sep  6 11:32:34.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016469284s
    Sep  6 11:32:36.745: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009461378s
    Sep  6 11:32:38.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01968208s
    Sep  6 11:32:40.750: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014517678s
    Sep  6 11:32:42.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.016993363s
    Sep  6 11:32:44.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016818493s
    Sep  6 11:32:46.756: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.020263167s
    Sep  6 11:32:48.757: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.021182361s
    Sep  6 11:32:50.755: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.019882142s
    Sep  6 11:32:52.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01665117s
    Sep  6 11:32:54.757: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.021342037s
    Sep  6 11:32:56.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.016743111s
    Sep  6 11:32:58.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01593927s
    Sep  6 11:33:00.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.017860881s
    Sep  6 11:33:02.747: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011864662s
    Sep  6 11:33:04.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.016365699s
    Sep  6 11:33:06.752: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016839432s
    Sep  6 11:33:08.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.018841926s
    Sep  6 11:33:10.744: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008789245s
    Sep  6 11:33:12.749: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01337269s
    Sep  6 11:33:14.754: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018246542s
    Sep  6 11:33:16.750: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014313631s
    Sep  6 11:33:18.751: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015684619s
    Sep  6 11:33:20.745: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009260165s
    Sep  6 11:33:22.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.017094915s
    Sep  6 11:33:24.757: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.02110541s
    Sep  6 11:33:26.753: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017207033s
    Sep  6 11:33:26.765: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.029635981s
    STEP: updating the pod 09/06/23 11:33:26.765
    Sep  6 11:33:27.300: INFO: Successfully updated pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036"
    STEP: waiting for pod running 09/06/23 11:33:27.3
    Sep  6 11:33:27.300: INFO: Waiting up to 2m0s for pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" in namespace "var-expansion-4539" to be "running"
    Sep  6 11:33:27.311: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Pending", Reason="", readiness=false. Elapsed: 10.975482ms
    Sep  6 11:33:29.324: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036": Phase="Running", Reason="", readiness=true. Elapsed: 2.023733642s
    Sep  6 11:33:29.324: INFO: Pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" satisfied condition "running"
    STEP: deleting the pod gracefully 09/06/23 11:33:29.324
    Sep  6 11:33:29.325: INFO: Deleting pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" in namespace "var-expansion-4539"
    Sep  6 11:33:29.342: INFO: Wait up to 5m0s for pod "var-expansion-b3180c28-29fe-4d9f-b7b7-29bd544c6036" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:34:01.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4539" for this suite. 09/06/23 11:34:01.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:34:01.395
Sep  6 11:34:01.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename daemonsets 09/06/23 11:34:01.396
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:01.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:01.422
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 09/06/23 11:34:01.458
STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 11:34:01.473
Sep  6 11:34:01.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:34:01.480: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:34:02.488: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:34:02.488: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:34:03.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 11:34:03.520: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/06/23 11:34:03.528
Sep  6 11:34:03.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 11:34:03.564: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 09/06/23 11:34:03.564
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/06/23 11:34:04.581
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7830, will wait for the garbage collector to delete the pods 09/06/23 11:34:04.581
Sep  6 11:34:04.644: INFO: Deleting DaemonSet.extensions daemon-set took: 8.051917ms
Sep  6 11:34:04.744: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.524713ms
Sep  6 11:34:07.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:34:07.648: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  6 11:34:07.650: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39039"},"items":null}

Sep  6 11:34:07.653: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39039"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:34:07.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7830" for this suite. 09/06/23 11:34:07.669
------------------------------
• [SLOW TEST] [6.279 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:34:01.395
    Sep  6 11:34:01.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename daemonsets 09/06/23 11:34:01.396
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:01.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:01.422
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 09/06/23 11:34:01.458
    STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 11:34:01.473
    Sep  6 11:34:01.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:34:01.480: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:34:02.488: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:34:02.488: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:34:03.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 11:34:03.520: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/06/23 11:34:03.528
    Sep  6 11:34:03.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 11:34:03.564: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 09/06/23 11:34:03.564
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/06/23 11:34:04.581
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7830, will wait for the garbage collector to delete the pods 09/06/23 11:34:04.581
    Sep  6 11:34:04.644: INFO: Deleting DaemonSet.extensions daemon-set took: 8.051917ms
    Sep  6 11:34:04.744: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.524713ms
    Sep  6 11:34:07.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:34:07.648: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  6 11:34:07.650: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39039"},"items":null}

    Sep  6 11:34:07.653: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39039"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:34:07.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7830" for this suite. 09/06/23 11:34:07.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:34:07.675
Sep  6 11:34:07.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 11:34:07.677
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:07.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:07.696
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Sep  6 11:34:07.705: INFO: Waiting up to 5m0s for pod "server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6" in namespace "pods-9446" to be "running and ready"
Sep  6 11:34:07.709: INFO: Pod "server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099224ms
Sep  6 11:34:07.709: INFO: The phase of Pod server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:34:09.723: INFO: Pod "server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017537893s
Sep  6 11:34:09.723: INFO: The phase of Pod server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6 is Running (Ready = true)
Sep  6 11:34:09.723: INFO: Pod "server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6" satisfied condition "running and ready"
Sep  6 11:34:09.788: INFO: Waiting up to 5m0s for pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403" in namespace "pods-9446" to be "Succeeded or Failed"
Sep  6 11:34:09.801: INFO: Pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403": Phase="Pending", Reason="", readiness=false. Elapsed: 12.900018ms
Sep  6 11:34:11.812: INFO: Pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024506483s
Sep  6 11:34:13.813: INFO: Pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025200843s
STEP: Saw pod success 09/06/23 11:34:13.813
Sep  6 11:34:13.814: INFO: Pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403" satisfied condition "Succeeded or Failed"
Sep  6 11:34:13.826: INFO: Trying to get logs from node kube-3 pod client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403 container env3cont: <nil>
STEP: delete the pod 09/06/23 11:34:13.87
Sep  6 11:34:13.887: INFO: Waiting for pod client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403 to disappear
Sep  6 11:34:13.891: INFO: Pod client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  6 11:34:13.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9446" for this suite. 09/06/23 11:34:13.895
------------------------------
• [SLOW TEST] [6.227 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:34:07.675
    Sep  6 11:34:07.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 11:34:07.677
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:07.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:07.696
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Sep  6 11:34:07.705: INFO: Waiting up to 5m0s for pod "server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6" in namespace "pods-9446" to be "running and ready"
    Sep  6 11:34:07.709: INFO: Pod "server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099224ms
    Sep  6 11:34:07.709: INFO: The phase of Pod server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:34:09.723: INFO: Pod "server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017537893s
    Sep  6 11:34:09.723: INFO: The phase of Pod server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6 is Running (Ready = true)
    Sep  6 11:34:09.723: INFO: Pod "server-envvars-dd4c34b1-b369-4547-8a1a-08e2a168d8e6" satisfied condition "running and ready"
    Sep  6 11:34:09.788: INFO: Waiting up to 5m0s for pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403" in namespace "pods-9446" to be "Succeeded or Failed"
    Sep  6 11:34:09.801: INFO: Pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403": Phase="Pending", Reason="", readiness=false. Elapsed: 12.900018ms
    Sep  6 11:34:11.812: INFO: Pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024506483s
    Sep  6 11:34:13.813: INFO: Pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025200843s
    STEP: Saw pod success 09/06/23 11:34:13.813
    Sep  6 11:34:13.814: INFO: Pod "client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403" satisfied condition "Succeeded or Failed"
    Sep  6 11:34:13.826: INFO: Trying to get logs from node kube-3 pod client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403 container env3cont: <nil>
    STEP: delete the pod 09/06/23 11:34:13.87
    Sep  6 11:34:13.887: INFO: Waiting for pod client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403 to disappear
    Sep  6 11:34:13.891: INFO: Pod client-envvars-8329485f-5d4b-4684-a1ea-a107d36f8403 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:34:13.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9446" for this suite. 09/06/23 11:34:13.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:34:13.903
Sep  6 11:34:13.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename svcaccounts 09/06/23 11:34:13.904
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:13.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:13.92
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Sep  6 11:34:13.936: INFO: created pod
Sep  6 11:34:13.936: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2608" to be "Succeeded or Failed"
Sep  6 11:34:13.941: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104973ms
Sep  6 11:34:15.950: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014021695s
Sep  6 11:34:17.955: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01911606s
STEP: Saw pod success 09/06/23 11:34:17.955
Sep  6 11:34:17.958: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep  6 11:34:47.959: INFO: polling logs
Sep  6 11:34:47.993: INFO: Pod logs: 
I0906 11:34:14.651679       1 log.go:198] OK: Got token
I0906 11:34:14.651742       1 log.go:198] validating with in-cluster discovery
I0906 11:34:14.652012       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0906 11:34:14.652057       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694000654, NotBefore:1694000054, IssuedAt:1694000054, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"6c761496-dc60-4145-815e-48c59d193c05"}}}
I0906 11:34:14.661397       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0906 11:34:14.665934       1 log.go:198] OK: Validated signature on JWT
I0906 11:34:14.666049       1 log.go:198] OK: Got valid claims from token!
I0906 11:34:14.666101       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694000654, NotBefore:1694000054, IssuedAt:1694000054, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"6c761496-dc60-4145-815e-48c59d193c05"}}}

Sep  6 11:34:47.993: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  6 11:34:48.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2608" for this suite. 09/06/23 11:34:48.017
------------------------------
• [SLOW TEST] [34.125 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:34:13.903
    Sep  6 11:34:13.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename svcaccounts 09/06/23 11:34:13.904
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:13.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:13.92
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Sep  6 11:34:13.936: INFO: created pod
    Sep  6 11:34:13.936: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2608" to be "Succeeded or Failed"
    Sep  6 11:34:13.941: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104973ms
    Sep  6 11:34:15.950: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014021695s
    Sep  6 11:34:17.955: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01911606s
    STEP: Saw pod success 09/06/23 11:34:17.955
    Sep  6 11:34:17.958: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Sep  6 11:34:47.959: INFO: polling logs
    Sep  6 11:34:47.993: INFO: Pod logs: 
    I0906 11:34:14.651679       1 log.go:198] OK: Got token
    I0906 11:34:14.651742       1 log.go:198] validating with in-cluster discovery
    I0906 11:34:14.652012       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0906 11:34:14.652057       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694000654, NotBefore:1694000054, IssuedAt:1694000054, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"6c761496-dc60-4145-815e-48c59d193c05"}}}
    I0906 11:34:14.661397       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0906 11:34:14.665934       1 log.go:198] OK: Validated signature on JWT
    I0906 11:34:14.666049       1 log.go:198] OK: Got valid claims from token!
    I0906 11:34:14.666101       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694000654, NotBefore:1694000054, IssuedAt:1694000054, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"6c761496-dc60-4145-815e-48c59d193c05"}}}

    Sep  6 11:34:47.993: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:34:48.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2608" for this suite. 09/06/23 11:34:48.017
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:34:48.029
Sep  6 11:34:48.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename ingress 09/06/23 11:34:48.031
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:48.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:48.056
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 09/06/23 11:34:48.058
STEP: getting /apis/networking.k8s.io 09/06/23 11:34:48.06
STEP: getting /apis/networking.k8s.iov1 09/06/23 11:34:48.06
STEP: creating 09/06/23 11:34:48.062
STEP: getting 09/06/23 11:34:48.077
STEP: listing 09/06/23 11:34:48.08
STEP: watching 09/06/23 11:34:48.082
Sep  6 11:34:48.082: INFO: starting watch
STEP: cluster-wide listing 09/06/23 11:34:48.083
STEP: cluster-wide watching 09/06/23 11:34:48.086
Sep  6 11:34:48.086: INFO: starting watch
STEP: patching 09/06/23 11:34:48.086
STEP: updating 09/06/23 11:34:48.093
Sep  6 11:34:48.102: INFO: waiting for watch events with expected annotations
Sep  6 11:34:48.102: INFO: saw patched and updated annotations
STEP: patching /status 09/06/23 11:34:48.102
STEP: updating /status 09/06/23 11:34:48.115
STEP: get /status 09/06/23 11:34:48.123
STEP: deleting 09/06/23 11:34:48.126
STEP: deleting a collection 09/06/23 11:34:48.139
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Sep  6 11:34:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5901" for this suite. 09/06/23 11:34:48.158
------------------------------
• [0.143 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:34:48.029
    Sep  6 11:34:48.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename ingress 09/06/23 11:34:48.031
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:48.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:48.056
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 09/06/23 11:34:48.058
    STEP: getting /apis/networking.k8s.io 09/06/23 11:34:48.06
    STEP: getting /apis/networking.k8s.iov1 09/06/23 11:34:48.06
    STEP: creating 09/06/23 11:34:48.062
    STEP: getting 09/06/23 11:34:48.077
    STEP: listing 09/06/23 11:34:48.08
    STEP: watching 09/06/23 11:34:48.082
    Sep  6 11:34:48.082: INFO: starting watch
    STEP: cluster-wide listing 09/06/23 11:34:48.083
    STEP: cluster-wide watching 09/06/23 11:34:48.086
    Sep  6 11:34:48.086: INFO: starting watch
    STEP: patching 09/06/23 11:34:48.086
    STEP: updating 09/06/23 11:34:48.093
    Sep  6 11:34:48.102: INFO: waiting for watch events with expected annotations
    Sep  6 11:34:48.102: INFO: saw patched and updated annotations
    STEP: patching /status 09/06/23 11:34:48.102
    STEP: updating /status 09/06/23 11:34:48.115
    STEP: get /status 09/06/23 11:34:48.123
    STEP: deleting 09/06/23 11:34:48.126
    STEP: deleting a collection 09/06/23 11:34:48.139
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:34:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5901" for this suite. 09/06/23 11:34:48.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:34:48.173
Sep  6 11:34:48.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename cronjob 09/06/23 11:34:48.173
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:48.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:48.192
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 09/06/23 11:34:48.194
STEP: Ensuring more than one job is running at a time 09/06/23 11:34:48.199
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/06/23 11:36:00.203
STEP: Removing cronjob 09/06/23 11:36:00.208
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  6 11:36:00.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2352" for this suite. 09/06/23 11:36:00.229
------------------------------
• [SLOW TEST] [72.068 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:34:48.173
    Sep  6 11:34:48.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename cronjob 09/06/23 11:34:48.173
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:34:48.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:34:48.192
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 09/06/23 11:34:48.194
    STEP: Ensuring more than one job is running at a time 09/06/23 11:34:48.199
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/06/23 11:36:00.203
    STEP: Removing cronjob 09/06/23 11:36:00.208
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:36:00.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2352" for this suite. 09/06/23 11:36:00.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:36:00.25
Sep  6 11:36:00.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-probe 09/06/23 11:36:00.25
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:36:00.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:36:00.285
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-5949dfd9-1196-499f-a570-02b23a25a280 in namespace container-probe-931 09/06/23 11:36:00.289
Sep  6 11:36:00.307: INFO: Waiting up to 5m0s for pod "busybox-5949dfd9-1196-499f-a570-02b23a25a280" in namespace "container-probe-931" to be "not pending"
Sep  6 11:36:00.310: INFO: Pod "busybox-5949dfd9-1196-499f-a570-02b23a25a280": Phase="Pending", Reason="", readiness=false. Elapsed: 3.266557ms
Sep  6 11:36:02.325: INFO: Pod "busybox-5949dfd9-1196-499f-a570-02b23a25a280": Phase="Running", Reason="", readiness=true. Elapsed: 2.017516601s
Sep  6 11:36:02.325: INFO: Pod "busybox-5949dfd9-1196-499f-a570-02b23a25a280" satisfied condition "not pending"
Sep  6 11:36:02.325: INFO: Started pod busybox-5949dfd9-1196-499f-a570-02b23a25a280 in namespace container-probe-931
STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 11:36:02.325
Sep  6 11:36:02.334: INFO: Initial restart count of pod busybox-5949dfd9-1196-499f-a570-02b23a25a280 is 0
Sep  6 11:36:52.607: INFO: Restart count of pod container-probe-931/busybox-5949dfd9-1196-499f-a570-02b23a25a280 is now 1 (50.272817964s elapsed)
STEP: deleting the pod 09/06/23 11:36:52.607
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  6 11:36:52.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-931" for this suite. 09/06/23 11:36:52.641
------------------------------
• [SLOW TEST] [52.402 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:36:00.25
    Sep  6 11:36:00.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-probe 09/06/23 11:36:00.25
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:36:00.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:36:00.285
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-5949dfd9-1196-499f-a570-02b23a25a280 in namespace container-probe-931 09/06/23 11:36:00.289
    Sep  6 11:36:00.307: INFO: Waiting up to 5m0s for pod "busybox-5949dfd9-1196-499f-a570-02b23a25a280" in namespace "container-probe-931" to be "not pending"
    Sep  6 11:36:00.310: INFO: Pod "busybox-5949dfd9-1196-499f-a570-02b23a25a280": Phase="Pending", Reason="", readiness=false. Elapsed: 3.266557ms
    Sep  6 11:36:02.325: INFO: Pod "busybox-5949dfd9-1196-499f-a570-02b23a25a280": Phase="Running", Reason="", readiness=true. Elapsed: 2.017516601s
    Sep  6 11:36:02.325: INFO: Pod "busybox-5949dfd9-1196-499f-a570-02b23a25a280" satisfied condition "not pending"
    Sep  6 11:36:02.325: INFO: Started pod busybox-5949dfd9-1196-499f-a570-02b23a25a280 in namespace container-probe-931
    STEP: checking the pod's current state and verifying that restartCount is present 09/06/23 11:36:02.325
    Sep  6 11:36:02.334: INFO: Initial restart count of pod busybox-5949dfd9-1196-499f-a570-02b23a25a280 is 0
    Sep  6 11:36:52.607: INFO: Restart count of pod container-probe-931/busybox-5949dfd9-1196-499f-a570-02b23a25a280 is now 1 (50.272817964s elapsed)
    STEP: deleting the pod 09/06/23 11:36:52.607
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:36:52.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-931" for this suite. 09/06/23 11:36:52.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:36:52.653
Sep  6 11:36:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 11:36:52.654
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:36:52.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:36:52.674
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-6feddc44-649b-4287-aa53-eafbda3bc54d 09/06/23 11:36:52.676
STEP: Creating a pod to test consume secrets 09/06/23 11:36:52.683
Sep  6 11:36:52.691: INFO: Waiting up to 5m0s for pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663" in namespace "secrets-5712" to be "Succeeded or Failed"
Sep  6 11:36:52.694: INFO: Pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663": Phase="Pending", Reason="", readiness=false. Elapsed: 3.425661ms
Sep  6 11:36:54.702: INFO: Pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011442966s
Sep  6 11:36:56.709: INFO: Pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018272748s
STEP: Saw pod success 09/06/23 11:36:56.709
Sep  6 11:36:56.709: INFO: Pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663" satisfied condition "Succeeded or Failed"
Sep  6 11:36:56.718: INFO: Trying to get logs from node kube-3 pod pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663 container secret-volume-test: <nil>
STEP: delete the pod 09/06/23 11:36:56.762
Sep  6 11:36:56.784: INFO: Waiting for pod pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663 to disappear
Sep  6 11:36:56.787: INFO: Pod pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 11:36:56.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5712" for this suite. 09/06/23 11:36:56.792
------------------------------
• [4.146 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:36:52.653
    Sep  6 11:36:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 11:36:52.654
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:36:52.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:36:52.674
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-6feddc44-649b-4287-aa53-eafbda3bc54d 09/06/23 11:36:52.676
    STEP: Creating a pod to test consume secrets 09/06/23 11:36:52.683
    Sep  6 11:36:52.691: INFO: Waiting up to 5m0s for pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663" in namespace "secrets-5712" to be "Succeeded or Failed"
    Sep  6 11:36:52.694: INFO: Pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663": Phase="Pending", Reason="", readiness=false. Elapsed: 3.425661ms
    Sep  6 11:36:54.702: INFO: Pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011442966s
    Sep  6 11:36:56.709: INFO: Pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018272748s
    STEP: Saw pod success 09/06/23 11:36:56.709
    Sep  6 11:36:56.709: INFO: Pod "pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663" satisfied condition "Succeeded or Failed"
    Sep  6 11:36:56.718: INFO: Trying to get logs from node kube-3 pod pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663 container secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 11:36:56.762
    Sep  6 11:36:56.784: INFO: Waiting for pod pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663 to disappear
    Sep  6 11:36:56.787: INFO: Pod pod-secrets-c07265f6-fbc8-4ddb-87c8-441adb678663 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:36:56.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5712" for this suite. 09/06/23 11:36:56.792
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:36:56.799
Sep  6 11:36:56.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 11:36:56.8
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:36:56.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:36:56.821
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-e88ef4b0-6b57-4665-aaea-f9f9a2daaa16 09/06/23 11:36:56.826
STEP: Creating secret with name s-test-opt-upd-e54b66ef-42da-46ce-abad-41ffc1ad87a9 09/06/23 11:36:56.831
STEP: Creating the pod 09/06/23 11:36:56.837
Sep  6 11:36:56.863: INFO: Waiting up to 5m0s for pod "pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212" in namespace "secrets-6450" to be "running and ready"
Sep  6 11:36:56.867: INFO: Pod "pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213696ms
Sep  6 11:36:56.867: INFO: The phase of Pod pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:36:58.870: INFO: Pod "pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212": Phase="Running", Reason="", readiness=true. Elapsed: 2.006706641s
Sep  6 11:36:58.870: INFO: The phase of Pod pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212 is Running (Ready = true)
Sep  6 11:36:58.870: INFO: Pod "pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-e88ef4b0-6b57-4665-aaea-f9f9a2daaa16 09/06/23 11:36:58.918
STEP: Updating secret s-test-opt-upd-e54b66ef-42da-46ce-abad-41ffc1ad87a9 09/06/23 11:36:58.927
STEP: Creating secret with name s-test-opt-create-a2dcf0ad-3df4-4614-99b6-6ef3322e4b8f 09/06/23 11:36:58.933
STEP: waiting to observe update in volume 09/06/23 11:36:58.94
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 11:37:03.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6450" for this suite. 09/06/23 11:37:03.046
------------------------------
• [SLOW TEST] [6.258 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:36:56.799
    Sep  6 11:36:56.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 11:36:56.8
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:36:56.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:36:56.821
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-e88ef4b0-6b57-4665-aaea-f9f9a2daaa16 09/06/23 11:36:56.826
    STEP: Creating secret with name s-test-opt-upd-e54b66ef-42da-46ce-abad-41ffc1ad87a9 09/06/23 11:36:56.831
    STEP: Creating the pod 09/06/23 11:36:56.837
    Sep  6 11:36:56.863: INFO: Waiting up to 5m0s for pod "pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212" in namespace "secrets-6450" to be "running and ready"
    Sep  6 11:36:56.867: INFO: Pod "pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213696ms
    Sep  6 11:36:56.867: INFO: The phase of Pod pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:36:58.870: INFO: Pod "pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212": Phase="Running", Reason="", readiness=true. Elapsed: 2.006706641s
    Sep  6 11:36:58.870: INFO: The phase of Pod pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212 is Running (Ready = true)
    Sep  6 11:36:58.870: INFO: Pod "pod-secrets-67c835c2-e4d9-4e50-a992-5049da66d212" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-e88ef4b0-6b57-4665-aaea-f9f9a2daaa16 09/06/23 11:36:58.918
    STEP: Updating secret s-test-opt-upd-e54b66ef-42da-46ce-abad-41ffc1ad87a9 09/06/23 11:36:58.927
    STEP: Creating secret with name s-test-opt-create-a2dcf0ad-3df4-4614-99b6-6ef3322e4b8f 09/06/23 11:36:58.933
    STEP: waiting to observe update in volume 09/06/23 11:36:58.94
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:37:03.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6450" for this suite. 09/06/23 11:37:03.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:37:03.06
Sep  6 11:37:03.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename configmap 09/06/23 11:37:03.062
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:03.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:03.093
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:37:03.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2152" for this suite. 09/06/23 11:37:03.135
------------------------------
• [0.081 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:37:03.06
    Sep  6 11:37:03.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename configmap 09/06/23 11:37:03.062
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:03.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:03.093
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:37:03.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2152" for this suite. 09/06/23 11:37:03.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:37:03.142
Sep  6 11:37:03.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubelet-test 09/06/23 11:37:03.143
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:03.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:03.162
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 09/06/23 11:37:03.174
Sep  6 11:37:03.174: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974" in namespace "kubelet-test-6857" to be "completed"
Sep  6 11:37:03.179: INFO: Pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974": Phase="Pending", Reason="", readiness=false. Elapsed: 4.643828ms
Sep  6 11:37:05.183: INFO: Pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009200598s
Sep  6 11:37:07.195: INFO: Pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020658371s
Sep  6 11:37:07.195: INFO: Pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:37:07.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6857" for this suite. 09/06/23 11:37:07.246
------------------------------
• [4.126 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:37:03.142
    Sep  6 11:37:03.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubelet-test 09/06/23 11:37:03.143
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:03.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:03.162
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 09/06/23 11:37:03.174
    Sep  6 11:37:03.174: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974" in namespace "kubelet-test-6857" to be "completed"
    Sep  6 11:37:03.179: INFO: Pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974": Phase="Pending", Reason="", readiness=false. Elapsed: 4.643828ms
    Sep  6 11:37:05.183: INFO: Pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009200598s
    Sep  6 11:37:07.195: INFO: Pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020658371s
    Sep  6 11:37:07.195: INFO: Pod "agnhost-host-aliases12bcadf1-ecf2-4a50-912b-c87f53076974" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:37:07.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6857" for this suite. 09/06/23 11:37:07.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:37:07.269
Sep  6 11:37:07.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename podtemplate 09/06/23 11:37:07.27
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:07.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:07.38
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  6 11:37:07.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4993" for this suite. 09/06/23 11:37:07.471
------------------------------
• [0.214 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:37:07.269
    Sep  6 11:37:07.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename podtemplate 09/06/23 11:37:07.27
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:07.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:07.38
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:37:07.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4993" for this suite. 09/06/23 11:37:07.471
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:37:07.483
Sep  6 11:37:07.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 11:37:07.484
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:07.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:07.556
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 09/06/23 11:37:07.558
Sep  6 11:37:07.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-525 cluster-info'
Sep  6 11:37:07.624: INFO: stderr: ""
Sep  6 11:37:07.624: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 11:37:07.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-525" for this suite. 09/06/23 11:37:07.627
------------------------------
• [0.159 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:37:07.483
    Sep  6 11:37:07.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 11:37:07.484
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:07.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:07.556
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 09/06/23 11:37:07.558
    Sep  6 11:37:07.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-525 cluster-info'
    Sep  6 11:37:07.624: INFO: stderr: ""
    Sep  6 11:37:07.624: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:37:07.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-525" for this suite. 09/06/23 11:37:07.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:37:07.642
Sep  6 11:37:07.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename conformance-tests 09/06/23 11:37:07.643
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:07.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:07.745
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 09/06/23 11:37:07.748
Sep  6 11:37:07.748: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Sep  6 11:37:07.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-7152" for this suite. 09/06/23 11:37:07.758
------------------------------
• [0.123 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:37:07.642
    Sep  6 11:37:07.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename conformance-tests 09/06/23 11:37:07.643
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:07.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:07.745
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 09/06/23 11:37:07.748
    Sep  6 11:37:07.748: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:37:07.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-7152" for this suite. 09/06/23 11:37:07.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:37:07.768
Sep  6 11:37:07.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename taint-multiple-pods 09/06/23 11:37:07.769
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:07.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:07.788
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Sep  6 11:37:07.790: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 11:38:07.811: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Sep  6 11:38:07.815: INFO: Starting informer...
STEP: Starting pods... 09/06/23 11:38:07.815
Sep  6 11:38:08.035: INFO: Pod1 is running on kube-3. Tainting Node
Sep  6 11:38:08.246: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7738" to be "running"
Sep  6 11:38:08.249: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.189486ms
Sep  6 11:38:10.256: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009673912s
Sep  6 11:38:10.256: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Sep  6 11:38:10.256: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7738" to be "running"
Sep  6 11:38:10.259: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.661502ms
Sep  6 11:38:10.259: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Sep  6 11:38:10.259: INFO: Pod2 is running on kube-3. Tainting Node
STEP: Trying to apply a taint on the Node 09/06/23 11:38:10.259
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/06/23 11:38:10.272
STEP: Waiting for Pod1 and Pod2 to be deleted 09/06/23 11:38:10.281
Sep  6 11:38:15.983: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  6 11:38:36.063: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/06/23 11:38:36.076
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:38:36.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-7738" for this suite. 09/06/23 11:38:36.087
------------------------------
• [SLOW TEST] [88.327 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:37:07.768
    Sep  6 11:37:07.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename taint-multiple-pods 09/06/23 11:37:07.769
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:37:07.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:37:07.788
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Sep  6 11:37:07.790: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  6 11:38:07.811: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Sep  6 11:38:07.815: INFO: Starting informer...
    STEP: Starting pods... 09/06/23 11:38:07.815
    Sep  6 11:38:08.035: INFO: Pod1 is running on kube-3. Tainting Node
    Sep  6 11:38:08.246: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7738" to be "running"
    Sep  6 11:38:08.249: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.189486ms
    Sep  6 11:38:10.256: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009673912s
    Sep  6 11:38:10.256: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Sep  6 11:38:10.256: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7738" to be "running"
    Sep  6 11:38:10.259: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.661502ms
    Sep  6 11:38:10.259: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Sep  6 11:38:10.259: INFO: Pod2 is running on kube-3. Tainting Node
    STEP: Trying to apply a taint on the Node 09/06/23 11:38:10.259
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/06/23 11:38:10.272
    STEP: Waiting for Pod1 and Pod2 to be deleted 09/06/23 11:38:10.281
    Sep  6 11:38:15.983: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Sep  6 11:38:36.063: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/06/23 11:38:36.076
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:38:36.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-7738" for this suite. 09/06/23 11:38:36.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:38:36.097
Sep  6 11:38:36.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 11:38:36.098
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:38:36.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:38:36.121
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 09/06/23 11:38:36.122
STEP: listing secrets in all namespaces to ensure that there are more than zero 09/06/23 11:38:36.128
STEP: patching the secret 09/06/23 11:38:36.133
STEP: deleting the secret using a LabelSelector 09/06/23 11:38:36.146
STEP: listing secrets in all namespaces, searching for label name and value in patch 09/06/23 11:38:36.154
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 11:38:36.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4828" for this suite. 09/06/23 11:38:36.162
------------------------------
• [0.073 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:38:36.097
    Sep  6 11:38:36.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 11:38:36.098
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:38:36.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:38:36.121
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 09/06/23 11:38:36.122
    STEP: listing secrets in all namespaces to ensure that there are more than zero 09/06/23 11:38:36.128
    STEP: patching the secret 09/06/23 11:38:36.133
    STEP: deleting the secret using a LabelSelector 09/06/23 11:38:36.146
    STEP: listing secrets in all namespaces, searching for label name and value in patch 09/06/23 11:38:36.154
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:38:36.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4828" for this suite. 09/06/23 11:38:36.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:38:36.17
Sep  6 11:38:36.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename cronjob 09/06/23 11:38:36.171
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:38:36.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:38:36.19
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 09/06/23 11:38:36.192
STEP: Ensuring a job is scheduled 09/06/23 11:38:36.198
STEP: Ensuring exactly one is scheduled 09/06/23 11:39:00.201
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/06/23 11:39:00.205
STEP: Ensuring the job is replaced with a new one 09/06/23 11:39:00.209
Sep  6 11:40:00.314: INFO: Warning: Found 0 jobs in namespace cronjob-3793
STEP: Removing cronjob 09/06/23 11:40:02.221
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  6 11:40:02.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3793" for this suite. 09/06/23 11:40:02.273
------------------------------
• [SLOW TEST] [86.113 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:38:36.17
    Sep  6 11:38:36.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename cronjob 09/06/23 11:38:36.171
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:38:36.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:38:36.19
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 09/06/23 11:38:36.192
    STEP: Ensuring a job is scheduled 09/06/23 11:38:36.198
    STEP: Ensuring exactly one is scheduled 09/06/23 11:39:00.201
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/06/23 11:39:00.205
    STEP: Ensuring the job is replaced with a new one 09/06/23 11:39:00.209
    Sep  6 11:40:00.314: INFO: Warning: Found 0 jobs in namespace cronjob-3793
    STEP: Removing cronjob 09/06/23 11:40:02.221
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:40:02.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3793" for this suite. 09/06/23 11:40:02.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:40:02.288
Sep  6 11:40:02.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:40:02.289
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:40:02.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:40:02.323
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:40:02.327
Sep  6 11:40:02.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251" in namespace "projected-5319" to be "Succeeded or Failed"
Sep  6 11:40:02.346: INFO: Pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251": Phase="Pending", Reason="", readiness=false. Elapsed: 9.748826ms
Sep  6 11:40:04.351: INFO: Pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014096806s
Sep  6 11:40:06.358: INFO: Pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021516396s
STEP: Saw pod success 09/06/23 11:40:06.358
Sep  6 11:40:06.358: INFO: Pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251" satisfied condition "Succeeded or Failed"
Sep  6 11:40:06.365: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251 container client-container: <nil>
STEP: delete the pod 09/06/23 11:40:06.38
Sep  6 11:40:06.399: INFO: Waiting for pod downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251 to disappear
Sep  6 11:40:06.401: INFO: Pod downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 11:40:06.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5319" for this suite. 09/06/23 11:40:06.405
------------------------------
• [4.123 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:40:02.288
    Sep  6 11:40:02.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:40:02.289
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:40:02.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:40:02.323
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:40:02.327
    Sep  6 11:40:02.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251" in namespace "projected-5319" to be "Succeeded or Failed"
    Sep  6 11:40:02.346: INFO: Pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251": Phase="Pending", Reason="", readiness=false. Elapsed: 9.748826ms
    Sep  6 11:40:04.351: INFO: Pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014096806s
    Sep  6 11:40:06.358: INFO: Pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021516396s
    STEP: Saw pod success 09/06/23 11:40:06.358
    Sep  6 11:40:06.358: INFO: Pod "downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251" satisfied condition "Succeeded or Failed"
    Sep  6 11:40:06.365: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251 container client-container: <nil>
    STEP: delete the pod 09/06/23 11:40:06.38
    Sep  6 11:40:06.399: INFO: Waiting for pod downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251 to disappear
    Sep  6 11:40:06.401: INFO: Pod downwardapi-volume-7be1cf46-9af2-433a-b45a-6ebf2d087251 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:40:06.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5319" for this suite. 09/06/23 11:40:06.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:40:06.411
Sep  6 11:40:06.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename statefulset 09/06/23 11:40:06.412
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:40:06.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:40:06.429
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2242 09/06/23 11:40:06.432
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 09/06/23 11:40:06.438
STEP: Creating stateful set ss in namespace statefulset-2242 09/06/23 11:40:06.448
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2242 09/06/23 11:40:06.455
Sep  6 11:40:06.460: INFO: Found 0 stateful pods, waiting for 1
Sep  6 11:40:16.472: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/06/23 11:40:16.472
Sep  6 11:40:16.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 11:40:16.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 11:40:16.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 11:40:16.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 11:40:16.671: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 11:40:26.684: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 11:40:26.684: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:40:26.720: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999025s
Sep  6 11:40:27.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991862688s
Sep  6 11:40:28.740: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987812658s
Sep  6 11:40:29.754: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.971734017s
Sep  6 11:40:30.758: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.958126843s
Sep  6 11:40:31.769: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.954194326s
Sep  6 11:40:32.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.943538922s
Sep  6 11:40:33.976: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.92953084s
Sep  6 11:40:34.981: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.735532642s
Sep  6 11:40:35.985: INFO: Verifying statefulset ss doesn't scale past 1 for another 731.441996ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2242 09/06/23 11:40:36.986
Sep  6 11:40:37.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 11:40:37.211: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 11:40:37.211: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 11:40:37.211: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 11:40:37.216: INFO: Found 1 stateful pods, waiting for 3
Sep  6 11:40:47.232: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:40:47.232: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:40:47.232: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 09/06/23 11:40:47.232
STEP: Scale down will halt with unhealthy stateful pod 09/06/23 11:40:47.232
Sep  6 11:40:47.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 11:40:47.469: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 11:40:47.469: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 11:40:47.469: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 11:40:47.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 11:40:47.588: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 11:40:47.588: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 11:40:47.588: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 11:40:47.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 11:40:47.712: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 11:40:47.712: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 11:40:47.712: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 11:40:47.712: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:40:47.716: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  6 11:40:57.741: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 11:40:57.741: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 11:40:57.741: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 11:40:57.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999635s
Sep  6 11:40:58.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995812612s
Sep  6 11:40:59.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977786315s
Sep  6 11:41:00.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968186618s
Sep  6 11:41:01.813: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.953139272s
Sep  6 11:41:02.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937199017s
Sep  6 11:41:03.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.932676234s
Sep  6 11:41:04.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.924732615s
Sep  6 11:41:05.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.915850482s
Sep  6 11:41:06.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 901.266054ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2242 09/06/23 11:41:07.865
Sep  6 11:41:07.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 11:41:08.261: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 11:41:08.261: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 11:41:08.261: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 11:41:08.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 11:41:08.471: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 11:41:08.471: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 11:41:08.471: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 11:41:08.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 11:41:08.598: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 11:41:08.598: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 11:41:08.598: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 11:41:08.598: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 09/06/23 11:41:18.613
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  6 11:41:18.613: INFO: Deleting all statefulset in ns statefulset-2242
Sep  6 11:41:18.616: INFO: Scaling statefulset ss to 0
Sep  6 11:41:18.625: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:41:18.628: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:41:18.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2242" for this suite. 09/06/23 11:41:18.654
------------------------------
• [SLOW TEST] [72.251 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:40:06.411
    Sep  6 11:40:06.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename statefulset 09/06/23 11:40:06.412
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:40:06.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:40:06.429
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2242 09/06/23 11:40:06.432
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 09/06/23 11:40:06.438
    STEP: Creating stateful set ss in namespace statefulset-2242 09/06/23 11:40:06.448
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2242 09/06/23 11:40:06.455
    Sep  6 11:40:06.460: INFO: Found 0 stateful pods, waiting for 1
    Sep  6 11:40:16.472: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/06/23 11:40:16.472
    Sep  6 11:40:16.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 11:40:16.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 11:40:16.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 11:40:16.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 11:40:16.671: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  6 11:40:26.684: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  6 11:40:26.684: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 11:40:26.720: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999025s
    Sep  6 11:40:27.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991862688s
    Sep  6 11:40:28.740: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987812658s
    Sep  6 11:40:29.754: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.971734017s
    Sep  6 11:40:30.758: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.958126843s
    Sep  6 11:40:31.769: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.954194326s
    Sep  6 11:40:32.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.943538922s
    Sep  6 11:40:33.976: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.92953084s
    Sep  6 11:40:34.981: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.735532642s
    Sep  6 11:40:35.985: INFO: Verifying statefulset ss doesn't scale past 1 for another 731.441996ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2242 09/06/23 11:40:36.986
    Sep  6 11:40:37.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 11:40:37.211: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  6 11:40:37.211: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 11:40:37.211: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 11:40:37.216: INFO: Found 1 stateful pods, waiting for 3
    Sep  6 11:40:47.232: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 11:40:47.232: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  6 11:40:47.232: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 09/06/23 11:40:47.232
    STEP: Scale down will halt with unhealthy stateful pod 09/06/23 11:40:47.232
    Sep  6 11:40:47.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 11:40:47.469: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 11:40:47.469: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 11:40:47.469: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 11:40:47.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 11:40:47.588: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 11:40:47.588: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 11:40:47.588: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 11:40:47.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  6 11:40:47.712: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  6 11:40:47.712: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  6 11:40:47.712: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  6 11:40:47.712: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 11:40:47.716: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Sep  6 11:40:57.741: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  6 11:40:57.741: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  6 11:40:57.741: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  6 11:40:57.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999635s
    Sep  6 11:40:58.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995812612s
    Sep  6 11:40:59.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977786315s
    Sep  6 11:41:00.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968186618s
    Sep  6 11:41:01.813: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.953139272s
    Sep  6 11:41:02.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937199017s
    Sep  6 11:41:03.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.932676234s
    Sep  6 11:41:04.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.924732615s
    Sep  6 11:41:05.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.915850482s
    Sep  6 11:41:06.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 901.266054ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2242 09/06/23 11:41:07.865
    Sep  6 11:41:07.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 11:41:08.261: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  6 11:41:08.261: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 11:41:08.261: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 11:41:08.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 11:41:08.471: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  6 11:41:08.471: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 11:41:08.471: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 11:41:08.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=statefulset-2242 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  6 11:41:08.598: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  6 11:41:08.598: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  6 11:41:08.598: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  6 11:41:08.598: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 09/06/23 11:41:18.613
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  6 11:41:18.613: INFO: Deleting all statefulset in ns statefulset-2242
    Sep  6 11:41:18.616: INFO: Scaling statefulset ss to 0
    Sep  6 11:41:18.625: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  6 11:41:18.628: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:41:18.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2242" for this suite. 09/06/23 11:41:18.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:41:18.664
Sep  6 11:41:18.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename resourcequota 09/06/23 11:41:18.665
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:18.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:18.684
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 09/06/23 11:41:18.686
STEP: Counting existing ResourceQuota 09/06/23 11:41:23.691
STEP: Creating a ResourceQuota 09/06/23 11:41:28.704
STEP: Ensuring resource quota status is calculated 09/06/23 11:41:28.71
STEP: Creating a Secret 09/06/23 11:41:30.721
STEP: Ensuring resource quota status captures secret creation 09/06/23 11:41:30.747
STEP: Deleting a secret 09/06/23 11:41:32.768
STEP: Ensuring resource quota status released usage 09/06/23 11:41:32.776
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  6 11:41:34.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7908" for this suite. 09/06/23 11:41:34.801
------------------------------
• [SLOW TEST] [16.148 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:41:18.664
    Sep  6 11:41:18.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename resourcequota 09/06/23 11:41:18.665
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:18.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:18.684
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 09/06/23 11:41:18.686
    STEP: Counting existing ResourceQuota 09/06/23 11:41:23.691
    STEP: Creating a ResourceQuota 09/06/23 11:41:28.704
    STEP: Ensuring resource quota status is calculated 09/06/23 11:41:28.71
    STEP: Creating a Secret 09/06/23 11:41:30.721
    STEP: Ensuring resource quota status captures secret creation 09/06/23 11:41:30.747
    STEP: Deleting a secret 09/06/23 11:41:32.768
    STEP: Ensuring resource quota status released usage 09/06/23 11:41:32.776
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:41:34.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7908" for this suite. 09/06/23 11:41:34.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:41:34.813
Sep  6 11:41:34.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename daemonsets 09/06/23 11:41:34.813
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:34.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:34.833
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Sep  6 11:41:34.855: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 09/06/23 11:41:34.862
Sep  6 11:41:34.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:41:34.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 09/06/23 11:41:34.866
Sep  6 11:41:34.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:41:34.893: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 11:41:35.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  6 11:41:35.909: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 09/06/23 11:41:35.919
Sep  6 11:41:35.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  6 11:41:35.963: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Sep  6 11:41:36.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:41:36.974: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/06/23 11:41:36.974
Sep  6 11:41:37.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:41:37.005: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 11:41:38.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:41:38.015: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 11:41:39.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:41:39.053: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 11:41:40.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:41:40.017: INFO: Node kube-2 is running 0 daemon pod, expected 1
Sep  6 11:41:41.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  6 11:41:41.015: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/06/23 11:41:41.026
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3879, will wait for the garbage collector to delete the pods 09/06/23 11:41:41.027
Sep  6 11:41:41.094: INFO: Deleting DaemonSet.extensions daemon-set took: 10.790222ms
Sep  6 11:41:41.194: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.315963ms
Sep  6 11:41:43.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:41:43.798: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  6 11:41:43.800: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40871"},"items":null}

Sep  6 11:41:43.803: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40871"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:41:43.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3879" for this suite. 09/06/23 11:41:43.841
------------------------------
• [SLOW TEST] [9.037 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:41:34.813
    Sep  6 11:41:34.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename daemonsets 09/06/23 11:41:34.813
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:34.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:34.833
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Sep  6 11:41:34.855: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 09/06/23 11:41:34.862
    Sep  6 11:41:34.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:41:34.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 09/06/23 11:41:34.866
    Sep  6 11:41:34.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:41:34.893: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 11:41:35.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  6 11:41:35.909: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 09/06/23 11:41:35.919
    Sep  6 11:41:35.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  6 11:41:35.963: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Sep  6 11:41:36.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:41:36.974: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/06/23 11:41:36.974
    Sep  6 11:41:37.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:41:37.005: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 11:41:38.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:41:38.015: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 11:41:39.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:41:39.053: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 11:41:40.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:41:40.017: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Sep  6 11:41:41.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  6 11:41:41.015: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/06/23 11:41:41.026
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3879, will wait for the garbage collector to delete the pods 09/06/23 11:41:41.027
    Sep  6 11:41:41.094: INFO: Deleting DaemonSet.extensions daemon-set took: 10.790222ms
    Sep  6 11:41:41.194: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.315963ms
    Sep  6 11:41:43.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:41:43.798: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  6 11:41:43.800: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40871"},"items":null}

    Sep  6 11:41:43.803: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40871"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:41:43.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3879" for this suite. 09/06/23 11:41:43.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:41:43.851
Sep  6 11:41:43.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename services 09/06/23 11:41:43.851
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:43.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:43.881
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-1585 09/06/23 11:41:43.884
STEP: creating service affinity-nodeport-transition in namespace services-1585 09/06/23 11:41:43.884
STEP: creating replication controller affinity-nodeport-transition in namespace services-1585 09/06/23 11:41:43.905
I0906 11:41:43.921117      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1585, replica count: 3
I0906 11:41:46.973179      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:41:47.002: INFO: Creating new exec pod
Sep  6 11:41:47.009: INFO: Waiting up to 5m0s for pod "execpod-affinitywrs4r" in namespace "services-1585" to be "running"
Sep  6 11:41:47.014: INFO: Pod "execpod-affinitywrs4r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.211918ms
Sep  6 11:41:49.018: INFO: Pod "execpod-affinitywrs4r": Phase="Running", Reason="", readiness=true. Elapsed: 2.009526717s
Sep  6 11:41:49.019: INFO: Pod "execpod-affinitywrs4r" satisfied condition "running"
Sep  6 11:41:50.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Sep  6 11:41:50.186: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep  6 11:41:50.186: INFO: stdout: ""
Sep  6 11:41:50.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c nc -v -z -w 2 10.233.6.34 80'
Sep  6 11:41:50.304: INFO: stderr: "+ nc -v -z -w 2 10.233.6.34 80\nConnection to 10.233.6.34 80 port [tcp/http] succeeded!\n"
Sep  6 11:41:50.304: INFO: stdout: ""
Sep  6 11:41:50.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c nc -v -z -w 2 10.2.20.103 31025'
Sep  6 11:41:50.409: INFO: stderr: "+ nc -v -z -w 2 10.2.20.103 31025\nConnection to 10.2.20.103 31025 port [tcp/*] succeeded!\n"
Sep  6 11:41:50.409: INFO: stdout: ""
Sep  6 11:41:50.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c nc -v -z -w 2 10.2.20.101 31025'
Sep  6 11:41:50.535: INFO: stderr: "+ nc -v -z -w 2 10.2.20.101 31025\nConnection to 10.2.20.101 31025 port [tcp/*] succeeded!\n"
Sep  6 11:41:50.535: INFO: stdout: ""
Sep  6 11:41:50.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:31025/ ; done'
Sep  6 11:41:50.744: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n"
Sep  6 11:41:50.744: INFO: stdout: "\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs"
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
Sep  6 11:41:50.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:31025/ ; done'
Sep  6 11:41:50.964: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n"
Sep  6 11:41:50.964: INFO: stdout: "\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r"
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
Sep  6 11:41:50.964: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1585, will wait for the garbage collector to delete the pods 09/06/23 11:41:50.98
Sep  6 11:41:51.045: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.319942ms
Sep  6 11:41:51.146: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.689881ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  6 11:41:54.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1585" for this suite. 09/06/23 11:41:54.212
------------------------------
• [SLOW TEST] [10.376 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:41:43.851
    Sep  6 11:41:43.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename services 09/06/23 11:41:43.851
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:43.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:43.881
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-1585 09/06/23 11:41:43.884
    STEP: creating service affinity-nodeport-transition in namespace services-1585 09/06/23 11:41:43.884
    STEP: creating replication controller affinity-nodeport-transition in namespace services-1585 09/06/23 11:41:43.905
    I0906 11:41:43.921117      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1585, replica count: 3
    I0906 11:41:46.973179      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  6 11:41:47.002: INFO: Creating new exec pod
    Sep  6 11:41:47.009: INFO: Waiting up to 5m0s for pod "execpod-affinitywrs4r" in namespace "services-1585" to be "running"
    Sep  6 11:41:47.014: INFO: Pod "execpod-affinitywrs4r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.211918ms
    Sep  6 11:41:49.018: INFO: Pod "execpod-affinitywrs4r": Phase="Running", Reason="", readiness=true. Elapsed: 2.009526717s
    Sep  6 11:41:49.019: INFO: Pod "execpod-affinitywrs4r" satisfied condition "running"
    Sep  6 11:41:50.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Sep  6 11:41:50.186: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Sep  6 11:41:50.186: INFO: stdout: ""
    Sep  6 11:41:50.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c nc -v -z -w 2 10.233.6.34 80'
    Sep  6 11:41:50.304: INFO: stderr: "+ nc -v -z -w 2 10.233.6.34 80\nConnection to 10.233.6.34 80 port [tcp/http] succeeded!\n"
    Sep  6 11:41:50.304: INFO: stdout: ""
    Sep  6 11:41:50.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c nc -v -z -w 2 10.2.20.103 31025'
    Sep  6 11:41:50.409: INFO: stderr: "+ nc -v -z -w 2 10.2.20.103 31025\nConnection to 10.2.20.103 31025 port [tcp/*] succeeded!\n"
    Sep  6 11:41:50.409: INFO: stdout: ""
    Sep  6 11:41:50.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c nc -v -z -w 2 10.2.20.101 31025'
    Sep  6 11:41:50.535: INFO: stderr: "+ nc -v -z -w 2 10.2.20.101 31025\nConnection to 10.2.20.101 31025 port [tcp/*] succeeded!\n"
    Sep  6 11:41:50.535: INFO: stdout: ""
    Sep  6 11:41:50.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:31025/ ; done'
    Sep  6 11:41:50.744: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n"
    Sep  6 11:41:50.744: INFO: stdout: "\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs\naffinity-nodeport-transition-dqkxr\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-tdphs"
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-dqkxr
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.744: INFO: Received response from host: affinity-nodeport-transition-tdphs
    Sep  6 11:41:50.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=services-1585 exec execpod-affinitywrs4r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:31025/ ; done'
    Sep  6 11:41:50.964: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:31025/\n"
    Sep  6 11:41:50.964: INFO: stdout: "\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r\naffinity-nodeport-transition-fpq5r"
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Received response from host: affinity-nodeport-transition-fpq5r
    Sep  6 11:41:50.964: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1585, will wait for the garbage collector to delete the pods 09/06/23 11:41:50.98
    Sep  6 11:41:51.045: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.319942ms
    Sep  6 11:41:51.146: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.689881ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:41:54.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1585" for this suite. 09/06/23 11:41:54.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:41:54.227
Sep  6 11:41:54.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename var-expansion 09/06/23 11:41:54.229
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:54.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:54.261
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 09/06/23 11:41:54.266
Sep  6 11:41:54.278: INFO: Waiting up to 5m0s for pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b" in namespace "var-expansion-2522" to be "Succeeded or Failed"
Sep  6 11:41:54.286: INFO: Pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.927436ms
Sep  6 11:41:56.298: INFO: Pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019134744s
Sep  6 11:41:58.298: INFO: Pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019648161s
STEP: Saw pod success 09/06/23 11:41:58.298
Sep  6 11:41:58.299: INFO: Pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b" satisfied condition "Succeeded or Failed"
Sep  6 11:41:58.307: INFO: Trying to get logs from node kube-3 pod var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b container dapi-container: <nil>
STEP: delete the pod 09/06/23 11:41:58.338
Sep  6 11:41:58.364: INFO: Waiting for pod var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b to disappear
Sep  6 11:41:58.368: INFO: Pod var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  6 11:41:58.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2522" for this suite. 09/06/23 11:41:58.373
------------------------------
• [4.153 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:41:54.227
    Sep  6 11:41:54.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename var-expansion 09/06/23 11:41:54.229
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:54.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:54.261
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 09/06/23 11:41:54.266
    Sep  6 11:41:54.278: INFO: Waiting up to 5m0s for pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b" in namespace "var-expansion-2522" to be "Succeeded or Failed"
    Sep  6 11:41:54.286: INFO: Pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.927436ms
    Sep  6 11:41:56.298: INFO: Pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019134744s
    Sep  6 11:41:58.298: INFO: Pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019648161s
    STEP: Saw pod success 09/06/23 11:41:58.298
    Sep  6 11:41:58.299: INFO: Pod "var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b" satisfied condition "Succeeded or Failed"
    Sep  6 11:41:58.307: INFO: Trying to get logs from node kube-3 pod var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b container dapi-container: <nil>
    STEP: delete the pod 09/06/23 11:41:58.338
    Sep  6 11:41:58.364: INFO: Waiting for pod var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b to disappear
    Sep  6 11:41:58.368: INFO: Pod var-expansion-560a4d5e-c843-4468-a4b2-5cad9375405b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:41:58.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2522" for this suite. 09/06/23 11:41:58.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:41:58.385
Sep  6 11:41:58.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubectl 09/06/23 11:41:58.386
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:58.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:58.406
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 09/06/23 11:41:58.408
Sep  6 11:41:58.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 create -f -'
Sep  6 11:41:59.031: INFO: stderr: ""
Sep  6 11:41:59.031: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/06/23 11:41:59.031
Sep  6 11:41:59.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:41:59.150: INFO: stderr: ""
Sep  6 11:41:59.150: INFO: stdout: "update-demo-nautilus-nbpph update-demo-nautilus-r5xh7 "
Sep  6 11:41:59.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-nbpph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:41:59.211: INFO: stderr: ""
Sep  6 11:41:59.211: INFO: stdout: ""
Sep  6 11:41:59.211: INFO: update-demo-nautilus-nbpph is created but not running
Sep  6 11:42:04.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:42:04.295: INFO: stderr: ""
Sep  6 11:42:04.295: INFO: stdout: "update-demo-nautilus-nbpph update-demo-nautilus-r5xh7 "
Sep  6 11:42:04.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-nbpph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:42:04.367: INFO: stderr: ""
Sep  6 11:42:04.367: INFO: stdout: "true"
Sep  6 11:42:04.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-nbpph -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:42:04.419: INFO: stderr: ""
Sep  6 11:42:04.419: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  6 11:42:04.419: INFO: validating pod update-demo-nautilus-nbpph
Sep  6 11:42:04.424: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:42:04.424: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:42:04.424: INFO: update-demo-nautilus-nbpph is verified up and running
Sep  6 11:42:04.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:42:04.477: INFO: stderr: ""
Sep  6 11:42:04.477: INFO: stdout: "true"
Sep  6 11:42:04.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:42:04.529: INFO: stderr: ""
Sep  6 11:42:04.529: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  6 11:42:04.529: INFO: validating pod update-demo-nautilus-r5xh7
Sep  6 11:42:04.533: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:42:04.533: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:42:04.533: INFO: update-demo-nautilus-r5xh7 is verified up and running
STEP: scaling down the replication controller 09/06/23 11:42:04.533
Sep  6 11:42:04.534: INFO: scanned /root for discovery docs: <nil>
Sep  6 11:42:04.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  6 11:42:05.625: INFO: stderr: ""
Sep  6 11:42:05.625: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/06/23 11:42:05.625
Sep  6 11:42:05.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:42:05.784: INFO: stderr: ""
Sep  6 11:42:05.784: INFO: stdout: "update-demo-nautilus-nbpph update-demo-nautilus-r5xh7 "
STEP: Replicas for name=update-demo: expected=1 actual=2 09/06/23 11:42:05.784
Sep  6 11:42:10.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:42:10.956: INFO: stderr: ""
Sep  6 11:42:10.956: INFO: stdout: "update-demo-nautilus-r5xh7 "
Sep  6 11:42:10.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:42:11.090: INFO: stderr: ""
Sep  6 11:42:11.090: INFO: stdout: "true"
Sep  6 11:42:11.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:42:11.196: INFO: stderr: ""
Sep  6 11:42:11.196: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  6 11:42:11.196: INFO: validating pod update-demo-nautilus-r5xh7
Sep  6 11:42:11.200: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:42:11.200: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:42:11.200: INFO: update-demo-nautilus-r5xh7 is verified up and running
STEP: scaling up the replication controller 09/06/23 11:42:11.2
Sep  6 11:42:11.202: INFO: scanned /root for discovery docs: <nil>
Sep  6 11:42:11.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  6 11:42:12.306: INFO: stderr: ""
Sep  6 11:42:12.306: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/06/23 11:42:12.306
Sep  6 11:42:12.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:42:12.497: INFO: stderr: ""
Sep  6 11:42:12.497: INFO: stdout: "update-demo-nautilus-qxpz6 update-demo-nautilus-r5xh7 "
Sep  6 11:42:12.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-qxpz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:42:12.598: INFO: stderr: ""
Sep  6 11:42:12.598: INFO: stdout: ""
Sep  6 11:42:12.598: INFO: update-demo-nautilus-qxpz6 is created but not running
Sep  6 11:42:17.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:42:17.794: INFO: stderr: ""
Sep  6 11:42:17.794: INFO: stdout: "update-demo-nautilus-qxpz6 update-demo-nautilus-r5xh7 "
Sep  6 11:42:17.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-qxpz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:42:17.920: INFO: stderr: ""
Sep  6 11:42:17.920: INFO: stdout: "true"
Sep  6 11:42:17.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-qxpz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:42:18.004: INFO: stderr: ""
Sep  6 11:42:18.004: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  6 11:42:18.004: INFO: validating pod update-demo-nautilus-qxpz6
Sep  6 11:42:18.009: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:42:18.009: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:42:18.009: INFO: update-demo-nautilus-qxpz6 is verified up and running
Sep  6 11:42:18.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:42:18.084: INFO: stderr: ""
Sep  6 11:42:18.084: INFO: stdout: "true"
Sep  6 11:42:18.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:42:18.152: INFO: stderr: ""
Sep  6 11:42:18.152: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  6 11:42:18.152: INFO: validating pod update-demo-nautilus-r5xh7
Sep  6 11:42:18.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:42:18.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:42:18.156: INFO: update-demo-nautilus-r5xh7 is verified up and running
STEP: using delete to clean up resources 09/06/23 11:42:18.156
Sep  6 11:42:18.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 delete --grace-period=0 --force -f -'
Sep  6 11:42:18.228: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:42:18.228: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 11:42:18.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get rc,svc -l name=update-demo --no-headers'
Sep  6 11:42:18.334: INFO: stderr: "No resources found in kubectl-2205 namespace.\n"
Sep  6 11:42:18.334: INFO: stdout: ""
Sep  6 11:42:18.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 11:42:18.402: INFO: stderr: ""
Sep  6 11:42:18.402: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  6 11:42:18.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2205" for this suite. 09/06/23 11:42:18.406
------------------------------
• [SLOW TEST] [20.030 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:41:58.385
    Sep  6 11:41:58.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubectl 09/06/23 11:41:58.386
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:41:58.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:41:58.406
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 09/06/23 11:41:58.408
    Sep  6 11:41:58.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 create -f -'
    Sep  6 11:41:59.031: INFO: stderr: ""
    Sep  6 11:41:59.031: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/06/23 11:41:59.031
    Sep  6 11:41:59.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 11:41:59.150: INFO: stderr: ""
    Sep  6 11:41:59.150: INFO: stdout: "update-demo-nautilus-nbpph update-demo-nautilus-r5xh7 "
    Sep  6 11:41:59.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-nbpph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 11:41:59.211: INFO: stderr: ""
    Sep  6 11:41:59.211: INFO: stdout: ""
    Sep  6 11:41:59.211: INFO: update-demo-nautilus-nbpph is created but not running
    Sep  6 11:42:04.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 11:42:04.295: INFO: stderr: ""
    Sep  6 11:42:04.295: INFO: stdout: "update-demo-nautilus-nbpph update-demo-nautilus-r5xh7 "
    Sep  6 11:42:04.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-nbpph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 11:42:04.367: INFO: stderr: ""
    Sep  6 11:42:04.367: INFO: stdout: "true"
    Sep  6 11:42:04.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-nbpph -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  6 11:42:04.419: INFO: stderr: ""
    Sep  6 11:42:04.419: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  6 11:42:04.419: INFO: validating pod update-demo-nautilus-nbpph
    Sep  6 11:42:04.424: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  6 11:42:04.424: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  6 11:42:04.424: INFO: update-demo-nautilus-nbpph is verified up and running
    Sep  6 11:42:04.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 11:42:04.477: INFO: stderr: ""
    Sep  6 11:42:04.477: INFO: stdout: "true"
    Sep  6 11:42:04.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  6 11:42:04.529: INFO: stderr: ""
    Sep  6 11:42:04.529: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  6 11:42:04.529: INFO: validating pod update-demo-nautilus-r5xh7
    Sep  6 11:42:04.533: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  6 11:42:04.533: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  6 11:42:04.533: INFO: update-demo-nautilus-r5xh7 is verified up and running
    STEP: scaling down the replication controller 09/06/23 11:42:04.533
    Sep  6 11:42:04.534: INFO: scanned /root for discovery docs: <nil>
    Sep  6 11:42:04.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Sep  6 11:42:05.625: INFO: stderr: ""
    Sep  6 11:42:05.625: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/06/23 11:42:05.625
    Sep  6 11:42:05.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 11:42:05.784: INFO: stderr: ""
    Sep  6 11:42:05.784: INFO: stdout: "update-demo-nautilus-nbpph update-demo-nautilus-r5xh7 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 09/06/23 11:42:05.784
    Sep  6 11:42:10.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 11:42:10.956: INFO: stderr: ""
    Sep  6 11:42:10.956: INFO: stdout: "update-demo-nautilus-r5xh7 "
    Sep  6 11:42:10.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 11:42:11.090: INFO: stderr: ""
    Sep  6 11:42:11.090: INFO: stdout: "true"
    Sep  6 11:42:11.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  6 11:42:11.196: INFO: stderr: ""
    Sep  6 11:42:11.196: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  6 11:42:11.196: INFO: validating pod update-demo-nautilus-r5xh7
    Sep  6 11:42:11.200: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  6 11:42:11.200: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  6 11:42:11.200: INFO: update-demo-nautilus-r5xh7 is verified up and running
    STEP: scaling up the replication controller 09/06/23 11:42:11.2
    Sep  6 11:42:11.202: INFO: scanned /root for discovery docs: <nil>
    Sep  6 11:42:11.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Sep  6 11:42:12.306: INFO: stderr: ""
    Sep  6 11:42:12.306: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/06/23 11:42:12.306
    Sep  6 11:42:12.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 11:42:12.497: INFO: stderr: ""
    Sep  6 11:42:12.497: INFO: stdout: "update-demo-nautilus-qxpz6 update-demo-nautilus-r5xh7 "
    Sep  6 11:42:12.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-qxpz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 11:42:12.598: INFO: stderr: ""
    Sep  6 11:42:12.598: INFO: stdout: ""
    Sep  6 11:42:12.598: INFO: update-demo-nautilus-qxpz6 is created but not running
    Sep  6 11:42:17.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  6 11:42:17.794: INFO: stderr: ""
    Sep  6 11:42:17.794: INFO: stdout: "update-demo-nautilus-qxpz6 update-demo-nautilus-r5xh7 "
    Sep  6 11:42:17.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-qxpz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 11:42:17.920: INFO: stderr: ""
    Sep  6 11:42:17.920: INFO: stdout: "true"
    Sep  6 11:42:17.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-qxpz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  6 11:42:18.004: INFO: stderr: ""
    Sep  6 11:42:18.004: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  6 11:42:18.004: INFO: validating pod update-demo-nautilus-qxpz6
    Sep  6 11:42:18.009: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  6 11:42:18.009: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  6 11:42:18.009: INFO: update-demo-nautilus-qxpz6 is verified up and running
    Sep  6 11:42:18.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  6 11:42:18.084: INFO: stderr: ""
    Sep  6 11:42:18.084: INFO: stdout: "true"
    Sep  6 11:42:18.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods update-demo-nautilus-r5xh7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  6 11:42:18.152: INFO: stderr: ""
    Sep  6 11:42:18.152: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  6 11:42:18.152: INFO: validating pod update-demo-nautilus-r5xh7
    Sep  6 11:42:18.156: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  6 11:42:18.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  6 11:42:18.156: INFO: update-demo-nautilus-r5xh7 is verified up and running
    STEP: using delete to clean up resources 09/06/23 11:42:18.156
    Sep  6 11:42:18.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 delete --grace-period=0 --force -f -'
    Sep  6 11:42:18.228: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  6 11:42:18.228: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  6 11:42:18.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get rc,svc -l name=update-demo --no-headers'
    Sep  6 11:42:18.334: INFO: stderr: "No resources found in kubectl-2205 namespace.\n"
    Sep  6 11:42:18.334: INFO: stdout: ""
    Sep  6 11:42:18.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1191164369 --namespace=kubectl-2205 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  6 11:42:18.402: INFO: stderr: ""
    Sep  6 11:42:18.402: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:42:18.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2205" for this suite. 09/06/23 11:42:18.406
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:42:18.415
Sep  6 11:42:18.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename svcaccounts 09/06/23 11:42:18.416
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:18.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:18.446
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Sep  6 11:42:18.478: INFO: created pod pod-service-account-defaultsa
Sep  6 11:42:18.478: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  6 11:42:18.495: INFO: created pod pod-service-account-mountsa
Sep  6 11:42:18.495: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  6 11:42:18.506: INFO: created pod pod-service-account-nomountsa
Sep  6 11:42:18.506: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  6 11:42:18.516: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  6 11:42:18.516: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  6 11:42:18.535: INFO: created pod pod-service-account-mountsa-mountspec
Sep  6 11:42:18.535: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  6 11:42:18.568: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  6 11:42:18.568: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  6 11:42:18.606: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  6 11:42:18.606: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  6 11:42:18.642: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  6 11:42:18.642: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  6 11:42:18.659: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  6 11:42:18.660: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  6 11:42:18.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4340" for this suite. 09/06/23 11:42:18.673
------------------------------
• [0.275 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:42:18.415
    Sep  6 11:42:18.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename svcaccounts 09/06/23 11:42:18.416
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:18.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:18.446
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Sep  6 11:42:18.478: INFO: created pod pod-service-account-defaultsa
    Sep  6 11:42:18.478: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Sep  6 11:42:18.495: INFO: created pod pod-service-account-mountsa
    Sep  6 11:42:18.495: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Sep  6 11:42:18.506: INFO: created pod pod-service-account-nomountsa
    Sep  6 11:42:18.506: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Sep  6 11:42:18.516: INFO: created pod pod-service-account-defaultsa-mountspec
    Sep  6 11:42:18.516: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Sep  6 11:42:18.535: INFO: created pod pod-service-account-mountsa-mountspec
    Sep  6 11:42:18.535: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Sep  6 11:42:18.568: INFO: created pod pod-service-account-nomountsa-mountspec
    Sep  6 11:42:18.568: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Sep  6 11:42:18.606: INFO: created pod pod-service-account-defaultsa-nomountspec
    Sep  6 11:42:18.606: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Sep  6 11:42:18.642: INFO: created pod pod-service-account-mountsa-nomountspec
    Sep  6 11:42:18.642: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Sep  6 11:42:18.659: INFO: created pod pod-service-account-nomountsa-nomountspec
    Sep  6 11:42:18.660: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:42:18.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4340" for this suite. 09/06/23 11:42:18.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:42:18.694
Sep  6 11:42:18.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename webhook 09/06/23 11:42:18.695
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:18.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:18.739
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/06/23 11:42:18.782
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:42:19.334
STEP: Deploying the webhook pod 09/06/23 11:42:19.398
STEP: Wait for the deployment to be ready 09/06/23 11:42:19.436
Sep  6 11:42:19.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 11:42:22.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 42, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 42, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 42, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 42, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/06/23 11:42:24.767
STEP: Verifying the service has paired with the endpoint 09/06/23 11:42:24.8
Sep  6 11:42:25.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 09/06/23 11:42:25.816
STEP: create a pod that should be denied by the webhook 09/06/23 11:42:25.843
STEP: create a pod that causes the webhook to hang 09/06/23 11:42:25.853
STEP: create a configmap that should be denied by the webhook 09/06/23 11:42:35.871
STEP: create a configmap that should be admitted by the webhook 09/06/23 11:42:35.917
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/06/23 11:42:35.946
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/06/23 11:42:35.955
STEP: create a namespace that bypass the webhook 09/06/23 11:42:35.964
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/06/23 11:42:35.981
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:42:36.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4926" for this suite. 09/06/23 11:42:36.071
STEP: Destroying namespace "webhook-4926-markers" for this suite. 09/06/23 11:42:36.093
------------------------------
• [SLOW TEST] [17.421 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:42:18.694
    Sep  6 11:42:18.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename webhook 09/06/23 11:42:18.695
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:18.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:18.739
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/06/23 11:42:18.782
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/06/23 11:42:19.334
    STEP: Deploying the webhook pod 09/06/23 11:42:19.398
    STEP: Wait for the deployment to be ready 09/06/23 11:42:19.436
    Sep  6 11:42:19.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  6 11:42:22.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 6, 11, 42, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 42, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 6, 11, 42, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 6, 11, 42, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/06/23 11:42:24.767
    STEP: Verifying the service has paired with the endpoint 09/06/23 11:42:24.8
    Sep  6 11:42:25.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 09/06/23 11:42:25.816
    STEP: create a pod that should be denied by the webhook 09/06/23 11:42:25.843
    STEP: create a pod that causes the webhook to hang 09/06/23 11:42:25.853
    STEP: create a configmap that should be denied by the webhook 09/06/23 11:42:35.871
    STEP: create a configmap that should be admitted by the webhook 09/06/23 11:42:35.917
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/06/23 11:42:35.946
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/06/23 11:42:35.955
    STEP: create a namespace that bypass the webhook 09/06/23 11:42:35.964
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/06/23 11:42:35.981
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:42:36.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4926" for this suite. 09/06/23 11:42:36.071
    STEP: Destroying namespace "webhook-4926-markers" for this suite. 09/06/23 11:42:36.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:42:36.12
Sep  6 11:42:36.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename containers 09/06/23 11:42:36.121
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:36.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:36.162
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 09/06/23 11:42:36.167
Sep  6 11:42:36.179: INFO: Waiting up to 5m0s for pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69" in namespace "containers-9579" to be "Succeeded or Failed"
Sep  6 11:42:36.194: INFO: Pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69": Phase="Pending", Reason="", readiness=false. Elapsed: 14.311292ms
Sep  6 11:42:38.201: INFO: Pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021535326s
Sep  6 11:42:40.433: INFO: Pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.253634671s
STEP: Saw pod success 09/06/23 11:42:40.433
Sep  6 11:42:40.433: INFO: Pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69" satisfied condition "Succeeded or Failed"
Sep  6 11:42:40.576: INFO: Trying to get logs from node kube-3 pod client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69 container agnhost-container: <nil>
STEP: delete the pod 09/06/23 11:42:40.662
Sep  6 11:42:41.342: INFO: Waiting for pod client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69 to disappear
Sep  6 11:42:41.709: INFO: Pod client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  6 11:42:41.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9579" for this suite. 09/06/23 11:42:42.745
------------------------------
• [SLOW TEST] [6.660 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:42:36.12
    Sep  6 11:42:36.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename containers 09/06/23 11:42:36.121
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:36.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:36.162
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 09/06/23 11:42:36.167
    Sep  6 11:42:36.179: INFO: Waiting up to 5m0s for pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69" in namespace "containers-9579" to be "Succeeded or Failed"
    Sep  6 11:42:36.194: INFO: Pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69": Phase="Pending", Reason="", readiness=false. Elapsed: 14.311292ms
    Sep  6 11:42:38.201: INFO: Pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021535326s
    Sep  6 11:42:40.433: INFO: Pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.253634671s
    STEP: Saw pod success 09/06/23 11:42:40.433
    Sep  6 11:42:40.433: INFO: Pod "client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69" satisfied condition "Succeeded or Failed"
    Sep  6 11:42:40.576: INFO: Trying to get logs from node kube-3 pod client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69 container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 11:42:40.662
    Sep  6 11:42:41.342: INFO: Waiting for pod client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69 to disappear
    Sep  6 11:42:41.709: INFO: Pod client-containers-ccd23148-1747-4e80-ae3a-a1303649ef69 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:42:41.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9579" for this suite. 09/06/23 11:42:42.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:42:42.784
Sep  6 11:42:42.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 11:42:42.785
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:42.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:42.964
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:42:42.969
Sep  6 11:42:43.000: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a" in namespace "downward-api-5829" to be "Succeeded or Failed"
Sep  6 11:42:43.048: INFO: Pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.309608ms
Sep  6 11:42:45.068: INFO: Pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067677862s
Sep  6 11:42:47.063: INFO: Pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063143598s
STEP: Saw pod success 09/06/23 11:42:47.063
Sep  6 11:42:47.064: INFO: Pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a" satisfied condition "Succeeded or Failed"
Sep  6 11:42:47.075: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a container client-container: <nil>
STEP: delete the pod 09/06/23 11:42:47.098
Sep  6 11:42:47.123: INFO: Waiting for pod downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a to disappear
Sep  6 11:42:47.126: INFO: Pod downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  6 11:42:47.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5829" for this suite. 09/06/23 11:42:47.13
------------------------------
• [4.362 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:42:42.784
    Sep  6 11:42:42.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 11:42:42.785
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:42.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:42.964
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:42:42.969
    Sep  6 11:42:43.000: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a" in namespace "downward-api-5829" to be "Succeeded or Failed"
    Sep  6 11:42:43.048: INFO: Pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.309608ms
    Sep  6 11:42:45.068: INFO: Pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067677862s
    Sep  6 11:42:47.063: INFO: Pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063143598s
    STEP: Saw pod success 09/06/23 11:42:47.063
    Sep  6 11:42:47.064: INFO: Pod "downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a" satisfied condition "Succeeded or Failed"
    Sep  6 11:42:47.075: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a container client-container: <nil>
    STEP: delete the pod 09/06/23 11:42:47.098
    Sep  6 11:42:47.123: INFO: Waiting for pod downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a to disappear
    Sep  6 11:42:47.126: INFO: Pod downwardapi-volume-ece1ba1f-c933-48a7-af6c-c5a59030672a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:42:47.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5829" for this suite. 09/06/23 11:42:47.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:42:47.148
Sep  6 11:42:47.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename replicaset 09/06/23 11:42:47.148
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:47.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:47.164
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/06/23 11:42:47.171
Sep  6 11:42:47.180: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1609" to be "running and ready"
Sep  6 11:42:47.184: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.145796ms
Sep  6 11:42:47.184: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:42:49.200: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.020802545s
Sep  6 11:42:49.201: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Sep  6 11:42:49.201: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 09/06/23 11:42:49.212
STEP: Then the orphan pod is adopted 09/06/23 11:42:49.225
STEP: When the matched label of one of its pods change 09/06/23 11:42:50.235
Sep  6 11:42:50.240: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 09/06/23 11:42:50.259
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:42:51.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1609" for this suite. 09/06/23 11:42:51.293
------------------------------
• [4.170 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:42:47.148
    Sep  6 11:42:47.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename replicaset 09/06/23 11:42:47.148
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:47.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:47.164
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/06/23 11:42:47.171
    Sep  6 11:42:47.180: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1609" to be "running and ready"
    Sep  6 11:42:47.184: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.145796ms
    Sep  6 11:42:47.184: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:42:49.200: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.020802545s
    Sep  6 11:42:49.201: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Sep  6 11:42:49.201: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 09/06/23 11:42:49.212
    STEP: Then the orphan pod is adopted 09/06/23 11:42:49.225
    STEP: When the matched label of one of its pods change 09/06/23 11:42:50.235
    Sep  6 11:42:50.240: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/06/23 11:42:50.259
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:42:51.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1609" for this suite. 09/06/23 11:42:51.293
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:42:51.318
Sep  6 11:42:51.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename secrets 09/06/23 11:42:51.319
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:51.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:51.339
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-53d76e98-bf0a-49b7-b7d4-2899e597e263 09/06/23 11:42:51.341
STEP: Creating a pod to test consume secrets 09/06/23 11:42:51.346
Sep  6 11:42:51.355: INFO: Waiting up to 5m0s for pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f" in namespace "secrets-3616" to be "Succeeded or Failed"
Sep  6 11:42:51.358: INFO: Pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.061129ms
Sep  6 11:42:53.363: INFO: Pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0078085s
Sep  6 11:42:55.365: INFO: Pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009644151s
STEP: Saw pod success 09/06/23 11:42:55.365
Sep  6 11:42:55.365: INFO: Pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f" satisfied condition "Succeeded or Failed"
Sep  6 11:42:55.369: INFO: Trying to get logs from node kube-2 pod pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f container secret-volume-test: <nil>
STEP: delete the pod 09/06/23 11:42:55.382
Sep  6 11:42:55.399: INFO: Waiting for pod pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f to disappear
Sep  6 11:42:55.402: INFO: Pod pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  6 11:42:55.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3616" for this suite. 09/06/23 11:42:55.406
------------------------------
• [4.094 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:42:51.318
    Sep  6 11:42:51.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename secrets 09/06/23 11:42:51.319
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:51.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:51.339
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-53d76e98-bf0a-49b7-b7d4-2899e597e263 09/06/23 11:42:51.341
    STEP: Creating a pod to test consume secrets 09/06/23 11:42:51.346
    Sep  6 11:42:51.355: INFO: Waiting up to 5m0s for pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f" in namespace "secrets-3616" to be "Succeeded or Failed"
    Sep  6 11:42:51.358: INFO: Pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.061129ms
    Sep  6 11:42:53.363: INFO: Pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0078085s
    Sep  6 11:42:55.365: INFO: Pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009644151s
    STEP: Saw pod success 09/06/23 11:42:55.365
    Sep  6 11:42:55.365: INFO: Pod "pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f" satisfied condition "Succeeded or Failed"
    Sep  6 11:42:55.369: INFO: Trying to get logs from node kube-2 pod pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f container secret-volume-test: <nil>
    STEP: delete the pod 09/06/23 11:42:55.382
    Sep  6 11:42:55.399: INFO: Waiting for pod pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f to disappear
    Sep  6 11:42:55.402: INFO: Pod pod-secrets-7b79fbad-2949-40be-b914-c5d2272ba66f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:42:55.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3616" for this suite. 09/06/23 11:42:55.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:42:55.413
Sep  6 11:42:55.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 11:42:55.414
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:55.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:55.482
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/06/23 11:42:55.484
Sep  6 11:42:55.495: INFO: Waiting up to 5m0s for pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8" in namespace "emptydir-4360" to be "Succeeded or Failed"
Sep  6 11:42:55.499: INFO: Pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.998438ms
Sep  6 11:42:57.515: INFO: Pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019504511s
Sep  6 11:42:59.516: INFO: Pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021030011s
STEP: Saw pod success 09/06/23 11:42:59.517
Sep  6 11:42:59.517: INFO: Pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8" satisfied condition "Succeeded or Failed"
Sep  6 11:42:59.525: INFO: Trying to get logs from node kube-2 pod pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8 container test-container: <nil>
STEP: delete the pod 09/06/23 11:42:59.536
Sep  6 11:42:59.558: INFO: Waiting for pod pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8 to disappear
Sep  6 11:42:59.561: INFO: Pod pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 11:42:59.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4360" for this suite. 09/06/23 11:42:59.565
------------------------------
• [4.158 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:42:55.413
    Sep  6 11:42:55.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 11:42:55.414
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:55.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:55.482
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/06/23 11:42:55.484
    Sep  6 11:42:55.495: INFO: Waiting up to 5m0s for pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8" in namespace "emptydir-4360" to be "Succeeded or Failed"
    Sep  6 11:42:55.499: INFO: Pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.998438ms
    Sep  6 11:42:57.515: INFO: Pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019504511s
    Sep  6 11:42:59.516: INFO: Pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021030011s
    STEP: Saw pod success 09/06/23 11:42:59.517
    Sep  6 11:42:59.517: INFO: Pod "pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8" satisfied condition "Succeeded or Failed"
    Sep  6 11:42:59.525: INFO: Trying to get logs from node kube-2 pod pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8 container test-container: <nil>
    STEP: delete the pod 09/06/23 11:42:59.536
    Sep  6 11:42:59.558: INFO: Waiting for pod pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8 to disappear
    Sep  6 11:42:59.561: INFO: Pod pod-0cf60f89-3bea-48b2-a553-f18f7ca847a8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:42:59.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4360" for this suite. 09/06/23 11:42:59.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:42:59.573
Sep  6 11:42:59.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename emptydir 09/06/23 11:42:59.574
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:59.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:59.593
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 09/06/23 11:42:59.595
Sep  6 11:42:59.602: INFO: Waiting up to 5m0s for pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5" in namespace "emptydir-3896" to be "Succeeded or Failed"
Sep  6 11:42:59.606: INFO: Pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.531854ms
Sep  6 11:43:01.621: INFO: Pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018725914s
Sep  6 11:43:03.622: INFO: Pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020087583s
STEP: Saw pod success 09/06/23 11:43:03.622
Sep  6 11:43:03.623: INFO: Pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5" satisfied condition "Succeeded or Failed"
Sep  6 11:43:03.635: INFO: Trying to get logs from node kube-3 pod pod-781705db-cdb6-487f-92b5-a80e24feb1c5 container test-container: <nil>
STEP: delete the pod 09/06/23 11:43:03.654
Sep  6 11:43:03.676: INFO: Waiting for pod pod-781705db-cdb6-487f-92b5-a80e24feb1c5 to disappear
Sep  6 11:43:03.680: INFO: Pod pod-781705db-cdb6-487f-92b5-a80e24feb1c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  6 11:43:03.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3896" for this suite. 09/06/23 11:43:03.684
------------------------------
• [4.118 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:42:59.573
    Sep  6 11:42:59.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename emptydir 09/06/23 11:42:59.574
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:42:59.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:42:59.593
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/06/23 11:42:59.595
    Sep  6 11:42:59.602: INFO: Waiting up to 5m0s for pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5" in namespace "emptydir-3896" to be "Succeeded or Failed"
    Sep  6 11:42:59.606: INFO: Pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.531854ms
    Sep  6 11:43:01.621: INFO: Pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018725914s
    Sep  6 11:43:03.622: INFO: Pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020087583s
    STEP: Saw pod success 09/06/23 11:43:03.622
    Sep  6 11:43:03.623: INFO: Pod "pod-781705db-cdb6-487f-92b5-a80e24feb1c5" satisfied condition "Succeeded or Failed"
    Sep  6 11:43:03.635: INFO: Trying to get logs from node kube-3 pod pod-781705db-cdb6-487f-92b5-a80e24feb1c5 container test-container: <nil>
    STEP: delete the pod 09/06/23 11:43:03.654
    Sep  6 11:43:03.676: INFO: Waiting for pod pod-781705db-cdb6-487f-92b5-a80e24feb1c5 to disappear
    Sep  6 11:43:03.680: INFO: Pod pod-781705db-cdb6-487f-92b5-a80e24feb1c5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:43:03.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3896" for this suite. 09/06/23 11:43:03.684
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:43:03.691
Sep  6 11:43:03.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename var-expansion 09/06/23 11:43:03.692
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:43:03.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:43:03.713
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 09/06/23 11:43:03.715
Sep  6 11:43:03.726: INFO: Waiting up to 5m0s for pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92" in namespace "var-expansion-3714" to be "Succeeded or Failed"
Sep  6 11:43:03.732: INFO: Pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92": Phase="Pending", Reason="", readiness=false. Elapsed: 5.761618ms
Sep  6 11:43:05.747: INFO: Pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020488679s
Sep  6 11:43:07.747: INFO: Pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020907464s
STEP: Saw pod success 09/06/23 11:43:07.747
Sep  6 11:43:07.748: INFO: Pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92" satisfied condition "Succeeded or Failed"
Sep  6 11:43:07.760: INFO: Trying to get logs from node kube-3 pod var-expansion-ea25c755-e864-4cde-9073-2e6727628d92 container dapi-container: <nil>
STEP: delete the pod 09/06/23 11:43:07.835
Sep  6 11:43:07.972: INFO: Waiting for pod var-expansion-ea25c755-e864-4cde-9073-2e6727628d92 to disappear
Sep  6 11:43:07.997: INFO: Pod var-expansion-ea25c755-e864-4cde-9073-2e6727628d92 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  6 11:43:07.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3714" for this suite. 09/06/23 11:43:08.005
------------------------------
• [4.337 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:43:03.691
    Sep  6 11:43:03.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename var-expansion 09/06/23 11:43:03.692
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:43:03.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:43:03.713
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 09/06/23 11:43:03.715
    Sep  6 11:43:03.726: INFO: Waiting up to 5m0s for pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92" in namespace "var-expansion-3714" to be "Succeeded or Failed"
    Sep  6 11:43:03.732: INFO: Pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92": Phase="Pending", Reason="", readiness=false. Elapsed: 5.761618ms
    Sep  6 11:43:05.747: INFO: Pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020488679s
    Sep  6 11:43:07.747: INFO: Pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020907464s
    STEP: Saw pod success 09/06/23 11:43:07.747
    Sep  6 11:43:07.748: INFO: Pod "var-expansion-ea25c755-e864-4cde-9073-2e6727628d92" satisfied condition "Succeeded or Failed"
    Sep  6 11:43:07.760: INFO: Trying to get logs from node kube-3 pod var-expansion-ea25c755-e864-4cde-9073-2e6727628d92 container dapi-container: <nil>
    STEP: delete the pod 09/06/23 11:43:07.835
    Sep  6 11:43:07.972: INFO: Waiting for pod var-expansion-ea25c755-e864-4cde-9073-2e6727628d92 to disappear
    Sep  6 11:43:07.997: INFO: Pod var-expansion-ea25c755-e864-4cde-9073-2e6727628d92 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:43:07.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3714" for this suite. 09/06/23 11:43:08.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:43:08.029
Sep  6 11:43:08.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename hostport 09/06/23 11:43:08.031
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:43:08.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:43:08.252
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/06/23 11:43:08.264
Sep  6 11:43:08.306: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2877" to be "running and ready"
Sep  6 11:43:08.371: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 65.0871ms
Sep  6 11:43:08.371: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:43:10.383: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.076402649s
Sep  6 11:43:10.383: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  6 11:43:10.383: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.2.20.101 on the node which pod1 resides and expect scheduled 09/06/23 11:43:10.383
Sep  6 11:43:10.392: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2877" to be "running and ready"
Sep  6 11:43:10.400: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.736358ms
Sep  6 11:43:10.400: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:43:12.411: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.018265799s
Sep  6 11:43:12.411: INFO: The phase of Pod pod2 is Running (Ready = false)
Sep  6 11:43:14.404: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.01177327s
Sep  6 11:43:14.404: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  6 11:43:14.404: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.2.20.101 but use UDP protocol on the node which pod2 resides 09/06/23 11:43:14.404
Sep  6 11:43:14.412: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2877" to be "running and ready"
Sep  6 11:43:14.419: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.630428ms
Sep  6 11:43:14.419: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:43:16.436: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.023601159s
Sep  6 11:43:16.436: INFO: The phase of Pod pod3 is Running (Ready = true)
Sep  6 11:43:16.436: INFO: Pod "pod3" satisfied condition "running and ready"
Sep  6 11:43:16.452: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2877" to be "running and ready"
Sep  6 11:43:16.460: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.511482ms
Sep  6 11:43:16.460: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:43:18.473: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.020293905s
Sep  6 11:43:18.473: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Sep  6 11:43:18.473: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/06/23 11:43:18.485
Sep  6 11:43:18.485: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.2.20.101 http://127.0.0.1:54323/hostname] Namespace:hostport-2877 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:43:18.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:43:18.488: INFO: ExecWithOptions: Clientset creation
Sep  6 11:43:18.488: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2877/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.2.20.101+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.20.101, port: 54323 09/06/23 11:43:18.67
Sep  6 11:43:18.670: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.2.20.101:54323/hostname] Namespace:hostport-2877 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:43:18.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:43:18.670: INFO: ExecWithOptions: Clientset creation
Sep  6 11:43:18.670: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2877/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.2.20.101%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.20.101, port: 54323 UDP 09/06/23 11:43:18.739
Sep  6 11:43:18.739: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.2.20.101 54323] Namespace:hostport-2877 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 11:43:18.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
Sep  6 11:43:18.740: INFO: ExecWithOptions: Clientset creation
Sep  6 11:43:18.740: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2877/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.2.20.101+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Sep  6 11:43:23.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-2877" for this suite. 09/06/23 11:43:23.826
------------------------------
• [SLOW TEST] [15.809 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:43:08.029
    Sep  6 11:43:08.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename hostport 09/06/23 11:43:08.031
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:43:08.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:43:08.252
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/06/23 11:43:08.264
    Sep  6 11:43:08.306: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2877" to be "running and ready"
    Sep  6 11:43:08.371: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 65.0871ms
    Sep  6 11:43:08.371: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:43:10.383: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.076402649s
    Sep  6 11:43:10.383: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  6 11:43:10.383: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.2.20.101 on the node which pod1 resides and expect scheduled 09/06/23 11:43:10.383
    Sep  6 11:43:10.392: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2877" to be "running and ready"
    Sep  6 11:43:10.400: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.736358ms
    Sep  6 11:43:10.400: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:43:12.411: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.018265799s
    Sep  6 11:43:12.411: INFO: The phase of Pod pod2 is Running (Ready = false)
    Sep  6 11:43:14.404: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.01177327s
    Sep  6 11:43:14.404: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  6 11:43:14.404: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.2.20.101 but use UDP protocol on the node which pod2 resides 09/06/23 11:43:14.404
    Sep  6 11:43:14.412: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2877" to be "running and ready"
    Sep  6 11:43:14.419: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.630428ms
    Sep  6 11:43:14.419: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:43:16.436: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.023601159s
    Sep  6 11:43:16.436: INFO: The phase of Pod pod3 is Running (Ready = true)
    Sep  6 11:43:16.436: INFO: Pod "pod3" satisfied condition "running and ready"
    Sep  6 11:43:16.452: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2877" to be "running and ready"
    Sep  6 11:43:16.460: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.511482ms
    Sep  6 11:43:16.460: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:43:18.473: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.020293905s
    Sep  6 11:43:18.473: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Sep  6 11:43:18.473: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/06/23 11:43:18.485
    Sep  6 11:43:18.485: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.2.20.101 http://127.0.0.1:54323/hostname] Namespace:hostport-2877 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:43:18.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:43:18.488: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:43:18.488: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2877/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.2.20.101+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.20.101, port: 54323 09/06/23 11:43:18.67
    Sep  6 11:43:18.670: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.2.20.101:54323/hostname] Namespace:hostport-2877 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:43:18.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:43:18.670: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:43:18.670: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2877/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.2.20.101%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.20.101, port: 54323 UDP 09/06/23 11:43:18.739
    Sep  6 11:43:18.739: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.2.20.101 54323] Namespace:hostport-2877 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  6 11:43:18.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    Sep  6 11:43:18.740: INFO: ExecWithOptions: Clientset creation
    Sep  6 11:43:18.740: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2877/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.2.20.101+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:43:23.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-2877" for this suite. 09/06/23 11:43:23.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:43:23.84
Sep  6 11:43:23.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 11:43:23.841
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:43:23.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:43:23.861
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Sep  6 11:43:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:25.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3560" for this suite. 09/06/23 11:44:25.357
------------------------------
• [SLOW TEST] [61.535 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:43:23.84
    Sep  6 11:43:23.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename custom-resource-definition 09/06/23 11:43:23.841
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:43:23.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:43:23.861
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Sep  6 11:43:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:25.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3560" for this suite. 09/06/23 11:44:25.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:25.384
Sep  6 11:44:25.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename daemonsets 09/06/23 11:44:25.385
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:25.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:25.406
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 09/06/23 11:44:25.425
STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 11:44:25.432
Sep  6 11:44:25.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:44:25.443: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:44:26.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:44:26.451: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:44:27.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 11:44:27.474: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 09/06/23 11:44:27.483
Sep  6 11:44:27.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 11:44:27.517: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:44:28.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 11:44:28.551: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:44:29.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 11:44:29.538: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:44:30.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  6 11:44:30.524: INFO: Node kube-1 is running 0 daemon pod, expected 1
Sep  6 11:44:31.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep  6 11:44:31.531: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/06/23 11:44:31.535
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8267, will wait for the garbage collector to delete the pods 09/06/23 11:44:31.535
Sep  6 11:44:31.601: INFO: Deleting DaemonSet.extensions daemon-set took: 11.70008ms
Sep  6 11:44:31.701: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.498587ms
Sep  6 11:44:33.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  6 11:44:33.606: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  6 11:44:33.609: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42237"},"items":null}

Sep  6 11:44:33.612: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42237"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:33.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8267" for this suite. 09/06/23 11:44:33.639
------------------------------
• [SLOW TEST] [8.261 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:25.384
    Sep  6 11:44:25.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename daemonsets 09/06/23 11:44:25.385
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:25.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:25.406
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 09/06/23 11:44:25.425
    STEP: Check that daemon pods launch on every node of the cluster. 09/06/23 11:44:25.432
    Sep  6 11:44:25.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:44:25.443: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:44:26.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:44:26.451: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:44:27.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 11:44:27.474: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 09/06/23 11:44:27.483
    Sep  6 11:44:27.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 11:44:27.517: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:44:28.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 11:44:28.551: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:44:29.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 11:44:29.538: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:44:30.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  6 11:44:30.524: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Sep  6 11:44:31.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep  6 11:44:31.531: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/06/23 11:44:31.535
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8267, will wait for the garbage collector to delete the pods 09/06/23 11:44:31.535
    Sep  6 11:44:31.601: INFO: Deleting DaemonSet.extensions daemon-set took: 11.70008ms
    Sep  6 11:44:31.701: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.498587ms
    Sep  6 11:44:33.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  6 11:44:33.606: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  6 11:44:33.609: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42237"},"items":null}

    Sep  6 11:44:33.612: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42237"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:33.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8267" for this suite. 09/06/23 11:44:33.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:33.647
Sep  6 11:44:33.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename container-runtime 09/06/23 11:44:33.648
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:33.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:33.671
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 09/06/23 11:44:33.673
STEP: wait for the container to reach Succeeded 09/06/23 11:44:33.682
STEP: get the container status 09/06/23 11:44:37.721
STEP: the container should be terminated 09/06/23 11:44:37.734
STEP: the termination message should be set 09/06/23 11:44:37.735
Sep  6 11:44:37.735: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 09/06/23 11:44:37.735
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:37.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1093" for this suite. 09/06/23 11:44:37.846
------------------------------
• [4.222 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:33.647
    Sep  6 11:44:33.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename container-runtime 09/06/23 11:44:33.648
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:33.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:33.671
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 09/06/23 11:44:33.673
    STEP: wait for the container to reach Succeeded 09/06/23 11:44:33.682
    STEP: get the container status 09/06/23 11:44:37.721
    STEP: the container should be terminated 09/06/23 11:44:37.734
    STEP: the termination message should be set 09/06/23 11:44:37.735
    Sep  6 11:44:37.735: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 09/06/23 11:44:37.735
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:37.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1093" for this suite. 09/06/23 11:44:37.846
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:37.87
Sep  6 11:44:37.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename runtimeclass 09/06/23 11:44:37.871
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:37.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:37.965
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Sep  6 11:44:38.005: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-709 to be scheduled
Sep  6 11:44:38.008: INFO: 1 pods are not scheduled: [runtimeclass-709/test-runtimeclass-runtimeclass-709-preconfigured-handler-7n8t5(527e7f3e-78f7-4535-b23d-5374c829e53a)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:40.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-709" for this suite. 09/06/23 11:44:40.035
------------------------------
• [2.180 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:37.87
    Sep  6 11:44:37.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename runtimeclass 09/06/23 11:44:37.871
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:37.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:37.965
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Sep  6 11:44:38.005: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-709 to be scheduled
    Sep  6 11:44:38.008: INFO: 1 pods are not scheduled: [runtimeclass-709/test-runtimeclass-runtimeclass-709-preconfigured-handler-7n8t5(527e7f3e-78f7-4535-b23d-5374c829e53a)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:40.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-709" for this suite. 09/06/23 11:44:40.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:40.051
Sep  6 11:44:40.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename namespaces 09/06/23 11:44:40.052
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:40.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:40.077
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 09/06/23 11:44:40.079
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:40.1
STEP: Creating a service in the namespace 09/06/23 11:44:40.103
STEP: Deleting the namespace 09/06/23 11:44:40.117
STEP: Waiting for the namespace to be removed. 09/06/23 11:44:40.142
STEP: Recreating the namespace 09/06/23 11:44:46.154
STEP: Verifying there is no service in the namespace 09/06/23 11:44:46.192
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:46.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2838" for this suite. 09/06/23 11:44:46.2
STEP: Destroying namespace "nsdeletetest-4564" for this suite. 09/06/23 11:44:46.209
Sep  6 11:44:46.212: INFO: Namespace nsdeletetest-4564 was already deleted
STEP: Destroying namespace "nsdeletetest-3877" for this suite. 09/06/23 11:44:46.212
------------------------------
• [SLOW TEST] [6.172 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:40.051
    Sep  6 11:44:40.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename namespaces 09/06/23 11:44:40.052
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:40.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:40.077
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 09/06/23 11:44:40.079
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:40.1
    STEP: Creating a service in the namespace 09/06/23 11:44:40.103
    STEP: Deleting the namespace 09/06/23 11:44:40.117
    STEP: Waiting for the namespace to be removed. 09/06/23 11:44:40.142
    STEP: Recreating the namespace 09/06/23 11:44:46.154
    STEP: Verifying there is no service in the namespace 09/06/23 11:44:46.192
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:46.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2838" for this suite. 09/06/23 11:44:46.2
    STEP: Destroying namespace "nsdeletetest-4564" for this suite. 09/06/23 11:44:46.209
    Sep  6 11:44:46.212: INFO: Namespace nsdeletetest-4564 was already deleted
    STEP: Destroying namespace "nsdeletetest-3877" for this suite. 09/06/23 11:44:46.212
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:46.223
Sep  6 11:44:46.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename pods 09/06/23 11:44:46.224
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:46.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:46.254
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 09/06/23 11:44:46.256
STEP: submitting the pod to kubernetes 09/06/23 11:44:46.257
STEP: verifying QOS class is set on the pod 09/06/23 11:44:46.266
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:46.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9311" for this suite. 09/06/23 11:44:46.281
------------------------------
• [0.071 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:46.223
    Sep  6 11:44:46.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename pods 09/06/23 11:44:46.224
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:46.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:46.254
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 09/06/23 11:44:46.256
    STEP: submitting the pod to kubernetes 09/06/23 11:44:46.257
    STEP: verifying QOS class is set on the pod 09/06/23 11:44:46.266
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:46.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9311" for this suite. 09/06/23 11:44:46.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:46.295
Sep  6 11:44:46.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:44:46.296
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:46.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:46.317
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-8a0eed34-918d-4a49-a5f0-9f63d5e2ede5 09/06/23 11:44:46.319
STEP: Creating a pod to test consume configMaps 09/06/23 11:44:46.324
Sep  6 11:44:46.333: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc" in namespace "projected-7786" to be "Succeeded or Failed"
Sep  6 11:44:46.340: INFO: Pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.298509ms
Sep  6 11:44:48.345: INFO: Pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012003078s
Sep  6 11:44:50.346: INFO: Pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01212962s
STEP: Saw pod success 09/06/23 11:44:50.346
Sep  6 11:44:50.346: INFO: Pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc" satisfied condition "Succeeded or Failed"
Sep  6 11:44:50.350: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc container agnhost-container: <nil>
STEP: delete the pod 09/06/23 11:44:50.365
Sep  6 11:44:50.381: INFO: Waiting for pod pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc to disappear
Sep  6 11:44:50.384: INFO: Pod pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:50.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7786" for this suite. 09/06/23 11:44:50.388
------------------------------
• [4.099 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:46.295
    Sep  6 11:44:46.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:44:46.296
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:46.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:46.317
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-8a0eed34-918d-4a49-a5f0-9f63d5e2ede5 09/06/23 11:44:46.319
    STEP: Creating a pod to test consume configMaps 09/06/23 11:44:46.324
    Sep  6 11:44:46.333: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc" in namespace "projected-7786" to be "Succeeded or Failed"
    Sep  6 11:44:46.340: INFO: Pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.298509ms
    Sep  6 11:44:48.345: INFO: Pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012003078s
    Sep  6 11:44:50.346: INFO: Pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01212962s
    STEP: Saw pod success 09/06/23 11:44:50.346
    Sep  6 11:44:50.346: INFO: Pod "pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc" satisfied condition "Succeeded or Failed"
    Sep  6 11:44:50.350: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc container agnhost-container: <nil>
    STEP: delete the pod 09/06/23 11:44:50.365
    Sep  6 11:44:50.381: INFO: Waiting for pod pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc to disappear
    Sep  6 11:44:50.384: INFO: Pod pod-projected-configmaps-80503548-8e10-4734-aeed-b4d09ae1b2fc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:50.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7786" for this suite. 09/06/23 11:44:50.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:50.401
Sep  6 11:44:50.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename downward-api 09/06/23 11:44:50.402
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:50.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:50.421
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 09/06/23 11:44:50.424
Sep  6 11:44:50.435: INFO: Waiting up to 5m0s for pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8" in namespace "downward-api-3339" to be "Succeeded or Failed"
Sep  6 11:44:50.444: INFO: Pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.665277ms
Sep  6 11:44:52.448: INFO: Pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013578063s
Sep  6 11:44:54.449: INFO: Pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013953403s
STEP: Saw pod success 09/06/23 11:44:54.449
Sep  6 11:44:54.450: INFO: Pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8" satisfied condition "Succeeded or Failed"
Sep  6 11:44:54.453: INFO: Trying to get logs from node kube-3 pod downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8 container dapi-container: <nil>
STEP: delete the pod 09/06/23 11:44:54.459
Sep  6 11:44:54.477: INFO: Waiting for pod downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8 to disappear
Sep  6 11:44:54.480: INFO: Pod downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:54.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3339" for this suite. 09/06/23 11:44:54.483
------------------------------
• [4.089 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:50.401
    Sep  6 11:44:50.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename downward-api 09/06/23 11:44:50.402
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:50.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:50.421
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 09/06/23 11:44:50.424
    Sep  6 11:44:50.435: INFO: Waiting up to 5m0s for pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8" in namespace "downward-api-3339" to be "Succeeded or Failed"
    Sep  6 11:44:50.444: INFO: Pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.665277ms
    Sep  6 11:44:52.448: INFO: Pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013578063s
    Sep  6 11:44:54.449: INFO: Pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013953403s
    STEP: Saw pod success 09/06/23 11:44:54.449
    Sep  6 11:44:54.450: INFO: Pod "downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8" satisfied condition "Succeeded or Failed"
    Sep  6 11:44:54.453: INFO: Trying to get logs from node kube-3 pod downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8 container dapi-container: <nil>
    STEP: delete the pod 09/06/23 11:44:54.459
    Sep  6 11:44:54.477: INFO: Waiting for pod downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8 to disappear
    Sep  6 11:44:54.480: INFO: Pod downward-api-fc9737c3-0e1b-4ffc-a07d-09f01d58a0b8 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:54.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3339" for this suite. 09/06/23 11:44:54.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:54.492
Sep  6 11:44:54.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename kubelet-test 09/06/23 11:44:54.493
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:54.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:54.512
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Sep  6 11:44:54.522: INFO: Waiting up to 5m0s for pod "busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df" in namespace "kubelet-test-7000" to be "running and ready"
Sep  6 11:44:54.526: INFO: Pod "busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332904ms
Sep  6 11:44:54.526: INFO: The phase of Pod busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:44:56.539: INFO: Pod "busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df": Phase="Running", Reason="", readiness=true. Elapsed: 2.017017754s
Sep  6 11:44:56.539: INFO: The phase of Pod busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df is Running (Ready = true)
Sep  6 11:44:56.539: INFO: Pod "busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  6 11:44:56.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7000" for this suite. 09/06/23 11:44:56.588
------------------------------
• [2.113 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:54.492
    Sep  6 11:44:54.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename kubelet-test 09/06/23 11:44:54.493
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:54.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:54.512
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Sep  6 11:44:54.522: INFO: Waiting up to 5m0s for pod "busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df" in namespace "kubelet-test-7000" to be "running and ready"
    Sep  6 11:44:54.526: INFO: Pod "busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332904ms
    Sep  6 11:44:54.526: INFO: The phase of Pod busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df is Pending, waiting for it to be Running (with Ready = true)
    Sep  6 11:44:56.539: INFO: Pod "busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df": Phase="Running", Reason="", readiness=true. Elapsed: 2.017017754s
    Sep  6 11:44:56.539: INFO: The phase of Pod busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df is Running (Ready = true)
    Sep  6 11:44:56.539: INFO: Pod "busybox-scheduling-fc55a706-70ff-4a00-ab20-abe2f4ca24df" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:44:56.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7000" for this suite. 09/06/23 11:44:56.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/06/23 11:44:56.606
Sep  6 11:44:56.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
STEP: Building a namespace api object, basename projected 09/06/23 11:44:56.608
STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:56.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:56.683
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 09/06/23 11:44:56.685
Sep  6 11:44:56.755: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9" in namespace "projected-1269" to be "Succeeded or Failed"
Sep  6 11:44:56.771: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.422232ms
Sep  6 11:44:58.786: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031896129s
Sep  6 11:45:00.781: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026727643s
Sep  6 11:45:02.784: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029046898s
STEP: Saw pod success 09/06/23 11:45:02.784
Sep  6 11:45:02.785: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9" satisfied condition "Succeeded or Failed"
Sep  6 11:45:02.794: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9 container client-container: <nil>
STEP: delete the pod 09/06/23 11:45:02.801
Sep  6 11:45:02.820: INFO: Waiting for pod downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9 to disappear
Sep  6 11:45:02.823: INFO: Pod downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  6 11:45:02.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1269" for this suite. 09/06/23 11:45:02.826
------------------------------
• [SLOW TEST] [6.228 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/06/23 11:44:56.606
    Sep  6 11:44:56.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1191164369
    STEP: Building a namespace api object, basename projected 09/06/23 11:44:56.608
    STEP: Waiting for a default service account to be provisioned in namespace 09/06/23 11:44:56.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/06/23 11:44:56.683
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 09/06/23 11:44:56.685
    Sep  6 11:44:56.755: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9" in namespace "projected-1269" to be "Succeeded or Failed"
    Sep  6 11:44:56.771: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.422232ms
    Sep  6 11:44:58.786: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031896129s
    Sep  6 11:45:00.781: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026727643s
    Sep  6 11:45:02.784: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029046898s
    STEP: Saw pod success 09/06/23 11:45:02.784
    Sep  6 11:45:02.785: INFO: Pod "downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9" satisfied condition "Succeeded or Failed"
    Sep  6 11:45:02.794: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9 container client-container: <nil>
    STEP: delete the pod 09/06/23 11:45:02.801
    Sep  6 11:45:02.820: INFO: Waiting for pod downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9 to disappear
    Sep  6 11:45:02.823: INFO: Pod downwardapi-volume-02b9af4e-edb9-4365-bd90-c7feedb313c9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  6 11:45:02.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1269" for this suite. 09/06/23 11:45:02.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Sep  6 11:45:02.836: INFO: Running AfterSuite actions on node 1
Sep  6 11:45:02.836: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Sep  6 11:45:02.836: INFO: Running AfterSuite actions on node 1
    Sep  6 11:45:02.836: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.062 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6187.332 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h43m7.598023392s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

